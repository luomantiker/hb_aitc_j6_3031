"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["60913"],{76463:function(e,n,i){e.exports=i.p+"static/image/calibration_percentile_bimodal.ac8e31a5.png"},97117:function(e,n,i){e.exports=i.p+"static/image/calibration_percentile_ln.d45d73fc.png"},4810:function(e,n,i){e.exports=i.p+"static/image/calibration_percentile_longtail.e798aad6.png"},38091:function(e,n,i){e.exports=i.p+"static/image/calibration_v2_workflow.1d50ec14.png"},82873:function(e,n,i){i.r(n);var t=i(85893),s=i(50065),r=i(95895),a=i(4810),o=i(76463),l=i(97117),c=i(38091);function d(e){let n=Object.assign({h1:"h1",a:"a",p:"p",h2:"h2",ol:"ol",li:"li",code:"code",pre:"pre",span:"span",div:"div",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",img:"img",strong:"strong",em:"em",ul:"ul"},(0,s.ah)(),e.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.h1,{id:"calibration-tutorial",children:[(0,t.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#calibration-tutorial",children:"#"}),"Calibration Tutorial"]}),"\n",(0,t.jsx)(r.Z,{}),"\n",(0,t.jsx)(n.p,{children:"In quantization, an important step is to determine the quantized parameters, a reasonable initial quantized parameter can significantly improve the accuracy of the model and speed up the convergence of the model.\nCalibration is the process of inserting an Observer into the floating-point model, using a small amount of training data, and counting the distribution of the data at various points during the forward process of the model to determine a reasonable quantized parameter.\nAlthough it is possible to do quantized awareness training without Calibration, but in general it is beneficial and not detrimental to quantized awareness training, so it is recommended that you make this step a required option."}),"\n",(0,t.jsxs)(n.h2,{id:"process-and-example",children:[(0,t.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#process-and-example",children:"#"}),"Process and Example"]}),"\n",(0,t.jsx)(n.p,{children:"The overall flow of Calibration and QAT is shown below:"}),"\n",(0,t.jsx)("img",{src:c,alt:"calibration_v2_workflow",width:"450"}),"\n",(0,t.jsx)(n.p,{children:"Following is a description of each step:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Build and train the floating-point model. Refer to the section ",(0,t.jsx)(n.a,{href:"/3.0.22/en/guide/plugin/qat_quickstart/qat_quickstart.html#getting-floating-point-model",children:"Getting Floating-point Model"})," in the quick start section of horizon_plugin_pytorch."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Insert the Observer node into the floating-point model. Refer to the section ",(0,t.jsx)(n.a,{href:"/3.0.22/en/guide/plugin/qat_quickstart/qat_quickstart.html#calibration",children:"Calibration"})," in the quick start section of horizon_plugin_pytorch.\nBefore converting the floating-point model using the ",(0,t.jsx)(n.code,{children:"prepare"})," method, you need to set ",(0,t.jsx)(n.code,{children:"qconfig"})," for the model. Refer to the section ",(0,t.jsx)(n.a,{href:"/3.0.22/en/guide/plugin/user_guide/qconfig.html",children:"QConfig in Detail"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Set ",(0,t.jsx)(n.code,{children:"fake quantize"})," state to ",(0,t.jsx)(n.code,{children:"CALIBRATION"}),"."]}),"\n",(0,t.jsx)(n.pre,{className:"code",children:(0,t.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,t.jsxs)(n.code,{className:"language-python",meta:"",children:[(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"horizon"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"quantization"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"set_fake_quantize"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(model, horizon.quantization.FakeQuantState.CALIBRATION)"})]}),"\n"]})})}),"\n",(0,t.jsxs)(n.p,{children:["There are three states of ",(0,t.jsx)(n.code,{children:"fake quantize"}),", which need to be set to the corresponding state of ",(0,t.jsx)(n.code,{children:"fake quantize"})," of the model before ",(0,t.jsx)(n.code,{children:"QAT"}),", ",(0,t.jsx)(n.code,{children:"calibration"}),", and ",(0,t.jsx)(n.code,{children:"validation"}),", respectively.\nIn the calibration state, only the statistics of the inputs and outputs of each operator are observed. In the QAT state, pseudo-quantization is performed in addition to observing the statistics.\nIn validation state, no statistics are observed and only pseudo-quantization is performed."]}),"\n",(0,t.jsx)(n.pre,{className:"code",children:(0,t.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,t.jsxs)(n.code,{className:"language-python",meta:"",children:[(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"class"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"FakeQuantState"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"("}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"Enum"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"):"})]}),"\n",(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    QAT "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"qat"'})]}),"\n",(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    CALIBRATION "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"calibration"'})]}),"\n",(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    VALIDATION "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"validation"'})]}),"\n"]})})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Perform calibration. Feed the prepared calibration data to the model, and the model will be observed by the observer during the forward process to observe the relevant statistics."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Set the model state to eval and set the ",(0,t.jsx)(n.code,{children:"fake quantize"})," state to ",(0,t.jsx)(n.code,{children:"VALIDATION"}),"."]}),"\n",(0,t.jsx)(n.pre,{className:"code",children:(0,t.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,t.jsxs)(n.code,{className:"language-python",meta:"",children:[(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"model"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"eval"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"()"})]}),"\n",(0,t.jsxs)(n.span,{className:"line line-number",children:[(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"horizon"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"quantization"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"set_fake_quantize"}),(0,t.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(model, horizon.quantization.FakeQuantState.VALIDATION)"})]}),"\n"]})})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Verify the effect of ",(0,t.jsx)(n.code,{children:"calibration"}),". If you are satisfied with the result, you can directly convert the model to fixed-point or perform quantized awareness training based on it, if not, you can adjust the parameters in ",(0,t.jsx)(n.code,{children:"calibration qconfig"})," to continue calibration."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h2,{id:"common-algorithms-introduction",children:[(0,t.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#common-algorithms-introduction",children:"#"}),"Common Algorithms Introduction"]}),"\n",(0,t.jsxs)(n.div,{className:"rspress-directive info",children:[(0,t.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,t.jsx)(n.div,{className:"rspress-directive-content",children:(0,t.jsx)(n.p,{children:"Reference the API documentation at the end of this section for a description of the parameters of each operator."})})]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{align:"left",children:"Algorithm"}),(0,t.jsx)(n.th,{align:"left",children:"Speed Rank"}),(0,t.jsx)(n.th,{align:"left",children:"Accuracy Rank"}),(0,t.jsx)(n.th,{align:"left",children:"Easy-to-use Rank"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"min_max"}),(0,t.jsx)(n.td,{align:"left",children:"1"}),(0,t.jsx)(n.td,{align:"left",children:"5"}),(0,t.jsx)(n.td,{align:"left",children:"1"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"percentile"}),(0,t.jsx)(n.td,{align:"left",children:"2"}),(0,t.jsx)(n.td,{align:"left",children:"4"}),(0,t.jsx)(n.td,{align:"left",children:"4"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"mse"}),(0,t.jsx)(n.td,{align:"left",children:"4"}),(0,t.jsx)(n.td,{align:"left",children:"1"}),(0,t.jsx)(n.td,{align:"left",children:"2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"kl"}),(0,t.jsx)(n.td,{align:"left",children:"5"}),(0,t.jsx)(n.td,{align:"left",children:"2"}),(0,t.jsx)(n.td,{align:"left",children:"3"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"mix"}),(0,t.jsx)(n.td,{align:"left",children:"3"}),(0,t.jsx)(n.td,{align:"left",children:"2"}),(0,t.jsx)(n.td,{align:"left",children:"1"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The performance of several popular calibration methods is shown in the table above, where smaller numbers are better, speed indicates the time taken to calibrate the same data,\naccuracy indicates how well the method calibrates on most models, and easy-to-use indicates the complexity of the method's tuning parameters."}),"\n",(0,t.jsx)(n.p,{children:"For a same model, the accuracy/speed of different methods with different parameters can be quite different, and some recent research work has shown that no one method can achieve the best accuracy on all models,\nand that its parameters need to be adjusted specifically. So it is recommended that you try all of these calibration methods."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"min_max. This method only counts the sliding average of the maximum and minimum values, and is used for quickly determining general parameters such as batch size, average_constant, and so on."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"percentile. This method has the highest accuracy upper limit among all the methods, but it is also the most troublesome to adjust, so if the accuracy requirement can be satisfied by other methods or the default parameters of this method,\nthen it is not recommended to spend too much time on adjusting the parameters. The percentile can be adjusted by two parameters, bins and percentile. The more bins there are, the smaller the interval between the candidates of max, the finer the granularity of tuning,\nbut it also means higher computation time. It is recommended to determine the percentile first and then adjust the bins, alternating between the two iterations to narrow down the tuning range until a satisfactory result is achieved.\nIn most cases, the bins of 2048 provides enough granularity for tuning, so there is no need to adjust this parameter separately. The following is the tuning path of a model:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{align:"left",children:"Order"}),(0,t.jsx)(n.th,{align:"left",children:"percentile"}),(0,t.jsx)(n.th,{align:"left",children:"bins"}),(0,t.jsx)(n.th,{align:"left",children:"Accuracy"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"1"}),(0,t.jsx)(n.td,{align:"left",children:"99.99"}),(0,t.jsx)(n.td,{align:"left",children:"2048"}),(0,t.jsx)(n.td,{align:"left",children:"53.75"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"2"}),(0,t.jsx)(n.td,{align:"left",children:"99.99"}),(0,t.jsx)(n.td,{align:"left",children:"4096"}),(0,t.jsx)(n.td,{align:"left",children:"54.38"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"3"}),(0,t.jsx)(n.td,{align:"left",children:"99.995"}),(0,t.jsx)(n.td,{align:"left",children:"4096"}),(0,t.jsx)(n.td,{align:"left",children:"16.25"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"4"}),(0,t.jsx)(n.td,{align:"left",children:"99.985"}),(0,t.jsx)(n.td,{align:"left",children:"4096"}),(0,t.jsx)(n.td,{align:"left",children:"32.67"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"5"}),(0,t.jsx)(n.td,{align:"left",children:"99.9875"}),(0,t.jsx)(n.td,{align:"left",children:"4096"}),(0,t.jsx)(n.td,{align:"left",children:"57.06"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"6"}),(0,t.jsx)(n.td,{align:"left",children:"99.9875"}),(0,t.jsx)(n.td,{align:"left",children:"8192"}),(0,t.jsx)(n.td,{align:"left",children:"62.84"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"7"}),(0,t.jsx)(n.td,{align:"left",children:"99.98875"}),(0,t.jsx)(n.td,{align:"left",children:"8192"}),(0,t.jsx)(n.td,{align:"left",children:"57.62"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{align:"left",children:"8"}),(0,t.jsx)(n.td,{align:"left",children:"99.988125"}),(0,t.jsx)(n.td,{align:"left",children:"8192"}),(0,t.jsx)(n.td,{align:"left",children:"63.15"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"In this example, it can be seen that the accuracy is improved by about 10% after careful adjustment.\nThere is a big difference between the inputs and outputs of different ops in the model, a set of global percentile parameters may be hard to satisfy the needs of all ops,\nif you need higher accuracy, you can find a better global parameter by the above method, and then use debug tool to find a few ops with bigger error, and then set up the percentile parameters of these ops individually.\nRefer to the qconfig setting. Below is a list of some common data distributions that are prone to large errors:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"calibration_percentile_longtail",src:a})}),"\n",(0,t.jsx)(n.p,{children:"For ultra-long-tailed distributions, the percentile should be set to a smaller value, 99.9 in the picture is a better value."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"calibration_percentile_bimodal",src:o})}),"\n",(0,t.jsx)(n.p,{children:"The value domain is too large, and the distribution is not concentrated in one place, this situation either retain the tail or ignore the tail will bring a large loss of accuracy,\nshould be adjusted in the training of floating-point model by adjusting the weight decay and other parameters to avoid this situation."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"calibration_percentile_ln",src:l})}),"\n",(0,t.jsx)(n.p,{children:"The layernorm output distribution will show a number of very high concentration of the region, at this time the percentile adjusted in accordance with the normal method will not have any effect on the quantization results, you need to increase the percentile adjustment amplitude."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"mse. The only parameter that can be adjusted is stride, the default stride is 1, it will gradually try the 100th quantile of the maximum value and select the value corresponding to the quantile with the smallest error (L2 distance) before and after the quantization and dequantization. This method is time-consuming for large models, if you increase stride within a reasonable range, you can reduce the time-consumption under the premise of guaranteeing the accuracy, but if stride is adjusted too large, the accuracy will be affected. Note that adjusting the parameters of this method can only optimize the time consumption, but not improve the accuracy significantly."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"kl. There are two adjustable parameters, bin and update_interval, it is not recommended to adjust the default bin because this method is too time consuming. The update_interval is 1 by default, it can be adjusted to reduce the time consuming, but you need to make sure that the update_interval is smaller than the total calibration step, otherwise you can not get a normal quantization parameter."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"mix.This method is a hybrid calibration, and for each place where statistics are needed, different parameters of the percentile method are tried, and the method with the smallest error (L2 distance) before and after the quantization and dequantization. There is a high degree of automation and no parameters that need to be adjusted."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h2,{id:"tuning-technique",children:[(0,t.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#tuning-technique",children:"#"}),"Tuning Technique"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"When calibration is performed, the more data the better. However, because of the marginal effect, when the amount of data reaches a certain level, the improvement of accuracy will be very limited. If your training set is small, you can use all of it for calibration. If your training set is large, you can select a subset of the right size in combination with the calibration time consumed, and it is recommended to calibrate at least 10 - 100 steps."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The data can be flipped horizontally augmentation, do not do mosaic augmentation, try to use the pre-processing + training data of the infer stage for calibration."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The Batch size should be as large as possible, but can be reduced if the data are noisy or if there are many outliers in the model. This parameter should be determined when trying the min max method."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The average_constant indicates the effect of each step on the maximum and minimum values, the smaller average_constant is, the smaller the effect of the current step is, and the larger the effect of the historical sliding average is. This parameter needs to be adjusted between 0.01 ~ 0.5 with the amount of data. When there is enough data (step > 100), average_constant can be 0.01,\nwhen there is not enough data, average_constant can be increased as appropriate, in extreme case, there are only 2 steps of data, average_constant can be 0.5. This parameter should be determined when trying the min max method, and all other methods will follow this parameter after that."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"When the accuracy of calibration model is good, fixing the quantization parameter of the feature map for QAT training can achieve better results, while when the accuracy is poor, the quantization parameter obtained from calibration cannot be fixed. There is no clear standard about whether the accuracy is good or bad, we need to try.\nFor example, if the accuracy of a certain model is 100, and if the accuracy of calibration is 50, then the accuracy is not good, but if the accuracy of calibration is 95, then whether the accuracy can reach the degree of fixing the quantization parameters of feature map needs to be tried, and the usual practice is to do experiments to compare the fixing and not fixing."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Priority to try min max method, the method is the fastest, used to run through the calibration process, adjust and determine the batch size and average_constant two parameters, and then try percentile, kl, mse and mix four methods and select the most effective method."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h2,{id:"observer-parameters",children:[(0,t.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#observer-parameters",children:"#"}),"Observer Parameters"]}),"\n",(0,t.jsx)("a",{id:"module-horizon_plugin_pytorch.quantization.observer_v2"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int = 512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,t.jsx)(n.p,{children:"KL observer."}),"\n",(0,t.jsx)(n.p,{children:"KL observer based on histogram. Histogram is calculated online\nand won\u2019t be saved."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"bins"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Number of histograms bins."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"update_interval"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Interval of computing KL entropy and update min/max.\nKLObserver will constantly collect histograms of activations,\nbut only perform KL calculation when update_interval is satisfied.\nif it is set to 1, KL entropy will be computed every forward step.\nLarger interval guarantees less time and does no harm to\ncalibration accuracy. Set it to the total calibration steps can\nachieve best performance. update_interval must be no greater than\ntotal calibration steps, otherwise no min/max will be computed."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," (",(0,t.jsx)(n.code,{children:"Union"}),"[",(0,t.jsx)(n.code,{children:"dtype"}),", ",(0,t.jsx)(n.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," (",(0,t.jsx)(n.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," (",(0,t.jsx)(n.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsx)(n.p,{children:"Defines the computation performed at every call."}),"\n",(0,t.jsx)(n.p,{children:"Should be overridden by all subclasses."}),"\n",(0,t.jsxs)(n.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,t.jsx)(n.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int = 1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,t.jsx)(n.p,{children:"MSE observer."}),"\n",(0,t.jsx)(n.p,{children:"Observer module for computing the quantization parameters based on the\nMean Square Error (MSE) between the original tensor and the quantized one."}),"\n",(0,t.jsx)(n.p,{children:"This observer linear searches the quantization scales that minimize MSE."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"stride"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Searching stride. Larger value gives smaller search space,\nwhich means less computing time but possibly poorer accuracy.\nDefault is 1. Suggests no greater than 20."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," (",(0,t.jsx)(n.code,{children:"Union"}),"[",(0,t.jsx)(n.code,{children:"dtype"}),", ",(0,t.jsx)(n.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," (",(0,t.jsx)(n.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," (",(0,t.jsx)(n.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsx)(n.p,{children:"Defines the computation performed at every call."}),"\n",(0,t.jsx)(n.p,{children:"Should be overridden by all subclasses."}),"\n",(0,t.jsxs)(n.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,t.jsx)(n.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver (averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,t.jsx)(n.p,{children:"Min max observer."}),"\n",(0,t.jsx)(n.p,{children:"This observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average\nminimum and maximum of incoming tensors, and uses this statistic to compute\nthe quantization parameters."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," (",(0,t.jsx)(n.code,{children:"Union"}),"[",(0,t.jsx)(n.code,{children:"dtype"}),", ",(0,t.jsx)(n.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," (",(0,t.jsx)(n.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," (",(0,t.jsx)(n.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsxs)(n.p,{children:["Record the running minimum and maximum of ",(0,t.jsx)(n.code,{children:"x"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MixObserver (averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,t.jsx)(n.p,{children:"Mix observer."}),"\n",(0,t.jsx)(n.p,{children:"This observer computes the quantization parameters based on multiple\ncalibration methods and selects the quantization parameters with the\nsmallest quantization error."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," (",(0,t.jsx)(n.code,{children:"Union"}),"[",(0,t.jsx)(n.code,{children:"dtype"}),", ",(0,t.jsx)(n.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," (",(0,t.jsx)(n.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," (",(0,t.jsx)(n.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsx)(n.p,{children:"Defines the computation performed at every call."}),"\n",(0,t.jsx)(n.p,{children:"Should be overridden by all subclasses."}),"\n",(0,t.jsxs)(n.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,t.jsx)(n.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver (percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,t.jsx)(n.p,{children:"Percentile observer."}),"\n",(0,t.jsx)(n.p,{children:"Percentile observer based on histogram. Histogram is calculated online\nand won\u2019t be saved. The minimum and maximum are moving averaged to compute\nthe quantization parameters."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"percentile"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Index percentile of histrogram"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"bins"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Number of histograms bins."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," (",(0,t.jsx)(n.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," (",(0,t.jsx)(n.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," (",(0,t.jsx)(n.code,{children:"Union"}),"[",(0,t.jsx)(n.code,{children:"dtype"}),", ",(0,t.jsx)(n.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," (",(0,t.jsx)(n.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," (",(0,t.jsx)(n.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," (",(0,t.jsx)(n.code,{children:"Optional"}),"[",(0,t.jsx)(n.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsx)(n.p,{children:"Defines the computation performed at every call."}),"\n",(0,t.jsx)(n.p,{children:"Should be overridden by all subclasses."}),"\n",(0,t.jsxs)(n.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,t.jsx)(n.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,t.jsx)("a",{id:"module-horizon_plugin_pytorch.quantization"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver (averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, quant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)"]})}),"\n",(0,t.jsx)(n.p,{children:"MovingAverageMinMax Observer."}),"\n",(0,t.jsx)(n.p,{children:"Observer module for computing the quantization parameters based on the\nmoving average of the min and max values."}),"\n",(0,t.jsx)(n.p,{children:"This observer computes the quantization parameters based on the moving\naverages of minimums and maximums of the incoming tensors. The module\nrecords the average minimum and maximum of incoming tensors, and uses this\nstatistic to compute the quantization parameters."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," \u2013 Quantized data type"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," \u2013 Quantization scheme to be used, only support\nper_tensor_symmetric scheme"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"reduce_range"})," \u2013 Reduces the range of the quantized data type by 1 bit"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," \u2013 Minimum quantization value."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," \u2013 Maximum quantization value."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," \u2013 Whether use sync quantize"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," \u2013 Arguments for register data buffer"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsxs)(n.p,{children:["Record the running minimum and maximum of ",(0,t.jsx)(n.code,{children:"x"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver (averaging_constant=0.01, ch_axis=0, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)"]})}),"\n",(0,t.jsx)(n.p,{children:"MovingAveragePerChannelMinMax Observer."}),"\n",(0,t.jsx)(n.p,{children:"Observer module for computing the quantization parameters based on the\nrunning per channel min and max values."}),"\n",(0,t.jsx)(n.p,{children:"This observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum\nof incoming tensors, and uses this statistic to compute the quantization\nparameters."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"averaging_constant"})," \u2013 Averaging constant for min/max."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ch_axis"})," \u2013 Channel axis"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"dtype"})," \u2013 Quantized data type"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"qscheme"})," \u2013 Quantization scheme to be used, Only support\nper_channel_symmetric"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_min"})," \u2013 Minimum quantization value."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quant_max"})," \u2013 Maximum quantization value."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"is_sync_quantize"})," \u2013 whether use sync quantize"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"factory_kwargs"})," \u2013 Arguments for register data buffer"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"forward (x_orig)"})}),"\n",(0,t.jsx)(n.p,{children:"Defines the computation performed at every call."}),"\n",(0,t.jsx)(n.p,{children:"Should be overridden by all subclasses."}),"\n",(0,t.jsxs)(n.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,t.jsx)(n.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]})]})}function h(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,s.ah)(),e.components);return n?(0,t.jsx)(n,Object.assign({},e,{children:(0,t.jsx)(d,e)})):d(e)}n.default=h,h.__RSPRESS_PAGE_META={},h.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fplugin%2Fuser_guide%2Fcalibration.mdx"]={toc:[{id:"process-and-example",text:"Process and Example",depth:2},{id:"common-algorithms-introduction",text:"Common Algorithms Introduction",depth:2},{id:"tuning-technique",text:"Tuning Technique",depth:2},{id:"observer-parameters",text:"Observer Parameters",depth:2}],title:"Calibration Tutorial",frontmatter:{}}},95895:function(e,n,i){i(39710);var t=i(85893),s=i(67294),r=i(45687);i(20388);let a={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function o(e,n,i){let t=Object.keys(a).includes(n)?n:i;return a[t](e)}n.Z=e=>{let{defaultLocale:n="en-US"}=e,i=(0,r.Vi)().page.readingTimeData,a=(0,r.Jr)(),l=(0,r.e7)(),[c,d]=(0,s.useState)(o(i,a,n));return(0,s.useEffect)(()=>{d(o(i,a,n))},[a,i]),(0,t.jsx)("span",{"data-dark":String(l),className:"rp-reading-time",children:c})}}}]);