"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["10759"],{41681:function(e,n,s){e.exports=s.p+"static/image/model_conversion_flowchart.a6ceefb4.png"},13901:function(e,n,s){e.exports=s.p+"static/image/workflow.d9a0ce41.png"},88086:function(e,n,s){s.r(n);var o=s(85893),i=s(50065),t=s(95895),r=s(13901),a=s(41681);function l(e){let n=Object.assign({h1:"h1",a:"a",h2:"h2",p:"p",img:"img",strong:"strong",div:"div",ul:"ul",li:"li",ol:"ol",h3:"h3",code:"code",pre:"pre",span:"span"},(0,i.ah)(),e.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.h1,{id:"ptq-conversion-principle-and-process",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#ptq-conversion-principle-and-process",children:"#"}),"PTQ Conversion Principle and Process"]}),"\n",(0,o.jsx)(t.Z,{}),"\n",(0,o.jsxs)(n.h2,{id:"overview",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#overview",children:"#"}),"Overview"]}),"\n",(0,o.jsx)(n.p,{children:"The process of the model from training to conversion to running on the development board is shown below:"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"image",src:r})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model Training"}),": It is the process of getting a usable model by using public deep learning frameworks such as TensorFlow, PyTorch, Caffe, etc.\nThe trained model will serve as the input for the model conversion stage.\nThe toolchain does not provide training-related libraries or tools.\nFor the detailed supported public learning frameworks, please refer to the instructions in the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/model_prepare.html",children:"Floating-point Model Preparation"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model Conversion"}),": Taking the floating-point model obtained from model training as input,\nthis stage transforms floating-point models into models that can run efficiently on the Horizon computing platform through important steps such as model structure optimization, model calibration quantization and model compilation.\nTo verify the usability of the heterogeneous model, the toolchain also provides you with performance analysis, accuracy analysis, and a rich set of exception debugging tools and recommendations.\nFor more information, please refer to ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/quantize_compile.html",children:"Model Quantization and Compilation"})," section."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Embedded Application development"}),": The toolchain supports application development in both X86 emulation environment and real embedded environment.\nIn case you are not convenient to use the development board, you can debug the program and verify the calculation results in the emulation environment.\nIn order to reduce the cost of simulation verification, the toolchain provides the exact same simulation library interface as the embedded interface, only with different compilation configurations.\nFor more information, please refer to ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/runtime/runtime_dev.html",children:"Embedded Application Development"})," section."]}),"\n","\n",(0,o.jsxs)(n.h2,{id:"ptq-conversion-process",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#ptq-conversion-process",children:"#"}),"PTQ Conversion Process"]}),"\n",(0,o.jsxs)(n.p,{children:["The complete the model development process with the Horizon toolchain involves five important stages: ",(0,o.jsx)(n.strong,{children:"Floating-point Model Preparation"}),", ",(0,o.jsx)(n.strong,{children:"Model Checking"}),", ",(0,o.jsx)(n.strong,{children:"Model Conversion"}),", ",(0,o.jsx)(n.strong,{children:"Performance Evaluation"}),", and ",(0,o.jsx)(n.strong,{children:"Accuracy Evaluation"}),", as shown in the figure below."]}),"\n",(0,o.jsx)("img",{src:a,width:"900"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Floating-point model"}),", as the output of the ",(0,o.jsx)(n.strong,{children:"Floating-point Model Preparation"})," stage, will serve as the input of the model conversion tool.\nThe floating-point model is usually trained on basis of some open source deep learning frameworks.\nNote that the model must be exported to a format supported by Horizon Robotics.\nFor more information, please refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/model_prepare.html",children:"Floating-point Model Preparation"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Model Checking"})," stage is used to ensure that the model is computing platform compliant.\nHorizon Robotics provides specified tools to complete model validation, and for non-compliance, such tools will explicitly give you the specific operator information for the non-compliance,\nso that you can easily adjust the model with the description of the operator constraints.\nFor more information, please refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/check_model.html",children:"Model Checking"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Model Conversion"})," stage converts the floating-point model to the board-side deployable model supported by Horizon Robotics.\nTo run models efficiently on the Horizon computing platform, critical steps such as model optimization, quantization, and compilation are completed by Horizon's model conversion tools. Horizon's model quantization method has undergone long-term technological and production validation,\nand can guarantee an accuracy loss of less than 1% on most typical deep learning models.\nFor more details about model conversion please refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/prepare_calibration_data.html",children:"Prepare Calibration Data"}),"  and ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/quantize_compile.html",children:"Model Quantization and Compilation"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Performance Evaluation"})," stage contains a series of tools to evaluate the model performance. Before deploying your application, you can use these tools to verify that the model performance meets application requirements.\nFor some cases where the performance is not as good as expected, you can optimize the models based on Horizon's model optimization advices.\nFor more information, please refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/performance_evaluation.html",children:"Model Performance Analysis"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Accuracy Evaluation"})," stage contains a series of tools to evaluate the accuracy of the model.\nIn most cases, Horizon's converted-models can maintain almost the same accuracy as the original floating-point model.\nBefore application deployment, you can use these tools to verify that the accuracy of the model meets the expectations.\nFor some cases where the accuracy is not as good as expected, you can optimize the models based on Horizon's model optimization advices.\nFor more information about evaluation, please refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/accuracy_evaluation.html",children:"Model Accuracy Analysis"}),"."]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive warning",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Attention"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"In general, you can get a qualified runtime model after model conversion.\nHowever, make sure that the performance and accuracy of the model are in line with the application requirements.\nHorizon Robotics strongly suggest you that each conversion shall be followed by the evaluation steps of performance and accuracy."}),"\n",(0,o.jsx)(n.li,{children:"The model conversion process will generate onnx models and bc model, which are intermediate products and only facilitate you to verify the accuracy of the model.\nTherefore, the compatibility between versions is not guaranteed.\nWhen using the evaluation script in the example to evaluate the onnx model in a single image or on a test set, please use the onnx model generated by the current version of the tool."}),"\n"]})})]}),"\n",(0,o.jsxs)(n.h2,{id:"model-conversion-process-detailed-explanation",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-conversion-process-detailed-explanation",children:"#"}),"Model Conversion Process Detailed Explanation"]}),"\n",(0,o.jsx)(n.p,{children:"So how to convert the floating-point models trained by using the opensource ML frameworks (such as Caffe, TensorFlow, PyTorch, etc.) to the Horizon hardware supported fixed-point models?"}),"\n",(0,o.jsx)(n.p,{children:"In most cases, the threshold values and weights of the floating-point models obtained from either the opensource ML frameworks or trained by yourself are floating-point numbers (float32) and each number occupies 4 bytes."}),"\n",(0,o.jsx)(n.p,{children:"However, by converting the floating-point numbers to fixed-point numbers (int8), each number occupies only 1 byte, thus the computation operations in the embedded runtime can be dramatically reduced."}),"\n",(0,o.jsx)(n.p,{children:"Therefore, it brings significant performance boost by converting the floating-point models to fixed-point models with no loss or very small loss."}),"\n",(0,o.jsx)(n.p,{children:"Typically, model conversion can be divided into the following steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Check if there are unsupported OPs in the models to be converted."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Prepare 20~100 images for calibration use at the conversion stage."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Convert the floating-point models to fixed-point models using the floating-point conversion tools."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Evaluate the performance and accuracy of the converted models to ensure that there isn't huge difference in model accuracy before and after the conversion."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Run models in simulator/dev board to validate model performance and accuracy."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.h3,{id:"model-checking",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-checking",children:"#"}),"Model Checking"]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.p,{children:["If you need to do this process in the sample folder, you need to execute the ",(0,o.jsx)(n.code,{children:"00_init.sh"})," script in the folder first to get the corresponding original model and dataset."]})})]}),"\n",(0,o.jsxs)(n.p,{children:["Before converting the floating-point models into the fixed-point models, we should check if there are Horizon hardwares unsupported OPs in the floating-point models using the ",(0,o.jsx)(n.code,{children:"hb_compile"})," tool.\nIf yes, the tool will report the unsupported OP(s). The usage of the ",(0,o.jsx)(n.code,{children:"hb_compile"})," tool for checking the model can be found in ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/check.html",children:"Model Checking"})," section."]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive tip",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Tip"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.p,{children:["\nMore information about Horizon hardware supported OPs, refer to ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/appendix/supported_op_list/onnx_operator_support_list.html",children:"Toolchain Operator Support Constraint List-ONNX Operator Support List"}),"."]})})]}),"\n","\n",(0,o.jsxs)(n.h3,{id:"calibration-image-preparation",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#calibration-image-preparation",children:"#"}),"Calibration Image Preparation"]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.p,{children:["If you need to do this process in the sample folder, you need to execute the ",(0,o.jsx)(n.code,{children:"00_init.sh"})," script in the folder first to get the corresponding original model and dataset."]})})]}),"\n",(0,o.jsx)(n.p,{children:"When converting the floating-point models, you need to prepare 20~100 images for calibration use at the calibration stage."}),"\n",(0,o.jsx)(n.p,{children:"Input image formats may vary by input type and layout. In this stage, because both original (JPG, etc.) and the processed images are valid,\nyou can either feed the calibration images used in the model training or feed your own processed images."}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.p,{children:["The format of the calibration dataset should be ",(0,o.jsx)(n.code,{children:"npy"})," ."]})})]}),"\n",(0,o.jsx)(n.p,{children:"We recommend you preprocessing the calibration images on your own, you need to complete the operations such as image channel (BGR/RGB), data layout (NHWC/NCHW), and image resizing/padding (Resize&Padding).\nThe tool will then feed the images to the calibration stage after loading them as npy format."}),"\n",(0,o.jsx)(n.p,{children:"Taking Resnet50 as an example, the required transformer operations are as follows:"}),"\n",(0,o.jsx)(n.pre,{className:"code",children:(0,o.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,o.jsxs)(n.code,{className:"language-python",meta:"",children:[(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    transformers "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" ["})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"PILResizeTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(size"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"256"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"),"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"                              "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Adjust the short side to 256 with bilinear interpolation method"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"PILCenterCropTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(size"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"224"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"),"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"                          "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Perform center cropping with 224*224 with PIL method"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"HWC2CHWTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(),"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"                                        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Convert NHWC to NCHW"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"ScaleTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(scale_value"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"/"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"255"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"),"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"                       "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Scale pixel value ranges to [0,1]"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"MeanTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(means"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"np."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"array"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(["}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.485"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.456"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.406"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"])),"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"      "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Normalize the data by the average of the data set"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"ScaleTransformer"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"( "})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"            scale_value"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"np."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"array"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(["}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"/"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.229"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"/"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.224"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:", "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"/"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.225"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"]))"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Normalize the data by the standard deviation of the data set"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    ]"})}),"\n"]})})}),"\n",(0,o.jsxs)(n.h3,{id:"model-conversion",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-conversion",children:"#"}),"Model Conversion"]}),"\n",(0,o.jsxs)(n.p,{children:["When you confirm that the floating-point model can be successfully converted, you can then convert the floating-point model to a Horizon hardware supported fixed-point model by using the ",(0,o.jsx)(n.code,{children:"hb_compile"})," tool."]}),"\n",(0,o.jsxs)(n.p,{children:["This process requires you to pass a configuration file(*.yaml) containing conversion requirements. For specific configuration file settings and the insturctions of each parameter,\nrefer to the descriptions in sections ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/convert.html#config_parameter",children:"Specific Parameter Information"})," and ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/convert.html#full_yaml",children:"Configuration File Template"})," ."]}),"\n",(0,o.jsxs)(n.p,{children:["When the model conversion process ends, it also prints the level of similarity between the floating-point model and fixed-point model to the log, you can therefore judge the similarity before and after conversion according to the ",(0,o.jsx)(n.code,{children:"Quantized Cosine"})," field."]}),"\n",(0,o.jsxs)(n.p,{children:["If the value of ",(0,o.jsx)(n.code,{children:"Quantized Cosine"})," is very close to 1, so the performance of the fixed-point model should be very close to that of the floating-point model before the conversion."]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"CosineSimilarity"})," in the log refers to the very first image in the calibration images, it cannot fully represent the model accuracy before and after the conversion."]})})]}),"\n",(0,o.jsxs)(n.p,{children:["After a successful model conversion, the model outputs and information files for each phase will be generated in the generated folder (",(0,o.jsx)(n.code,{children:"model_output"})," by default), where the model output files will be used in subsequent phases."]}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["You can use the ",(0,o.jsx)(n.code,{children:"03_classification/03_resnet50/03_build.sh"})," script to experience the ",(0,o.jsx)(n.code,{children:"hb_compile"})," tool doing the model quantized compilation."]}),"\n",(0,o.jsxs)(n.li,{children:["If you want to learn more about the model conversion workflow, please read ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/quantize_compile.html",children:"Model Quantification and Compilation"})," ."]}),"\n"]})})]}),"\n",(0,o.jsxs)(n.h3,{id:"single-image-inference",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#single-image-inference",children:"#"}),"Single Image Inference"]}),"\n",(0,o.jsx)(n.p,{children:"The accuracy of the fixed-point model generated by the floating-point conversion must be evaluated."}),"\n",(0,o.jsx)(n.p,{children:"You should have good understanding of the input/output structures of the model. You should also be able to accurately preprocess the input images of the model, postprocess the model outputs, and write the model execution scripts on your own."}),"\n",(0,o.jsxs)(n.p,{children:["You can refer to the sample code in ",(0,o.jsx)(n.code,{children:"04_inference.sh"})," of the sample in Horizon model convert sample package."]}),"\n",(0,o.jsx)(n.p,{children:"The code logic is as follows:"}),"\n",(0,o.jsx)(n.pre,{className:"code",children:(0,o.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,o.jsxs)(n.code,{className:"language-python",meta:"",children:[(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" numpy "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"as"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" np"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Load Horizon dependency library"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" horizon_tc_ui"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"hb_runtime "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" HBRuntime"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"from"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" postprocess "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" postprocess"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number"}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Prepare the input for model running, here the `input.py` is the processed data"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"data "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" np"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"load"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"input.npy"'}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Load model file"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"sess "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"HBRuntime"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"("}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"model.bc"'}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:")"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Obtain the model input & output node information"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"input_names "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" sess"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"input_names"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"output_names "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" sess"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"output_names"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number"}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Model inference, here we assume the model has only one input"})}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# The return value is a list that corresponds in order to the names specified by output_names"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"output "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" sess"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"run"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(output_names, {input_names["}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"]: data})"})]}),"\n",(0,o.jsx)(n.span,{className:"line line-number"}),"\n",(0,o.jsx)(n.span,{className:"line line-number",children:(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-comment)"},children:"# Postprocessing"})}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"top_five_label_probs "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"postprocess"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-punctuation)"},children:"(output)"})]}),"\n"]})})}),"\n",(0,o.jsx)(n.p,{children:"Once the script used, you can verify its own accuracy by inputting a single image."}),"\n",(0,o.jsxs)(n.p,{children:["For example, the input to this script is a picture of a zebra, preprocessing the image data from rgb to the data type configured by input_type_rt(for informaiton about intermediate types, refer to the ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/quantize_compile.html#conversion_interpretation",children:"Model Conversion Interpretation"})," ).\nThen, infer the model by passing the above image data by using the ",(0,o.jsx)(n.code,{children:"HB_HBIRRuntime"})," command, post-processing after inference, and finally print out 5 of its most likely types."]}),"\n",(0,o.jsxs)(n.p,{children:["The output of the script is shown as follows with the most possible class being ",(0,o.jsx)(n.code,{children:"label: 340"}),":"]}),"\n",(0,o.jsx)(n.pre,{className:"code",children:(0,o.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,o.jsxs)(n.code,{className:"language-bash",meta:"",children:[(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"I0108"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"18"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:":11:47.398328"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"140427646048000"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"cls_inference.py:89]"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"The"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"input"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"picture"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"is"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"classified"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"to"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"be:"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"label"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"340"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:":"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"prob"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.97"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"label"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"292"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:":"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"prob"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.02"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"label"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"282"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:":"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"prob"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.00"})]}),"\n",(0,o.jsxs)(n.span,{className:"line line-number",children:[(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"label"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"83"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:":"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"prob"}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,o.jsx)(n.span,{style:{color:"var(--shiki-token-constant)"},children:"0.00"})]}),"\n"]})})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"label"})," uses the ImageNet label classes, where the corresponding class of ",(0,o.jsx)(n.code,{children:"340"})," is zebra, so the inference result is correct."]}),"\n","\n",(0,o.jsxs)(n.h3,{id:"model-accuracy-evaluations",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-accuracy-evaluations",children:"#"}),"Model Accuracy Evaluations"]}),"\n",(0,o.jsx)(n.p,{children:"It's insufficient to determine the model accuracy by single image inference, so you still need to use scripts to evalute the model accuracy after the conversion."}),"\n",(0,o.jsx)(n.p,{children:"To do so, you need some coding work to enable the model to loop the image inference and compare the inference results with standard results to get model accuracy results."}),"\n",(0,o.jsxs)(n.p,{children:["In model accuracy evaluations, images must be ",(0,o.jsx)(n.strong,{children:"pre-processed"})," and the model output must be ",(0,o.jsx)(n.strong,{children:"post-processed"}),", so here we provide a Python script as a sample."]}),"\n",(0,o.jsx)(n.p,{children:"The logic of this script is the same as that of single image inference, yet it must run on the entire dataset."}),"\n",(0,o.jsx)(n.p,{children:"The script can evaluate the model output results and generate evaluation results."}),"\n",(0,o.jsxs)(n.p,{children:["Because it takes a long time to run the script, you can set the number of threads to run the evaluation by specifying the ",(0,o.jsx)(n.code,{children:"PARALLEL_PROCESS_NUM"})," environment variable."]}),"\n",(0,o.jsx)(n.p,{children:"After the execution of this script, you can get the accuracy of the converted fixed-point model from the output of the script."}),"\n",(0,o.jsxs)(n.div,{className:"rspress-directive info",children:[(0,o.jsx)(n.div,{className:"rspress-directive-title",children:"Note"}),(0,o.jsx)(n.div,{className:"rspress-directive-content",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Model accuracy may vary slightly due to the differences of operating systems and dependencies."}),"\n",(0,o.jsx)(n.li,{children:"Model accuracy may vary slightly by iteration."}),"\n"]})})]}),"\n",(0,o.jsxs)(n.h3,{id:"reference-supported-calibration-methods",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#reference-supported-calibration-methods",children:"#"}),"[Reference] Supported Calibration Methods"]}),"\n",(0,o.jsx)(n.p,{children:"We currently support the following calibration methods:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Default"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"Default"})," is a strategy that automatically searches the calibrated quantization parameters to obtain a relatively good combination."]}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Mix"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"Mix"})," is a search strategy that integrates multiple calibration methods, which automatically identifies quantization-sensitive nodes, selects the best from a group of calibration methods at node granularity, and finally build a hybrid calibration method absorbing the advantages of multiple calibration methods."]}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"KL"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"KL"})," learns from the ",(0,o.jsx)(n.a,{href:"http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf",target:"_blank",rel:"noopener noreferrer",children:"Solution proposed by TensorRT"}),", uses the KL entropy value to traverse the data distribution of each quantized layer, and determines threshold value by searching for the lowest KL entropy value."]}),"\n",(0,o.jsxs)(n.p,{children:["As this method can cause more data saturation and smaller data quantization granularity, it more suitable than ",(0,o.jsx)(n.code,{children:"max"})," for those neural network models with more concentrated data distribution."]}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"Max"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"Max"})," refers to a calibration method that automatically selects the max value in quantized layer as the threshold."]}),"\n",(0,o.jsx)(n.p,{children:"This method can cause oversized quantization granularity; however, it also causes less saturated points than the KL method, which makes it suitable for those neural network models with more discrete data distribution."}),"\n",(0,o.jsxs)(n.h3,{id:"reference-op-list",children:[(0,o.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#reference-op-list",children:"#"}),"[Reference] OP List"]}),"\n",(0,o.jsxs)(n.p,{children:["For more information about the operators and corresponding constraints currently supported by Horizon Algorithm Toolchain, please refer to ",(0,o.jsx)(n.a,{href:"/3.0.22/en/guide/appendix/supported_op_list/onnx_operator_support_list.html",children:"Toolchain Operator Support Constraint List"}),"."]})]})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,o.jsx)(n,Object.assign({},e,{children:(0,o.jsx)(l,e)})):l(e)}n.default=c,c.__RSPRESS_PAGE_META={},c.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fptq%2Fptq_workflow.mdx"]={toc:[{id:"overview",text:"Overview",depth:2},{id:"ptq-conversion-process",text:"PTQ Conversion Process",depth:2},{id:"model-conversion-process-detailed-explanation",text:"Model Conversion Process Detailed Explanation",depth:2},{id:"model-checking",text:"Model Checking",depth:3},{id:"calibration-image-preparation",text:"Calibration Image Preparation",depth:3},{id:"model-conversion",text:"Model Conversion",depth:3},{id:"single-image-inference",text:"Single Image Inference",depth:3},{id:"model-accuracy-evaluations",text:"Model Accuracy Evaluations",depth:3},{id:"reference-supported-calibration-methods",text:"[Reference] Supported Calibration Methods",depth:3},{id:"reference-op-list",text:"[Reference] OP List",depth:3}],title:"PTQ Conversion Principle and Process",frontmatter:{}}},95895:function(e,n,s){s(39710);var o=s(85893),i=s(67294),t=s(45687);s(20388);let r={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function a(e,n,s){let o=Object.keys(r).includes(n)?n:s;return r[o](e)}n.Z=e=>{let{defaultLocale:n="en-US"}=e,s=(0,t.Vi)().page.readingTimeData,r=(0,t.Jr)(),l=(0,t.e7)(),[c,h]=(0,i.useState)(a(s,r,n));return(0,i.useEffect)(()=>{h(a(s,r,n))},[r,s]),(0,o.jsx)("span",{"data-dark":String(l),className:"rp-reading-time",children:c})}}}]);