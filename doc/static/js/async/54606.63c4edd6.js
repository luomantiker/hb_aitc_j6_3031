"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["54606"],{34181:function(e,n,s){s.r(n);var r=s(85893),i=s(50065),o=s(95895);function t(e){let n=Object.assign({h1:"h1",a:"a",p:"p",strong:"strong",em:"em",ul:"ul",li:"li",code:"code"},(0,i.ah)(),e.components);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.h1,{id:"nnmultiscaledeformableattention",children:[(0,r.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#nnmultiscaledeformableattention",children:"#"}),"nn.MultiScaleDeformableAttention"]}),"\n",(0,r.jsx)(o.Z,{}),"\n",(0,r.jsx)(n.p,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.em,{children:"class"})," horizon_plugin_pytorch.nn.MultiScaleDeformableAttention (embed_dims: int = 256, num_heads: int = 8, num_levels: int = 4, num_points: int = 4, im2col_step: int = 64, dropout: float = 0.1, batch_first: bool = False, value_proj_ratio: float = 1.0, split_weight_mul: bool = False, split_batch: bool = False)"]})}),"\n",(0,r.jsx)(n.p,{children:"An attention module used in Deformable-Detr."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2010.04159.pdf",target:"_blank",rel:"noopener noreferrer",children:"Deformable DETR: Deformable Transformers for End-to-End Object Detection."}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"embed_dims"})," (",(0,r.jsx)(n.code,{children:"int"}),") \u2013 The embedding dimension of Attention.\nDefault: 256."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"num_heads"})," (",(0,r.jsx)(n.code,{children:"int"}),") \u2013 Parallel attention heads. Default: 8."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"num_levels"})," (",(0,r.jsx)(n.code,{children:"int"}),") \u2013 The number of feature map used in\nAttention. Default: 4."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"num_points"})," (",(0,r.jsx)(n.code,{children:"int"}),") \u2013 The number of sampling points for\neach query in each head. Default: 4."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"im2col_step"})," (",(0,r.jsx)(n.code,{children:"int"}),") \u2013 The step used in image_to_column.\nDefault: 64."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"dropout"})," (",(0,r.jsx)(n.code,{children:"float"}),") \u2013 A Dropout layer on inp_identity.\nDefault: 0.1."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"batch_first"})," (",(0,r.jsx)(n.code,{children:"bool"}),") \u2013 Key, Query and Value are shape of\n(batch, n, embed_dim)\nor (n, batch, embed_dim). Default to False."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"value_proj_ratio"})," (",(0,r.jsx)(n.code,{children:"float"}),") \u2013 The expansion ratio of value_proj.\nDefault: 1.0."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"split_weight_mul"})," (",(0,r.jsx)(n.code,{children:"bool"}),") \u2013 Whether split attention weight mul onto each level\noutputs. Enable this can reduce memory usage in qat training."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"split_batch"})," (",(0,r.jsx)(n.code,{children:"bool"}),") \u2013 Whether Compute each batch at a time. Enable this can\nreduce memory usage in qat training."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"forward (query: Tensor | QTensor, key: Tensor | QTensor | None = None, value: Tensor | QTensor | None = None, identity: Tensor | QTensor | None = None, query_pos: Tensor | QTensor | None = None, key_padding_mask: Tensor | None = None, reference_points: Tensor | QTensor | None = None, spatial_shapes: Tensor | None = None)"})}),"\n",(0,r.jsx)(n.p,{children:"Forward Function of MultiScaleDeformAttention."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"query"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),"]) \u2013 Query of Transformer with shape\n(num_query, bs, embed_dims)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"key"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),", ",(0,r.jsx)(n.code,{children:"None"}),"]) \u2013 The key tensor with shape\n(num_key, bs, embed_dims)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"value"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),", ",(0,r.jsx)(n.code,{children:"None"}),"]) \u2013 The value tensor with shape\n(num_key, bs, embed_dims)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"identity"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),", ",(0,r.jsx)(n.code,{children:"None"}),"]) \u2013 The tensor used for addition, with the\nsame shape as query. Default None. If None,\nquery will be used."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"query_pos"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),", ",(0,r.jsx)(n.code,{children:"None"}),"]) \u2013 The positional encoding for query.\nDefault: None."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"key_padding_mask"})," (",(0,r.jsx)(n.code,{children:"Optional"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),"]) \u2013 ByteTensor for query, with\nshape [bs, num_key]."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"reference_points"})," (",(0,r.jsx)(n.code,{children:"Union"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),", ",(0,r.jsx)(n.code,{children:"QTensor"}),", ",(0,r.jsx)(n.code,{children:"None"}),"]) \u2013 The normalized reference\npoints with shape (bs, num_query, num_levels, 2),\nall elements is range in [0, 1], top-left (0,0),\nbottom-right (1, 1), including padding area.\nor (bs, num_query, num_levels, 4), add\nadditional two dimensions is (w, h) to\nform reference boxes."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"spatial_shapes"})," (",(0,r.jsx)(n.code,{children:"Optional"}),"[",(0,r.jsx)(n.code,{children:"Tensor"}),"]) \u2013 Spatial shape of features in\ndifferent levels. int tensor with shape (num_levels, 2),\nlast dimension represents (h, w)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Returns:"}),"\nthe same shape with query."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Return type:"}),"\nTensor"]}),"\n"]})]})}function l(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,r.jsx)(n,Object.assign({},e,{children:(0,r.jsx)(t,e)})):t(e)}n.default=l,l.__RSPRESS_PAGE_META={},l.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fplugin%2Fplugin_api_reference%2Fhorizon_operator%2Fhorizon_plugin_pytorch_nn_MultiScaleDeformableAttention.md"]={toc:[],title:"nn.MultiScaleDeformableAttention",frontmatter:{}}},95895:function(e,n,s){s(39710);var r=s(85893),i=s(67294),o=s(45687);s(20388);let t={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function l(e,n,s){let r=Object.keys(t).includes(n)?n:s;return t[r](e)}n.Z=e=>{let{defaultLocale:n="en-US"}=e,s=(0,o.Vi)().page.readingTimeData,t=(0,o.Jr)(),d=(0,o.e7)(),[c,a]=(0,i.useState)(l(s,t,n));return(0,i.useEffect)(()=>{a(l(s,t,n))},[t,s]),(0,r.jsx)("span",{"data-dark":String(d),className:"rp-reading-time",children:c})}}}]);