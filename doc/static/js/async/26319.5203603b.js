"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["26319"],{45232:function(e,t,n){e.exports=n.p+"static/image/ptq_accuracy_tune_flow.096c9051.png"},76141:function(e,t,n){n.r(t);var i=n(85893),a=n(50065),c=n(95895),o=n(45232);function r(e){let t=Object.assign({h1:"h1",a:"a",p:"p",div:"div",strong:"strong",h2:"h2",h3:"h3",ol:"ol",li:"li",ul:"ul"},(0,a.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.h1,{id:"model-accuracy-tuning",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#model-accuracy-tuning",children:"#"}),"Model Accuracy Tuning"]}),"\n",(0,i.jsx)(c.Z,{}),"\n",(0,i.jsx)(t.p,{children:"Based on the accuracy evaluation in the previous section, you may find that the accuracy is less than expected.\r\nThis section introduces you how to perform accuracy tuning with the accuracy tuning tools and functions to reduce quantization accuracy loss or\r\nto assist you in locating the cause of quantization accuracy loss when you experience a loss of quantization accuracy during the PTQ model conversion."}),"\n",(0,i.jsxs)(t.div,{className:"rspress-directive warning",children:[(0,i.jsx)(t.div,{className:"rspress-directive-title",children:"Attention"}),(0,i.jsx)(t.div,{className:"rspress-directive-content",children:(0,i.jsxs)(t.p,{children:["All the quantization accuracy tuning below refers to the ",(0,i.jsx)(t.strong,{children:"calibrated_model.onnx"})," quantization accuracy tuning generated during the quantization process described in the previous section."]})})]}),"\n",(0,i.jsxs)(t.h2,{id:"accuracy-tuning-advice",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#accuracy-tuning-advice",children:"#"}),"Accuracy Tuning Advice"]}),"\n",(0,i.jsx)(t.p,{children:"You can tune the model accuracy by adjusting the quantization method or the computation accuracy as follows:"}),"\n",(0,i.jsxs)(t.h3,{id:"quantization-method",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#quantization-method",children:"#"}),"Quantization Method"]}),"\n",(0,i.jsx)(t.p,{children:"You can try to adjust the model quantization method by configuring different quantization methods, quantization parameter search methods, or by trying to configure the independent calibration functions:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Configure the calibration method"}),"\n",(0,i.jsxs)(t.p,{children:["You can try to adjust the model calibration method, such as kl, max, and other calibration methods,\r\nthe configuration method can be found in section ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/quant_config.html",children:"The quant_config Introduction"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Configure the quantization parameter search methods"}),"\n",(0,i.jsx)(t.p,{children:"Two different granularity calibration parameter search methods are supported:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"modelwise_search: search for quantization parameters at the model level, this method allows multiple calibration methods to be configured at once, which will find a minimally quantization-loss calibration method by comparing the quantization loss metric (configurable) of the model output before and after quantization."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"layerwise_search: search for quantization parameters at the node level, this method calculates the quantization loss metric (configurable) based on the model output before and after quantization for each node and assigns the calibration method with the the minimally quantization loss to that node."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["The configuration method can be found in section ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/quant_config.html",children:"The quant_config Introduction"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Independent Quantization Function Configuration"}),"\n",(0,i.jsxs)(t.p,{children:["Enable the independent quantization mode can reduce the computational resource consumption, you can try to configure the parameters per_channel, asymmetric, bias_correction,\r\nthe configuration method can be found in section ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/quant_config.html",children:"The quant_config Introduction"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.h3,{id:"computation-accuracy",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#computation-accuracy",children:"#"}),"Computation Accuracy"]}),"\n",(0,i.jsxs)(t.p,{children:["In addition to the configuration of the quantization method, you can try to configure the computation accuracy (dtype) of the model operator to try to accuracy tuning,\r\ncurrently we support configuring the computation accuracy of the operator at three levels: model, op_type, and op_name, and the supported configuration types include int8, int16, float16, and float32.\r\nThe configuration method can be found in section ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hb_compile/quant_config.html",children:"The quant_config Introduction"}),"."]}),"\n",(0,i.jsxs)(t.h2,{id:"accuracy-debug-tool",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#accuracy-debug-tool",children:"#"}),"Accuracy Debug Tool"]}),"\n",(0,i.jsxs)(t.p,{children:["If you want to locate the exact operators that caused the loss of quantization accuracy, we also provide you with the accuracy debug tool to assist you in locating them,\r\nwhich can be found in section ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/accuracy_debug.html",children:"Accuracy Debug Tool"}),"."]}),"\n",(0,i.jsxs)(t.h2,{id:"quantization-accuracy-tuning-flow",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#quantization-accuracy-tuning-flow",children:"#"}),"Quantization Accuracy Tuning Flow"]}),"\n",(0,i.jsx)(t.p,{children:"Based on our previous experience with typical model accuracy tuning processes,\r\nbelow we provide you with an accuracy tuning process that balances ease of use and practicality:"}),"\n",(0,i.jsx)("img",{src:o,width:"400"}),"\n",(0,i.jsx)(t.p,{children:"The tuning flowchart is described in detail below:"}),"\n",(0,i.jsxs)("table",{children:[(0,i.jsxs)("colgroup",{children:[(0,i.jsx)("col",{style:{width:"15%"}}),(0,i.jsx)("col",{style:{width:"20%"}}),(0,i.jsx)("col",{style:{width:"40%"}}),(0,i.jsx)("col",{style:{width:"25%"}})]}),(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"Tuning Area"})}),(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"Milestone"})}),(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"Detailed Description"})}),(0,i.jsx)("td",{children:(0,i.jsx)("b",{children:"Auxiliary Function"})})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"Default int8 quantization model accuracy"}),(0,i.jsx)("td",{children:"Verify whether the int8 quantization accuracy loss meets your expectations."}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"Without any configuration of the quantization parameters, perform the model conversion using the default int8 quantization,\r\ntest the loss of model quantization accuracy, and evaluate whether the accuracy can meet the standard."})}),(0,i.jsx)("td",{rowspan:"3",children:(0,i.jsxs)("ol",{className:"list-decimal pl-5 my-4 leading-7",children:[(0,i.jsx)("li",{children:"Quantization functions such as per_channel, asymmetric, bias_correction, etc. can be configured at various stages of tuning."}),(0,i.jsx)("li",{children:"The accuracy debug tool can assist in locating operators in the model that have a high loss of accuracy."})]})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{rowspan:"2",children:"Mixed computation accuracy tuning"}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"Verify the upper limit of model accuracy when the model is quantized at full int16,\r\nand determine whether subsequent tuning can be completed using mixed-accuracy."})}),(0,i.jsx)("td",{children:(0,i.jsx)(t.p,{children:"Configure all_nodes_type=int16 via quant_config, and try to configure all nodes in the model with int16 high-accuracy computation,\r\nto obtain the upper limit of the quantization accuracy expression capability of the model at int16.\r\nIf all_int16 meets the accuracy standard, the subsequent mixed-accuracy configuration will use int16."})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"By explicitly specifying the computational accuracy of the operator, complete the mixed accuracy fine-tuning."}),(0,i.jsx)("td",{children:(0,i.jsxs)("ol",{className:"list-decimal pl-5 my-4 leading-7",children:[(0,i.jsx)("li",{children:"Use the accuracy debug tool to analyze the nodes with high quantization loss in the int8 calibration model and configure them for high-accuracy int16 computation via quant_config to complete the quantization accuracy fine-tuning, where:"}),(0,i.jsx)("ul",{className:"list-disc pl-5 my-4 leading-7",children:(0,i.jsx)("li",{children:"Choose the calibration method when mixed accuracy configuration: if you use int8+int16 mixed accuracy, you can use the calibration method automatically chosen by the system when all_nodes_type=int16 configuration."})}),(0,i.jsx)("li",{children:"If using the debug tool to set the sensitive nodes to high accuracy does not effectively improve the model accuracy, you can analyze the model structure and try to set the typical operators or sub-structures that have a higher risk of quantitative loss to high accuracy:"}),(0,i.jsxs)("ul",{className:"list-disc pl-5 my-4 leading-7",children:[(0,i.jsx)("li",{children:"Set the index computation substructure of Gridsample to high accuracy in the bev projection process."}),(0,i.jsx)("li",{children:"Set the calibration head structure part of multi-task as high accuracy."}),(0,i.jsx)("li",{children:"Set the quantization sensitive operators such as layernorm and softmax as high accuracy."})]})]})})]})]})]})]})}function s(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,a.ah)(),e.components);return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(r,e)})):r(e)}t.default=s,s.__RSPRESS_PAGE_META={},s.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fptq%2Fptq_usage%2Faccuracy_tune.mdx"]={toc:[{id:"accuracy-tuning-advice",text:"Accuracy Tuning Advice",depth:2},{id:"quantization-method",text:"Quantization Method",depth:3},{id:"computation-accuracy",text:"Computation Accuracy",depth:3},{id:"accuracy-debug-tool",text:"Accuracy Debug Tool",depth:2},{id:"quantization-accuracy-tuning-flow",text:"Quantization Accuracy Tuning Flow",depth:2}],title:"Model Accuracy Tuning",frontmatter:{}}},95895:function(e,t,n){n(39710);var i=n(85893),a=n(67294),c=n(45687);n(20388);let o={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function r(e,t,n){let i=Object.keys(o).includes(t)?t:n;return o[i](e)}t.Z=e=>{let{defaultLocale:t="en-US"}=e,n=(0,c.Vi)().page.readingTimeData,o=(0,c.Jr)(),s=(0,c.e7)(),[h,u]=(0,a.useState)(r(n,o,t));return(0,a.useEffect)(()=>{u(r(n,o,t))},[o,n]),(0,i.jsx)("span",{"data-dark":String(s),className:"rp-reading-time",children:h})}}}]);