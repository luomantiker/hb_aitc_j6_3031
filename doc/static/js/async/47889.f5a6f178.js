"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["47889"],{2271:function(n,e,i){i.r(e);var s=i(85893),r=i(50065),t=i(95895);function a(n){let e=Object.assign({p:"p",strong:"strong",em:"em",ul:"ul",li:"li",code:"code"},(0,r.ah)(),n.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.Z,{}),"\n","\n",(0,s.jsx)(e.p,{}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int = 512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,s.jsx)(e.p,{children:"KL observer."}),"\n",(0,s.jsx)(e.p,{children:"KL observer based on histogram. Histogram is calculated online\nand won\u2019t be saved."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"bins"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Number of histograms bins."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"update_interval"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Interval of computing KL entropy and update min/max.\nKLObserver will constantly collect histograms of activations,\nbut only perform KL calculation when update_interval is satisfied.\nif it is set to 1, KL entropy will be computed every forward step.\nLarger interval guarantees less time and does no harm to\ncalibration accuracy. Set it to the total calibration steps can\nachieve best performance. update_interval must be no greater than\ntotal calibration steps, otherwise no min/max will be computed."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," (",(0,s.jsx)(e.code,{children:"Union"}),"[",(0,s.jsx)(e.code,{children:"dtype"}),", ",(0,s.jsx)(e.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," (",(0,s.jsx)(e.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," (",(0,s.jsx)(e.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,s.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,s.jsxs)(e.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,s.jsx)(e.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int = 1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,s.jsx)(e.p,{children:"MSE observer."}),"\n",(0,s.jsx)(e.p,{children:"Observer module for computing the quantization parameters based on the\nMean Square Error (MSE) between the original tensor and the quantized one."}),"\n",(0,s.jsx)(e.p,{children:"This observer linear searches the quantization scales that minimize MSE."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"stride"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Searching stride. Larger value gives smaller search space,\nwhich means less computing time but possibly poorer accuracy.\nDefault is 1. Suggests no greater than 20."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," (",(0,s.jsx)(e.code,{children:"Union"}),"[",(0,s.jsx)(e.code,{children:"dtype"}),", ",(0,s.jsx)(e.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," (",(0,s.jsx)(e.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," (",(0,s.jsx)(e.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,s.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,s.jsxs)(e.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,s.jsx)(e.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver (averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,s.jsx)(e.p,{children:"Min max observer."}),"\n",(0,s.jsx)(e.p,{children:"This observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average\nminimum and maximum of incoming tensors, and uses this statistic to compute\nthe quantization parameters."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," (",(0,s.jsx)(e.code,{children:"Union"}),"[",(0,s.jsx)(e.code,{children:"dtype"}),", ",(0,s.jsx)(e.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," (",(0,s.jsx)(e.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," (",(0,s.jsx)(e.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsxs)(e.p,{children:["Record the running minimum and maximum of ",(0,s.jsx)(e.code,{children:"x"}),"."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.MixObserver (averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,s.jsx)(e.p,{children:"Mix observer."}),"\n",(0,s.jsx)(e.p,{children:"This observer computes the quantization parameters based on multiple\ncalibration methods and selects the quantization parameters with the\nsmallest quantization error."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," (",(0,s.jsx)(e.code,{children:"Union"}),"[",(0,s.jsx)(e.code,{children:"dtype"}),", ",(0,s.jsx)(e.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," (",(0,s.jsx)(e.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," (",(0,s.jsx)(e.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,s.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,s.jsxs)(e.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,s.jsx)(e.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver (percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None = None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)"]})}),"\n",(0,s.jsx)(e.p,{children:"Percentile observer."}),"\n",(0,s.jsx)(e.p,{children:"Percentile observer based on histogram. Histogram is calculated online\nand won\u2019t be saved. The minimum and maximum are moving averaged to compute\nthe quantization parameters."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"percentile"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Index percentile of histrogram"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"bins"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Number of histograms bins."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," (",(0,s.jsx)(e.code,{children:"float"}),") \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," (",(0,s.jsx)(e.code,{children:"int"}),") \u2013 Channel axis."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," (",(0,s.jsx)(e.code,{children:"Union"}),"[",(0,s.jsx)(e.code,{children:"dtype"}),", ",(0,s.jsx)(e.code,{children:"QuantDType"}),"]) \u2013 Quantized data type."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," (",(0,s.jsx)(e.code,{children:"qscheme"}),") \u2013 Quantization scheme to be used."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Min quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"int"}),"]) \u2013 Max quantization value. Will follow dtype if unspecified."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," (",(0,s.jsx)(e.code,{children:"bool"}),") \u2013 If sync statistics when training with multiple\ndevices."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," (",(0,s.jsx)(e.code,{children:"Optional"}),"[",(0,s.jsx)(e.code,{children:"Dict"}),"]) \u2013 kwargs which are passed to factory functions for\nmin_val and max_val."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,s.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,s.jsxs)(e.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,s.jsx)(e.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]}),"\n",(0,s.jsx)(e.p,{}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver (averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, quant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)"]})}),"\n",(0,s.jsx)(e.p,{children:"MovingAverageMinMax Observer."}),"\n",(0,s.jsx)(e.p,{children:"Observer module for computing the quantization parameters based on the\nmoving average of the min and max values."}),"\n",(0,s.jsx)(e.p,{children:"This observer computes the quantization parameters based on the moving\naverages of minimums and maximums of the incoming tensors. The module\nrecords the average minimum and maximum of incoming tensors, and uses this\nstatistic to compute the quantization parameters."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," \u2013 Quantized data type"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," \u2013 Quantization scheme to be used, only support\nper_tensor_symmetric scheme"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"reduce_range"})," \u2013 Reduces the range of the quantized data type by 1 bit"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," \u2013 Minimum quantization value."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," \u2013 Maximum quantization value."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," \u2013 Whether use sync quantize"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," \u2013 Arguments for register data buffer"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsxs)(e.p,{children:["Record the running minimum and maximum of ",(0,s.jsx)(e.code,{children:"x"}),"."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:[(0,s.jsx)(e.em,{children:"class"})," horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver (averaging_constant=0.01, ch_axis=0, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)"]})}),"\n",(0,s.jsx)(e.p,{children:"MovingAveragePerChannelMinMax Observer."}),"\n",(0,s.jsx)(e.p,{children:"Observer module for computing the quantization parameters based on the\nrunning per channel min and max values."}),"\n",(0,s.jsx)(e.p,{children:"This observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum\nof incoming tensors, and uses this statistic to compute the quantization\nparameters."}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"averaging_constant"})," \u2013 Averaging constant for min/max."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ch_axis"})," \u2013 Channel axis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"dtype"})," \u2013 Quantized data type"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"qscheme"})," \u2013 Quantization scheme to be used, Only support\nper_channel_symmetric"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_min"})," \u2013 Minimum quantization value."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"quant_max"})," \u2013 Maximum quantization value."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"is_sync_quantize"})," \u2013 whether use sync quantize"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"factory_kwargs"})," \u2013 Arguments for register data buffer"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"forward (x_orig)"})}),"\n",(0,s.jsx)(e.p,{children:"Defines the computation performed at every call."}),"\n",(0,s.jsx)(e.p,{children:"Should be overridden by all subclasses."}),"\n",(0,s.jsxs)(e.p,{children:["NOTE:\nAlthough the recipe for forward pass needs to be defined within\nthis function, one should call the ",(0,s.jsx)(e.code,{children:"Module"})," instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them."]})]})}function c(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:e}=Object.assign({},(0,r.ah)(),n.components);return e?(0,s.jsx)(e,Object.assign({},n,{children:(0,s.jsx)(a,n)})):a(n)}e.default=c,c.__RSPRESS_PAGE_META={},c.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fplugin%2Fapi_processed%2Fcalibration_api.md"]={toc:[],title:"",frontmatter:{}}},95895:function(n,e,i){i(39710);var s=i(85893),r=i(67294),t=i(45687);i(20388);let a={"zh-CN":n=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${n.minutes>=1?`${Math.ceil(n.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":n=>`Estimated reading time: ${n.minutes>=1?`${Math.ceil(n.minutes)} minutes`:"less than 1 minute"}`};function c(n,e,i){let s=Object.keys(a).includes(e)?e:i;return a[s](n)}e.Z=n=>{let{defaultLocale:e="en-US"}=n,i=(0,t.Vi)().page.readingTimeData,a=(0,t.Jr)(),o=(0,t.e7)(),[l,d]=(0,r.useState)(c(i,a,e));return(0,r.useEffect)(()=>{d(c(i,a,e))},[a,i]),(0,s.jsx)("span",{"data-dark":String(o),className:"rp-reading-time",children:l})}}}]);