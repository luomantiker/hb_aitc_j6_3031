"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["19680"],{85786:function(e,t,o){e.exports=o.p+"static/image/concat_tensor.da4da5e4.png"},70635:function(e,t,o){e.exports=o.p+"static/image/equal_tensor.88ae5444.png"},45388:function(e,t,o){e.exports=o.p+"static/image/output_shape.d71ddc7a.jpg"},60329:function(e,t,o){e.exports=o.p+"static/image/where_operator.f7cb17cd.png"},4771:function(e,t,o){o.r(t);var n=o(85893),i=o(50065),s=o(95895),r=o(85786),a=o(45388),l=o(60329),h=o(70635);function c(e){let t=Object.assign({h1:"h1",a:"a",h2:"h2",ul:"ul",li:"li",p:"p",code:"code",div:"div",ol:"ol",table:"table",thead:"thead",tr:"tr",th:"th",tbody:"tbody",td:"td",pre:"pre",span:"span",br:"br",strong:"strong",img:"img"},(0,i.ah)(),e.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(t.h1,{id:"post-training-quantization-ptq-faq",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#post-training-quantization-ptq-faq",children:"#"}),"Post-training Quantization (PTQ) FAQ"]}),"\n",(0,n.jsx)(s.Z,{}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-the-two-forms-of-bpu-acceleration-and-cpu-computation",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-the-two-forms-of-bpu-acceleration-and-cpu-computation",children:"#"}),"How to understand the two forms of BPU acceleration and CPU computation?"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"BPU acceleration: It means that the operator can be quantized and accelerated by BPU hardware when the model is inferred at the board side, where most of the operators ( such as conv, and so on) are directly supported by the hardware.\nSome will be replaced with other operators to achieve acceleration (e.g., gemm will be replaced with conv); and others depend on specific contexts (e.g., Reshape, Transpose need to be BPU operators before and after) to be quantized passively."}),"\n",(0,n.jsx)(t.li,{children:"CPU computation: For operators in the model that cannot be directly or indirectly accelerated by the BPU hardware, our toolchain will put them on the CPU for computation, and the runtime prediction library will also automatically complete the heterogeneous scheduling of the execution hardware during model inference."}),"\n"]}),"\n",(0,n.jsxs)(t.h2,{id:"how-does-model-segmentation-affect-the-performance",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-does-model-segmentation-affect-the-performance",children:"#"}),"How does model segmentation affect the performance?"]}),"\n",(0,n.jsx)(t.p,{children:"When the model has CPU operators that cannot be accelerated in the middle of BPU operators, some performance loss will be introduced when switching computation between BPU and CPU operators,\nthus introducing a certain performance loss in two ways."}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CPU operator performance is much lower than that of the BPU operator."}),"\n",(0,n.jsx)(t.li,{children:"The heterogeneous scheduling between CPU and BPU also introduces quantization and dequantization operators (running on CPU), and because the internal computation needs to traverse the data, its time consumption will be proportional to the shape size."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["The above CPU operator and quantization and dequantization operators can be measured by passing ",(0,n.jsx)(t.code,{children:"profile_path"})," parameter through the board-side tool hrt_model_exec. Horizon Robotics recommends that you try to build the model with BPU operator to get better performance."]}),"\n",(0,n.jsxs)(t.h2,{id:"why-some-ops-supported-by-bpu-at-the-tail-part-of-the-model-running-on-cpu",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#why-some-ops-supported-by-bpu-at-the-tail-part-of-the-model-running-on-cpu",children:"#"}),"Why some OPs supported by BPU at the tail part of the model running on CPU?"]}),"\n",(0,n.jsx)(t.p,{children:"First, we need to understand the following two concepts:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Currently supports only the Conv operator at the tail of the model to be output with int32 high precision, while all other operators can only be output with int8 low accuracy."}),"\n",(0,n.jsx)(t.li,{children:"Normally, the model conversion will fuse Conv with its subsequent BN and ReLU/ReLU6 in the optimization stage for calculation. However, due to the limitation of the BPU hardware itself, Conv, which is output with int32 high precision at the end of the model, does not support operator fusion."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["Therefore, if the model is ended with Conv+ReLU/ReLU6, then to ensure the overall accuracy of the quantized model, the Conv will by default use int32 output, while the ReLU/ReLU6 will run on the CPU. Similarly, the other tail-part OPs who run on the CPU are also because that the Conv OP needs higher-accuracy output. However, Horizon supports running these operators on the BPU by configuring ",(0,n.jsx)(t.code,{children:"quant_config"})," in the yaml file to get better performance, but introduces some loss of accuracy."]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-the-compiler-optimization-level-parameters-in-yaml-files",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-the-compiler-optimization-level-parameters-in-yaml-files",children:"#"}),"How to understand the compiler optimization level parameters in yaml files?"]}),"\n",(0,n.jsx)(t.p,{children:"In the yaml configuration file of the model conversion, the compilation parameter group provides the optimize_level parameter to select the optimization level of the model compilation, the available range is O0~O2. Among which:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"O0 without any optimization, the fastest compilation speed, suitable for use in model conversion function verification, debugging different calibration methods."}),"\n",(0,n.jsx)(t.li,{children:"O1 - O2 as the optimization level gets higher, the search space during compilation and optimization gets larger."}),"\n",(0,n.jsx)(t.li,{children:"The compiler's optimization strategy is not at the operator granularity level, but is a global optimization for the whole model."}),"\n"]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-compile-to-get-a-multi-batch-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-compile-to-get-a-multi-batch-model",children:"#"}),"How to compile to get a multi-batch model?"]}),"\n",(0,n.jsx)(t.p,{children:"According to the original model type, we will discuss this issue by dividing it into dynamic input models and non-dynamic input models."}),"\n",(0,n.jsxs)(t.div,{className:"rspress-directive info",children:[(0,n.jsx)(t.div,{className:"rspress-directive-title",children:"Note"}),(0,n.jsx)(t.div,{className:"rspress-directive-content",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["The ",(0,n.jsx)(t.code,{children:"input_batch"})," parameter can only be used only for the model which first dimension of the ",(0,n.jsx)(t.code,{children:"input_shape"})," is 1 (if the model has multiple inputs, the first dimension of the ",(0,n.jsx)(t.code,{children:"input_shape"})," needs to be 1 for all inputs), and only effective when the original onnx model itself supports multi-batch inference. This parameter only supports specifying a single value, which will act on all inputs of the model when the model has multiple inputs."]}),"\n",(0,n.jsxs)(t.li,{children:["The size of each calibration data shape should be the same as the size of ",(0,n.jsx)(t.code,{children:"input_shape"}),"."]}),"\n"]})})]}),"\n",(0,n.jsx)(t.p,{children:"Dynamic input model: If the original model is a dynamic input model, for example, ? x3x224x224 (dynamic input models must use the input_shape parameter to specify the model input information)."}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["When input_shape is configured as 1x3x224x224, if you want to compile the model to get a multi-batch model, you can use ",(0,n.jsx)(t.code,{children:"input_batch"})," parameter, then each calibration data shape size is 1x3x224x224."]}),"\n",(0,n.jsxs)(t.li,{children:["When the first dimension of input_shape is an integer greater than 1, the original model itself will be recognized as a multi-batch model, and the ",(0,n.jsx)(t.code,{children:"input_batch"})," parameter can not be used, and you need to pay attention to the size of each calibration data shape. For example, if the input_shape is 4x3x224x224, the size of each calibration data shape need to be 4x3x224x224."]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Non-dynamic input model."}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["When the input shape[0] of the input is 1, you can use the ",(0,n.jsx)(t.code,{children:"input_batch"})," parameter.\nEach calibration data shape size is the same as the original model shape."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["When the input shape[0] is not 1, the ",(0,n.jsx)(t.code,{children:"input_batch"})," parameter is not supported."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.h2,{id:"is-it-normal-for-the-order-of-model-inputs-to-change-during-the-conversion-of-a-multi-input-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#is-it-normal-for-the-order-of-model-inputs-to-change-during-the-conversion-of-a-multi-input-model",children:"#"}),"Is it normal for the order of model inputs to change during the conversion of a multi-input model?"]}),"\n",(0,n.jsx)(t.p,{children:"This is normal. It is possible for the model input order to change during the conversion of a multi-input model.\nThe possible cases are shown in the following example."}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"original floating-point model input order: input1, input2, input3."}),"\n",(0,n.jsx)(t.li,{children:"original.onnx model input order: input1, input2, input3."}),"\n",(0,n.jsx)(t.li,{children:"quanti.bc model input order: input2, input1, input3."}),"\n",(0,n.jsx)(t.li,{children:"hbm model input order: input3, input2, input1."}),"\n"]}),"\n",(0,n.jsxs)(t.div,{className:"rspress-directive warning",children:[(0,n.jsx)(t.div,{className:"rspress-directive-title",children:"Attention"}),(0,n.jsx)(t.div,{className:"rspress-directive-content",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"When you do accuracy consistency alignment, please make sure the input order is correct, otherwise it may affect the accuracy result."}),"\n",(0,n.jsxs)(t.li,{children:["If you want to check the hbm model input order, you can use the ",(0,n.jsx)(t.code,{children:"hb_model_info"})," command to check it.\nThe input order listed in the input_parameters info group is the hbm model input order."]}),"\n"]})})]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-prepare-a-floating-point-model-for-horizon-support-conversion",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-prepare-a-floating-point-model-for-horizon-support-conversion",children:"#"}),"How to prepare a floating-point model for Horizon support conversion?"]}),"\n",(0,n.jsx)(t.p,{children:"The Horizon PTQ floating-point conversion toolchain supports both Caffe1.0 and ONNX (10 \u2264 opset_version \u2264 19 and ir_version \u2264 9) model formats, and the ONNX export method is as follows:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Training Framework"}),(0,n.jsx)(t.th,{children:"ONNX Export Methods"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Caffe1.0"}),(0,n.jsx)(t.td,{children:"Horizon native support, no need to export ONNX"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Othere frameworks"}),(0,n.jsx)(t.td,{children:(0,n.jsx)(t.a,{href:"https://github.com/onnx/tutorials",target:"_blank",rel:"noopener noreferrer",children:"ONNX Tutorials"})})]})]})]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-verify-the-correctness-of-the-original-floating-point-onnx-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-verify-the-correctness-of-the-original-floating-point-onnx-model",children:"#"}),"How to verify the correctness of the original floating-point ONNX model?"]}),"\n",(0,n.jsxs)(t.p,{children:["The toolchain uses an interface based on the public ONNXRuntime wrapper to implement model parsing and forwarding when converting models.\nSo we should check the legitimacy of the original floating-point ONNX model itself (i.e., whether it is capable of inference properly) and whether accuracy bias was introduced during the export of ONNX from the training framework before using the toolchain.\nSpecific tests can be found in the ",(0,n.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_tool/hbruntime.html",children:"The HBRuntime Inference Library"})," section."]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-convert-the-model-of-fp16",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-convert-the-model-of-fp16",children:"#"}),"How to convert the model of fp16?"]}),"\n",(0,n.jsx)(t.p,{children:"The toolchain does not currently support direct conversion of fp16 models, but you can convert them to fp32 models first, and then use the Horizon toolchain for quantization conversion."}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-modify-the-input-layout-of-the-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-modify-the-input-layout-of-the-model",children:"#"}),"How to modify the input layout of the model?"]}),"\n",(0,n.jsx)(t.p,{children:"J6 PTQ tool no longer supports to modify the input layout format of the model during the conversion process, PTQ will keep your model layout unchanged during the conversion process.\nIf you need to modify the layout, you can directly modify the model before exporting onnx to finish the layout change."}),"\n",(0,n.jsx)(t.p,{children:"If modifications are required at the model level, it is recommended that they be implemented in the following way:"}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsx)(t.li,{children:"Add transpose to the DL framework so that its input layout is NHWC and re-export the ONNX model;"}),"\n",(0,n.jsx)(t.li,{children:"Use the onnx library to modify the model directly, as referenced below:"}),"\n"]}),"\n",(0,n.jsx)(t.pre,{className:"code",children:(0,n.jsx)(t.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,n.jsxs)(t.code,{className:"language-cpp",meta:"",children:[(0,n.jsx)(t.span,{className:"line line-number",children:(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"import onnx"})}),"\n",(0,n.jsx)(t.span,{className:"line line-number"}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"onnx_model "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-function)"},children:"load"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"("}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:"'./model.onnx'"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:")"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"graph "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx_model"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"graph"})]}),"\n",(0,n.jsx)(t.span,{className:"line line-number"}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"new_node "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"helper"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-function)"},children:"make_node"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"(         # Create Transpose node"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"           "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"Transpose"'}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"           inputs "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" ["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"data"'}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"            # New input node name"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" customization"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"           outputs "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" ["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"data_oringal"'}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]    # Original input node name"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"           perm "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" ["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"3"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"2"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]           # NHWC"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"--\x3e"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"NCHW"})]}),"\n",(0,n.jsx)(t.span,{className:"line line-number",children:(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:")"})}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"graph"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"node"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-function)"},children:"insert"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"("}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" new_node)            # Insert Transpose at the very front of the model"})]}),"\n",(0,n.jsx)(t.span,{className:"line line-number"}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"d "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"graph"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"input["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"type"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"tensor_type"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"shape"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"dim    # Configure "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"new"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" model inputs"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" NHWC"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"d"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"dim_value "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"1"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"d"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"1"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"dim_value "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"224"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"d"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"2"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"dim_value "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"224"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"d"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"3"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"dim_value "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"6"})]}),"\n",(0,n.jsx)(t.span,{className:"line line-number"}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx_model"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"graph"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"input["}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"0"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"]"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"name "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"="}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" f"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"data"'}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"    # Configure the model input name"})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"checker"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-function)"},children:"check_model"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"(onnx_model)        # Model check "})]}),"\n",(0,n.jsxs)(t.span,{className:"line line-number",children:[(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-constant)"},children:"onnx"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:"."}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-function)"},children:"save"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:"(onnx_model"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-punctuation)"},children:","}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-string-expression)"},children:'"new_model.onnx"'}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:")     # Save the "}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"new"}),(0,n.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" model"})]}),"\n"]})})}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-shape_inference_failonnx",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-shape_inference_failonnx",children:"#"}),"How to understand shape_inference_fail.onnx?"]}),"\n",(0,n.jsx)(t.p,{children:"This model will only be output when the PTQ model conversion fails, it has no special meaning,\nyou can provide this model and the log file of the model conversion to Horizon for debug analysis."}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-that-the-model-input-and-output-shape-in-log-is-0",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-that-the-model-input-and-output-shape-in-log-is-0",children:"#"}),"How to understand that the model input and output shape in log is 0?"]}),"\n",(0,n.jsxs)(t.p,{children:["The model transformation log prints the name and shape of the input and output nodes of the model, but sometimes the shape is 0, as shown in the following figure.\nThis occurs mainly in the batch dimension, because the output shape of some models are dynamic (stored using dim_param), and converting the tool forward inference model (shape_inference) uses a '?' to take up space, and the log print will show 0.",(0,n.jsx)(t.br,{}),"\n","This is as expected and does not affect the model transformation and does not need to be of undue concern."]}),"\n",(0,n.jsx)("img",{src:a,alt:"output_shape",width:"850"}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-that-the-operator-changes-in-the-optimized-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-that-the-operator-changes-in-the-optimized-model",children:"#"}),"How to understand that the operator changes in the optimized model?"]}),"\n",(0,n.jsxs)(t.p,{children:["In order to improve the performance of the model on the board, the model transformation has an Optimizer module which performs some graphs optimization on the model to improve the performance of the model on the board.\nThe Optimizer module mainly includes functions such as ",(0,n.jsx)(t.strong,{children:"Constant Folding"}),", ",(0,n.jsx)(t.strong,{children:"Operator Replacement"}),", ",(0,n.jsx)(t.strong,{children:"Operator Property/Input Update"}),", ",(0,n.jsx)(t.strong,{children:"Operator Fusion"}),", and ",(0,n.jsx)(t.strong,{children:"Operator Move"}),"."]}),"\n",(0,n.jsxs)(t.div,{className:"rspress-directive warning",children:[(0,n.jsx)(t.div,{className:"rspress-directive-title",children:"Warning"}),(0,n.jsx)(t.div,{className:"rspress-directive-content",children:(0,n.jsx)(t.p,{children:"The Optimizer module causes a change in the operator of the your model, and this module performs some equivalent graph optimization of the model."})})]}),"\n",(0,n.jsxs)(t.h2,{id:"in-the-conversion-process-what-are-the-several-scenarios-in-which-the-calibrated-cosine-column-appears-as-nan-in-the-print-log-of-the-hb_compile-tool-and-what-causes-such-problems",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#in-the-conversion-process-what-are-the-several-scenarios-in-which-the-calibrated-cosine-column-appears-as-nan-in-the-print-log-of-the-hb_compile-tool-and-what-causes-such-problems",children:"#"}),"In the conversion process, what are the several scenarios in which the Calibrated Cosine column appears as nan in the print log of the hb_compile tool and what causes such problems?"]}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Case 1: The second input to the where operator in the model is -Inf"})}),"\n",(0,n.jsx)(t.p,{children:"During the optimization phase of the transformation process, in order to be able to ensure that the where operator can be quantized, this operator will be split into a combination of several quantizable operators during the optimization phase. When the second input of where is -Inf, there will be a case of nan in the transformation process. The reason is as follows:"}),"\n",(0,n.jsx)("img",{src:l,alt:"where_operator",width:"900"}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Case 2: All 0 tensor in the model"})}),"\n",(0,n.jsx)(t.p,{children:"1). concat result in all 0 tensor."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{src:r})}),"\n",(0,n.jsx)(t.p,{children:"Taking the above structure as an example, the two inputs to concat are all 0 constants, so it is possible that the subsequent output after slice may have a branch in which the data flowing are all 0, and lead to illegal thresholds (i.e., thresholds of 0) in HzCalibration."}),"\n",(0,n.jsx)(t.p,{children:"2). equal result in all 0 tensor."}),"\n",(0,n.jsx)("img",{src:h,alt:"equal_tensor",width:"450"}),"\n",(0,n.jsx)(t.p,{children:"Taking the above structure as an example, the input onnx::Not_3 to the model is a bool type tensor, which after cast+equal results in all false data in the tensor, and after cast(bool -> float32) results in an all 0 tensor."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.div,{className:"rspress-directive warning",children:[(0,n.jsx)(t.div,{className:"rspress-directive-title",children:"Warning"}),(0,n.jsx)(t.div,{className:"rspress-directive-content",children:(0,n.jsx)(t.p,{children:"This case requires the customer to try to test with multiple sets of data, thus ruling out the possibility that the model has all 0 caused by the calibration data."})})]}),"\n",(0,n.jsxs)(t.h2,{id:"is-there-hardware-support-for-model-input-color-space-conversion",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#is-there-hardware-support-for-model-input-color-space-conversion",children:"#"}),"Is there hardware support for model input color space conversion?"]}),"\n",(0,n.jsxs)(t.p,{children:["BPU supports conversion between common model input formats (e.g. nv12 -> rgb/bgr) and data normalization, which can be configured via yaml file.\nSee the description of ",(0,n.jsx)(t.code,{children:"input_type_train"}),", ",(0,n.jsx)(t.code,{children:"input_type_rt"}),", ",(0,n.jsx)(t.code,{children:"mean_value"}),", ",(0,n.jsx)(t.code,{children:"scale_value"})," and ",(0,n.jsx)(t.code,{children:"std_value"})," parameters."]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-understand-compiler-optimization-levels",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-understand-compiler-optimization-levels",children:"#"}),"How to understand compiler optimization levels?"]}),"\n",(0,n.jsxs)(t.p,{children:["The yaml file ",(0,n.jsx)(t.code,{children:"optimize_level"})," parameter configures the compiler optimization level from ",(0,n.jsx)(t.code,{children:"O0 to O2"}),", and the higher the optimization level, the larger the search space is available and usually more time consuming.\nThe optimization level does not perform some defined optimization strategy at the operator granularity level, and most of the operator optimizations are independent of the optimization level (they are not time-consuming).\nThe optimization level is mainly for global optimization, which is the analysis and optimization of the whole model."]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-handle-quantizationdequantization-operators-in-the-first-and-last-parts-of-the-model",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-handle-quantizationdequantization-operators-in-the-first-and-last-parts-of-the-model",children:"#"}),"How to handle quantization/dequantization operators in the first and last parts of the model?"]}),"\n",(0,n.jsx)(t.p,{children:"By default, PTQ toolchain inserts quantization operator at the beginning of featuremap input model to implement the mapping of input data from float32 to int8, and inserts dequantization operator at the end of all models to implement the mapping of output data from int8 (or int32 by default if BPU ends with conv) to float32.\nThe quantization/dequantization operators are not efficient on the CPU, especially when the data shape is large."}),"\n",(0,n.jsx)(t.p,{children:"Therefore, we prefer to integrate quantization/dequantization operations into pre- and post-processing, which is the most efficient way."}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-use-unitconv-operator-to-optimize-model-performance",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-use-unitconv-operator-to-optimize-model-performance",children:"#"}),"How to use unitconv operator to optimize model performance?"]}),"\n",(0,n.jsxs)(t.p,{children:["For an operator in the model to run on BPU, besides satisfying the BPU support conditions, it also needs to be able to find its quantization threshold on calibration.\nThe quantization thresholds of some non-computation intensive operators (such as concat, reshape, etc.) depend on the featuremap Tensor of the upstream and downstream operators.\nTherefore, if these operators are at the beginning and end of the model they will run on the CPU by default.",(0,n.jsx)(t.br,{}),"\n","At this point, for more efficient performance, unitconv can be inserted before/after this operator to introduce a new quantization threshold statistic, which in turn can be quantized on the BPU."]}),"\n",(0,n.jsxs)(t.p,{children:["However, it should be noted that this approach may introduce some quantization loss.\nTake the model output by ",(0,n.jsx)(t.code,{children:"conv+reshape+concat"})," structure as an example, by default, the toolchain will output conv with high accuracy of int32, and then send it to reshape and concat on CPU after dequantization to float32.\nIf unitconv is inserted after concat, the whole structure will run on BPU with low accuracy of int8.\nAt this point, although the final unitconv can still be output with high accuracy of int32, the accuracy compression of the previous conv output has already introduced a certain amount of quantization loss.\nTherefore, please consider whether to insert unitconv to optimize the performance."]}),"\n",(0,n.jsxs)(t.h2,{id:"whether-int16int32-calculations-are-supported",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#whether-int16int32-calculations-are-supported",children:"#"}),"Whether int16/int32 calculations are supported?"]}),"\n",(0,n.jsxs)(t.p,{children:["Most of the operators use int8 calculation by default, and some of them support int16 and fp16 calculation, and the range of operator support continues to expand, see ",(0,n.jsx)(t.a,{href:"/3.0.22/en/guide/appendix/supported_op_list/onnx_operator_support_list.html",children:"Toolchain Operator Support Constraints List - ONNX Operator Support List"}),", in addition:"]}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsx)(t.li,{children:"If the BPU part of the model ends in Conv, the operator defaults to int32 high accuracy output;"}),"\n",(0,n.jsx)(t.li,{children:"The DSP hardware is also int8/int16/float32 capable."}),"\n"]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-handle-the-calibration-data-of-the-model-correctly",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-handle-the-calibration-data-of-the-model-correctly",children:"#"}),"How to handle the calibration data of the model correctly?"]}),"\n",(0,n.jsxs)(t.p,{children:["For the preparation of PTQ model calibration data, refer to the  ",(0,n.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_usage/prepare_calibration_data.html",children:"Prepare Calibration Data"})," section.\nAlso, for featuremap input models, please do your own preprocessing of the data and save it as an npy file of the desired input type via the numpy.save interface."]}),"\n",(0,n.jsxs)(t.h2,{id:"how-to-dump-model-middle-tier-output",children:[(0,n.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#how-to-dump-model-middle-tier-output",children:"#"}),"How to dump model middle tier output?"]}),"\n",(0,n.jsxs)(t.p,{children:["During the model conversion phase, if ",(0,n.jsx)(t.code,{children:"debug_mode"})," parameter in the yaml file is configured with ",(0,n.jsx)(t.code,{children:"dump_all_layers_output"}),", adds a dequantization output node for each convolution and matmul operator, which significantly degrades the performance of the model once it is on the board."]}),"\n",(0,n.jsxs)(t.p,{children:["Among them, the ",(0,n.jsx)(t.code,{children:"output_nodes"})," parameter can specify any node in the model as an output node, which is more convenient for us to debug and tune.\nIn addition, the ",(0,n.jsx)(t.code,{children:"hb_verifier"})," tool can be used to compare the consistency of the fixed-point model quantized_model.bc with the hbm model on the upper board."]}),"\n",(0,n.jsxs)(t.p,{children:["During the board deployment phase, the ",(0,n.jsx)(t.code,{children:"hrt_model_exec"})," tool also supports saving node outputs (including nodes specified with the output_nodes parameter) in bin or txt format, as described in ",(0,n.jsx)(t.a,{href:"/3.0.22/en/guide/ucp/runtime/tool_introduction/hrt_model_exec.html",children:"The hrt_model_exec Tool Introduction"})," \u3002"]})]})}function d(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,i.ah)(),e.components);return t?(0,n.jsx)(t,Object.assign({},e,{children:(0,n.jsx)(c,e)})):c(e)}t.default=d,d.__RSPRESS_PAGE_META={},d.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fptq%2Fptq_faq_troubleshooting%2Fptq_faq.mdx"]={toc:[{id:"how-to-understand-the-two-forms-of-bpu-acceleration-and-cpu-computation",text:"How to understand the two forms of BPU acceleration and CPU computation?",depth:2},{id:"how-does-model-segmentation-affect-the-performance",text:"How does model segmentation affect the performance?",depth:2},{id:"why-some-ops-supported-by-bpu-at-the-tail-part-of-the-model-running-on-cpu",text:"Why some OPs supported by BPU at the tail part of the model running on CPU?",depth:2},{id:"how-to-understand-the-compiler-optimization-level-parameters-in-yaml-files",text:"How to understand the compiler optimization level parameters in yaml files?",depth:2},{id:"how-to-compile-to-get-a-multi-batch-model",text:"How to compile to get a multi-batch model?",depth:2},{id:"is-it-normal-for-the-order-of-model-inputs-to-change-during-the-conversion-of-a-multi-input-model",text:"Is it normal for the order of model inputs to change during the conversion of a multi-input model?",depth:2},{id:"how-to-prepare-a-floating-point-model-for-horizon-support-conversion",text:"How to prepare a floating-point model for Horizon support conversion?",depth:2},{id:"how-to-verify-the-correctness-of-the-original-floating-point-onnx-model",text:"How to verify the correctness of the original floating-point ONNX model?",depth:2},{id:"how-to-convert-the-model-of-fp16",text:"How to convert the model of fp16?",depth:2},{id:"how-to-modify-the-input-layout-of-the-model",text:"How to modify the input layout of the model?",depth:2},{id:"how-to-understand-shape_inference_failonnx",text:"How to understand shape_inference_fail.onnx?",depth:2},{id:"how-to-understand-that-the-model-input-and-output-shape-in-log-is-0",text:"How to understand that the model input and output shape in log is 0?",depth:2},{id:"how-to-understand-that-the-operator-changes-in-the-optimized-model",text:"How to understand that the operator changes in the optimized model?",depth:2},{id:"in-the-conversion-process-what-are-the-several-scenarios-in-which-the-calibrated-cosine-column-appears-as-nan-in-the-print-log-of-the-hb_compile-tool-and-what-causes-such-problems",text:"In the conversion process, what are the several scenarios in which the Calibrated Cosine column appears as nan in the print log of the hb_compile tool and what causes such problems?",depth:2},{id:"is-there-hardware-support-for-model-input-color-space-conversion",text:"Is there hardware support for model input color space conversion?",depth:2},{id:"how-to-understand-compiler-optimization-levels",text:"How to understand compiler optimization levels?",depth:2},{id:"how-to-handle-quantizationdequantization-operators-in-the-first-and-last-parts-of-the-model",text:"How to handle quantization/dequantization operators in the first and last parts of the model?",depth:2},{id:"how-to-use-unitconv-operator-to-optimize-model-performance",text:"How to use unitconv operator to optimize model performance?",depth:2},{id:"whether-int16int32-calculations-are-supported",text:"Whether int16/int32 calculations are supported?",depth:2},{id:"how-to-handle-the-calibration-data-of-the-model-correctly",text:"How to handle the calibration data of the model correctly?",depth:2},{id:"how-to-dump-model-middle-tier-output",text:"How to dump model middle tier output?",depth:2}],title:"Post-training Quantization (PTQ) FAQ",frontmatter:{}}},95895:function(e,t,o){o(39710);var n=o(85893),i=o(67294),s=o(45687);o(20388);let r={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function a(e,t,o){let n=Object.keys(r).includes(t)?t:o;return r[n](e)}t.Z=e=>{let{defaultLocale:t="en-US"}=e,o=(0,s.Vi)().page.readingTimeData,r=(0,s.Jr)(),l=(0,s.e7)(),[h,c]=(0,i.useState)(a(o,r,t));return(0,i.useEffect)(()=>{c(a(o,r,t))},[r,o]),(0,n.jsx)("span",{"data-dark":String(l),className:"rp-reading-time",children:h})}}}]);