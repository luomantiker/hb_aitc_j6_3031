"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["46878"],{93611:function(e,t,n){e.exports=n.p+"static/svg/segmented_deploy.4699b5b9.svg"},26267:function(e,t,n){e.exports=n.p+"static/svg/segmented_deploy_method.b8091296.svg"},50510:function(e,t,n){n.r(t);var i=n(85893),o=n(50065),a=n(95895),s=n(93611),d=n(26267);function r(e){let t=Object.assign({h1:"h1",a:"a",h2:"h2",p:"p",img:"img",ol:"ol",li:"li"},(0,o.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.h1,{id:"model-segmented-deployment",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#model-segmented-deployment",children:"#"}),"Model Segmented Deployment"]}),"\n",(0,i.jsx)(a.Z,{}),"\n",(0,i.jsxs)(t.h2,{id:"scenario",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#scenario",children:"#"}),"Scenario"]}),"\n",(0,i.jsx)(t.p,{children:"In some scenarios, there may be a need to split the model trained as a whole into multiple segments for on-board deployment.\nFor example, for the two-stage detection model in the below picture, if the DPP needs to be executed on the CPU, and the output (roi) of the DPP needs to be the input of RoiAlign,\nthen the model needs to be split into Stage1 and Stage2 according to the annotation of the dotted box and compiled separately for on-board deployment.\nWhen running on the board, the fixed-point data output from the backbone will be directly used as the input to RoiAlign."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"segmented_deploy",src:s})}),"\n",(0,i.jsxs)(t.h2,{id:"usage",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#usage",children:"#"}),"Usage"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"segmented_deploy_method",src:d})}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Model modification: As shown in the picture above, based on a model which can normally be trained in quantized awareness, you need to insert the QuantStub after the cutoff point of the model segments before prepare_qat.\nNote that if horizon_plugin_pytorch.quantization.QuantStub is used, scale = None must be set."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"QAT training: train the modified model on quantized awareness as a whole, the inserted QuantStub will record the scale of the input data of the Stage2 model into the buffer."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Fixed-point model conversion: convert the whole trained QAT model to a fixed-point model using the convert interface."}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:"Segmentation and compilation: segment the model according to the form after deploying on the board, then export and compile the segmented model respectively.\nNote that although the input of Stage2 is quantized data during training, the example_input of Stage2 when exporting still needs to be the floating-point form, the inserted QuantStub in Stage2 will configure the correct scale for the data and quantize it."}),"\n"]}),"\n"]})]})}function l(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,o.ah)(),e.components);return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(r,e)})):r(e)}t.default=l,l.__RSPRESS_PAGE_META={},l.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fplugin%2Fadvanced_tutorial%2Fsegmented_deploy.mdx"]={toc:[{id:"scenario",text:"Scenario",depth:2},{id:"usage",text:"Usage",depth:2}],title:"Model Segmented Deployment",frontmatter:{}}},95895:function(e,t,n){n(39710);var i=n(85893),o=n(67294),a=n(45687);n(20388);let s={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function d(e,t,n){let i=Object.keys(s).includes(t)?t:n;return s[i](e)}t.Z=e=>{let{defaultLocale:t="en-US"}=e,n=(0,a.Vi)().page.readingTimeData,s=(0,a.Jr)(),r=(0,a.e7)(),[l,h]=(0,o.useState)(d(n,s,t));return(0,o.useEffect)(()=>{h(d(n,s,t))},[s,n]),(0,i.jsx)("span",{"data-dark":String(r),className:"rp-reading-time",children:l})}}}]);