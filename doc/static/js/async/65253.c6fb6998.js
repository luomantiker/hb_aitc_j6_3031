"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["65253"],{76985:function(e,t,n){n.r(t);var o=n(85893),i=n(50065),r=n(95895);function a(e){let t=Object.assign({h1:"h1",a:"a",p:"p",pre:"pre",code:"code",span:"span"},(0,i.ah)(),e.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.h1,{id:"introduction",children:[(0,o.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#introduction",children:"#"}),"Introduction"]}),"\n",(0,o.jsx)(r.Z,{}),"\n",(0,o.jsx)(t.p,{children:"The quantization indicates a technique for performing computations and storing tensors with the bit widths lower than the floating-point accuracy. Quantized models perform some or all operations on the tensor using integers rather than floating-point values. Compared to typical FP32 models, taking INT8 quantization as an example, resulting in a 4x reduction in model size and a 4x reduction in memory bandwidth requirements. The hardware support for INT8 computation is typically 2 to 4 times faster than FP32 computation. The quantization is primarily a technique to accelerate the inference, and the quantization operations are only supported for forward computation."}),"\n",(0,o.jsx)(t.p,{children:"horizon_plugin_pytorch provides the BPU-adapted quantization operations and supports quantization-aware training (QAT). The QAT uses fake-quantization modules to model the quantization errors in forward computation and backpropagation. Note that the computation process of the QAT is performed by using floating-point operations. At the end of the QAT, horizon_plugin_pytorch provides the conversion functions to convert the trained model to a fixed-point model, using a more compact model for representation and high-performance vectorization on the BPU."}),"\n",(0,o.jsx)(t.p,{children:"This section gives you a detailed introduction to horizon_plugin_pytorch quantitative training tool developed on the basis of PyTorch."}),"\n",(0,o.jsxs)(t.p,{children:["Horizon_plugin_pytorch is developed based on PyTorch, in order to reduce the learning cost of you, we refer to the design of PyTorch on quantized awareness training.\r\nThis doc doesn't repeat the contents which PyTorch doc already contains, if you want to understand the details of the tool, we recommend that you read the ",(0,o.jsx)(t.a,{href:"https://github.com/pytorch/pytorch/tree/v2.0.0/torch/ao",target:"_blank",rel:"noopener noreferrer",children:"Official source code"})," or the python source code of this tool.\r\nTo ensure a smooth experience, we recommend that you first read the PyTorch documentation and familiarize yourself with the quantized awareness training and deployment tools provided by PyTorch."]}),"\n",(0,o.jsx)(t.p,{children:"For the purpose of brevity, the code in the documentation has been replaced with the following aliases by default:"}),"\n",(0,o.jsx)(t.pre,{className:"code",children:(0,o.jsx)(t.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,o.jsxs)(t.code,{className:"language-python",meta:"",children:[(0,o.jsxs)(t.span,{className:"line line-number",children:[(0,o.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"import"}),(0,o.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" horizon_plugin_pytorch "}),(0,o.jsx)(t.span,{style:{color:"var(--shiki-token-keyword)"},children:"as"}),(0,o.jsx)(t.span,{style:{color:"var(--shiki-color-text)"},children:" horizon"})]}),"\n"]})})})]})}function s(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,i.ah)(),e.components);return t?(0,o.jsx)(t,Object.assign({},e,{children:(0,o.jsx)(a,e)})):a(e)}t.default=s,s.__RSPRESS_PAGE_META={},s.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fplugin%2Fintroduce.mdx"]={toc:[],title:"Introduction",frontmatter:{}}},95895:function(e,t,n){n(39710);var o=n(85893),i=n(67294),r=n(45687);n(20388);let a={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function s(e,t,n){let o=Object.keys(a).includes(t)?t:n;return a[o](e)}t.Z=e=>{let{defaultLocale:t="en-US"}=e,n=(0,r.Vi)().page.readingTimeData,a=(0,r.Jr)(),c=(0,r.e7)(),[h,d]=(0,i.useState)(s(n,a,t));return(0,i.useEffect)(()=>{d(s(n,a,t))},[a,n]),(0,o.jsx)("span",{"data-dark":String(c),className:"rp-reading-time",children:h})}}}]);