"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["57151"],{56990:function(e,t,n){e.exports=n.p+"static/image/qat_ptq_contrast.0ec33c90.png"},29649:function(e,t,n){e.exports=n.p+"static/image/toolchain_framework.62614baf.png"},38194:function(e,t,n){e.exports=n.p+"static/image/usage_process.28419803.png"},83753:function(e,t,n){n.r(t);var i=n(85893),o=n(50065),a=n(95895),r=n(56990),s=n(29649),d=n(38194);function l(e){let t=Object.assign({h1:"h1",a:"a",p:"p",strong:"strong"},(0,o.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.h1,{id:"toolchain-overview",children:[(0,i.jsx)(t.a,{className:"header-anchor","aria-hidden":"true",href:"#toolchain-overview",children:"#"}),"Toolchain Overview"]}),"\n",(0,i.jsx)(a.Z,{}),"\n",(0,i.jsxs)(t.p,{children:["Horizon J6 algorithm toolchain (hereinafter referred to as ",(0,i.jsx)(t.strong,{children:"the toolchain"}),") is a complete set of artificial intelligence edge algorithm solution,\r\nwhich can help you quantify floating-point models into fixed-point models and rapidly deploy self-developed algorithm models on Horizon computing platforms."]}),"\n",(0,i.jsx)(t.p,{children:"Currently, most of the models trained on GPUs are floating-point models, that is, the parameters are stored in the float data type."}),"\n",(0,i.jsx)(t.p,{children:"The computing platforms with Horizon's BPU architecture use the int8 calculation precision (common precision for computing platforms in the industry) and is capable of running fixed-point quantization models."}),"\n",(0,i.jsx)(t.p,{children:"The quantization is the process of converting a trained floating-point model to a fixed-point model."}),"\n",(0,i.jsx)(t.p,{children:"In addition, model quantization can effectively reduce the model size, accelerate the speed of deep-learning inference, therefore, it is also widely studied and applied in academia and industry."}),"\n",(0,i.jsx)(t.p,{children:"Depending on whether to adjust the parameters after quantization, we can classify the quantization methods into post-training quantization (PTQ) and quantization-aware training (QAT)."}),"\n",(0,i.jsx)(t.p,{children:"The difference in operation between these two methods is shown in the following diagram (Left: PTQ; Right: QAT)."}),"\n",(0,i.jsx)("img",{src:r,width:"860"}),"\n",(0,i.jsxs)(t.p,{children:["The PTQ uses a batch of calibration data to calibrate the trained models, which converts the trained FP32 model directly into a fixed-point computational model without any training of the original model.\r\nAs the quantization process is simple and fast, requiring a few adjustments to the hyperparameters and no training, this method has been widely used in a large number of end-side and cloud-side deployment scenarios.\r\nWe recommend that you to try the PTQ method first to see if it meets your requirements on the deployment accuracy and performance.\r\nFor more information about the PTQ scheme, please read ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ptq/ptq_workflow.html",children:"Post-training Quantization (PTQ)"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["The QAT is to quantize the trained model before training it again.\r\nSince the fixed-point values cannot be used for backward gradient calculation, the actual procedure is to insert fake quantization nodes in front of some operators to obtain the truncated values of the data flowing through the op during the training,\r\nso that they can be easily used when quantizing the nodes during the deployment of the quantization models.\r\nWe need to continuously optimize the accuracy during training to obtain the best quantization parameters.\r\nSince the model training is involved, it requires the developers to have higher levels of technical skills.\r\nFor more information about the QAT scheme, please read ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/plugin/introduce.html",children:"Quantized Awareness Training (QAT)"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"The toolchain consists of PTQ, QAT, embedded compilation, etc. The schematic diagram of the toolchain composition is as follows:"}),"\n",(0,i.jsx)("img",{src:s,width:"950"}),"\n",(0,i.jsx)(t.p,{children:"Runtime SDK provides runtime library support.\r\nThe runtime library contains two parts, ARM and x86, which are used to execute models on Horizon computing platform and x86 simulation platform respectively."}),"\n",(0,i.jsxs)(t.p,{children:["For more information about embedded application development, please read ",(0,i.jsx)(t.a,{href:"/3.0.22/en/guide/ucp/runtime/runtime_dev.html",children:"Embedded Application Development"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["In addition, the toolchain provides rich development ",(0,i.jsx)(t.strong,{children:"tools"}),", ",(0,i.jsx)(t.strong,{children:"samples"}),", and ",(0,i.jsx)(t.strong,{children:"model releases"})," with a large number of built-in algorithmic models to help you get started and to improve your development efficiency."]}),"\n",(0,i.jsx)(t.p,{children:"The overall workflow of using the toolchain is shown in the figure below.\r\nWe recommend that you to try the PTQ method first to see if it meets your requirements on the deployment accuracy and performance."}),"\n",(0,i.jsx)("img",{src:d,width:"1000"})]})}function h(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,o.ah)(),e.components);return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(l,e)})):l(e)}t.default=h,h.__RSPRESS_PAGE_META={},h.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fpreface%2Ftoolchain_overview.mdx"]={toc:[],title:"Toolchain Overview",frontmatter:{}}},95895:function(e,t,n){n(39710);var i=n(85893),o=n(67294),a=n(45687);n(20388);let r={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function s(e,t,n){let i=Object.keys(r).includes(t)?t:n;return r[i](e)}t.Z=e=>{let{defaultLocale:t="en-US"}=e,n=(0,a.Vi)().page.readingTimeData,r=(0,a.Jr)(),d=(0,a.e7)(),[l,h]=(0,o.useState)(s(n,r,t));return(0,o.useEffect)(()=>{h(s(n,r,t))},[r,n]),(0,i.jsx)("span",{"data-dark":String(d),className:"rp-reading-time",children:l})}}}]);