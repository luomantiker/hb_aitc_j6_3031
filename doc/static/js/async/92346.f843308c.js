"use strict";(self.webpackChunkrspress_doc_template=self.webpackChunkrspress_doc_template||[]).push([["92346"],{120:function(e,n,s){e.exports=s.p+"static/image/oe_package_sample.abbde74e.png"},69534:function(e,n,s){s.r(n);var l=s(85893),i=s(50065),r=s(95895),a=s(120);function o(e){let n=Object.assign({h1:"h1",a:"a",p:"p",pre:"pre",code:"code",span:"span",ul:"ul",li:"li",h2:"h2"},(0,i.ah)(),e.components);return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(n.h1,{id:"oe-development-kit-samples-introduction",children:[(0,l.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#oe-development-kit-samples-introduction",children:"#"}),"OE Development Kit Samples Introduction"]}),"\n",(0,l.jsx)(r.Z,{}),"\n",(0,l.jsx)(n.p,{children:"The algorithmic toolchain covers the key steps of model training (floating-point and quantization training, optional), conversion, performance/accuracy validation, deployment and inference.\nTo facilitate your quick experience and learning, rich and comprehensive samples are provided in the OE Development Kit.\nTo facilitate your understanding and use of these samples, this article will provide a detailed introduction to these samples."}),"\n",(0,l.jsx)(n.p,{children:"First of all, after you obtain the OE development package, the directory structure of the unzipped samples package is shown below:"}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-text",meta:"",children:[(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"samples # Samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 ai_toolchain "})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 horizon_model_convert_sample # Floating-point model to fixed-point model conversion sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 horizon_model_train_sample # Floating-point model training framework sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u2514\u2500\u2500 model_zoo # Model library to hold source and runtime models for toolchain sample model compilation"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 model_zoo -> ai_toolchain/model_zoo "})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u2514\u2500\u2500 ucp_tutorial # Unify Compute Platform UCP sample package, providing the necessary dependencies for UCP and related samples "})}),"\n",(0,l.jsx)(n.span,{className:"line",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"}})})]})})}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"samples"})," directory provides model training sample, floating-point to fixed-point model sample, and UCP related sample in the Unify Compute Platform.\nBelow is a distribution of general processes and samples used at each stage of the toolchain:"]}),"\n",(0,l.jsx)("img",{src:a,alt:"oe_package_sample",width:"1000"}),"\n",(0,l.jsx)(n.p,{children:"The samples correspond to different stages of use in the flow of the toolchain shown above:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"The model conversion sample contains the processes of model checking, calibration data processing, model quantization compilation, single image inference, and accuracy evaluation in the model PTQ conversion phase."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"The model training sample contains dataset packing scripts, model config files, training scripts, and other tool scripts for the floating-point model training (optional) and QAT quantization training (optional) phases,\nso that the QAT quantization training strategy can be attempted when the expected accuracy is not achieved by applying model accuracy optimization tools."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"UCP board-side deployment samples, including visual processing, model inference, high-performance operator library, custom operator and object detection full process sample.\nThe model inference sample provides compilation scripts, run scripts and source code for the model deployment stage."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h2,{id:"model-conversion-sample-horizon_model_convert_sample",children:[(0,l.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-conversion-sample-horizon_model_convert_sample",children:"#"}),"Model Conversion Sample (horizon_model_convert_sample)"]}),"\n",(0,l.jsxs)(n.p,{children:["The toolchain provides model conversion samples in the ",(0,l.jsx)(n.code,{children:"samples/ai_toolchain/horizon_model_convert_sample"})," folder, and the sample package directory structure is shown below:"]}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-text",meta:"",children:[(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 01_common          # Calibration data, data loading, preprocessing code"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 03_classification  # Classification model conversion sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 04_detection       # Detection model conversion Sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 07_segmentation    # Segmentation model conversion sample"})}),"\n",(0,l.jsx)(n.span,{className:"line",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"}})})]})})}),"\n",(0,l.jsx)(n.p,{children:"The OE package not only provides PTQ model conversion samples, but additionally contains one-click run scripts for model checking, calibration data preprocessing, conversion compilation, and inference."}),"\n",(0,l.jsxs)(n.p,{children:["Take 03_resnet50 in the ",(0,l.jsx)(n.code,{children:"horizon_model_convert_sample/03_classification"})," directory as an example of the relevant script:"]}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-text",meta:"",children:[(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 00_init.sh            # Get the calibration dataset and original model script corresponding to the sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 01_check.sh           # Model checking script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 02_preprocess.sh      # Calibration data preprocessing script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 03_build.sh           # Model quantization compilation script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 04_inference.sh       # Single image inference script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 05_evaluate.sh        # Accuracy evaluation script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 resnet50_config.yaml  # Model compilation yaml configuration file"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 postprocess.py        # Model post-processing code"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 preprocess.py         # Data preprocessing code"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u2514\u2500\u2500 README.cn.md"})}),"\n",(0,l.jsx)(n.span,{className:"line",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"}})})]})})}),"\n",(0,l.jsxs)(n.p,{children:["For a tutorial on how to use the PTQ model conversion sample, please refer to ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ptq/ptq_sample/general_description.html",children:"PTQ Model Conversion Samples"})," section."]}),"\n",(0,l.jsxs)(n.h2,{id:"model-training-sample-horizon_model_train_sample",children:[(0,l.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#model-training-sample-horizon_model_train_sample",children:"#"}),"Model Training Sample (horizon_model_train_sample)"]}),"\n",(0,l.jsxs)(n.p,{children:["The toolchain provides model training samples in the ",(0,l.jsx)(n.code,{children:"samples/ai_toolchain/horizon_model_train_sample"})," directory, and the sample package structure is shown below:"]}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-text",meta:"",children:[(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 plugin_basic        # The horizon_plugin_pytorch basic sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u2514\u2500\u2500 scripts               "})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u251C\u2500\u2500 configs         # The model config file"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 bev"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 classification"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 detection"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 disparity_pred"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 keypoint"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 lane_pred"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 lidar_bevfusion"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 lidar_multi_task"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 map"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 occ"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 opticalflow_pred"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 segmentation"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u251C\u2500\u2500 track_pred"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2502   \u2514\u2500\u2500 traj_pred"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u251C\u2500\u2500 examples        # Remove the config mechanism samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"    \u2514\u2500\u2500 tools           # Execute the script"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 calops.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 compare_dump_data.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 compile_perf_hbir.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 create_data.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 dataset_converters"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 datasets"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 export_hbir.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 export_onnx.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 gen_camera_param_nusc.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 homography_generator.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 infer_hbir.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 model_checker.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 predict.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 quant_analysis.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 reference_points_generator.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 gen_reference_points_nusc.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u251C\u2500\u2500 train.py"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"        \u2514\u2500\u2500 validation_hbir.py"})}),"\n",(0,l.jsx)(n.span,{className:"line",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"}})})]})})}),"\n",(0,l.jsx)(n.p,{children:"Model training is usually achieved by simply using the following commands:"}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-bash",meta:"",children:[(0,l.jsxs)(n.span,{className:"line line-number",children:[(0,l.jsx)(n.span,{style:{color:"var(--shiki-token-function)"},children:"python3"}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"tools/"}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"${TOOLS} "}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"--config"}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:" "}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-token-string)"},children:"configs/"}),(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"${CONFIGS}"})]}),"\n"]})})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["$","{CONFIGS}"," needs to be configured as the path to the config file corresponding to model training in the configs folder, which defines the model structure, dataset loading, and the whole set of training procedures,\nand the sample provides models including classification, detection, segmentation, and optical flow estimation tasks."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"The tools folder provides execution scripts containing dataset processing, model training, conversion compilation, computation statistics, etc.\nThe functions of the relevant scripts are as follows."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"calops.py: Network modeling computational volume statistics tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"compile_perf_hbir.py: Compilation and perf tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"create_data.py: Dataset preprocessing tool for preprocessing Kitti3D radar dataset."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"dataset_converters: Conversion script for different dataset formats are provided under the folder."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"datasets: Dataset package and data visualization script are provided under the folder."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"export_hbir.py: HBIR model export tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"export_onnx.py: ONNX model export tool, exported ONNX model can only be used for visualization, does not support inference."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"gen_camera_param_nusc.py: Script to get camera internal and external parameters from nuscenes."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"homography_generator.py: Script to compute the ego2img matrix."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"reference_points_generator.py: Script to calculate model input reference points from the homography matrix."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"gen_reference_points_nusc.py: Script to get model input reference points from nuscenes."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"infer_hbir.py: Single image prediction tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"model_checker.py: Model checking tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"predict.py: Prediction tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"quant_analysis.py: Accuracy debug tool."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"train.py: Model training script, support floating-point model training, quantization training function."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"validation_hbir.py: Accuracy verification tool to provide fully aligned results on the upper board."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For a tutorial on using the model training samples, please refer to the introduction in the ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/advanced_content/hat/introduction.html",children:"Horizon Torch Samples"})," section."]}),"\n",(0,l.jsxs)(n.h2,{id:"unify-compute-platform-sample-ucp_tutorial",children:[(0,l.jsx)(n.a,{className:"header-anchor","aria-hidden":"true",href:"#unify-compute-platform-sample-ucp_tutorial",children:"#"}),"Unify Compute Platform Sample (ucp_tutorial)"]}),"\n",(0,l.jsxs)(n.p,{children:["The toolchain provides sample source code and running scripts for the Unify Compute Platform UCP in the ",(0,l.jsx)(n.code,{children:"samples/ai_toolchain/ucp_tutorial"})," directory, and the sample package structure is shown below:"]}),"\n",(0,l.jsx)(n.pre,{className:"code",children:(0,l.jsx)(n.pre,{className:"shiki css-variables has-line-number",style:{backgroundColor:"var(--shiki-color-background)"},tabIndex:"0",children:(0,l.jsxs)(n.code,{className:"language-text",meta:"",children:[(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"ucp_tutorial"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 all-round                     # Object detection full process sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 custom_operator               # Custom operator sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 dsp_sample                # DSP sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u2514\u2500\u2500 gpu_sample                # Development sample for calling board-side GPU based on OpenCL interface"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 deps_aarch64                  # AArch64 public dependency directory  "})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 deps_x86                      # X86 emulation public dependency directory  "})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 dnn                           # DNN samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 ai_benchmark              # AI Benchmark sample"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u2514\u2500\u2500 basic_samples             # Model inference basic samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 hpl                           # HPL samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 code                      # Source code directory"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u2514\u2500\u2500 hpl_samples               # HPL sample minimum executable environment"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u251C\u2500\u2500 vp                            # VP samples"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u251C\u2500\u2500 code                      # Source code directory"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"|   \u2514\u2500\u2500 vp_samples                # VP sample minimum executable environment"})}),"\n",(0,l.jsx)(n.span,{className:"line line-number",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"},children:"\u2514\u2500\u2500 tools                         # Tools"})}),"\n",(0,l.jsx)(n.span,{className:"line",children:(0,l.jsx)(n.span,{style:{color:"var(--shiki-color-text)"}})})]})})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["all-round: Object detection full process sample, for detailed introduction and usage tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/full_process_sample.html",children:"Object Detection Full Process Sample"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["custom_operator: Custom operator sample, including the DSP sample and development sample for calling board-side GPU based on OpenCL interface, for detailed introduction and usage tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/plugin/introduction.html",children:"UCP Custom Operator"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"deps_aarch64: AArch64 public dependency directory, containing UCP dependency library and header file, etc."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"deps_x86: X86 emulation public dependency directory"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"dnn: DNN samples, containing as below:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["ai_benchmark provides performance and accuracy evaluation samples of common models for embedded application development.\nFor detailed introduction and usage tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/runtime/ai_benchmark.html",children:"AI Benchmark User Guide"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["basic_samples provides model inference related to the use of shallow and deep samples, designed to help you familiarize and learn the model inference related to the interface and a variety of advanced features, detailed introduction and use of the tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/runtime/basic_sample.html",children:"Basic Sample User Guide"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["hpl: HPL samples including sample source code and minimal executable environment, detailed introduction and use of the tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/hpl/hpl6_sample.html",children:"High Performance Library - Sample"})]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["vp: VP samples including sample source code and minimal executable environment, detailed introduction and use of the tutorial, please refer to the introduction in the section ",(0,l.jsx)(n.a,{href:"/3.0.22/en/guide/ucp/vp/vp6_sample/vp6_Overview.html",children:"Visual Process - Sample"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"tools: The Unify Compute Platform UCP provides tools."}),"\n"]}),"\n"]})]})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:n}=Object.assign({},(0,i.ah)(),e.components);return n?(0,l.jsx)(n,Object.assign({},e,{children:(0,l.jsx)(o,e)})):o(e)}n.default=c,c.__RSPRESS_PAGE_META={},c.__RSPRESS_PAGE_META["3.0.22%2Fen%2Fguide%2Fappendix%2Fcommunity_articles%2Foe_package_sample.mdx"]={toc:[{id:"model-conversion-sample-horizon_model_convert_sample",text:"Model Conversion Sample (horizon_model_convert_sample)",depth:2},{id:"model-training-sample-horizon_model_train_sample",text:"Model Training Sample (horizon_model_train_sample)",depth:2},{id:"unify-compute-platform-sample-ucp_tutorial",text:"Unify Compute Platform Sample (ucp_tutorial)",depth:2}],title:"OE Development Kit Samples Introduction",frontmatter:{}}},95895:function(e,n,s){s(39710);var l=s(85893),i=s(67294),r=s(45687);s(20388);let a={"zh-CN":e=>`\u{9884}\u{8BA1}\u{9605}\u{8BFB}\u{65F6}\u{95F4}: ${e.minutes>=1?`${Math.ceil(e.minutes)} \u{5206}\u{949F}`:"\u5C0F\u4E8E 1 \u5206\u949F"}`,"en-US":e=>`Estimated reading time: ${e.minutes>=1?`${Math.ceil(e.minutes)} minutes`:"less than 1 minute"}`};function o(e,n,s){let l=Object.keys(a).includes(n)?n:s;return a[l](e)}n.Z=e=>{let{defaultLocale:n="en-US"}=e,s=(0,r.Vi)().page.readingTimeData,a=(0,r.Jr)(),c=(0,r.e7)(),[t,d]=(0,i.useState)(o(s,a,n));return(0,i.useEffect)(()=>{d(o(s,a,n))},[a,s]),(0,l.jsx)("span",{"data-dark":String(c),className:"rp-reading-time",children:t})}}}]);