[{"id":325,"title":"量化模型训练","content":"Bev多任务模型训练#\n\nBEV参考算法基于Horizon Torch Samples（地平线自研深度学习框架）开发，关于Horizon Torch\nSamples的使用介绍可以参考Horizon Torch Samples使用文档。BEV参考算法的训练config位于HAT/configs/bev/路径下。\n下文以HAT/configs/bev/bev_ipm_efficientnetb0_multitask_nuscenes.py为例介绍如何配置并训练BEV参考算\n法。\n\n\n训练流程#\n\n如果你只是想简单的把 Bev 的模型训练起来，那么可以首先阅读一下这一章的内容。和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n这里以nuscense数据集为例，可以从 https://www.nuscenes.org/nuscenes 下载数据集\n。同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb和val_lmdb就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。metas中为分割模型需要的地图信息。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的bev多任务网络了。\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证也可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。我们在HAT中提供了模型检查的接口，可以在定义好量化模型\n之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\n\n\n其中，model 下面的 type\n表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。例如，如果我们想训练一个backbone为res\nnet50的模型，只需要将 model 下面的 backbone 替换掉就可以。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口torch.utils.data.DataLoader，表示的是将 batch_size 大小的图片组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量， CocoFromLMDB\n表示从lmdb数据集中读取图片，路径也就是我们在第一部分数据集准备中提到的路径。transforms 下面包含着一系列的数据增强。\nval_data_loader 中除了图片翻转(RandomFlip), 其他的数据变换和 data_loader 一致。你也可以通过在 transforms\n中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(WarmupStepLrUpdater)，在训练过程中验证模型的指标(Valid\nation)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。当然训练一个好的检测模\n型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以得到伪量化模型了，该模型仅使用calibrat\nion即可达到目标：\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的calibration_trainer\n\n\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置quantize=True，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。","routePath":"/guide/advanced_content/hat/examples/bev","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":241},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":377},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":652},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":797},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":843},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1081},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1138},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1197},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1424},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1481},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1632},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2051},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":3,"charIndex":2778},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2929}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":326,"title":"汽车关键点检测模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在汽车关键点数据集 CarFusion 上从头开始训练一个关键点检测模型，包括浮点，量化和定点模型。\n\nCarFusion\n是一个汽车关键点数据集，数据集中标注了汽车的12个关键点，包含汽车前后左右的车轮轮心，车灯位置以及车顶角点。开始模型训练之前，第一步是准备好数据集，这里我们需要\n先在官网申请数据集，官网链接为：CarFusion，下拉可以找到数据集申请表单，提交表单后可以收到数据集下载链接邮件。下载解压缩后数据目录结构如下所示：\n\n\n\n这里我们只考虑更简单的情形，即从检测好的汽车图片中检测关键点。因此，我们需要首先根据数据标注框从图片中裁剪出汽车。运行一下命令即可一键将数据转换为裁剪好的格式。\n\n\n\n得到的文件夹组织样式是这样的：\n\n\n\n其中 keypoints_test.json 和 keypoints_train.json 分别包含测试集和训练集的标注。\n每个样本以jsonge字典格式存储，样式为： \"img_path\":\nkeypoints_list，其中keypoints_list形状为(12,3)，每行前两个元素为关键点坐标(x,y)，第三个元素为关键点是否有效的标注，当关键\n点不在图内或无效时，第三个元素为0。\n\n接着，可以将数据打包为 LMDB 格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n打包完成之后，目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\nconfigs/keypoint/keypoint_efficientnetb0_carfusion.py 包含了本教程中模型训练相关的所有设置。\n\n在网络开始训练之前，你可以使用以下命令先计算一下网络的计算量和参数数量：\n\n\n\n下一步就可以开始训练。训练可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n由于HAT算法包使用了注册机制，每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。 train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是 \"float\" 、\n\"calibration\"，分别可以完成浮点模型和量化模型的校准，其中量化模型的训练依赖于上一步浮点训练产出的浮点模型。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化、定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、 QAT\n的指标，分别为浮点、量化的指标。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 和 \"calibration\"时， 分别可以完成对训练好的浮点模型、量化模型的验证。\n\n对于定点模型精度验证，也可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT提供了 infer_hbir.py 脚本对各阶段训练好的模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。我们在HAT中提供了模型检查的接口，可以在定义好量化模型\n之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\n针对轻量的汽车关键点检测任务，网络模型 HeatmapKeypointModel\n使用efficienet-b0作为backbone，加上三层转置卷积层生成热力图，然后从热力图解码关键点坐标。我们通过在 config 配置文件中定义\nmodel 这样的一个 dict 型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n模型包含 backbone ，转置卷积构成的 decode_head ， losses 和 post_process\n模块，在HeatmapKeypointModel中, 输入为裁剪好的汽车图片，backbone 用于提取图像特征， decoder 上采样输出 heatmap\n， losses 使用根据heatmap位置加权的 MSELoss 来作为训练的 loss ，post_process 使用 HeatmapDecoder\n将heatmap输出转换为关键点位置预测值。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 data_loader 和 val_data_loader 这两个 dict\n来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例，数据增强使用了 RandomFlip 、 Resize 、\nRandomPadLdmkData 和 GaussianNoise。对于关键点检测任务，还需要使用 GenerateHeatmapTarget\n从关键点标注生成热力图(Heatmap)目标。\n\n\n\n因为最终跑在 BPU 上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式， 所以HAT在 ToTensor\n数据transform中提供了 to_yuv=True 的选项，将 RGB 格式图片转换为 YUV444 的格式。 HAT还提供了\nbatch_processor 接口对数据进行批处理，但此处没有添加额外的增强处理。其中 loss_collector 是一个获取当前批量数据的 loss\n的函数，由于模型返回值为 (pred, loss) 的 Tuple，因此取模型输出tuple的index=1的值，即为 loss 。\n\n\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n训练策略#\n\nconfigs/keypoint/keypoint_efficientnetb0_carfusion.py 文件中的 float_trainer ，\ncalibration_trainer 分别对应浮点与量化模型的训练策略。下面以 float_trainer 训练策略示例：\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。其中 model_convert_pipeline 定义了模型开始训练前的转换操作，例如此处模型首先载入 Efficientnet-b0 在\nImageNet 上的预训练模型。 optimizer 此处使用的是 AdamW 优化器，配合 lr=0.001 的学习率和 weight_decay=0.05\n。实验表明较大的weight_decay值能够帮助训练量化性能更好的浮点模型。 callbacks\n定义了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式\nCosLrUpdater，在训练过程中验证模型的指标(Validation)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中\n实现的操作，也可以按照这种dict的方式添加。 train_metrics 和 val_metrics\n分别定义了模型训练和验证时监测的指标(Metric)。train_metrics 中的 LossShow 可以监测训练损失。 val_metrics 中的\nPCKMetric 计算了在一定距离阈值内与相应真实关键点重合的关键点的百分比；MeanKeypointDist 计算了关键点预测点与真实关键点的平均距离误差。\n\n总之 float_trainer 负责将模型整个浮点训练的逻辑给串联起来。\n\n\n量化模型校准#\n\n当我们有了纯浮点模型后，我们首先通过校准（Calibration）操作使用一些数据样本计算模型量化各层的缩放（scale）参数，从而对模型进行int8量化。相关\n的配置为：\n\n\n\n\n模型检查编译和仿真上板精度验证#\n\n对于HAT来说，量化模型的意义在于可以在BPU上直接运行。因此，对于量化模型的检查和编译是必须的。 前文提到的 compile_perf_hbir\n脚本也可以在定义好量化模型之后，先检查能否在BPU上正常运行， 并可通过 validation_hbir 脚本获取模型上板精度。用法同前文。","routePath":"/guide/advanced_content/hat/examples/car_keypoint","lang":"zh","toc":[{"text":"模型训练","id":"模型训练","depth":2,"charIndex":676},{"text":"导出定点模型","id":"导出定点模型","depth":2,"charIndex":1101},{"text":"模型验证","id":"模型验证","depth":2,"charIndex":1147},{"text":"模型推理","id":"模型推理","depth":2,"charIndex":1367},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":2,"charIndex":1425},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":2,"charIndex":1484},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1711},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1768},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":2177},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2744},{"text":"量化模型校准","id":"量化模型校准","depth":3,"charIndex":3565},{"text":"模型检查编译和仿真上板精度验证","id":"模型检查编译和仿真上板精度验证","depth":3,"charIndex":3665}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":327,"title":"量化模型训练","content":"CenterPoint检测模型训练#\n\n这篇教程主要是告诉大家如何利用HAT在数据集 nuscenes 上从头开始训练一个 CenterPoint 模型，包括浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，可以在 nuscenes数据集 下载 1.0 版本的完整数据文件。\n\n下载后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，tmp_data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集，meta中为评测脚本需要的初始化信息，具体信息是从\nnuscenes 原始数据集中拷贝得来。\n\n同时，为了训练 nuscenes 点云数据，还需要为 nuscenes 数据集生成每个单独的训练目标的点云数据， 并将其存储在\ntmp_nuscenes/lidar/nuscenes_gt_database 的 .bin 格式的文件中，文件保存目录可根据需要更改。\n同时，需要为这部分数据生成 .pkl 格式的包含数据信息的文件。\n此外，训练数据集初始化过程中需要读取全部数据集中每个sample的类别信息，并进行重采样操作，我们可以提前生成对应的信息并保存成 .pkl\n格式的文件，可以加速训练过程。 通过运行下面的命令来创建上述数据：\n\n\n\n执行上述命令后，生成的文件目录如下：\n\n\n\n其中，nuscenes_gt_database 和 nuscenes_dbinfos_train.pkl 是训练是用于采样的样本，而\nnuscenes_dbinfos_train.pkl 则包含训练数据集初始化需要的信息。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的CenterPoint网络了。在网络训练开始之前，你可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n由于HAT算法包使用了一种巧妙的注册机制，使得每一个训练任务都可以按照这种train.py加上config配置文件的形式启动。\ntrain.py是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的config配置文件里面。\nconfig文件里面提供了模型构建、数据读取等关键的dict。\n\n\n模型构建#\n\nCenterPoint 的网络结构可以参考 论文 ，这里不做详细介绍。 我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中， model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\n训练脚本在启动之后，会调用 build_model 接口，将这样一个dict类型的model变成类型为 torch.nn.Module 类型的model。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口 torch.utils.data.DataLoader，表示的是将 batch_size 大小的样本组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量， data_path 路径也就是我们在第一部分数据集准备中提到的路径。 transforms\n下面包含着一系列的数据增强。 val_data_loader 中只有读取点云文件、Format 和 Collect3D。 你也可以通过在 transforms\n中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CyclicLrUpdater)，在训练过程中验证模型的指标(Validatio\nn)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\nfloat_trainer 负责将整个训练的逻辑给串联起来，其中也会负责模型的pretrain。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了。不过这里需要说明的是，Cent\nerPoint在量化训练过程中建议加上calibration的流程。calibration可以为QAT的量化训练提供一个更好的初始化参数。\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的qat_trainer。\n\n\n\n\nmodel_convert_pipeline参数值不同#\n\n当我们训练量化模型的时候，需要设置model_convert_pipeline，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float和qat两阶段的训练过程，相应的我们可以验证这两个阶段训练出来的模型性能，只需要相应\n的运行以下两条命令即可：\n\n\n\n同时，我们还提供了quantization模型的性能测试，只需要运行以下命令，但需要注意是必须要先导出hbir：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n模型推理和结果可视化#\n\n如果你希望可以看到训练出来的模型对于雷达点云的检测效果，我们的tools文件夹下面同样提供了点云预测及可视化的脚本，你只需要运行以下脚本即可：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile_perf_hbir 的工具用来将量化模型编译成可以上板运行的 hbm\n文件，同时该工具也能预估在BPU上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/centerpoint","lang":"zh","toc":[{"text":"模型构建","id":"模型构建","depth":2,"charIndex":1116},{"text":"数据增强","id":"数据增强","depth":2,"charIndex":1375},{"text":"训练策略","id":"训练策略","depth":2,"charIndex":1762},{"text":"model_convert_pipeline参数值不同","id":"model_convert_pipeline参数值不同","depth":2,"charIndex":2587},{"text":"训练策略不同","id":"训练策略不同","depth":2,"charIndex":2760},{"text":"导出定点模型","id":"导出定点模型","depth":2,"charIndex":2938},{"text":"模型验证","id":"模型验证","depth":2,"charIndex":2984},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":2,"charIndex":3205},{"text":"模型推理和结果可视化","id":"模型推理和结果可视化","depth":2,"charIndex":3264},{"text":"模型检查和编译","id":"模型检查和编译","depth":2,"charIndex":3353}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":328,"title":"MobileNetV1分类模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在 ImageNet 上训练一个state-of-art的浮点和定点模型。ImageNet\n是图像分类里用的最多的数据集，很多最先进的图像分类研究都会优先基于这个数据集做好验证。\n虽然有很多方法在社区或者其他途径里可以获取到state-of-art的分类模型，但从头训一个state-of-art的分类模型仍然不是一个简单的任务。\n本篇教程将会重点讲叙从数据集准备开始如何在 ImageNet 上训练出一个state-of-art的模型，包括浮点、量化和定点三种模式。\n\n其中 ImageNet 数据集可以从 ImageNet官网 进行下载，下载之后的数据集格式为：\n\n\n\n这里我们使用 MobileNetV1 的例子来详细介绍整个分类的流程。\n\n\n训练流程#\n\n如果你只是想简单的使用 HAT 的接口来进行简单的实验，那么首先阅读一下这一小节的内容是个不错的选择。 对于所有的训练和评测的任务， HAT 统一采用\ntools + config 的形式来完成。 在准备好原始数据集之后，通过以下的流程，我们可以方便的完成整个训练的流程。\n\n\n数据集准备#\n\n首先是数据集打包，打包数据集与原始数据集在处理速度上有明显的优势，这里我们选择与 PyTorch 一脉相承的 LMDB 的打包方法， 当然由于HAT在处理\ndataset 上的灵活性，其他形式的数据集打包和读取形式，如 MXRecord 也是可以独立支持的。\n\n在 tools/datasets 目录下提供了 cityscapes 、 imagenet 、 voc 、mscoco 这些常见数据集的打包脚本。 例如\nimagenet_packer 的脚本，可以利用 torchvision 提供的默认公开数据集处理方法直接将原始的公开 ImageNet 数据集转成 Numpy\n或者 Tensor 的格式， 最后将得到的数据统一用 msgpack 的方法压缩到 LMDB 的文件中。\n\n这个过程可以很方便通过下面的脚本完成数据集打包:\n\n\n\n在完成数据集打包之后，可以得到含有 ImageNet 的 LMDB 数据集。下一步就可以开始训练。\n\n\n模型训练#\n\n准备打包好数据集之后，便可以开始训练模型。只需要运行下面的命令就可以启动训练：\n\n\n\n由于HAT算法包使用了注册机制，使得每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是 \"float\" 、 \"calibration\" ，分别可以完成浮点模型、量化模型的训练。\n其中量化模型的训练依赖于上一步浮点训练产出的浮点模型，具体内容见上一节量化的细节。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、\nCalibration 和 Quantized 的指标，分别为浮点、量化和完全定点的指标，详细说明见量化细节。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 、 \"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT提供了 infer_hbir.py 脚本对各阶段训练好的模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型编译#\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n针对地平线不同架构的BPU，可以在 configs/classification/mobilenetv1_imagenet.py 中设置 march =\nMarch.NASH_E, march = March.NASH_M 。\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们仍使用 MobileNetV1 作为示例，对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n训练条件#\n\n在超过100w张图片的 ImageNet 上训练一个深度学习模型是非常消耗资源的，主要的瓶颈在于矩阵计算和数据读取。\n\n对于矩阵计算，这里非常建议使用一个高性能的GPU来替代CPU作为训练工具，另外同时使用多个GPU可以有效的降低训练时间。\n\n对于数据读取，推荐使用更好的CPU和SSD存储。多线程的CPU加速和更好的SSD存储对于数据读取有很大帮助。 需要注意的是，整个 ImageNet\n大概会占用300G的存储资源，所以SSD存储至少得有300G的存储空间。\n\n\n网络结构#\n\nHAT 或者其他一些社区都可以找到 MobileNetV1 丰富的实现方法，因此，MobileNetV1 的具体实现在这里并不赘述。\n\n在 HAT 的 config 中，我们可以直接用下面的 dict 就可以构建一个浮点的 MobileNetV1 分类模型。可以直接修改 backbone\n中的配置参数来达到修改模型目的。\n\n\n\n模型除了 backbone 之外，还有losses模块，在常见的分类模型中，我们一般直接采用 Cross-Entropy\n作为训练的loss，但是越来越多的实验证明，在分类的loss中加上 Label-Smooth 对训练的结果有帮助，尤其是在结合 Cosine 的lr更新方法上。\n\n通常在定义好一个模型之后，尤其是一些公版模型，我们会有检查计算量的需求。HAT通过 calops 的工具来计算模型的计算量，执行方法如下：\n\n\n\n这种计算量的统计工具，是可以同时支持浮点和定点模型的。\n\n\n数据增强#\n\n关于ImageNet训练的数据增强已经逐渐形成一种共识，我们以 torchvision 提供的数据增强作为基础，构建分类训练的数据增强， 包括\nRandomResizedCrop ， RandomHorizontalFlip， ColorJitter 。\n\n因为最终跑在BPU上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式， 所以HAT提供 BgrToYuv444\n的数据增强来将 RGB 转到 YUV444 的格式。 为了优化训练过程，HAT 使用了 batch_processor，可将一些增强处理放在\nbatch_processor 中优化训练：\n\n\n\n对应的 batch_processor 部分：\n\n\n\n验证集的数据转换相对简单很多，最主要的区别是做短边 Resize 到256，和 CenterCrop 。其他的颜色空间转换和训练集是一样的。\n\n\n\n\n训练策略#\n\n在 ImageNet 上训练不同分类模型的训练策略大体上一致，但也有微小的区别。这里我们主要介绍有效果提升的细节。\n\nCosine 的学习策略配合 Warmup 与普通的 StepLr 有一些提升效果。适当延长epoch的训练长度对于小模型也有提升。\n\n另外，只对 weight 的参数施加 L2 norm\n也是推荐的训练策略。configs/classification/mobilenetv1_imagenet.py 文件中的 float_trainer，\ncalibration_trainer， int_trainer 分别对应浮点、量化、定点模型的训练策略。下面以 float_trainer 训练策略为例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n这里主要讲一下HAT的分类中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，HAT在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的策略并不统一，这里简单描述分类模型训练中的常见策略。\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。因为有浮点预训练模型，所以量化训练的学习率 Lr 可以很小，\n一般可以从0.001或0.0001开始，并可以搭配 StepLrUpdater 做1-2次 scale=0.1 的 Lr 调整；同时训练的长度不用很长。 此外\nweight decay 也会对训练结果有一定影响。\n\n\n预训练模型#\n\nHAT 已经提供了丰富的在 ImageNet 上的预训练模型，可以参照 modelzoo 的内容，所有模型都在发布包中。","routePath":"/guide/advanced_content/hat/examples/classification","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":345},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":491},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":922},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":1226},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1272},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1523},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1581},{"text":"定点模型编译","id":"定点模型编译","depth":3,"charIndex":1640},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1874},{"text":"训练条件","id":"训练条件","depth":3,"charIndex":1952},{"text":"网络结构","id":"网络结构","depth":3,"charIndex":2194},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":2612},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":3021},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":3342},{"text":"预训练模型","id":"预训练模型","depth":2,"charIndex":3724}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":329,"title":"Deformable DETR检测模型训练","content":"#\n\n这篇教程以Deform-detr为例，告诉大家如何使用HAT算法包训练一个定点的检测模型。\n在开始量化感知训练，也就是定点模型训练之前，首先需要训练一个精度较高的纯浮点模型，然后基于这个纯浮点模型做finetune，就可以快速的训练出定点模型。\n所以我们从训练一个纯浮点的Deform-detr模型开始讲起。\n\n\n训练流程#\n\n和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools + config\n的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，这里我们下载MSCOCO的 train2017.zip 和 val2017.zip\n做为网络的训练集和验证集，同时需要下载相应的标签数据 annotations_trainval2017.zip，解压缩之后数据目录结构如下所示：\n\n\n\n同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的Deformable Detr检测网络了。在网络训练开始之前，你可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。 和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、\nCalibration 、 Qat和 Quantized 的指标：\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\"、\"qat\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n\n模型结构#\n\nDeformable DETR的网络结构可以参考 论文 ，这里不做详细介绍。我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中， model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\n例如，如果我们想训练一个backbone为efficientnet的模型，只需要将 model 下面的 backbone 替换掉，并相应设置 neck 就行。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口torch.utils.data.DataLoader，表示的是将 batch_size 大小的图片组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量， CocoFromLMDB\n表示从lmdb数据集中读取图片，路径也就是我们在第一部分数据集准备中提到的路径。 transforms 下面包含着一系列的数据增强。\nval_data_loader 则没有施加数据增强。 你也可以通过在 transforms 中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 其中优化器是由 custom_param_optimizer 包裹的 AdamW 优化器，可以对模型中的backbone和norm\nlayers进行更精细的优化参数控制。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式（StepDecayLrUpdater），在训练过程中验证模型的指标（Valida\ntion），以及保存（Checkpoint）模型的操作。 当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了。\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。 Deformable\nDETR在量化训练过程中建议加上calibration的流程。calibration可以为QAT的量化训练提供一个更好的初始化参数。\n此时我们使用的训练策略来自于config文件中的calibration_trainer和qat_trainer。\n\n\n\n我们首先通过calibration为QAT获取一个好的初始化参数，calibration加载训练好的浮点模型，然后采用 mse 的校准方式搜索量化参数， 其中\nFloat2Calibration 将模型由浮点模型转为校准模型。\n\n\n\n接着，我们在calibration模型的基础上进行QAT量化训练。初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少。 注意到\nconverter 中的 Float2QAT 将模型由浮点模型转换为QAT模型，再加载的calibration模型权重。 经过量化训练，Deformable\nDETR量化模型的精度可以达到浮点模型精度的99%以上。","routePath":"/guide/advanced_content/hat/examples/deform_detr","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":159},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":255},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":593},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":787},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":833},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1066},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1123},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1182},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1409},{"text":"模型结构","id":"模型结构","depth":3,"charIndex":1417},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1680},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2066},{"text":"量化模型训练","id":"量化模型训练","depth":2,"charIndex":2696}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":330,"title":"DenseTNT轨迹预测模型训练","content":"#\n\n这篇教程主要是告诉大家如何在数据集 Argoverse 1.1 上从头开始训练一个 DenseTNT 模型，包括浮点、量化和定点模型。\n\n\n训练流程#\n\n如果你只是想简单的把 Bev 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，可以在 Argoverse 1 数据集 下载。 需要下载：Training 和 Validation。\n同时需要准备 HD Map数据。\n\n下载后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集，打包完成之后，${target-data-dir} 目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 、 \"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nDenseTNT 的网络结构可以参考 论文 ，这里不做详细介绍。 我们通过在 config 配置文件中定义 model 这样的一个 dict\n型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n\n数据加载#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 data_loader 和 val_data_loader 这两个 dict\n来实现的，分别对应着训练集和验证集的处理流程。\n\n\n\nbatch_processor 中传入一个 loss_collector 函数，用于获取当前批量数据的 loss ，如下所示：\n\n\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n\n\n训练策略#\n\n在 SceneFlow 数据集上训练浮点模型使用 Cosine 的学习策略配合 Warmup， 以及对 weight 的参数施加 L2 norm。\nconfigs/traj_pred/densetnt_vectornet_argoverse1.py 文件中的 float_trainer,\nqat_trainer, int_trainer 分别对应浮点、量化、定点模型的训练策略。 下面为 float_trainer 训练策略示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练\n章节的内容。这里主要讲一下中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。 因为有浮点预训练模型，所以量化训练的学习率 Lr 可以很小， 一般可以从\n0.001 或 0.0001 开始，并可以搭配 StepLrUpdater 做 1-2 次 scale=0.1 的 Lr 调整； 同时训练的长度不用很长。此外\nweight decay 也会对训练结果有一定影响。\n\nDenseTNT 示例模型的量化训练策略可见 configs/traj_pred/densetnt_vectornet_argoverse1.py 文件。","routePath":"/guide/advanced_content/hat/examples/densetnt","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":72},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":209},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":529},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":672},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":718},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":958},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1015},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1074},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1301},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1358},{"text":"数据加载","id":"数据加载","depth":3,"charIndex":1463},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1671},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":1899}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":331,"title":"Docker 镜像说明","content":"#\n\n\n如何启动#\n\n为方便使用HAT算法包，我们提供了一键启动docker环境的命令，以简化使用过程中的环境搭建步骤。在获取发布包后先进入发布包所在的路径，然后根据\nDocker容器部署\n一节的内容完成基础Docker环境安装、添加Docker组用户、拉取Docker镜像之后，只需使用以下命令，就可以获取成功运行HAT示例的所有环境。\n\n\n\n注解\n\n此处的版本号 仅为示例，请将其替换为您实际获取到的镜像版本号。\n\n其中 openexplorer/ai_toolchain_ubuntu_22_j6_gpu:{version} 是镜像名，-v\n用于将本地的路径挂载到docker路径下。\n\n\n获取执行脚本#\n\n成功启动docker环境之后，从发布包中获取 release_packages 的压缩包，解压之后会看到 configs 和 tools 两个文件夹目录。\n所有环境都拥有之后，你就可以按照后面的训练教程来一步步的使用HAT训练出一个定点模型。","routePath":"/guide/advanced_content/hat/examples/docker","lang":"zh","toc":[{"text":"如何启动","id":"如何启动","depth":2,"charIndex":3},{"text":"获取执行脚本","id":"获取执行脚本","depth":2,"charIndex":297}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":332,"title":"FCOS检测模型训练","content":"#\n\n这篇教程以FCOS-efficientnet为例，告诉大家如何使用HAT算法包训练一个定点的检测模型。\n在开始量化感知训练，也就是定点模型训练之前，首先需要训练一个精度较高的纯浮点模型，然后基于这个纯浮点模型做finetune，就可以快速的训练出定点模型。\n所以我们从训练一个纯浮点的FCOS-efficientnet模型开始讲起。\n\n\n训练流程#\n\n如果你只是想简单的把 Bev 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，这里我们下载MSCOCO的 train2017.zip 和 val2017.zip\n做为网络的训练集和验证集，同时需要下载相应的标签数据 annotations_trainval2017.zip，解压缩之后数据目录结构如下所示：\n\n\n\n同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的FCOS-efficientnet检测网络了。在网络训练开始之前，你可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 、 \"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n\n模型结构#\n\nfcos的网络结构可以参考 论文， 这里不做详细介绍。我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中， model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。\n这样定义模型的好处在于我们可以很方便的替换我们想要的结构。例如，如果我们想训练一个backbone为resnet50的模型，只需要将 model 下面的\nbackbone 替换掉就可以。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口torch.utils.data.DataLoader，表示的是将 batch_size 大小的图片组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量， CocoFromLMDB\n表示从lmdb数据集中读取图片，路径也就是我们在第一部分数据集准备中提到的路径。 transforms 下面包含着一系列的数据增强。\nval_data_loader 中除了图片翻转（RandomFlip），其他的数据变换和 data_loader 一致。 你也可以通过在 transforms\n中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式（WarmupStepLrUpdater），在训练过程中验证模型的指标（Valid\nation），以及保存（Checkpoint）模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\nfloat_trainer 负责将整个训练的逻辑给串联起来，其中也会负责模型的pretrain。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了。\n不过这里需要说明的是，FCOS在量化训练过程中建议加上calibration的流程。calibration可以为QAT的量化训练提供一个更好的初始化参数。\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的qat_trainer。\n\n\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置quantize=True，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。","routePath":"/guide/advanced_content/hat/examples/fcos","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":171},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":308},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":646},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":842},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":888},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1128},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1185},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1244},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1471},{"text":"模型结构","id":"模型结构","depth":3,"charIndex":1479},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1716},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2136},{"text":"量化模型训练","id":"量化模型训练","depth":2,"charIndex":2729},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":2,"charIndex":2958},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":3109}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":333,"title":"量化模型训练","content":"FCOS3D检测模型训练#\n\n这篇教程以FCOS3D-efficientnetb0为例，告诉大家如何使用HAT算法包训练一个定点的3D检测模型。\n在开始量化感知训练，也就是定点模型训练之前，首先需要训练一个精度较高的纯浮点模型，然后基于这个纯浮点模型做finetune，就可以快速的训练出定点模型。\n所以我们从训练一个纯浮点的FCOS3D-efficientnetb0模型开始讲起。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集。这里我们训练 FCOS3D 模型使用的是开源的 nuscenes 数据集。解压缩之后数据目录结构如下所示：\n\n\n\n同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb和val_lmdb就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的FCOS3D-efficientnetb0检测网络了。\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n由于HAT算法包使用了一种巧妙的注册机制，使得每一个训练任务都可以按照这种train.py加上config配置文件的形式启动。\ntrain.py是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的config配置文件里面。\nconfig文件里面提供了模型构建、数据读取等关键的dict。\n\n\n模型构建#\n\nFCOS3D的网络结构可以参考 论文， 这里不做详细介绍。我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中， model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。 这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的\nepoch次数，以及优化器的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(WarmupStepLrUpdater)，在训练过程中验证模型的指标(Valid\nation)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了：\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的 qat_trainer 和\ncalibration_trainer 。\n\n\n\n\nmodel_convert_pipeline参数值不同#\n\n当我们训练量化模型的时候，需要设置相应的model_convert_pipeline，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float，calibration和qat三阶段的训练过程，相应的我们可以验证这三个阶段训练出\n来的模型性能，只需要相应的运行以下两条命令即可：\n\n\n\n同时，我们还提供了quantization模型的性能测试，但需要注意是必须要先导出hbir：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n结果可视化#\n\n如果你希望可以看到训练出来的模型对于单张图片的检测效果，我们的tools文件夹下面同样提供了单张图片预测及可视化的脚本，你只需要运行以下脚本即可：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile_perf_hbir 的工具用来将量化模型编译成可以上板运行的 hbm\n文件，同时该工具也能预估在BPU上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/fcos3d","lang":"zh","toc":[{"text":"模型构建","id":"模型构建","depth":2,"charIndex":735},{"text":"数据增强","id":"数据增强","depth":2,"charIndex":910},{"text":"训练策略","id":"训练策略","depth":2,"charIndex":1038},{"text":"model_convert_pipeline参数值不同","id":"model_convert_pipeline参数值不同","depth":2,"charIndex":1706},{"text":"训练策略不同","id":"训练策略不同","depth":2,"charIndex":1882},{"text":"导出定点模型","id":"导出定点模型","depth":2,"charIndex":2060},{"text":"模型验证","id":"模型验证","depth":2,"charIndex":2106},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":2,"charIndex":2329},{"text":"结果可视化","id":"结果可视化","depth":2,"charIndex":2388},{"text":"模型检查和编译","id":"模型检查和编译","depth":2,"charIndex":2474}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":334,"title":"Occupancy预测模型","content":"#\n\nOccupancy预测模型基于Horizon Torch Samples（地平线自研深度学习框架）开发，关于Horizon Torch\nSamples的使用介绍可以参考Horizon Torch\nSamples使用文档。Occupancy预测模型的训练config位于configs/occ/路径下。\n下文以configs/occ/flashocc_henet_lss_occ3d_nuscenes.py为例介绍如何配置并训练Occupancy预测模型。\n\n\n训练流程#\n\n如果你只是想简单的把 flashocc_henet_lss_occ3d_nuscenes\n的模型训练起来，那么可以首先阅读一下这一章的内容。和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools + config\n的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n这里以nuscense数据集为例，可以从 https://www.nuscenes.org/nuscenes 下载数据集,\n对于Occupancy预测任务，还需要下载OCC的GT，可以从\nhttps://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Predic\ntion 下载数据集。 这里建议将下载好的数据集，解压到nuscense数据集文件夹中的 occ3d/gts 下。\n同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。\n\n\n\n上面这条命令会在 ./tmp_data/nuscenes/occ3d 下面生成 nuscenes_infos_train.pkl 和\nnuscenes_infos_val.pkl 两个文件, 接下来执行下面命令进行打包：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb和val_lmdb就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的Occupancy预测模型了。\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证也可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。我们在HAT中提供了模型检查的接口，可以在定义好量化模型\n之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\n\n\n其中，model 下面的 type\n表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。例如，如果我们想训练一个backbone为res\nnet50的模型，只需要将 model 下面的 backbone 替换掉就可以。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程, 详细信息见\nconfigs/occ/flashocc_henet_lss_occ3d_nuscenes.py。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CosineAnnealingLrUpdater)，在训练过程中验证模型的指标(\nValidation)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。当然训练一个好的检测模\n型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以得到伪量化模型了，该模型仅使用calibrat\nion即可达到目标：\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的calibration_trainer\n\n\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置quantize=True，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。","routePath":"/guide/advanced_content/hat/examples/flashocc","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":232},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":398},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":918},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":1068},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1114},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1352},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1409},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1468},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1695},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1752},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1903},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2068},{"text":"量化模型训练","id":"量化模型训练","depth":3,"charIndex":2616},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":3,"charIndex":2800},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2951}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":335,"title":"GaNet车道线检测模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在车道线数据集 CuLane 上从头开始训练一个 GaNet 模型，包括浮点、量化和定点模型。\n\nCuLane 是车道线检测中用的比较多的数据集，很多先进的车道线检测研究都会优先基于这个数据集做好验证。\n开始训练模型之前，第一步是准备好数据集，这里我们下载官方的数据集以及相应的标签数据 CuLaneDataset ，需要注意的是，\nannotations_new.tar.gz 这个文件必须要最后解压。 解压缩之后数据目录结构如下所示：\n\n\n\n其中 list/train.txt 里面是训练数据的路径，list/test.txt 里面是测试数据的路径。\n\n\n训练流程#\n\n如果你只是想简单的把 GaNet 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n为了提升训练速度，我们对原始的数据集做了一个打包，将其转换为 LMDB 格式的数据集。只需要运行下面的脚本， 就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集，打包完成之后，${target-data-dir} 目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 test_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\n在网络开始训练之前，你可以使用以下命令先计算一下网络的计算量和参数数量：\n\n\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n由于HAT算法包使用了注册机制，使得每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是 \"float\"、\"calibration\"，\n分别可以完成浮点模型、量化模型的训练，其中量化模型的训练依赖于上一步浮点训练产出的浮点模型。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 、 \"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nGaNet 的网络结构可以参考 论文，这里不做详细介绍。 我们通过在 config 配置文件中定义 model 这样的一个 dict\n型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n模型除了 backbone 之外，还有 neck、 head、 targets、post_process 和 losses 模块，在 GaNet\n中，backbone 主要是提取图像的特征，neck 主要是特征增强，head 主要是由特征来得到预测的车道线关键点的分数和偏移。 targets 是训练时从\ngt 中得到训练的target, post_process 主要是后处理部分，推理的时候使用。losses 部分采用论文中的 LaneFastFocalLoss\n和 L1Loss 来作为训练的 loss，loss_weight 是对应的 loss 的权重。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 train_data_loader 和 val_data_loader 这两个\ndict 来实现的，分别对应着训练集和验证集的处理流程。 以 train_data_loader 为例，数据增强使用了 FixedCrop、\nRandomFlip、 Resize、RandomSelectOne、 RGBShift、 HueSaturationValue、\nJPEGCompress、MeanBlur、 MedianBlur、 RandomBrightnessContrast、ShiftScaleRotate 和\nRandomResizedCrop 来增加训练数据的多样性，增强模型的泛化能力。\n\n\n\n因为最终跑在 BPU 上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式，所以HAT提供 BgrToYuv444\n的数据增强来将 RGB 转到 YUV444 的格式。 为了优化训练过程，HAT使用了 batch_processor ，可将一些增强处理放在\nbatch_processor 中优化训练：\n\n\n\n其中 loss_collector 是一个获取当前批量数据的 loss 的函数。\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n\n\n训练策略#\n\n在 CuLane 数据集上训练浮点模型使用 Cosine 的学习策略配合 Warmup，以及对 weight 的参数施加L2 norm。\nconfigs/lane_pred/ganet/ganet_mixvargenet_culane.py 文件中的\nfloat_trainer、calibration_trainer、int_trainer 分别对应浮点、量化、定点模型的训练策略。 下面为\nfloat_trainer 训练策略示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。这里主要讲一下\nHAT 的车道线检测中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，HAT在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。 因为有浮点预训练模型，所以量化训练的学习率 Lr 可以很小， 一般可以从\n0.001 或 0.0001 开始，并可以搭配 StepLrUpdater 做1-2次 scale=0.1 的 Lr 调整； 同时训练的长度不用很长。此外\nweight decay 也会对训练结果有一定影响。\n\nGaNet 示例模型的量化训练策略可见 configs/lane_pred/ganet/ganet_mixvargenet_culane.py 文件。","routePath":"/guide/advanced_content/hat/examples/ganet","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":300},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":439},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":644},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":995},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1041},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1281},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1338},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1397},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1624},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1681},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":2065},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2654},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":2884}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":336,"title":"量化模型训练","content":"Lidar融合多任务感知模型#\n\n这篇教程主要是告诉大家如何利用HAT在自动驾驶数据集 Nuscenes 上训练一个Lidar融合多任务感知模型，包括浮点、量化和定点模型。 下文以配置\nconfigs/lidar_bevfusion/bevfusion_pointpillar_henet_multisensor_multitask_nuscen\nes.py 为例介绍如何配置并训练Lidar融合多任务感知模型。\n\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes\n是一个多模态多任务自动驾驶感知模型，接受两种 camera 和 lidar 两种模态的输入，输出动态要素3d检测框和3d占用网格预测。\n\n\n训练流程#\n\n如果你只是想简单的把 bevfusion_pointpillar_henet_multisensor_multitask_nuscenes\n的模型训练起来，那么可以首先阅读一下这一章的内容。和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools + config\n的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n这里以nuscense数据集为例，可以从 https://www.nuscenes.org/nuscenes 下载数据集,\n对于Occupancy预测任务，还需要下载OCC的GT，可以从\nhttps://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Predic\ntion 下载数据集。 这里建议将下载好的数据集，解压到nuscense数据集文件夹中的 occ3d/gts 下。 然后可以运行以下命令，将lidar,\nimages, occ gt等数据一起打包成lmdb格式：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb和val_lmdb就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的lidar融合多任务感知模型\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes 了。\n\n首先可以使用以下命令评估模型的计算量：\n\n\n\n如果你想要复现整个模型浮点和定点训练流程，你需要：\n\n\n\n以上命令分别完成camera输入的浮点模型预训练，浮点模型训练和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练\n章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证也可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。我们在HAT中提供了模型检查的接口，可以在定义好量化模型\n之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\n\n\n其中，model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes 模型主要由 lidar network ，\ncamera_network 和 bev_decoders 三个部分组成， 其中 bev_decoders 包含 BEVFormerDetDecoder 和\nBevformerOccDetDecoder 两个模块，分别输出3d检测框和3d占用网格预测。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。 例如训练集的定义为：\n\n\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n对于这个多网融合模型，我们建议先训练 lidar 输入的模型，再训练 camera 输入的模型，最后训练 lidar + camera\n的融合模型，以获得较好的效果。 其中 lidar 输入的模型可以参考 centerpoint 模型训练， camera 输入的模型可以参考\n“configs/lidar_bevfusion/bevformer_henet_camera_multitask_nuscenes_pretrain.py”\nconfig训练浮点预训练模型，命令为：\n\n\n\n雷达融合多任务模型的 float_trainer 的配置如下，可以看到我们分别加载了camera输入和lidar输入的预训练模型。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CosineAnnealingLrUpdater)，在训练过程中验证模型的指标(\nValidation)，以及保存(Checkpoint)模型的操作。 当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。当然训练一个好的检测模\n型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以得到伪量化模型了:\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型, 其中calibration流程可以为QAT的量化训练提供一个更好的初始化参数。\n\n我们使用模版配置了 Float2Calibration 和 Float2QAT 将模型分别转换为calibration和qat模型，具体的calibration\n和 QAT 的config为：\n\n\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率相比float小很多，\n训练的epoch次数也大大减少，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。","routePath":"/guide/advanced_content/hat/examples/lidar_fusion","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":336},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":527},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":930},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":1175},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1221},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1459},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1516},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1575},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1802},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1859},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":2153},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2275},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":3338}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":337,"title":"lidarMultiTask模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在数据集 nuscenes 上从头开始训练一个 lidarMultiTask 模型，包括浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，可以在 nuscenes 数据集 下载 v1.0 版本的完整数据文件和 nuScenes-lidarseg\n的完整数据文件。\n\n下载后，解压并按照如下方式组织文件夹结构，其中，lidarseg文件夹摆放规则可参照 nuscenes 官方Tutorials。\n\n\n\n为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。其中，lidarMultiTask 模型只使用了 nuscenes\n数据集的点云部分文件。 只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，tmp_data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb\n就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集，meta中为评测脚本需要的初始化信息，具体信息是从nuscenes原始数据集中拷贝得来。\n\n同时，为了训练nuscenes点云数据，还需要为nuscenes数据集生成每个单独的训练目标的点云数据，并将其存储在\ntmp_nuscenes/lidar/nuscenes_gt_database 的 .bin\n格式的文件中，文件保存目录可根据需要更改。同时，需要为这部分数据生成 .pkl 格式的包含数据信息的文件。\n此外，训练数据集初始化过程中需要读取全部数据集中每个sample的类别信息，并进行重采样操作，我们可以提前生成对应的信息并保存成 .pkl\n格式的文件，可以加速训练过程。 通过运行下面的命令来创建上述数据:\n\n\n\n执行上述命令后，生成的文件目录如下：\n\n\n\n其中， nuscenes_gt_database 和 nuscenes_dbinfos_train.pkl 是训练是用于采样的样本，而\nnuscenes_dbinfos_train.pkl 则包含训练数据集初始化需要的信息。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的lidarMultiTask网络了。在网络训练开始之前，你可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n由于HAT算法包使用了一种巧妙的注册机制，使得每一个训练任务都可以按照这种train.py加上config配置文件的形式启动。\ntrain.py是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的config配置文件里面。\nconfig文件里面提供了模型构建、数据读取等关键的dict。\n\n\n模型构建#\n\nlidarMultiTask 的网络结构主要借鉴 CenterPoint\n模型，在模型层面，主要修改了neck部分并添加了分割的输出头，详细可参考下面的config文件。 我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中， model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\n训练脚本在启动之后，会调用 build_model 接口，将这样一个dict类型的model变成类型为 torch.nn.Module 类型的model。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口 torch.utils.data.DataLoader，表示的是将 batch_size 大小的样本组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量，data_path 路径也就是我们在第一部分数据集准备中提到的路径。transforms\n下面包含着一系列的数据增强。而 val_data_loader 中只有读取点云文件、生成分割标签、数据Reformat等操作。 你也可以通过在\ntransforms 中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CyclicLrUpdater)，在训练过程中验证模型的指标(Validatio\nn)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\nfloat_trainer 负责将整个训练的逻辑给串联起来，其中也会负责模型的pretrain。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了。\n不过这里需要说明的是，lidarMultiTask模型在量化训练过程中建议加上calibration的流程。calibration可以为QAT的量化训练提供一个\n更好的初始化参数。\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的qat_trainer。\n\n\n\n\nmodel_convert_pipeline参数值不同#\n\n当我们训练量化模型的时候，需要设置model_convert_pipeline，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的二分之一，训练的epoch次数也随之减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址或者calibration校准后的模型地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float和qat两阶段的训练过程，相应的我们可以验证这两个阶段训练出来的模型性能，只需要相应\n的运行以下两条命令即可：\n\n\n\n同时，我们还提供了quantization模型的性能测试，只需要运行以下命令，但需要注意是必须要先导出hbir：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n模型推理和结果可视化#\n\n如果你希望可以看到训练出来的模型对于雷达点云的检测和分割效果，我们的tools文件夹下面同样提供了点云预测及可视化的脚本，你只需要运行以下脚本即可：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile_perf_hbir 的工具用来将量化模型编译成可以上板运行的 hbm\n文件，同时该工具也能预估在BPU上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/lidar_multitask","lang":"zh","toc":[{"text":"数据集准备","id":"数据集准备","depth":2,"charIndex":78},{"text":"浮点模型训练","id":"浮点模型训练","depth":2,"charIndex":918},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1216},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1525},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1913},{"text":"量化模型训练","id":"量化模型训练","depth":2,"charIndex":2502},{"text":"model_convert_pipeline参数值不同","id":"model_convert_pipeline参数值不同","depth":2,"charIndex":2744},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2917},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":3116},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":3162},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":3383},{"text":"模型推理和结果可视化","id":"模型推理和结果可视化","depth":3,"charIndex":3442},{"text":"模型检查和编译","id":"模型检查和编译","depth":3,"charIndex":3534}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":338,"title":"MapTROE 模型训练","content":"#\n\nMapTROE 参考算法基于 Horizon Algorithm Toolkit（HAT，地平线自研深度学习算法包）开发。训练config位于\nconfigs/map/ 路径下。\n下文以configs/map/maptroe_henet_tinym_bevformer_nuscenes.py为例介绍如何配置并训练 MapTROE\n参考算法。\n\n\n训练流程#\n\n如果你只是想简单的把 MapTROE 的模型训练起来，那么可以首先阅读一下这一章的内容。和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n这里以nuscense数据集为例，可以从 https://www.nuscenes.org/nuscenes 下载数据集\n。同时，为了提升训练的速度，我们对原始的jpg格式的数据集做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集。此外，MapTROE模型还需要导航地图(SD\nmap)进行地图融合来提升模型精度，导航地图可以从开放街道地图(OpenStreetMap, OSM)\nhttps://www.openstreetmap.org 中获取。 打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb和val_lmdb就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集。metas中包含模型需要的地图信息。osm中包含地图融合需\n要的sd map信息。\n\n\n模型训练#\n\n数据集准备好之后，就可以开始 MapTROE 模型的训练了。\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n以上命令分别完成float、calibration、qat阶段的训练，其中calibration阶段的训练需要以训练好的浮点模型为基础，qat阶段的训练则需要训\n练好的calibration模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成qat训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的float、calibration和qat模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float\n、Calibration 和 QAT 的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 float、calibration、qat\n时，分别可以完成对训练好的浮点模型、calibration模型、qat模型的验证。\n\n定点模型精度验证也可使用下面命令，但需要注意，必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。我们在HAT中提供了模型检查的接口，可以在定义好量化模型\n之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\n\n\n其中，model 下面的 type\n表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。例如，如果我们想训练一个backbone为res\nnet18的模型，只需要将 model 下面的 backbone 替换掉就可以。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。如下所示：\n\n\n\n其中type直接用的pytorch自带的接口torch.utils.data.DataLoader，表示的是将 batch_size 大小的图片组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量， data_path 表示lmdb数据集的路径，map_path\n表示地图数据的路径，sd_map_path表示导航地图的路径，也就是我们在第一部分数据集准备中提到的路径。transforms\n下面包含着一系列的数据增强。你也可以通过在 transforms 中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CosineAnnealingLrUpdater)，在训练过程中验证模型的指标(\nValidation)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\n\n注意\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n\n量化训练#\n\n使用 float_trainer\n的设置，可以训练出来一个高精度的浮点模型。当我们有了浮点模型之后，就可以开始训练相应的量化模型了。此时我们使用的训练策略来自于config文件中的\ncalibration_trainer 和 qat_trainer。\n\n\n\n量化训练其实是在纯浮点训练基础上的finetue，因此量化训练的时候，学习率 qat_lr 要比 float_lr\n小得多，qat训练的epoch次数也大大减少，最重要的是 model 定义的时候，calibration模型的 pretrained\n需要设置成已经训练出来的纯浮点模型的地址，qat模型则需要设置为校准出来的calibration模型地址。\n\n\nqconfig的设置#\n\n当我们训练量化模型的时候，需要设置模型的qconfig，浮点模型会按照qconfig的设置被转换成对应的量化模型，qconfig设置如下：\n\n\n\n其中，cali_qconfig_setter 和 qat_qconfig_setter\n分别为calibration模型和qat模型对应的qconfig设置，关于qconfig的设置方法与调试步骤，比如默认qconfig的设置、量化敏感算子设置等，\n请阅读 量化感知训练-Qconfig详解 章节的内容。","routePath":"/guide/advanced_content/hat/examples/maptroe","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":176},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":316},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":723},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":924},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":971},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1214},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1271},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1330},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1535},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1592},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1743},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2120},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":2544},{"text":"qconfig的设置","id":"qconfig的设置","depth":3,"charIndex":2861}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":339,"title":"Motr多目标跟踪模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在目标跟踪数据集 MOT17 上从头开始训练一个 Motr 模型，包括浮点、量化和定点模型。\n\nMOT17 是多目标跟踪中用的比较多的数据集，很多先进的多目标跟踪研究都会优先基于这个数据集做好验证。\n开始训练模型之前，第一步是准备好数据集，这里我们下载官方的数据集以及相应的标签数据 MOT17DATASET 。\n\n解压缩之后数据目录结构如下所示：\n\n\n\n其中 MOT17-02-DPM 视频序列的名字，det.txt 为检测器的结果，比如 MOT17-02-DPM 文件夹中的就是使用 DPM 算法检测的结果，gt\n下的 gt.txt 为标签数据，img1 中为图像数据。\n\n\n训练流程#\n\n如果你只是想简单的把 Motr 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。 在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n由于官方的测试集无gt，因此我们将训练集拆成一半，每个视频前一半帧作为训练集，后一半帧作为验证集。 HAT 提供脚本将训练集拆分，只需要运行下面的脚本：\n\n\n\n运行完上面脚本后，将会得到下面生成类似下面结构的文件夹：\n\n\n\n为了提升训练速度，我们对原始的数据集做了一个打包，将其转换为 LMDB 格式的数据集。只需要运行下面的脚本就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集。\n\n评测精度时，需要用到验证数据集的标签数据，因此我们做一个软连接，如下所示：\n\n\n\n打包和软连接完成之后，${target-data-dir} 目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 test_lmdb 就是打包之后的训练数据集和验证数据集，test_gt 里面是验证集的标签数据，接下来就可以开始训练模型。\n\n\n模型训练#\n\n由于 Motr 模型中的 qim 模块输入依赖于一些后处理，因此我们将整个 Motr 模型拆分到两个config中。\n\n除去生成定点模型、编译、模型checker、模型计算量需要使用 qim 的 config，其余均使用 motr 的 config\n即可，详细使用情况可见下面章节。\n\n以下描述中，第一个模型为 Motr 的基础模块，第二个模型为 qim 模块，未特别注明则是两个模块串联的模型。\n\n在网络开始训练之前，你可以使用以下命令先计算一下网络的计算量和参数数量：\n\n\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n由于HAT算法包使用了注册机制，使得每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。 train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是\n\"float\"、\"calibration\"、\"qat\"，分别可以完成浮点模型、量化模型的训练，其中量化模型的训练依赖于上一步浮点训练产出的浮点模型。\n\n对于该模型，量化模型到定点模型的转化需要使用 qim 模块的 config。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为\nFloat、Calibration、QAT 和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\"、\"qat\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nMotr 的网络结构可以参考 论文，这里不做详细介绍。我们通过在 config 配置文件中定义 model 这样的一个 dict\n型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n模型除了 backbone 之外，还有 head、 criterion、 post_process 和 track_embed 模块， 在 Motr 中,\nbackbone 主要是提取图像的特征，head 主要是由特征来得到预测的类别、位置和特征。 criterion 是训练时计算 loss 的模块,\npost_process 主要是后处理部分，track_embed 是用来更新已跟踪上的目标query的模块(即 qim 模块)。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 data_loader 和 val_data_loader 这两个 dict\n来实现的，分别对应着训练集和验证集的处理流程。 以 data_loader 为例，数据增强使用了\nSeqRandomFlip、RandomSelectOne、SeqResize、SeqRandomSizeCrop、SeqToTensor、SeqNormali\nze 来增加训练数据的多样性，增强模型的泛化能力。\n\n因为最终跑在 BPU 上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式，所以HAT提供 SeqBgrToYuv444\n的数据增强来将 RGB 转到 YUV444 的格式。\n\n\n\nbatch_processor 中传入一个 loss_collector 函数，用于获取当前批量数据的 loss，如下所示：\n\n\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n\n\n训练策略#\n\n在 Mot17 数据集上训练浮点模型使用 Stepdecay 的学习策略， 以及对 weight 的参数施加 L2 norm。\nconfigs/track_pred/motr_efficientnetb3_mot17.py 文件中的 float_trainer，\ncalibration_trainer， qat_trainer，int_trainer 分别对应浮点、量化、定点模型的训练策略。 下面为\nfloat_trainer 训练策略示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。 这里主要讲一下\nHAT 的多目标跟踪模型中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，HAT在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。 因为有浮点预训练模型，所以量化训练的学习率 Lr 可以很小，一般可以从\n0.001 或 0.0001 开始，并可以搭配 StepLrUpdater 做1-2次 scale=0.1 的 Lr 调整；同时训练的长度不用很长。 此外\nweight decay 也会对训练结果有一定影响。\n\nMotr 示例模型的量化训练策略可见 configs/track_pred/motr_efficientnetb3_mot17.py 文件。","routePath":"/guide/advanced_content/hat/examples/motr","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":310},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":449},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":831},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":1428},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1474},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1720},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1777},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1836},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":2063},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":2120},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":2438},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2883},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":3118}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":340,"title":"多机使用说明","content":"#\n\n\n环境准备#\n\n准备两台开发机，确保两者直接可以免密登录。\n\n保证两台开发机上需要运行的代码的的文件目录和内容是一样的。\n\n\n启动docker#\n\n在第一台机器上启动docker。\n\n\n\n其中 096e26686644 是第一步启动容器的镜像名字，可以使用 docker ps 查看。\n\n在第二台机器上启动docker。\n\n\n\n这里注意映射ssh的端口号变了，第一台机器是10022，第二台机器是10023，保证两者不一样即可。\n\n为了保证两台开发机docker内部的开发目录是一样的，可以使用 -v 挂载相同的开发目录即可。\n\n另外可以测试两个docker容器之间是否可以免密登录：使用ifconfig命令查看第二台机器上的容器的ip地址，例如是：172.17.0.12\n，回到第一台机器上的容器，使用 ssh -p 22 172.17.0.12，观察两台机器是否可以免密登录。如果可以了就可以进行下一步了。\n\n\n使用torchrun启动多机多卡训练脚本#\n\n\n\nhostip1 表示第一台机器上的容器的ip地址，使用 ifconfig 即可查看。\n\n--nnodes 2 ：2表示总机器数。\n\n--nproc_per_node\n4：4表示每个机器上gpu的数量。（你可能需要手动把configs/classification/mobilenetv1_imagenet.py文件中的gpu数\n量改为4个）\n\n运行该命令就可以看到多机多卡的事例正确运行了。","routePath":"/guide/advanced_content/hat/examples/multi_machine","lang":"zh","toc":[{"text":"环境准备","id":"环境准备","depth":2,"charIndex":3},{"text":"启动docker","id":"启动docker","depth":2,"charIndex":65},{"text":"使用torchrun启动多机多卡训练脚本","id":"使用torchrun启动多机多卡训练脚本","depth":2,"charIndex":410}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":341,"title":"PointPillars检测模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在雷达点云数据集 KITTI-3DObject 上从头开始训练一个 PointPillars\n模型，包括浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，我们在KITTI官网下载 3DObject据集 ， 包括4个文件:\n\n 1. left color images of object data set\n 2. velodyne point clouds\n 3. camera calibration matrices of object data set\n 4. taining labels of object data set\n\n下载上述4个文件后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了创建KITTI点云数据，首先需要加载原始的点云数据并生成相关的包含目标标签和标注框的数据标注文件，同时还需要为KITTI数据集生成每个单独的训练目标的点云数\n据，并将其存储在 data/kitti/gt_database 的 .bin 格式的文件中，此外，需要为训练数据或者验证数据生成 .pkl\n格式的包含数据信息的文件。随后，通过运行下面的命令来创建KITTI数据：\n\n\n\n执行上述命令后，生成的文件目录如下：\n\n\n\n同时，为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集，kitti3d_gt_database 和\nkitti3d_dbinfos_train.pkl 是训练是用于采样的样本。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始训练浮点型的PointPillars检测网络了。在网络训练开始之前，你可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n由于HAT算法包使用了一种巧妙的注册机制，使得每一个训练任务都可以按照这种train.py加上config配置文件的形式启动。 train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的config配置文件里面。\nconfig文件里面提供了模型构建、数据读取等关键的dict。\n\n\n模型构建#\n\nPointPillars 的网络结构可以参考 论文， 这里不做详细介绍。我们通过在config配置文件中定义 model\n这样的一个dict型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n其中，model 下面的 type 表示定义的模型名称，剩余的变量表示模型的其他组成部分。这样定义模型的好处在于我们可以很方便的替换我们想要的结构。\n训练脚本在启动之后，会调用 build_model 接口，将这样一个dict类型的model变成类型为 torch.nn.Module 类型的model。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在config配置文件中定义 data_loader 和 val_data_loader\n这两个dict来实现的，分别对应着训练集和验证集的处理流程。以 data_loader 为例：\n\n\n\n其中type直接用的pytorch自带的接口 torch.utils.data.DataLoader，表示的是将 batch_size 大小的图片组合到一起。\n这里面唯一需要关注的可能是 dataset 这个变量，data_path 路径也就是我们在第一部分数据集准备中提到的路径。transforms\n下面包含着一系列的数据增强。val_data_loader 中只有除了点云Pillar化（Voxelization）和Reformat。 你也可以通过在\ntransforms 中插入新的dict实现自己希望的数据增强操作。\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，相应的训练策略同样都定义在其中的config文件中，从 float_trainer\n这个变量就可以看出来。\n\n\n\nfloat_trainer\n从大局上定义了我们的训练方式，包括使用多卡分布式训练（distributed_data_parallel_trainer），模型训练的epoch次数，以及优化器\n的选择。 同时 callbacks\n中体现了模型在训练过程中使用到的小策略以及您想实现的操作，包括学习率的变换方式(CyclicLrUpdater)，在训练过程中验证模型的指标(Validatio\nn)，以及保存(Checkpoint)模型的操作。当然，如果你有自己希望模型在训练过程中实现的操作，也可以按照这种dict的方式添加。\nfloat_trainer 负责将整个训练的逻辑给串联起来，其中也会负责模型的pretrain。\n\n注解\n\n如果需要复现精度，config中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，你应该对config文件的功能有了一个比较清楚的认识。然后通过前面提到的训练脚本，就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了：\n\n\n\n可以看到，我们的配置文件没有改变，只改变了 stage 的类型。此时我们使用的训练策略来自于config文件中的 qat_trainer 和\ncalibration_trainer 。\n\n\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置 quantize=True ，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 retrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float、calibration和qat三阶段的训练过程，相应的我们可以验证这三个阶段训练出\n来的模型性能，只需要相应的运行以下两条命令即可：\n\n\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n模型推理和结果可视化#\n\nHAT提供了 infer_hbir.py 脚本对各阶段训练好的模型的推理结果进行可视化展示：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile_perf_hbir 的工具用来将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU\n上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/pointpillars","lang":"zh","toc":[{"text":"数据集准备","id":"数据集准备","depth":2,"charIndex":86},{"text":"浮点模型训练","id":"浮点模型训练","depth":2,"charIndex":796},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1095},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1353},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1746},{"text":"量化模型训练","id":"量化模型训练","depth":3,"charIndex":2335},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":3,"charIndex":2510},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2663},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":2840},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":2886},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":3096},{"text":"模型推理和结果可视化","id":"模型推理和结果可视化","depth":3,"charIndex":3155},{"text":"模型检查和编译","id":"模型检查和编译","depth":3,"charIndex":3219}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":342,"title":"PointPillars检测模型训练(无 config)","content":"#\n\n这篇教程主要是告诉大家如何利用 HAT 在雷达点云数据集 KITTI-3DObject 上从头开始训练一个 PointPillars\n模型，包括浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，我们在KITTI官网下载 3DObject据集， 包括4个文件：\n\n 1. left color images of object data set\n 2. velodyne point clouds\n 3. camera calibration matrices of object data set\n 4. taining labels of object data set\n\n下载上述4个文件后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了创建KITTI点云数据，首先需要加载原始的点云数据并生成相关的包含目标标签和标注框的数据标注文件，同时还需要为KITTI数据集生成每个单独的训练目标的点云数\n据，并将其存储在 data/kitti/gt_database 的 .bin 格式的文件中，此外，需要为训练数据或者验证数据生成 .pkl\n格式的包含数据信息的文件。随后，通过运行下面的命令来创建KITTI数据：\n\n\n\n执行上述命令后，生成的文件目录如下：\n\n\n\n同时，为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集，kitti3d_gt_database 和\nkitti3d_dbinfos_train.pkl 是训练是用于采样的样本。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始模型训练相关的内容。\n\n\n模型构建#\n\nPointPillars 的网络结构可以参考 论文 ，这里不做详细介绍。\n\n从模型训练到编译上板的整个流程大致如下：\n\n\n\n从上图中，主要用到了三个阶段的模型，即 Float Model、QAT Model、Quantized Model。其中：\n\n * Float Model: 即是一般的浮点模型。\n * QAT Model: 即是插入伪量化结点的模型。\n * Quantized Model: 即量化后的模型，参数为INT8类型。\n\n此外，在训练、 编译等流程中又分别会使用不同结构或状态的模型：\n\n * model: 完整的模型结构，即包含模型 前处理、网络结构、后处理，主要用于训练、评测。\n * deploy_model: 只包含 网络结构 （可以编译到 hbm 中的部分），不包含 前处理 和 后处理，主要用于编译。\n\n我们通过定义一个 PointPillarsModel 类来与模型结构相关的所有内容，包含上述三个阶段：Float Model、QAT\nModel、Quantized Model 以及两种状态: model 和 deploy_model：\n\n\n\n至此， PointPillarsModel 中已定义了与模型相关的所有内容，在使用时可以很方便的通过 PointPillarsModel.xxx()\n获取相应的模型结构。\n\n在完成网络结构的定义之后，我们可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n\n数据增强#\n\n类似 模型构建 部分，我们通过定义一个 DataHelper 来实现数据相关内容，包括 transforms ， data_loader 等：\n\n\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，不同阶段的模型(浮点、QAT) 其训练策略也会略有不同，因此，我们也把训练策略内容(\noptimizer, lr_schedule) 等也定义在 PointPillarsModel 里面：\n\n\n\n注解\n\n如果需要复现精度，示例代码中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，我们已经完成了对模型训练相关所有模块的定义，接下来就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。\n\n如果你只是单纯的想启动这样的训练任务，只需要运行下面的命令就可以：\n\n\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了：\n\n\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置quantize=True，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的 finetue。\n因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少，最重要的是 model 定义的时候，我们的 pretrained\n需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float和qat两阶段的训练过程，相应的我们可以验证这两个阶段训练出来的模型性能，只需要相应\n的运行以下两条命令即可：\n\n\n\n同时，我们还提供了quantization模型的性能测试，只需要运行以下命令：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n结果可视化#\n\n如果你希望可以看到训练出来的模型对于单帧雷达点云的检测效果，我们的tools文件夹下面同样提供了点云预测及可视化的脚本，你只需要运行以下脚本即可：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile 的工具用来将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在BPU上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/pointpillars_v2","lang":"zh","toc":[{"text":"数据集准备","id":"数据集准备","depth":2,"charIndex":88},{"text":"浮点模型训练","id":"浮点模型训练","depth":2,"charIndex":797},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":833},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1461},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1544},{"text":"量化模型训练","id":"量化模型训练","depth":2,"charIndex":1881},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":3,"charIndex":1960},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2111},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":2290},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":2494},{"text":"结果可视化","id":"结果可视化","depth":3,"charIndex":2553},{"text":"模型检查和编译","id":"模型检查和编译","depth":3,"charIndex":2639}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":343,"title":"PointPillars检测模型训练(无config && 无HAT Trainer)","content":"#\n\n这篇教程旨在详细介绍如何将 HAT 作为一个模型库（Model Library），并将其与用户自定义的训练框架结合使用，在雷达点云数据集\nKITTI-3DObject 上从头开始训练一个 PointPillars 模型。 文档中我们将探讨如何将 HAT\n中的模型集成到您的训练流程中，包括数据预处理、模型构建等关键步骤，帮助您成功地将 HAT 集成到您的训练框架中，并训练出 PointPillars\n的浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，我们在KITTI官网下载 3DObject据集，包括4个文件:\n\n 1. left color images of object data set\n 2. velodyne point clouds\n 3. camera calibration matrices of object data set\n 4. taining labels of object data set\n\n下载上述4个文件后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了创建KITTI点云数据，首先需要加载原始的点云数据并生成相关的包含目标标签和标注框的数据标注文件，同时还需要为KITTI数据集生成每个单独的训练目标的点云数\n据，并将其存储在 data/kitti/gt_database 的 .bin 格式的文件中，此外，需要为训练数据或者验证数据生成 .pkl\n格式的包含数据信息的文件。 随后，通过运行下面的命令来创建KITTI数据:\n\n\n\n执行上述命令后，生成的文件目录如下：\n\n\n\n同时，为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。 只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应着转换训练数据集和验证数据集，打包完成之后，data目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，也是网络最终读取的数据集，kitti3d_gt_database 和\nkitti3d_dbinfos_train.pkl 是训练是用于采样的样本。\n\n\n浮点模型训练#\n\n数据集准备好之后，就可以开始模型训练相关的内容。\n\n\n模型构建#\n\nPointPillars的网络结构可以参考 论文 ，这里不做详细介绍。\n\n从模型训练到编译上板的整个流程大致如下：\n\n\n\n从上图中，主要用到了三个阶段的模型，即 Float Model、QAT Model、Quantized Model。其中：\n\n * Float Model: 即是一般的浮点模型。\n * QAT Model: 即是插入伪量化结点的模型。\n * Quantized Model: 即量化后的模型，参数为INT8类型。\n\n此外，在训练、 编译等流程中又分别会使用不同结构或状态的模型：\n\n * model: 完整的模型结构，即包含模型 前处理、网络结构、后处理，主要用于训练、评测。\n * deploy_model: 只包含 网络结构 （可以编译到 hbm 中的部分），不包含 前处理 和 后处理，主要用于编译。\n\n我们通过定义一个 PointPillarsModel 类来与模型结构相关的所有内容，包含上述三个阶段：Float Model、QAT\nModel、Quantized Model 以及两种状态: model 和 deploy_model：\n\n\n\n至此，PointPillarsModel 中已定义了与模型相关的所有内容，在使用时可以很方便的通过 PointPillarsModel.xxx()\n获取相应的模型结构。\n\n在完成网络结构的定义之后，我们可以使用以下命令先测试一下网络的计算量和参数数量：\n\n\n\n\n数据增强#\n\n类似 模型构建 部分，我们通过定义一个 DataHelper 来实现数据相关内容，包括 transforms ， data_loader 等：\n\n\n\n\n训练策略#\n\n为了训练一个精度高的模型，好的训练策略是必不可少的。对于每一个训练任务而言，不同阶段的模型(浮点、QAT)\n其训练策略也会略有不同，因此，我们也把训练策略内容(optimizer, lr_schedule) 等也定义在 PointPillarsModel 里面：\n\n\n\n注解\n\n如果需要复现精度，示例代码中的训练策略最好不要修改。否则可能会有意外的训练情况出现。\n\n通过上面的介绍，我们已经完成了对模型训练相关所有模块的定义。不过，在开始训练之前，我们还需要有一个模型训练框架（比如可以使用HAT内置的训练框架，或者其他任意框\n架，可以完成训练即可）。 例如，下面的示例代码就是我们按照 Pytorch 的官方教程 和\n示例代码，进行少量修改，构建的一个简单支持单机多卡DDP的\"训练框架\"：\n\n\n\n至此，我们已经完成了模型训练和评测的所需内容，读者可以在示例中查看完整代码。接下来就可以训练一个高精度的纯浮点的检测模型。\n当然训练一个好的检测模型不是我们最终的目的，它只是做为一个pretrain为我们后面训练定点模型服务的。可以通过以下命令，启动浮点模型训练：\n\n\n\n\n量化模型训练#\n\n当我们有了纯浮点模型之后，就可以开始训练相应的定点模型了。和浮点训练的方式一样，我们只需要通过运行下面的脚本就可以训练定点模型了：\n\n\n\n通过上文中搭建模型结构部分时，我们已经了解到 Float Model 和 QAT Model 有些不同，主要体现在以下方面。\n\n\nquantize参数的值不同#\n\n当我们训练量化模型的时候，需要设置 quantize=True，此时相应的浮点模型会被转换成量化模型，相关代码如下：\n\n\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。\n\n\n训练策略不同#\n\n正如我们之前所说，量化训练其实是在纯浮点训练基础上的finetue。因此量化训练的时候，我们的初始学习率设置为浮点训练的十分之一，训练的epoch次数也大大减少\n，最重要的是 model 定义的时候，我们的 pretrained 需要设置成已经训练出来的纯浮点模型的地址。\n\n做完这些简单的调整之后，就可以开始训练我们的量化模型了。\n\n\n模型验证#\n\n模型训练完成之后，我们还可以验证训练出来的模型性能。由于我们提供了float和qat两阶段的训练过程，相应的我们可以验证这两个阶段训练出来的模型性能，只需要相应\n的运行以下两条命令即可：\n\n\n\n同时，我们还提供了quantization模型的性能测试，只需要运行以下命令：\n\n\n\n这个显示出来的精度才是最终的int8模型的真正精度，当然这个精度和qat验证阶段的精度应该是保持十分接近的。\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n结果可视化#\n\n如果你希望可以看到训练出来的模型对于单帧雷达点云的检测效果，我们的tools文件夹下面同样提供了点云预测及可视化的脚本，你只需要运行以下脚本即可：\n\n\n\n\n模型检查和编译#\n\n在训练完成之后，可以使用 compile 的工具用来将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在BPU上的运行性能，可以采用以下脚本：\n\n","routePath":"/guide/advanced_content/hat/examples/pointpillars_v2_no_hat_trainer","lang":"zh","toc":[{"text":"数据集准备","id":"数据集准备","depth":2,"charIndex":218},{"text":"浮点模型训练","id":"浮点模型训练","depth":2,"charIndex":928},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":964},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1590},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1673},{"text":"量化模型训练","id":"量化模型训练","depth":2,"charIndex":2167},{"text":"quantize参数的值不同","id":"quantize参数的值不同","depth":3,"charIndex":2310},{"text":"训练策略不同","id":"训练策略不同","depth":3,"charIndex":2462},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":2640},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":2844},{"text":"结果可视化","id":"结果可视化","depth":3,"charIndex":2903},{"text":"模型检查和编译","id":"模型检查和编译","depth":3,"charIndex":2989}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":344,"title":"PwcNet光流预测模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在光流数据集 FlyingChairs 上从头开始训练一个 PwcNet 模型，包括浮点、量化和定点模型。\n\nFlyingChairs 是光流预测中用的比较多的数据集，很多先进的光流预测研究都会优先基于这个数据集做好验证。\n开始训练模型之前，第一步是准备好数据集，这里我们下载官方的数据集 FlyingChairs.zip 作为训练和验证集。 同时需要下载相应的标签数据\nFlyingChairs_train_val.txt，解压缩之后数据目录结构如下所示：\n\n\n\n\n训练流程#\n\n如果你只是想简单的把 PwcNet 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。 在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n为了提升训练速度，我们对原始的数据集做了一个打包，将其转换为 LMDB 格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集，打包完成之后，目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\n在网络开始训练之前，你可以使用以下命令先计算一下网络的计算量和参数数量：\n\n\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n由于HAT算法包使用了注册机制，使得每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。 train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是\n\"float\"、\"calibration\"、\"qat\"，分别可以完成浮点模型、量化模型的训练，其中量化模型的训练依赖于上一步浮点训练产出的浮点模型。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为\nFloat、Calibration、QAT 和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\"、\"qat\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示。\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nPwcNet 的网络结构可以参考 论文 和 社区TensorFlow版本，这里不做详细介绍。 我们通过在 config 配置文件中定义 model 这样的一个\ndict 型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n模型除了 backbone 之外，还有 head和 losses 模块，在PwcNet中，backbone主要是提取两张图像的特征，head\n主要是由特征来得到预测的光流图。 losses 部分采样论文中的LnNormLoss来作为训练的 loss，loss_weights是特征层对应的 loss\n和权重。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 data_loader 和 val_data_loader 这两个 dict\n来实现的，分别对应着训练集和验证集的处理流程。 以 data_loader 为例，数据增强使用了\nRandomCrop、RandomFlip、SegRandomAffine 和 FlowRandomAffineScale。\n\n\n\n因为最终跑在 BPU 上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式，所以HAT提供 BgrToYuv444\n的数据增强来将 RGB 转到 YUV444 的格式。 为了优化训练过程，HAT使用了 batch_processor，可将一些增强处理放在\nbatch_processor 中优化训练：\n\n\n\n其中 loss_collector 是一个获取当前批量数据的 loss 的函数。\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n\n\n训练策略#\n\n在 FlyingChairs 数据集上训练浮点模型使用 Cosine 的学习策略配合 Warmup，以及对 weight 的参数施加L2 norm。\nconfigs/opticalflow_pred/pwcnet/pwcnet_pwcnetneck_flyingchairs.py 文件中的\nfloat_trainer，calibration_trainer，qat_trainer，int_trainer 分别对应浮点、量化、定点模型的训练策略。\n下面以 float_trainer 训练策略示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练\n章节的内容。这里主要讲一下HAT的光流预测中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，HAT在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。因为有浮点预训练模型，所以量化训练的学习率 Lr 可以很小，一般可以从 0.001\n或 0.0001 开始，并可以搭配 StepLrUpdater 做1-2次 scale=0.1 的 Lr 调整；同时训练的长度不用很长。此外 weight\ndecay 也会对训练结果有一定影响。\n\nPwcNet 示例模型的量化训练策略可见\nconfigs/opticalflow_pred/pwcnet/pwcnet_pwcnetneck_flyingchairs.py 文件。","routePath":"/guide/advanced_content/hat/examples/pwcnet","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":254},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":395},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":579},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":936},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":982},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1228},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1284},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1343},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1570},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1627},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1902},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2344},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":2606}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":345,"title":"QCNet轨迹预测模型训练","content":"#\n\n这篇教程主要是告诉大家如何在数据集 Argoverse 2 上从头开始训练一个 QCNet 模型，包括浮点、量化和定点模型。\nQCNet是一个轨迹预测模型，可以参考论文Query-Centric Trajectory Prediction.\n\n\n训练流程#\n\n如果你只是想简单的把 QCNet 的模型训练起来，那么可以首先阅读一下这一章的内容。 和其他任务一样，对于所有的训练，评测任务，HAT统一采用 tools +\nconfig 的形式来完成。在准备好原始数据集之后，可以通过下面的流程，方便地完成整个训练的流程。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，可以在 Argoverse 2 数据集 下载。 需要下载：Training , Validation 和\nTest。\n\n下载后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集，打包完成之后, 目标目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 val_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n由于QCNet采用相对时空编码，引入了时空不变性，在模型部署的时候我们可以复用历史帧编码的特征。\n为了方便模型评测，这里我们使用一个额外的模型用于生成历史帧的特征，类似地，可以使用以下命令导出模型：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点和量化模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration 和\nQat 的指标， 前者可以得到浮点模型的指标、后两者分别为量化校准和量化训练得到模型的指标。\n\n\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nQCNet 的网络结构可以参考 论文 ，这里不做详细介绍。 我们通过在 config 配置文件中定义 model 这样的一个 dict\n型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n\n数据加载#\n\n跟 model 的定义一样，训练和验证阶段的 dataloader 是在 config 配置文件中定义 data_loader 和\nval_data_loader 这两个 dict 来实现的，分别对应着训练集和验证集的处理流程。\n\nQCNet并没有添加复杂的数据增强，因此 transforms=None. collate_fn 定义了如何将单个数据整合为batch,\n其中包含一些数据对齐操作。\n\n\n\nconfig 里还定义了 batch_processor 对batch进行处理：\n\n\n\n其中 batch_processor 中传入一个 loss_collector 函数，用于获取当前批量数据的 loss ，如下所示：\n\n\n\n\n训练策略#\n\n首先介绍QCNet 在 Argoverse2 数据集上训练浮点模型的策略。我们使用 AdamW 作为优化器，设置 lr=5e-4,\nweight_decay=1e-4，但是不对 bias 、 nn.Embedding 以及归一化层施加 weight decay， 因此此处使用\ncustom_param_optimizer 能够简单地达到这一设置。我们使用 Cosine 的学习率更新策略，并将 warmup\n长度设置为1个epoch，模型共训练64个epoch。 下面为config中 float_trainer 的完整配置示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练\n章节的内容。这里主要讲一下中如何定义和使用量化模型。\n\nQCnet 示例模型的量化训练策略可见 configs/traj_pred/qcnet_oe_argoverse2.py 文件，主要分为量化校准\ncalibration 和量化训练 qat 两个阶段。\n\n量化校准 calibration 的配置为:\n\n\n\n量化训练 qat 的配置为:\n\n\n\n其中 float2calibration 和 float2qat 分别定义了浮点到校准模型以及浮点到量化训练模型的模型转换过程；\ncali_qconfig_setter 和 qat_qconfig_setter\n分别为calibration模型和qat模型对应的qconfig设置，关于qconfig的设置方法与调试步骤，比如默认qconfig的设置、量化敏感算子设置等，\n请阅读 量化感知训练-Qconfig详解 章节的内容。\n\n\n量化敏感度算子排序#\n\n我们随checkpoint文件夹提供了 \"output_prob_L1_sensitive_ops.pt\" 和\n\"output_pred_L1_sensitive_ops.pt\" 文件，如果你想自己计算得到这个敏感度文件的话，需要进行以下流程：\n\n首先使用default模板训练一个 calibration 模型， 设置为：\n\n\n\n然后训练一个qconfig为int8默认模板的calibration模型，将保存的calibration模型重命名为\n\"calibration-checkpoint-best-defaultQconfig.pth.tar\".\n\n接着调用敏感度分析工具，逐层比较float模型和calibration模型的相似度，命令为：\n\n\n\n这条命令对应config中这段配置：\n\n\n\n其中的关键步骤，请阅读 量化感知训练-精度调优工具使用指南 章节的内容。","routePath":"/guide/advanced_content/hat/examples/qcnet","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":125},{"text":"数据集准备","id":"数据集准备","depth":3,"charIndex":264},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":558},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":701},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":850},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1025},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1082},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1141},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1368},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1425},{"text":"数据加载","id":"数据加载","depth":3,"charIndex":1527},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":1851},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":2129},{"text":"量化敏感度算子排序","id":"量化敏感度算子排序","depth":3,"charIndex":2591}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":346,"title":"执行脚本","content":"#\n\n在HAT中，可以直接使用和修改的功能主要包含tools和configs两部分的内容。其中tools主要是核心的功能模块，包含训练验证可视化等。而configs主\n要是包含功能模块执行过程中可以配置的选项和参数。\n\n这篇教程告诉大家tools主要包含哪些核心功能，以及config的开发规范和使用流程。\n\n大部分情况下tools工具的执行模式都是需要输入一个config，除了部分与数据集或者单张图可视化相关的工具。因此通用的执行范式可以总结成这样：\n\n\n\n这里主要介绍一下tools的核心功能和对外接口。\n\n\ntools#\n\n目前常用的tools有多个python脚本，每个脚本具有不同的功能。\n\ntrain.py 是训练工具，常用的参数为：\n\n参数                                         描述\n--stage {float, calibration, qat}          不同的训练和预测阶段。\n--config CONFIG, -c CONFIG                 config文件路径。\n--device-ids DEVICE_IDS, -ids DEVICE_IDS   运行的GPU列表。\n--dist-url DIST_URL                        多机运行指定的server地址，默认是auto。\n--launcher {torch}                         多机运行的启动方式。\n--pipeline-test                            是否运行pipeline测试。\n--opts                                     通过命令行修改config参数。\n--opts-overwrite                           是否可以覆盖config参数。\n--level                                    除了rank0之外的logging level。\n\npredict.py 是预测工具，常用的参数为：\n\n参数                                             描述\n--stage {float, calibration, qat, int_infer}   不同的训练和预测阶段。\n--config CONFIG, -c CONFIG                     config文件路径。\n--device-ids DEVICE_IDS, -ids DEVICE_IDS       运行的GPU列表。\n--dist-url DIST_URL                            多机运行指定的server地址，默认是auto。\n--backend                                      多机或者多卡通信使用的backend。\n--ckpt                                         预测使用的checkpoint文件。\n--launcher {torch}                             多机运行的启动方式。\n--pipeline-test                                是否运行pipeline测试。\n\nmodel_checker.py 是检查模型是否能够正常在BPU运行的工具，常用参数有：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n\nvalidation_hbir.py 是精度验证工具，提供上板完全对齐的结果。常用参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n--stage {align_bpu}          不同验证阶段。\n\ncalops.py 是网络计算量统计工具，常用参数列别为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n--input-shape                输入shape。\n--method                     使用计算的方式。\n\ncompile_perf_hbir.py 是编译和perf工具，常用参数列表分别为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件目录。\n--opt {0,1,2}                编译时的优化选项。\n--jobs JOBS                  编译时的线程数。\n--model_path MODEL_PATH      qat hbir 模型路径。\n\ninfer_hbir.py 是用来做单图预测的工具。常用的参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n--model-inputs               指定的模型输入。\n--save-path                  可视化结果的保存路径。\n--use-dataset                是否使用数据集的数据。\n\ncreate_data.py: 用来预处理Kitti3D雷达数据集。常用的参数列表为：\n\n参数           描述\n--dataset    数据集名称。\n--root-dir   数据集所在根目录。\n\nexport_onnx.py 用来将模型导出为ONNX（只用于可视化，不支持推理）。常用的参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n\nexport_hbir.py 用来将模型导出为hbir。常用的参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n--save-path                  hbir保存的路径。\n\ngen_camera_param_nusc.py: 从nuscenes中获取相机内外参的脚本。常用的参数列表为：\n\n参数               描述\n--data-path      数据集所在根目录。\n--save-path      保存的路径。\n--save-by-city   是否根据城市保存。\n--version        数据集版本。\n\ngen_reference_points_nusc.py: 从nuscenes中获取模型输入参考点的脚本。常用的参数列表为：\n\n参数                           描述\n--data-path                  数据集所在根目录。\n--save-path                  保存的路径。\n--save-by-city               是否根据城市保存。\n--version                    数据集版本。\n--config CONFIG, -c CONFIG   config文件路径。\n\nhomography_generator.py: 计算ego到图像的转换矩阵的脚本。常用的参数列表为：\n\n参数                         描述\n--sensor2ego-translation   传感器到ego坐标系的平移矩阵。\n--sensor2ego-rotation      传感器到ego坐标系的旋转矩阵。\n--camera-intrinsic         相机内外参矩阵。\n--save-path                保存的路径。\n\nreference_points_generator.py: 从单应性矩阵计算模型输入参考点的脚本。常用的参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n--homography                 转换矩阵路径。\n--save-path                  保存的路径。\n\nquant_analysis.py: 量化训练分析工具。常用的参数列表为：\n\n参数                           描述\n--config CONFIG, -c CONFIG   config文件路径。\n\ndatasets 目录是数据集相关的打包和可视化工具。","routePath":"/guide/advanced_content/hat/examples/scripts","lang":"zh","toc":[{"text":"tools","id":"tools","depth":2,"charIndex":257}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":347,"title":"UNet分割模型训练","content":"#\n\n作为HAT分割任务的示例，这篇教程主要向大家展示如何使用HAT在 Cityscapes 数据集上训练一个 state-of-the-art 的浮点和定点模型。\n\nCityscapes 是一个城市驾驶场景的图像数据集，包含了5000张具有像素级标注的图像，图像中的物体被分为19个类别。\n分割任务相对复杂，对模型能力的要求较高，使用小模型在分割任务上取得较好的指标并不是很轻易的事。 本教程将从零开始，详细描述如何使用HAT在 Cityscapes\n数据集上训练一个 state-of-the-art 的分割模型，并在浮点模型基础上进行量化训练，最终得到一个定点模型。\n\n\n训练流程#\n\n\n数据集下载#\n\n要下载 Cityscapes 数据集，首先需要在 官方网站 注册一个账号。\n\n之后便可在 下载页面 下载需要的数据集文件， 这里我们只需要 gtFine_trainvaltest.zip 和\n*[leftImg8bit_trainvaltest.zip 两个文件。\n\n同时， Cityscapes 数据集官方还提供了一个脚本用于数据的下载和处理，见 Github链接 。 首先使用如下命令安装官方工具：\n\n\n\n然后使用官方工具下载所需数据集文件（注意，使用此工具下载仍需要登录上面注册的账号）。\n\n\n\n最后对下载好的文件进行解包即可（可选）：\n\n\n\n\n数据集打包#\n\n为了高效地读取数据，我们推荐预先将数据集打包为 LMDB 格式。 HAT提供了 cityscapes_packer.py\n脚本来方便地将数据集从原始公开的格式转换为 numpy.ndarray或 torch.Tensor ，使用 msgpack 对数据进行封装，并最终打包为\nLMDB 文件。\n\n数据集打包的命令如下：\n\n\n\n生成的lmdb文件保存在 ${data-dir}/train_lmdb 和 ${data-dir}/val_lmdb 路径下。\n\n\n模型训练#\n\n将数据集打包为 LMDB 文件后，就可以开始模型的训练了。HAT提供了 train.py训练脚本来方便地配合 config 文件实现模型训练。\n\n模型训练的命令如下，开始训练之前请确保将 unet_mobilenetv1_cityscapes.py\n配置文件中的数据集路径（data_rootdir）设置为数据集 LMDB 文件所在位置。\n\n\n\n以上命令分别完成浮点模型和定点模型的训练，其中定点模型的训练需要以训练好的浮点模型为基础，具体内容请阅读 量化感知训练 章节的内容。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似，--stage 后面的参数为 \"float\"、\"calibration\" 时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n\n模型结构#\n\n\n\n分割模型主要由 backbone， neck ， head 三部分组成，这里我们使用了 MobileNetV1_0.25\n作为backbone，MobileNet是一种轻量高效的网络结构。neck则使用了 Unet 结构，这种结构可以综合各个尺度上的\nfeaturemap，保留精细的空间信息。 head 为一个卷积层，负责输出最终的分割结果。\n\n我们使用了 FocalLoss 作为损失函数。 FocalLoss 可视为一种动态权重的交叉熵损失函数，可以较好地解决类别不平衡带来的训练困难的问题。\n\nUnet 的层次化结构和 FPN 的思想一致，很适合采用相同的训练方式，即在 Unet 的每个 scale 上都构造一个输出， 将此输出和对应尺寸的\nground truth 构造损失函数进行训练，通过对网络的各个尺度都进行监督，为网络训练提供更加丰富的参考信息，降低训练难度，提高训练速度和最终精度。\n同时考虑到我们需要的最终结果是最大尺寸网络输出(scale=4)，为了避免其他尺寸的梯度过大影响到最大尺寸输出的精度，我们按照 scale\n为损失函数增加了相应的权重，scale 越大的层权重越小。\n\n通常在定义好一个模型之后，尤其是一些公版模型，我们会有检查计算量的需求。HAT提供了 calops.py 来计算模型的计算量，命令如下：\n\n\n\n这种计算量的统计工具，是可以同时支持浮点和定点模型的。\n\n\n数据预处理#\n\n首先，我们使用 LabelRemap 将数据标签重映射至 [0, 18] 的区间。\n\n对于训练集， SegRandomAffine 可以对图片进行随机仿射变换来进行数据增强，我们只配置了随机的缩放，没有做任何旋转操作。\n\n由于训练采用了类 FPN 的方式，我们需要将标签缩放至不同的大小，用于模型不同 scale 的训练。\n\n因为最终跑在BPU上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的格式， 所以HAT提供 ImageBgrToYuv444\n的数据增强来将 RGB 数据转到 YUV444 的格式。\n\n最后，归一化对于深度学习模型训练是必须的。\n\n值得注意的是，这里我们使用了 MultiBatchProcessor 进行任务的训练，此 Processor 支持在GPU上以 batch\n为单位对数据做预处理。 由于分割任务的数据预处理过程相对复杂，若使用CPU来做，会出现处理瓶颈。使用GPU预处理数据会提高显存使用量，导致最大 batch\nsize 降低。 但即使如此，最终的训练速度也有可观的提升。\n\n\n\n验证集的数据预处理与训练集相比不需要做随机的仿射变换和多尺度缩放，其他步骤一致，不再赘述。\n\n\n训练策略#\n\n分割任务可以看作是像素级的分类任务，因此训练策略也和分类任务高度相似，首先在保证可以收敛的前提下尽可能调大学习率可以提高训练速度；\n在使用某一学习率训练至精度不再增长时，适当将学习率调小，模型便可继续收敛，精度进一步提升；\n最终训练完成后对测试集精度和训练集精度进行比较，若训练集精度高于测试集过多，则可以认为模型出现了过拟合，此时适当增大 weight decay\n可以增强模型的泛化能力，减少过拟合，从而获得更高的测试集精度。\n\n\n\n\n量化训练策略#\n\n量化训练的目的是在训练好的浮点模型基础上对数据进行模拟量化，来模拟定点计算的过程，使得经过量化训练的模型转为定点模型时的精度损失降到最低。\n\n由于浮点模型经过了充分的训练已经收敛到了较优的状态，量化训练通常只需要对模型进行微调即可，学习率不能设置过大，可以从 1e-4\n的数量级开始尝试，其他参数一般和浮点训练一致即可。同样地，微调模型所需的训练量较低，一般训练几十个 epoch 就可以了。\n\n由于量化训练之前模型已经有一个不错的精度，精度提升的区间较小，且对数据进行了模拟量化，会导致训练过程波动较大。\n此时需要一些耐心，认真观察，从波动中看出趋势，适量调整参数，才能取得最好的结果。\n\n\n预训练模型#\n\nHAT已经提供了此例子的预训练模型，所有模型都在发布包中。","routePath":"/guide/advanced_content/hat/examples/segmentation","lang":"zh","toc":[{"text":"训练流程","id":"训练流程","depth":2,"charIndex":288},{"text":"数据集下载","id":"数据集下载","depth":3,"charIndex":296},{"text":"数据集打包","id":"数据集打包","depth":3,"charIndex":580},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":817},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":1066},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":1112},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1349},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1406},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1465},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1692},{"text":"模型结构","id":"模型结构","depth":3,"charIndex":1700},{"text":"数据预处理","id":"数据预处理","depth":3,"charIndex":2315},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2848},{"text":"量化训练策略","id":"量化训练策略","depth":3,"charIndex":3075},{"text":"预训练模型","id":"预训练模型","depth":2,"charIndex":3381}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":348,"title":"StereoNet双目深度估计模型训练","content":"#\n\n这篇教程主要是告诉大家如何利用HAT在数据集 SceneFlow 上从头开始训练一个 StereoNet 模型，包括浮点、量化和定点模型。\n\n\n数据集准备#\n\n在开始训练模型之前，第一步是需要准备好数据集，可以在 SceneFlow 数据集 下载， 同时需要准备训练数据和验证数据集对应的文件列表，可以从 此处 下载\nSceneFlow_finalpass_train.txt 和 SceneFlow_finalpass_test.txt 。\n\n下载后，解压并按照如下方式组织文件夹结构：\n\n\n\n为了提升训练的速度，我们对数据信息文件做了一个打包，将其转换成lmdb格式的数据集。只需要运行下面的脚本，就可以成功实现转换：\n\n\n\n上面这两条命令分别对应转换训练数据集和验证数据集，打包完成之后，${target-data-dir} 目录下的文件结构应该如下所示：\n\n\n\ntrain_lmdb 和 test_lmdb 就是打包之后的训练数据集和验证数据集，接下来就可以开始训练模型。\n\n\n模型训练#\n\n在网络开始训练之前，你可以使用以下命令先计算一下网络的计算量和参数数量：\n\n\n\n下一步就可以开始训练。训练也可以通过下面的脚本来完成，在训练之前需要确认配置中数据集路径是否已经切换到已经打包好的数据集路径。\n\n\n\n由于HAT算法包使用了注册机制，使得每一个训练任务都可以按照这种 train.py 加上 config 配置文件的形式启动。 train.py\n是统一的训练脚本，与任务无关，我们需要训练什么样的任务、使用什么样的数据集以及训练相关的超参数设置都在指定的 config 配置文件里面。 上面的命令中\n--stage 后面的参数可以是 \"float\" 、 \"calibration\" 、 \"qat\"\n，分别可以完成浮点模型、量化模型的训练，其中量化模型的训练依赖于上一步浮点训练产出的浮点模型。\n\n\n导出定点模型#\n\n完成量化训练后，便可以开始导出定点模型。可以通过下面命令来导出：\n\n\n\n\n模型验证#\n\n在完成训练之后，可以得到训练完成的浮点、量化或定点模型。和训练方法类似，我们可以用相同方法来对训好的模型做指标验证，得到为 Float 、Calibration\n、 QAT 和 Quantized 的指标，分别为浮点、量化和完全定点的指标。\n\n\n\n和训练模型时类似， --stage 后面的参数为 \"float\" 、 \"calibration\"、 \"qat\"\n时，分别可以完成对训练好的浮点模型、量化模型的验证。\n\n定点模型精度验证可使用下面命令，但需要注意是必须要先导出hbir：\n\n\n\n\n模型推理#\n\nHAT 提供了 infer_hbir.py 脚本提供了对定点模型的推理结果进行可视化展示：\n\n\n\n\n仿真上板精度验证#\n\n除了上述模型验证之外，我们还提供和上板完全一致的精度验证方法，可以通过下面的方式完成：\n\n\n\n\n定点模型检查和编译#\n\n在HAT中集成的量化训练工具链主要是为了地平线的计算平台准备的，因此，对于量化模型的检查和编译是必须的。\n我们在HAT中提供了模型检查的接口，可以在定义好量化模型之后，先检查能否在 BPU 上正常运行：\n\n\n\n在模型训练完成后，可以通过 compile_perf_hbir 脚本将量化模型编译成可以上板运行的 hbm 文件，同时该工具也能预估在 BPU 上的运行性能：\n\n\n\n以上就是从数据准备到生成量化可部署模型的全过程。\n\n\n训练细节#\n\n在这个说明中，我们对模型训练需要注意的一些事项进行说明，主要为 config 的一些相关设置。\n\n\n模型构建#\n\nStereoNet 的网络结构可以参考 论文 ，这里不做详细介绍。 我们通过在 config 配置文件中定义 model 这样的一个 dict\n型变量，就可以方便的实现对模型的定义和修改。\n\n\n\n模型除了 backbone 之外，还有 head 、 post_process 、 losses 模块，在 StereoNet 中，backbone\n主要是提取图像的特征，head 主要是由特征来得到预测的视差值。 post_process 主要是后处理部分，losses 模块采用论文中的\nSmoothL1Loss 作为训练的 loss，loss_weights 是对应的 loss 的权重。\n\n\n数据增强#\n\n跟 model 的定义一样，数据增强的流程是通过在 config 配置文件中定义 data_loader 和 val_data_loader 这两个 dict\n来实现的，分别对应着训练集和验证集的处理流程。 以 data_loader 为例，数据增强使用了 RandomCrop、 ToTensor 和\nNormalize 来增加训练数据的多样性，增强模型的泛化能力。\n\n因为最终跑在 BPU 上的模型使用的是 YUV444 的图像输入，而一般的训练图像输入都采用 RGB 的形式，所以HAT提供 BgrToYuv444\n的数据增强来将 RGB 转到 YUV444 的格式。\n\n\n\nbatch_processor 中传入一个 loss_collector 函数，用于获取当前批量数据的 loss，如下所示：\n\n\n\n验证集的数据转换相对简单很多，如下所示：\n\n\n\n\n\n\n训练策略#\n\n在 SceneFlow 数据集上训练浮点模型使用 Cosine 的学习策略配合 Warmup， 以及对 weight 的参数施加 L2 norm。\nconfigs/disparity_pred/stereonet/stereonet_stereonetneck_sceneflow.py 文件中的\nfloat_trainer， calibration_trainer， qat_trainer，int_trainer 分别对应浮点、量化、定点模型的训练策略。\n下面为 float_trainer 训练策略示例：\n\n\n\n\n量化训练#\n\n关于量化训练中的关键步骤，比如准备浮点模型、算子替换、插入量化和反量化节点、设置量化参数以及算子的融合等，请阅读 量化感知训练 章节的内容。 这里主要讲一下\nHAT 的双目深度估计中如何定义和使用量化模型。\n\n在模型准备的好情况下，包括量化已有的一些模块完成之后，HAT在训练脚本中统一使用下面的脚本将浮点模型映射到定点模型上来。\n\n\n\n量化训练的整体策略可以直接沿用浮点训练的策略，但学习率和训练长度需要适当调整。 因为有浮点预训练模型，所以量化训练的学习率 Lr\n可以很小，一般可以从0.001或0.0001开始，并可以搭配 StepLrUpdater 做1-2次 scale=0.1 的 Lr 调整；\n同时训练的长度不用很长。此外 weight decay 也会对训练结果有一定影响。\n\nStereoNet 示例模型的量化训练策略可见\nconfigs/disparity_pred/stereonet/stereonet_stereonetneck_sceneflow.py 文件。","routePath":"/guide/advanced_content/hat/examples/stereonet","lang":"zh","toc":[{"text":"数据集准备","id":"数据集准备","depth":2,"charIndex":74},{"text":"模型训练","id":"模型训练","depth":3,"charIndex":445},{"text":"导出定点模型","id":"导出定点模型","depth":3,"charIndex":807},{"text":"模型验证","id":"模型验证","depth":3,"charIndex":853},{"text":"模型推理","id":"模型推理","depth":3,"charIndex":1106},{"text":"仿真上板精度验证","id":"仿真上板精度验证","depth":3,"charIndex":1163},{"text":"定点模型检查和编译","id":"定点模型检查和编译","depth":3,"charIndex":1222},{"text":"训练细节","id":"训练细节","depth":2,"charIndex":1449},{"text":"模型构建","id":"模型构建","depth":3,"charIndex":1506},{"text":"数据增强","id":"数据增强","depth":3,"charIndex":1811},{"text":"训练策略","id":"训练策略","depth":3,"charIndex":2202},{"text":"量化训练","id":"量化训练","depth":3,"charIndex":2469}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":349,"title":"执行引擎","content":"#\n\n在上一小节框架的介绍中，可以发现所有的 Data，Model，Callback\n等子模块构建完成之后，最终都会统一放到Engine中来完成执行。因此Engine是整个HAT的执行引擎，其重要性不言而喻。\n\n在HAT中，Engine定义了训练和预测的整个Pipeline。对于任何一个深度学习的项目，对给定的模型完成训练和预测的任务是必须要完成的事情。因此本小节主要介\n绍HAT中的Engine模块是如何实现的。\n\n\nEngine的执行流程#\n\n整个HAT的Engine中，最基础的PipeBase定义了所有的Callbacks可操作的运行阶段，而LoopBase定义了所有Engine的基础执行流程。如上\n图所示，整个Engine的执行流程也就是由丰富的Callbacks和与模型相关的Processor处理两部分组成。\n\n所有Callbacks可操作的运行阶段，总共可以分为on_loop_begin，on_epoch_begin，on_step_begin，on_batch_be\ngin，on_batch_end，on_step_end，on_epoch_end，on_loop_end八个阶段。八个阶段的执行顺序如上图所示，您可以根据自己\n的需要，在不同的阶段使用不同的Callback，当然同一个Callback也可以用在不同的阶段。比如常见的LrUpdater可以在on_epoch_begin和\non_step_begin这两个阶段使用，其他部分Callback的作用范围亦可如图所示。\n\nBatchProcessor则负责当前Batch中数据和模型的运行方式，包含模型常见的基本操作，如forward和backward等。除此之外，还有部分grad\n的更新操作也会被定义在这里。需要说明的是，在一些复杂的任务训练过程中，BatchProcessor也要求能做到更多轮次的迭代和更加丰富的grad操作。\n\n\nEngine结构#\n\n以LoopBase为基础，可以派生出丰富的执行引擎，如上图的Engine关系图所示。\n\n按照功能划分，LoopBase可以派生出以训练为主的Trainer和以预测为主的Predictor。\n\n * Trainer：负责所有和训练相关的流程，一般的深度学习相关的训练都需要用到。\n\n * Predictor：负责和预测相关的流程，常用的使用场景如Validation等。\n\n按照执行方式的不同，训练方式可以派生出不同的Trainer，如以torch.nn.parallel.DistributedDataParallel为基础的Dis\ntibutedDataParallelTrainer，以torch.nn.DataParallel为基础的DataParallelTrainer等等。不同的执行\n方式也需要不同的启动方式，具体可以参考不同Trainer中的launcher。","routePath":"/guide/advanced_content/hat/framework/engine","lang":"zh","toc":[{"text":"Engine的执行流程","id":"engine的执行流程","depth":2,"charIndex":210},{"text":"Engine结构","id":"engine结构","depth":2,"charIndex":813}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":350,"title":"框架","content":"#\n\n\n核心模块#\n\n上图为HAT框架的整体组织的抽象流程图，可以看到HAT的训练和验证流程是由四大核心模块组成的，分别为 Data\n，Model，Callback，Engine。这里先分别简单地介绍一下这些核心模块。\n\n * Data：负责HAT中所有的数据生产流程，包括负责迭代输出的Dataset，负责各项任务数据增强的Transforms，负责数据串联和打包batch的Co\n   llate，以及负责数据采样流程的Sampler。所有的数据生产流程最终通过Dataloader的接口统一的组织起来。\n\n * Model：负责HAT中所有模型的搭建流程。在HAT中，模型一般分为backbone，neck，head或者task\n   module等子模块，由统一的structure来负责串联所有的子模块完成搭建最终的模型。structure中除了有常见的任务之外，还有GraphMode\n   l专门用来处理多任务相关的模型结构搭建。\n\n * Callback：负责在Engine执行的过程中，来动态调整训练状态的模块。其定位类似于Torch中模型的Hooks，可以在不修改Engine代码的情况下\n   ，根据Engine提供的训练状态，在指定的可修改位置上动态的调整。整个Engine的可修改位置主要包括：on_loop_begin(end)，on_epo\n   ch_begin(end)，on_step_begin(end)，on_batch_begin(end)。\n\n * Engine：主要负责训练或者预测的流程搭建和执行，其中训练的模块为Trainer，预测的模块为Predictor。所有的其他模块，如Data，Model\n   ，Callback等都会在构建完成之后输入到Engine中来，由Engine统一调度，完成训练或者预测的全部流程。\n\n除了核心的四大模块之外，还有其他一些辅助的模块，如 Profiler ，Metric，Visualize等。\n\n * Profiler：作为HAT的prof工具，主要用来辅助定位训练或者验证过程中出现的速度瓶颈。\n\n * Metric：主要用来做数据集训练或者测试过程中的指标验证，它其实是Model的一种特殊情况，和具体的数据集强绑定。\n\n * Visualize：主要是用来完成相关数据集的可视化工作。\n\n\n构建训练流程#\n\n1.针对任意一个数据集，构建Data需要的所有子模块。\n\n首先搭建用于迭代输出的Dataset，在迭代输出中通过Transform对数据进行处理，最常见的就是训练中的数据增强操作，测试中的数据预处理等。通过Sample\nr采样器控制Dataset的输出顺序，最终用Collate逐个串联，并最终完成一个Batch训练数据的打包。由DataLoader统一完成所有流程调度，并将Ba\ntch的训练数据作为结构，输入到训练流程中。\n\n2.针对任意一个模型，构建Model所需要的所有子模块，如Backbone，Neck等。\n\n使用Structure将所有的子模块串联在一起，形成一个完整的带训练状态的模型，这个模型也将作为训练对象输入到训练流程中。\n\n3.针对训练的任务，选择或者定义合适的Callback来动态调整训练过程中的训练状态。\n\n如在每次训练中定时输出训练的结果，或者每次训练中动态调整训练的学习率。虽然Callback的定义和Engine是分开的，但执行流程是嵌入到Engine的完整流程\n中。\n\n4.针对训练的环境，构建合适的Engine作为训练引擎。\n\n如常见的多卡训练环境可以选择DistributedDataParallelTrainer或者DataParallelTrainer。Engine可以将所有已经构\n建的模块，包括Data,Model，或者其他模块如Callback，Metric，Profiler统一的组织在一起，完成训练所需要全部环境的初始化。需要注意的是\n，Engine里面的模块并不是所有的都是必须的。\n\n5.最后，统一使用选择好的Engine中的fit接口，完成训练的全部流程。\n\n以上就是HAT框架的整体结构和训练的抽象流程，本节开始的图不仅仅反应了构建的数据流，同时也包括模块和模块之间的调用关系。对于训练而言，最核心的部分应该就是Eng\nine，全面理解Engine的运行流程，也就可以理解整个HAT的数据流。","routePath":"/guide/advanced_content/hat/framework/framework","lang":"zh","toc":[{"text":"核心模块","id":"核心模块","depth":2,"charIndex":3},{"text":"构建训练流程","id":"构建训练流程","depth":2,"charIndex":992}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":351,"title":"常见问题","content":"#\n\n\nDocker 容器中无法 import hat？#\n\n若您在 GPU Docker 中运行示例提示 GPU RuntimeError，或者加载 hat 出现 ModuleNotFoundError: No\nmodule named 'hat'、Segmentation fault (core dumped) 等类似报错，那么可以按照如下两个方向进行排查：\n\n 1. 使用 OE 包中提供的 run_docker.sh 脚本启动 GPU Docker 容器;\n 2. 在容器中执行如下脚本来检测 torch 和 cuda 是否正常。若 CUDA 无法调用，可以使用 nvidia-smi 检查下驱动版本是否符合要求。因为\n    OE 包自 v1.1.60 版本开始，升级了 torch 环境为 1.1.30+cu116，对应的驱动要求可见：环境部署 - Docker容器部署。\n\n\n\n\n如何找到 config 文件中模块的具体实现？#\n\n模块对应的实现为 type 对应的 value，可将 type 值理解为 class，在 build model 时会实例化该类，\n类的路径可通过以下两种方式获取：\n\n 1. 各模块的具体实现存放在 hat/models/模块名/，也可以在 init.py 中查看该路径下是否包含该 class， 例如：在 bev_mt_ipm\n    模型的配置文件中，neck 的 dict[\"type\"] 为 \"FastSCNNNeck\"， 则 FastSCNNNeck 的实现在\n    hat/models/necks/fast_scnn.py； 另外，特定 task 的模块定义会在 task_modules\n    下（head，后处理，decoder）， 例如：检测分支的 head 配置项为 \"CenterPoint3dHead\"，是 task_modules\n    包含部分， 则其实现在 hat/models/task_modules/centerpoint3d/head.py;\n 2. 使用 grep 命令查找，例如：\n\n\n\n\n为什么 config 能正常打印，但 launch trainer 失败？#\n\n例如，以下报错需要修改 config 中的 device_ids 为当前可使用的 GPU 资源；若报错包含 “CUDA out of memory”，则需修改\nbatch_size_per_gpu 为当前资源能支持的 batch size。\n\n\n\n\n为什么配置 data 运行的 device 不生效？#\n\n请在 hat/engine/processors/processor.py 中检查 to_cuda 是否成功， 不成功可能是因为 data 不是 tensor\n类型导致未调用起 device。\n\n\n为什么配置 trainer 的运行 device 不生效？#\n\ndevice 和 device_id 配置有优先级：如果使用默认的 train.py 的 launch 来调用的 trainer， 那么只有\ndevice-ids 是起作用的，device 配置不会起作用。 如果是直接调用的 trainer（predictor，loop base等），device\n可以直接起作用。\n\n\n如何设置验证/打印频率？#\n\n在 config 文件中有关于 callback 频率的参数，验证频率可以通过以下方式修改，其他 callback 相关频率都以 “xxx_frep”\n形式设置：\n\n\n\n\n为什么 plt.show 图像不显示？#\n\n可能是由于当前 Linux 系统无图形界面导致，可以在 hat/visualize 路径下各数据集对应代码中添加\nplt.savefig('./test.png',dpi=300)，保存在本地后再查看。\n\n\n如何 new dataset？#\n\ndataloader 内部类调用流程图如下所示（以 Coco 数据集为例），其中所有类和接口都在config中调用， 数据加载通过调用 dataloader\n实现，dataloader 加载的数据集 dataset 由 Coco 类返回，Coco 类读取的是lmdb 格式的打包数据，该数据通过调用 packer\n接口实现，实现流程：packer 调用 CocoDetectionPacker 类将 CocoDetection\n返回的数据（image，label…）打包成设置的格式（lmdb，mxrecord）。\n\n实现一个 dataset 需要对以上涉及的类重写，根据数据集的 format（也可将数据集的 format 调整为原 dataset 的格式）重写\n__getitem()__ 。\n\n\n如何恢复意外中断的训练？#\n\n可以通过在 config 的 {stage}_trainer 中配置 resume_optimizer 和 resume_epoch_or_step\n字段来恢复意外中断的训练，或仅恢复 optimizer 来进行 fine-tune， 例如在 config 文件的 trainer 中增加以下字段：\n\n\n\n\n如何可视化模型结构？#\n\n支持 hb_model_info 工具通过 -v 参数可视化 ；也可参考：如何可视化地平线的参考算法模型结构。\n\n\n如何修改 calibration 的方法？#\n\n可以修改 config 中的 calibration_trainer 字段，新增 qconfig_params 字典，参考如下：\n\n\n\n\nDETR模型是否支持 deformable conv？#\n\n该算子主要在 deformable-DETR 中的 deformable attention 结构中使用，地平线还暂不支持，但可以使用已经支持的\ngridsample 算子进行替代。\n\n\n在BEV 模型中，为什么 Lss 的精度低于 IPM？#\n\nLss 的输入分辨率是 256x704，低于 IPM 的 512x960，所以精度更低。\n\n\nBEV 模型中的 Lss 输入的约束条件是什么？#\n\nLss 在 grid_sample 前会做维度折叠，所以对 input_feature 的 h、w 有算子编译的约束条件，目前为：H,W ∈ [1, 1024]\n且 H*W ≤ 720*1024。\n\n\n如何解决BEV 模型中 Lss 的 mul(depth,feature) 耗时大？#\n\n可以先通过 grid_sample 算子将 featuremap 的 H、W 转换为 [128,128] 后再做 mul 计算。\n\n\nBEV 模型中 Lss 的 voxelpooling 如何实现？选取多少个点？#\n\n为了不遗失坐落在相同 voxel 中的点云特征，我们会对每个 voxel 采样 10 次， 并将每个点云特征相加得到 128x128x64 的 BEV\n特征图，对应代码如下：\n\n\n\n\n\n\nBEV 模型的 Lss 如何选择参考点？#\n\npoint 的生成在 _gen_reference_point，会将 feature 范围外的无效点置为较大的值。为了不取到无效点，会使用\ntopk（k=10，训练速度较快）将取值较小的前 10 个点进行集合。\n\n\nBEV 模型的 Lss 如何处理 gridsample 输入较大的情况？#\n\nLss 模型因为会在 gridsample 算子前将 3 个维度折叠为 1 个维度，因此其 H*W 容易超出 720*1024 的 BPU\n算子约束限制。此时建议在维度折叠（即 dfeat = dfeat.view(B, 1, -1, H * W)）前，先对可能超限的维度进行拆分，分别计算\ngridsample，最后再将结果叠加。\n\n\n为什么 BEV 模型的 gkt 精度指标较低？有什么优势？#\n\n基于 Transformer 的 gkt 精度低于 IPM 的主要原因是开源数据集的数据量不够，我们尝试加入一些业务数据后可以高于 IPM 的精度。而且 gkt\n模型的鲁棒性更好，能够解决相机偏移导致的精度影响。\n\n\nBEV 模型中 gkt 的 transfomer 是否是 global 的？#\n\n不是 global 的，选取的是 kernel（3x3） 的 feature 做注意力，因此是 kernel-transformer。\n\n\nBEV 模型是否支持公版 bevformer？#\n\nJ6已支持优化版bevformer。\n\n\nBEV模型检测和分割任务之间是否会制约精度？#\n\n会有影响，多任务中的检测任务精度会有所下降，需要通过训练策略做平衡。\n\n\nPointPillars 模型的 latency 包括哪几部分？#\n\nPointpillars 目前的 latency包含两部分：前一部分为前处理（voxel化），即点云输入后到 head 的时间；后一部分为后处理时间。\n\n\n点云量对 PointPillars 模型性能影响如何？#\n\n有效点云数越多则前处理耗时越长，具体到配置项为 Voxelization cfg 参数组：\n\n\n\n\nPointPillars 模型的前处理包含哪些操作？#\n\n前处理为 pillar（voxel）化，对应阶段为 voxeliza tation，流程示意图如下所示：\n\n\nPointPillars 模型支持几类任务检测？如何扩展？#\n\n目前 PointPillars_Kitti_Car 模型只支持 car 一类检测，另外我们也提供了多类别检测模型\ncenterpoint_pointpillar_nuscenes。\n\n\nPointPillars 的板端 hbm 模型的输入数据如何生成？如何预处理？#\n\n数据预处理包括 reshape 和 padding（padding 至（1,1,150000,4），其中 150000 为 bin\n文件中的最大点云量），参考代码如下所示：\n\n\n\n\n为什么 PointPillars 模型的板端 perf 数据与官方指标不一致？#\n\n在板端使用 hrt_model_exec perf 工具评估性能时请指定真实的 input_file，否则工具会采用随机生成的点云数据，可能导致 perf\n指标不准确。\n\n\nPointPillars 模型中，PTQ 方案是否支持对 Lidar 模型转换？#\n\n功能链路上是支持的，可以从训练框架中导出 ONNX 模型进行 PTQ 转换检查。 但是从既有经验看，Lidar 点云模型走 PTQ\n方案量化的精度风险较大，主要原因包括：点云比较稀疏，数据分布情况对量化不友好等。\n\n\ncenterpoint 和 pointpillars 能否互用？#\n\n两个模型可以互用，只需要对应修改数据集所涉及的相关配置，例如：点云范围、预测类别，以及后处理的 anchor、target 等，详情说明可见该技术贴\ncenterpoint使用kitti数据集过滤后数据集没了 的评论区中的config示例。\n\n\nDeepLabv3+ 模型如何实现 Resize 和 Argmax 算子的 BPU 加速？#\n\n可参考该技术贴 地平线deeplabv3+参考算法如何实现resize和argmax算子的BPU加速？ 。","routePath":"/guide/advanced_content/hat/hat_faq","lang":"zh","toc":[{"text":"Docker 容器中无法 import hat？","id":"docker-容器中无法-import-hat","depth":2,"charIndex":3},{"text":"如何找到 config 文件中模块的具体实现？","id":"如何找到-config-文件中模块的具体实现","depth":2,"charIndex":397},{"text":"为什么 config 能正常打印，但 launch trainer 失败？","id":"为什么-config-能正常打印但-launch-trainer-失败","depth":2,"charIndex":893},{"text":"为什么配置 data 运行的 device 不生效？","id":"为什么配置-data-运行的-device-不生效","depth":2,"charIndex":1058},{"text":"为什么配置 trainer 的运行 device 不生效？","id":"为什么配置-trainer-的运行-device-不生效","depth":2,"charIndex":1186},{"text":"如何设置验证/打印频率？","id":"如何设置验证打印频率","depth":2,"charIndex":1379},{"text":"为什么 plt.show 图像不显示？","id":"为什么-pltshow-图像不显示","depth":2,"charIndex":1480},{"text":"如何 new dataset？","id":"如何-new-dataset","depth":2,"charIndex":1606},{"text":"如何恢复意外中断的训练？","id":"如何恢复意外中断的训练","depth":2,"charIndex":1975},{"text":"如何可视化模型结构？","id":"如何可视化模型结构","depth":2,"charIndex":2144},{"text":"如何修改 calibration 的方法？","id":"如何修改-calibration-的方法","depth":2,"charIndex":2215},{"text":"DETR模型是否支持 deformable conv？","id":"detr模型是否支持-deformable-conv","depth":2,"charIndex":2308},{"text":"在BEV 模型中，为什么 Lss 的精度低于 IPM？","id":"在bev-模型中为什么-lss-的精度低于-ipm","depth":2,"charIndex":2432},{"text":"BEV 模型中的 Lss 输入的约束条件是什么？","id":"bev-模型中的-lss-输入的约束条件是什么","depth":2,"charIndex":2509},{"text":"如何解决BEV 模型中 Lss 的 mul(depth,feature) 耗时大？","id":"如何解决bev-模型中-lss-的-muldepthfeature-耗时大","depth":2,"charIndex":2637},{"text":"BEV 模型中 Lss 的 voxelpooling 如何实现？选取多少个点？","id":"bev-模型中-lss-的-voxelpooling-如何实现选取多少个点","depth":2,"charIndex":2748},{"text":"BEV 模型的 Lss 如何选择参考点？","id":"bev-模型的-lss-如何选择参考点","depth":2,"charIndex":2884},{"text":"BEV 模型的 Lss 如何处理 gridsample 输入较大的情况？","id":"bev-模型的-lss-如何处理-gridsample-输入较大的情况","depth":2,"charIndex":3015},{"text":"为什么 BEV 模型的 gkt 精度指标较低？有什么优势？","id":"为什么-bev-模型的-gkt-精度指标较低有什么优势","depth":2,"charIndex":3223},{"text":"BEV 模型中 gkt 的 transfomer 是否是 global 的？","id":"bev-模型中-gkt-的-transfomer-是否是-global-的","depth":2,"charIndex":3364},{"text":"BEV 模型是否支持公版 bevformer？","id":"bev-模型是否支持公版-bevformer","depth":2,"charIndex":3475},{"text":"BEV模型检测和分割任务之间是否会制约精度？","id":"bev模型检测和分割任务之间是否会制约精度","depth":2,"charIndex":3522},{"text":"PointPillars 模型的 latency 包括哪几部分？","id":"pointpillars-模型的-latency-包括哪几部分","depth":2,"charIndex":3584},{"text":"点云量对 PointPillars 模型性能影响如何？","id":"点云量对-pointpillars-模型性能影响如何","depth":2,"charIndex":3697},{"text":"PointPillars 模型的前处理包含哪些操作？","id":"pointpillars-模型的前处理包含哪些操作","depth":2,"charIndex":3777},{"text":"PointPillars 模型支持几类任务检测？如何扩展？","id":"pointpillars-模型支持几类任务检测如何扩展","depth":2,"charIndex":3861},{"text":"PointPillars 的板端 hbm 模型的输入数据如何生成？如何预处理？","id":"pointpillars-的板端-hbm-模型的输入数据如何生成如何预处理","depth":2,"charIndex":3986},{"text":"为什么 PointPillars 模型的板端 perf 数据与官方指标不一致？","id":"为什么-pointpillars-模型的板端-perf-数据与官方指标不一致","depth":2,"charIndex":4119},{"text":"PointPillars 模型中，PTQ 方案是否支持对 Lidar 模型转换？","id":"pointpillars-模型中ptq-方案是否支持对-lidar-模型转换","depth":2,"charIndex":4248},{"text":"centerpoint 和 pointpillars 能否互用？","id":"centerpoint-和-pointpillars-能否互用","depth":2,"charIndex":4400},{"text":"DeepLabv3+ 模型如何实现 Resize 和 Argmax 算子的 BPU 加速？","id":"deeplabv3-模型如何实现-resize-和-argmax-算子的-bpu-加速","depth":2,"charIndex":4558}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":352,"title":"简介","content":"#\n\nHorizon-Torch-Samples是基于Pytorch和Pytorch\nplugin的接口开发的算法工具，旨在为地平线BPU提供高效且用户友好的算法工具包。\n\nHorizon-Torch-Samples所依赖的PyTorch是一个针对深度学习，并且使用GPU和CPU来优化的tensor library\n(张量库)，是目前最受欢迎的深度学习框架之一。而Pytorch\nplugin是基于Pytorch开发的一套量化算法工具，专注于与计算平台相贴近的量化功能实现，其量化算法与地平线计算平台深度耦合，利用该工具训练得到的量化模型均\n可以正常编译和运行在地平线BPU（BERNOULLI/BAYES/NASH）上。\n\nHorizon-Torch-Samples作为地平线开发的算法包基础框架，面向所有的算法用户开发和研究的用户，其量化训练与地平线计算平台紧密相关，包含了\n浮点训练 --> QAT 训练 --> 定点转化预测 --> 模型检查编译(针对地平线 BPU) --> 上板精度仿真验证\n的整套流程。同时它还可以提供包含分类，检测，分割等常见的图像任务的SOTA(state-of-the-art)深度学习模型。\n\n\n特性#\n\n * 基于Pytorch和horizon_plugin_pytorch。\n * 包含从 浮点训练 到 上板精度仿真验证 的整套流程。\n * 包含分类、检测、分割等常见图像任务的SOTA模型，且所有示例都与地平线BPU兼容。\n\n\n示例模型#\n\nHorizon-Torch-Samples目前已包含以下深度学习模型。\n\n分类模型\n\n * mobilenetv1_imagenet\n\n * mobilenetv2_imagenet\n\n * resnet18_imagenet\n\n * resnet50_imagenet\n\n * vargnetv2_imagenet\n\n * efficientnet_imagenet\n\n * horizon_swin_transformer_imagenet\n\n * mixvargenet_imagenet\n\n * efficientnasnetm_imagenet\n\n * efficientnasnets_imagenet\n\n * vit_small_imagenet\n\n * henet_tinye_imagenet\n\n * henet_tinym_imagenet\n\n检测模型\n\n * fcos_efficientnetb0_mscoco\n\n * fcos_efficientnetb1_mscoco\n\n * fcos_efficientnetb2_mscoco\n\n * fcos_efficientnetb3_mscoco\n\n * detr_resnet50_mscoco\n\n * detr_efficientnetb3_mscoco\n\n * deform_detr_resnet50_mscoco\n\n * fcos3d_efficientnetb0_nuscenes\n\n * pointpillars_kitti_car\n\n * centerpoint_pointpillar_nuscenes\n\n分割模型\n\n * deeplabv3plus_efficientnetm0_cityscapes\n * deeplabv3plus_efficientnetm1_cityscapes\n * deeplabv3plus_efficientnetm2_cityscapes\n * fastscnn_efficientnetb0tiny_cityscapes\n * unet_mobilenetv1_cityscapes\n\n光流模型\n\n * pwcnet_pwcnetneck_flyingchairs\n\n车道线检测模型\n\n * ganet_mixvargenet_culane\n\n多目标跟踪模型\n\n * motr_efficientnetb3_mot17\n\n双目深度估计模型\n\n * stereonet_stereonetneck_sceneflow\n * stereonetplus_mixvargenet_sceneflow\n\nBev多任务模型\n\n * bev_ipm_efficientnetb0_multitask_nuscenes\n * bev_lss_efficientnetb0_multitask_nuscenes\n * bev_gkt_mixvargenet_multitask_nuscenes\n * bev_ipm_4d_efficientnetb0_multitask_nuscenes\n * detr3d_efficientnetb3_nuscenes\n * petr_efficientnetb3_nuscenes\n * bevformer_tiny_resnet50_detection_nuscenes\n * bev_cft_efficientnetb3_nuscenes\n * bev_sparse_henet_tinym_nuscenes\n\n关键点检测模型\n\n * keypoint_efficientnetb0_carfusion\n\nLidar多任务模型\n\n * centerpoint_mixvargnet_multitask_nuscenes\n\n轨迹预测模型\n\n * densetnt_vectornet_argoverse1\n * qcnet_oe_argoverse2\n\nOccupancy预测模型\n\n * flashocc_henet_lss_occ3d_nuscenes\n\n在线建图模型\n\n * maptroe_henet_tinym_bevformer_nuscenes\n\nLidar融合多任务模型\n\n * bevfusion_pointpillar_henet_multisensor_multitask_nuscenes\n\n以上模型中，resnet18_imagenet 、 resnet50_imagenet 、 vargnetv2_imagenet 、\nefficientnasnetm_imagenet 、 efficientnasnets_imagenet 、 efficientnet_imagenet 、\nmixvargenet_imagenet 、 vargnetv2_imagenet 、ganet_mixvargenet_culane 、\ndeeplabv3plus_efficientnetm0_cityscapes 、deeplabv3plus_efficientnetm1_cityscapes\n、 deeplabv3plus_efficientnetm2_cityscapes、\nfastscnn_efficientnetb0tiny_cityscapes、 bev_gkt_mixvargenet_multitask_nuscenes\n、bev_ipm_efficientnetb0_multitask_nuscenes 、\nbev_lss_efficientnetb0_multitask_nuscenes、 flashocc_henet_lss_occ3d_nuscenes、\ndetr3d_efficientnetb3_nuscenes 和 keypoint_efficientnetb0_carfusion\n只需要做calibration量化精度就能达到目标，详细精度参考model_zoo。","routePath":"/guide/advanced_content/hat/introduction","lang":"zh","toc":[{"text":"特性","id":"特性","depth":2,"charIndex":517},{"text":"示例模型","id":"示例模型","depth":2,"charIndex":636}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":353,"title":"模型仓库modelzoo","content":"#\n\n\nClassification#\n\nNETWORK                             FLOAT   QAT     QUANTIZATION   DATASET    INPUT SHAPE   BPU LATENCY (MS)   FPS\nmobilenetv1_imagenet                74.12   73.92   73.61          ImageNet   1x3x224x224   0.42               5069.83\nmobilenetv2_imagenet                72.65   72.51   72.11          ImageNet   1x3x224x224   0.45               4541.88\nresnet18_imagenet                   72.04   72.03   72.03          ImageNet   1x3x224x224   0.65               2346.22\nresnet50_imagenet                   77.37   76.99   76.94          ImageNet   1x3x224x224   1.12               1117.93\nvargnetv2_imagenet                  73.94   73.56   73.64          ImageNet   1x3x224x224   0.50               3763.68\nefficientnet_imagenet               74.31   74.23   74.18          ImageNet   1x3x224x224   0.49               3887.20\nhorizon_swin_transformer_imagenet   80.24   80.15   80.05          ImageNet   1x3x224x224   4.19               257.265367\nmixvargenet_imagenet                71.33   71.23   71.04          ImageNet   1x3x224x224   0.44               4728.13\nefficientnasnetm_imagenet           80.24   79.99   79.94          ImageNet   1x3x280x280   0.99               1305.20\nefficientnasnets_imagenet           76.63   76.23   76.03          ImageNet   1x3x300x300   0.56               2943.99\nvit_small_imagenet                  79.50   79.40   77.86          ImageNet   1x3x224x224   2.35               472.41\nhenet_tinye_imagenet                77.68   77.22   76.92          ImageNet   1x3x224x224   0.60               2713.99\nhenet_tinym_imagenet                78.38   77.95   77.62          ImageNet   1x3x224x224   0.48(J6M)          3451.43(J6M)\n\n\nDetection#\n\nFCOS\n\nNETWORK                      BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\nfcos_efficientnetb0_mscoco   efficientnetb0   36.26   35.79   35.59          MS COCO   1x3x512x512   -                  -\nfcos_efficientnetb1_mscoco   efficientnetb1   41.37   41.21   40.71          MS COCO   1x3x640x640   2.44               487.89\nfcos_efficientnetb2_mscoco   efficientnetb2   45.35   45.10   45.00          MS COCO   1x3x768x768   3.46               325.84\nfcos_efficientnetb3_mscoco   efficientnetb3   48.03   47.65   47.58          MS COCO   1x3x896x896   5.75               187.30\n\nDETR\n\nNETWORK                      BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE    BPU LATENCY (MS)   FPS\ndetr_resnet50_mscoco         resnet50         35.70   31.42   31.31          MS COCO   1x3x800x1333   28.63              35.27\ndetr_efficientnetb3_mscoco   efficientnetb3   37.21   35.95   35.99          MS COCO   1x3x800x1333   22.20              45.64\n\nDeform DETR\n\nNETWORK                       BACKBONE   FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE    BPU LATENCY (MS)   FPS\ndeform_detr_resnet50_mscoco   resnet50   44.34   44.65   44.80          MS COCO   1x3x800x1333   222.42             4.51\n\nFCOS3D\n\nNETWORK                          BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET    INPUT SHAPE   BPU LATENCY (MS)   FPS\nfcos3d_efficientnetb0_nuscenes   efficientnetb0   30.60   30.27   30.31          nuscenes   1x3x512x896   3.03               409.89\n\n\nSegmentation#\n\nUNet\n\nNETWORK                       BACKBONE      FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\nunet_mobilenetv1_cityscapes   MobileNetV1   68.02   67.56   67.53          Cityscapes   1x3x1024x2048   1.64               783.06\n\nDeeplab\n\nNETWORK                                   BACKBONE          FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\ndeeplabv3plus_efficientnetm0_cityscapes   EfficientNet-M0   76.30   76.22   76.12          Cityscapes   1x3x1024x2048   4.83               218.18\ndeeplabv3plus_efficientnetm1_cityscapes   EfficientNet-M1   77.94   77.64   77.65          Cityscapes   1x3x1024x2048   9.21               111.76\ndeeplabv3plus_efficientnetm2_cityscapes   EfficientNet-M2   78.82   78.65   78.63          Cityscapes   1x3x1024x2048   13.77              74.04\n\nFastScnn\n\nNETWORK                                  BACKBONE              FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\nfastscnn_efficientnetb0tiny_cityscapes   EfficientNet-B0lite   69.97   69.90   69.88          Cityscapes   1x3x1024x2048   2.27               493.85\n\n\nOpticalFlow#\n\nPwcNet\n\nNETWORK                          BACKBONE   FLOAT    QAT      QUANTIZATION   DATASET        INPUT SHAPE   BPU LATENCY (MS)   FPS\npwcnet_pwcnetneck_flyingchairs   PwcNet     1.4117   1.4112   1.4075         FlyingChairs   1x6x384x512   -                  -\n\n\nLidar#\n\nPointPillars\n\nNETWORK                  BACKBONE               FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\npointpillars_kitti_car   SequentialBottleNeck   77.31   76.86   76.76          KITTI3D   150000x4      185.42             28.12\n\nCenterPoint\n\nNETWORK                            BACKBONE               FLOAT                   QAT                     QUANTIZATION            DATASET        INPUT SHAPE             BPU LATENCY (MS)   FPS\ncenterpoint_pointpillar_nuscenes   SequentialBottleNeck   58.32(NDS) 48.04(MAP)   58.11(NDS) 47.85(MAP)   58.14(NDS) 47.81(MAP)   nuscenes det   1x5x20x40000, 40000x4   55.26              100.62\n\nLidarMultiTask\n\nNETWORK                                     BACKBONE      FLOAT                                  QAT                                    QUANTIZATION                           DATASET               INPUT SHAPE             BPU LATENCY (MS)   FPS\ncenterpoint_mixvargnet_multitask_nuscenes   MixVarGENet   58.09(NDS) 47.27(MAP) 91.28(MeanIOU)   57.72(NDS) 46.76(MAP) 91.18(MeanIOU)   57.53(NDS) 46.28(MAP) 91.21(MeanIOU)   nuscenes det && seg   1x5x20x40000, 40000x4   53.56              125.52\n\n注解\n\nPointPillars 的指标是 Box3d Moderate 这项。\n\n\nLane Detection#\n\nGaNet\n\nNETWORK                    BACKBONE      FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\nganet_mixvargenet_culane   MixVarGENet   79.49   78.72   78.72          CuLane    1x3x320x800   0.94               1445.91\n\n\nMultiple Object Track#\n\nMotr\n\nNETWORK                     BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE                                       BPU LATENCY (MS)   FPS\nmotr_efficientnetb3_mot17   efficientnetb3   58.02   57.62   57.76          Mot17     1x3x800x1422, 1x256x2x128, 1x1x1x256, 1x4x2x128   14.99              68.31\n\n\nBinocular depth estimation#\n\nStereoNet\n\nNETWORK                               BACKBONE      FLOAT    QAT      QUANTIZATION   DATASET     INPUT SHAPE   BPU LATENCY (MS)   FPS\nstereonet_stereonetneck_sceneflow     StereoNeck    1.1270   1.1677   1.1685         SceneFlow   1x6x540x960   -                  -\nstereonetplus_mixvargenet_sceneflow   MixVarGENet   1.1270   1.1329   1.1351         SceneFlow   2x3x544x960   5.12               208.22\n\n\nBev#\n\nBev\n\nNETWORK                                        BACKBONE         FLOAT                                  QAT                                    QUANTIZATION                           DATASET               INPUT SHAPE                                                  BPU LATENCY (MS)   FPS\nbev_ipm_efficientnetb0_multitask_nuscenes      efficientnetb0   30.54(NDS) 21.70(MAP) 51.45(MeanIOU)   30.80(NDS) 21.66(MAP) 51.47(MeanIOU)   30.26(NDS) 21.56(MAP) 50.99(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x128x128x2                                     9.52               112.59\nbev_lss_efficientnetb0_multitask_nuscenes      efficientnetb0   30.06(NDS) 20.62(MAP) 51.80(MeanIOU)   30.10(NDS) 20.51(MAP) 51.78(MeanIOU)   30.08(NDS) 20.46(MAP) 51.47(MeanIOU)   nuscenes det && seg   6x3x256x704, 10x128x128x2, 10x128x128x2                      6.90               160.88\nbev_gkt_mixvargenet_multitask_nuscenes         MixVarGENet      28.10(NDS) 19.91(MAP) 48.52(MeanIOU)   27.98(NDS) 19.99(MAP) 48.42(MeanIOU)   27.90(NDS) 20.00(MAP) 48.35(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2,     16.31              64.30\n                                                                                                                                                                                                           6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2\nbev_ipm_4d_efficientnetb0_multitask_nuscenes   efficientnetb0   37.21(NDS) 22.00(MAP) 52.87(MeanIOU)   37.32(NDS) 22.21(MAP) 53.83(MeanIOU)   37.34(NDS) 22.15(MAP) 53.87(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x128x128x2, 1x64x128x128, 1x128x128x2          9.83               108.87\ndetr3d_efficientnetb3_nuscenes                 efficientnetb3   33.04(NDS) 27.52(MAP)                  32.84(NDS) 27.14(MAP)                  32.81(NDS) 27.06(MAP)                  nuscenes det          6x3x512x1408                                                 37.63              26.92\npetr_efficientnetb3_nuscenes                   efficientnetb3   37.65(NDS) 30.38(MAP)                  37.26(NDS) 29.29(MAP)                  37.40(NDS) 29.33(MAP)                  nuscenes det          6x3x512x1408                                                 77.10              13.05\nbevformer_tiny_resnet50_detection_nuscenes     resnet50         37.12(NDS) 26.79(MAP)                  37.16(NDS) 26.50(MAP)                  37.15(NDS) 26.59(MAP)                  nuscenes det          6x3x480x800, 1x2500x256, 1x50x50x2, 6x20x32x2, 1x100x50x2,   28.29(J6M)         35.90(J6M)\n                                                                                                                                                                                                           6x640x4x2, 1x2500x1\nbev_cft_efficientnetb3_nuscenes                efficientnetb3   32.79(NDS) 24.79(MAP)                  32.50(NDS) 24.47(MAP)                  32.42(NDS) 24.46(MAP)                  nuscenes det          6x3x512x1408                                                 36.50              27.75\nbev_sparse_henet_tinym_nuscenes                henet_tinym      54.19(NDS) 20.62(MAP)                  52.23(NDS) 20.62(MAP)                  -                                      nuscenes det          6x3x256x704, 6x4x4, 1x384x11, 1x384x256                      12.92(J6M)         79.68(J6M)\n\n\nKeypoint Detection#\n\nHeatmapKeypointModel\n\nNETWORK                             BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET     INPUT SHAPE   BPU LATENCY (MS)   FPS\nkeypoint_efficientnetb0_carfusion   efficientnetb0   94.33   94.30   94.31          carfusion   1x3x128x128   0.45               4550.72\n\n\nTrajectory Prediction#\n\nDenseTNT\n\nNETWORK                         BACKBONE    FLOAT    QAT      QUANTIZATION   DATASET       INPUT SHAPE                                                   BPU LATENCY (MS)   FPS\ndensetnt_vectornet_argoverse1   vectornet   1.2974   1.2989   1.3038         argoverse 1   30x9x19x32, 30x11x9x64, 30x1x1x96, 30x2x1x2048, 30x1x1x2048   11.30              144.725382\n\nQCNet\n\nNETWORK               BACKBONE   FLOAT   QAT     QUANTIZATION   DATASET       INPUT SHAPE   BPU LATENCY (MS)   FPS\nqcnet_oe_argoverse2   -          83.03   82.19   -              argoverse 2   输入见下方list     11.42(J6M)         189.39\n\n注解\n\nqcnet_oe_argoverse2 的指标是 HitRate 这项。\n\nqcnet_oe_argoverse2 模型输入shape为:\n\n1x30x10, 1x10x30x30, 1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x80,\n1x1x30x80, 1x1x30x80, 1x1x30x6, 1x1x30x6, 1x1x30x6, 1x1x30x6, 1x1x30x30,\n1x1x30x30, 1x1x30x30,1x30x5x128, 1x30x2x128, 1x80, 1x80, 1x1x80x80, 1x1x80x80,\n1x1x80x80,1x1x80x50, 1x1x80x50, 1x1x80x50, 1x80x50, 1x80x50, 1x80x50,\n1x80x50,1x30x30, 1x30x1, 1x80x80\n\n\nOccupancy Prediction#\n\nFlashOcc\n\nNETWORK                             BACKBONE               FLOAT    QAT      QUANTIZATION   DATASET          INPUT SHAPE                               BPU LATENCY (MS)   FPS\nflashocc_henet_lss_occ3d_nuscenes   henet_tinym_imagenet   0.3674   0.3661   0.3656         occ3d_nuscenes   6x3x512x960, 10x128x128x2, 10x128x128x2   8.39(J6M)          119.23(J6M)\n\n\nOnline Map Construction#\n\nMapTROE\n\nNETWORK                                  BACKBONE               FLOAT    QAT      QUANTIZATION   DATASET    INPUT SHAPE                                         BPU LATENCY (MS)   FPS\nmaptroe_henet_tinym_bevformer_nuscenes   henet_tinym_imagenet   0.6632   0.6577   0.6387         nuscenes   6x3x480x800, 1x1x50x100, 6x20x100x2, 1x100x100x2,   11.03(J6M)         93.77(J6M)\n                                                                                                            6x2000x4x2, 1x5000x1\n\n\nLidar Fusion#\n\nLidarFusion\n\nNETWORK                                                      BACKBONE               FLOAT                               QAT                                 QUANTIZATION   DATASET                 INPUT SHAPE                                                    BPU LATENCY (MS)   FPS\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes   henet_tinym_imagenet   64.28(NDS) 58.09(MAP) 51.77(MIOU)   62.91(NDS) 57.48(MAP) 52.51(MIOU)   -              nuscenes det && occ3d   1x5x20x40000, 40000x4, 6x3x512x960, 1x256x128x2, 6x5120x2x2,   24.87(J6M)         41.21(J6M)\n                                                                                                                                                                                                   1x16384x1","routePath":"/guide/advanced_content/hat/model_zoo","lang":"zh","toc":[{"text":"Classification","id":"classification","depth":2,"charIndex":3},{"text":"Detection","id":"detection","depth":2,"charIndex":1691},{"text":"Segmentation","id":"segmentation","depth":2,"charIndex":3249},{"text":"OpticalFlow","id":"opticalflow","depth":2,"charIndex":4425},{"text":"Lidar","id":"lidar","depth":2,"charIndex":4705},{"text":"Lane Detection","id":"lane-detection","depth":2,"charIndex":5934},{"text":"Multiple Object Track","id":"multiple-object-track","depth":2,"charIndex":6202},{"text":"Binocular depth estimation","id":"binocular-depth-estimation","depth":2,"charIndex":6554},{"text":"Bev","id":"bev","depth":2,"charIndex":6999},{"text":"Keypoint Detection","id":"keypoint-detection","depth":2,"charIndex":10393},{"text":"Trajectory Prediction","id":"trajectory-prediction","depth":2,"charIndex":10708},{"text":"Occupancy Prediction","id":"occupancy-prediction","depth":2,"charIndex":11755},{"text":"Online Map Construction","id":"online-map-construction","depth":2,"charIndex":12146},{"text":"Lidar Fusion","id":"lidar-fusion","depth":2,"charIndex":12685}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":354,"title":"如何开启 AMP","content":"#\n\nAMP全称为Automatic Mixed\nPrecision，即自动混合精度。AMP开启后，pytorch可以自动地在模型执行时将一些算子（如卷积和全连接）使用 float16\n进行计算，以达到提升计算速度、减少显存占用的效果。详见 pytorch官方文档。\n\nHAT中已经为AMP做好相关的工作，只需要在定义config文件中的 batch_processor 字段时将 enable_amp 参数设置为 True\n即可。\n\n注解\n\n在模型验证时为得到准确的指标，一般是不需开启AMP的，在定义 val_batch_processor 字段时请将 enable_amp 参数设置为 False\n。\n\n","routePath":"/guide/advanced_content/hat/tutorials/amp","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":355,"title":"数据校准","content":"#\n\n在量化训练(QAT)中，一个重要的步骤是确定量化参数 scale ，一个合理的 scale\n能够显著提升模型训练结果和加快模型的收敛速度。Calibration是通过用浮点模型在训练集上跑少数batch的数据（只跑forward过程，没有backwar\nd），统计这些数据的分布直方图，通过一定方法去计算出 min_value 和 max_value ，然后可以用这些 min_value 和 max_value\n去获取scale。当QAT的训练精度上不去的时候，在QAT的开始之前使用calibration做量化参数的微调，获取scale，可以为QAT提供更好的量化初始化\n参数，提升收敛速度和精度。\n\n\n如何定义 Calibration 模型#\n\n * 默认不需要对现有模型做任何修改\n   \n   类似于定义量化模型时需要设置 QAT QConfig ，Calibration时也需要对模型设置 Calibration QConfig 。不过，\n   Calibration QConfig 的设置相对来说比较简单，HAT框架已经实现对模型 Calibration QConfig\n   的默认设置，无需对模型做任何修改，即可使用Calibration。\n\n * 自定义模型子模块 Calibration QConfig\n   \n   在上文的默认情况下，会为模型的所有Module（继承自nn.Module）设置 Calibration QConfig\n   。因此，Calibration时也就会对所有 Module 的特征分布进行统计。如果有特殊需求，可以在模型内自定义实现\n   set_calibration_qconfig 方法：\n   \n   \n\n\n浮点模型做 Calibration#\n\nHAT中集成了Calibration功能，浮点模型做Calibration命令和正常训练相似，只需执行以下命令即可：\n\n\n\n需要注意的是 config 文件中 calibration_trainer 中的一些配置:\n\n\n\n1. 数据集的设置：\n\n做Calibration的数据集(dataset)不能是测试集(可以是训练集或其他数据)，但是做Calibration时用于数据增强的transforms\n可以和正常训练时的transforms保持一致，但是也可以设置成和validation的transforms一致，也可以自定义transforms。（哪种实验效\n果最好，暂时没有定论，都可以尝试。）\n\n2. Calibration 迭代的图片数目(可供参考)：\n\n * classification：图片张数一般可以500～1500张就可以取得不错的效果。\n * segmentation&&detection：图片张数可以100~300张左右。\n\n注解\n\n这些图片张数具体数目也不是固定的，上方的建议只是从已有的实验中总结的经验，可根据实际情况调整。\n\n\n使用Calibration模型做QAT训练#\n\n\n\nQAT时averaging_constant参数设置：\n\n量化时scale参数的更新规则是 scale = (1 - averaging_constant) * scale + averaging_constant *\ncurrent_scale 。\n\n在已有的一些实验中（主要是图像分类任务实验）发现，做完calibration后，把activation的scale固定住，不进行更新，即设置activation\n的 averaging_constant=0 , 并设置weight的 averaging_constant=1 ，效果可能会相对略好一些。\n\n注解\n\n这种设置并不适用于所有任务，在lidar任务中，固定scale，精度也可能会变差。可根据实际情况调整。\n\n接下来只需要执行正常的QAT训练命令，即可启动QAT训练：\n\n","routePath":"/guide/advanced_content/hat/tutorials/calibration","lang":"zh","toc":[{"text":"如何定义 Calibration 模型","id":"如何定义-calibration-模型","depth":2,"charIndex":306},{"text":"浮点模型做 Calibration","id":"浮点模型做-calibration","depth":2,"charIndex":737},{"text":"使用Calibration模型做QAT训练","id":"使用calibration模型做qat训练","depth":2,"charIndex":1237}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":356,"title":"计算量工具","content":"#\n\n\n计算量的定义#\n\n计算量是用来评估神经网络大小的常用工具。在常见的计算量工具中一般只统计两种操作的计算量，一个是卷积相关，另一个是全连接相关的。这两种都是和乘加计算相关的。\n\n以常见的 torch.nn.Conv2d 为例，输入数据的形状为 bs * c_in * w_in * h_in ，输出数据的形状为 bs * c_out *\nw_out * h_out ，卷积核的大小为 f 。则该 Conv2d 的计算量为 2 * bs * f * f * c_in * c_out * w_out\n* h_out 。 2 表示加法计算量和乘法计算量各一半。\n\n而 torch.nn.Linear 的情况里，输入数据的神经元个数为 c_in ，输出数据的神经元个数为 c_out\n。其实这种全连接层可以作为一个特殊的卷积层，输入输出的大小均为 1x1 ，卷积核大小也是 1x1 。则全连接层的计算量为 bs * (c_in * c_out\n+ c_out) ，需要注意是这里乘法和加法的计算量并不完全一致。\n\n注意\n\n量化模型和QAT模型和对应的浮点模型的计算量是完全一致的。\n\n\n计算量工具的使用方法#\n\n目前计算量工具支持统计计算量的op有三种，分别为torch.nn.Conv2d，torch.nn.Linear 以及\ntorch.nn.ConvTranspose2d。\n\n\n\n其中 config 中影响计算量的主要key是 test_model （或者 model ），以及 test_inputs 。 model\n相关的决定了计算量工具需要检查的模型，而 test_inputs 决定了输入的大小，除此之外，输入形状还可以通过 --input-shape B,C,H,W\n的输入参数决定。\n\n这里需要注意的是，其实计算量工具也可以支持matmul，使用方法是--method的时候选择fx。不过这个方法要求模型本身能够支持fx才可以。\n\n\n常见分类模型的计算量（输入大小为1x3x224x224）#\n\nNETWORK                   OPS(G)\nmobilenetv1 (alpha=1.0)   0.57\nmobilenetv2 (alpha=1.0)   0.31\nresnet18                  1.81\nresnet50                  3.86\nvargnetv2                 0.36\nefficientnetb0            0.39\n\n\n说明#\n\n一般有两种情况使用calops会出现结果不对或者计算错误。\n\n 1. 使用hook的method，但模型含有matmul或者其他明确不覆盖的算子。\n 2. 使用fx的method，但模型本身不支持fx，或者含有其他明确不覆盖的算子。","routePath":"/guide/advanced_content/hat/tutorials/calops","lang":"zh","toc":[{"text":"计算量的定义","id":"计算量的定义","depth":2,"charIndex":3},{"text":"计算量工具的使用方法","id":"计算量工具的使用方法","depth":2,"charIndex":492},{"text":"常见分类模型的计算量（输入大小为`1x3x224x224`）","id":"常见分类模型的计算量输入大小为1x3x224x224","depth":2,"charIndex":-1},{"text":"说明","id":"说明","depth":2,"charIndex":1078}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":357,"title":"如何编译模型","content":"#\n\nHAT集成了编译定点模型的能力，将pytorch的模型编译为可以上板运行的形式。您可以选择下面的方式来编译模型。\n\n\n输入为 config 文件#\n\n此方式使用 tools/compile_perf_hbir.py 脚本，命令如下：\n\n\n\n此命令会从config文件的 hbir_compiler 字段获取编译相关配置，举例说明如下：\n\n\n\n编译完成后，会生成以下几个文件：\n\n * tmp_compile/model.hbm ：编译出的模型文件。","routePath":"/guide/advanced_content/hat/tutorials/compile","lang":"zh","toc":[{"text":"输入为 config 文件","id":"输入为-config-文件","depth":2,"charIndex":61}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":358,"title":"config 文件介绍","content":"#\n\n使用HAT算法包训练模型通常只需使用一条命令就可以了，即：\n\n\n\n其中 /PATH/TO/CONFIG 就是模型训练对应的 config 文件，它负责定义了模型结构、数据集加载、以及整套的训练流程。\n\n本章节通过介绍config文件中一些固定的全局的关键字，以及它们是如何配置的，让您对config文件的内容以及作用有个大致的了解。\n\n\n全局关键字#\n\n * training_stage：模型训练的各个阶段，包括 float、qat 和 int_infer。\n\n * device_ids：模型训练使用的 gpu 列表。\n\n * cudnn_benchmark：是否打开cudnn benchmark。通常默认为 True 。\n\n * seed：是否设置随机数种子。通常默认为 None 。\n\n * log_rank_zero_only：简化多卡训练时的日志打印，只在第0卡上输出日志。通常默认为 True 。\n\n * model：参与 training 过程中的模型结构。type 表示模型的类型，如\n   Classifier、Segmentor、RetinaNet等等，分别对应分类、分割、检测中的某一类模型。它会在使用过程中被 build\n   成具体的类，余下的参数都是用于初始化这个类。\n\n * deploy_model：参与 deploy 过程的模型结构，主要用于模型编译。和 model 相比，大多数情况下只需要把损失函数以及后处理部分设置为\n   None 即可。\n\n * deploy_inputs：deploy 过程的模拟输入。不用关心具体的数值，只要保证格式满足输入要求即可。\n\n * data_loader：训练阶段的数据集加载流程。它的 type 是一个具体的类 torch.utils.data.DataLoader\n   ，余下的参数都是用于初始化这个类。相关参数的含义也可以参考 pytorch 官网提供的接口文档。这里 dataset\n   表示读取某个具体的数据集，例如ImageNet、MSCOCO、VOC等等，它的 transforms 表示在数据读取过程中添加的数据增强操作。\n\n * val_data_loader：验证模型性能阶段的数据集加载流程。和 data_loader 不同的地方在于 data_path 不同，以及去掉了\n   transforms 的过程和 sample 的过程。\n\n * batch_processor：模型在训练过程中每个迭代 stage 进行的操作，包括前向计算、梯度回传、参数更新等等。如果包含\n   batch_transforms 参数，表示一些数据增强的操作是在 gpu 上进行的，这可以大大加快训练速度。\n\n * val_batch_processor：模型在验证过程中每个迭代 stage 进行的操作，只包含前向计算。\n\n * metric_updater：模型训练过程中更新指标的方法，这个指标是用来验证训练的模型性能是否在提升。它通常是和 float_trainer 下面的\n   train_metrics 配合着使用。train_metrics 是具体的指标形式，metric_updater 只是提供一种更新方法。\n\n * val_metric_updater：训练出来的模型在验证性能的过程中更新指标的方法，这个指标用来验证最终训练出来的模型性能到底如何。它通常是和\n   float_trainer 下面的 val_metrics 配合着使用，和 metric_updater 同理。\n\n * float_trainer：浮点模型训练流程的配置。type 类型为 distributed_data_parallel_trainer\n   表示支持分布式训练，其余的参数分别定义了模型、数据集加载、优化器、训练 epoch 长度等等。其中 callbacks\n   表示训练过程中进行的一系列操作，比如模型保存、学习率更新、精度验证等等。 直接被 tools/train.py 文件调用的变量。\n\n * qat_trainer：qat 模型训练流程配置。参数的含义和 float_trainer 基本一致。直接被 tools/train.py\n   文件调用的变量。\n\n * int_infer_trainer：不包含训练流程，只是为了验证定点模型的精度。直接被 tools/train.py 文件调用的变量。\n\n * compile_cfg：编译相关的配置。out_dir 表示编译生成的 hbm 文件（部署模型）的输出路径。\n\n之所以称这些变量为全局关键字，是因为几乎每个 config 文件中都定义了以上这些变量，且对应的功能基本一致。因此通过对这篇文档的学习，您可以大致理解任意一个\nconfig 文件实现的功能。\n\n\n如何配置#\n\n这里主要介绍数据类型为 dict 的全局关键字的配置。\n\n数据类型为 dict 的全局关键字可以为两种：\n\n 1. 包含 type的，例如 model、 data_loader、 float_trainer等。\n\n 2. 不包含 type的，例如compile_cfg。\n\n它们的区别在于包含 type 的全局关键字本质可以看作是一个 class，它的type值可以是一个 string 变量，也可以是一个具体的 class，如果是\nstring， 在程序运行中同样会被 build 成一个相应的 class。这个dict中除掉type之外的其它keys的值都用于初始化这个\nclass。和全局关键字属性类似，这些keys的值可以是一个数值，也可以是一个包含type变量的dict，例如 data_loader 中的 dataset\n属性，以及这个 dataset 下面的 transforms 属性。\n\n对于没有 type 变量的全局关键字来说，它就是一个普通类型的 dict 变量，代码在运行过程中会通过其 keys 获取对应的 values。\n\n提示\n\n所有已经提供的config配置，可以保证正常运行和复现精度。如果因为环境配置和训练时间等原因，需要修改配置的话，那么相对的训练策略可能也需要更改。直接修改con\nfig中的个别配置有时候并不能得到想要的结果。","routePath":"/guide/advanced_content/hat/tutorials/config","lang":"zh","toc":[{"text":"全局关键字","id":"全局关键字","depth":2,"charIndex":171},{"text":"如何配置","id":"如何配置","depth":2,"charIndex":2019}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":359,"title":"set_qconfig 书写规范和自定义 qconfig 介绍","content":"#\n\n\nset_qconfig 方法书写规范#\n\n在对要量化的模型进行定义时，需要实现模型 set_qconfig 方法对量化方式进行配置。\n\n当前设置QConfig接口由 hat.utils.qconfig_manager 提供，set_qconfig 中调用\nhat.utils.qconfig_manager 实现对模块Qconfig的设置，例如：\n\n\n\n\n自定义 QAT QConfig 参数#\n\nHAT支持QAT训练时使用自定义QConfig，只需在config文件的 qat_solver 中配置 qconfig_params 参数即可:\n\n\n\nqconfig_params\n主要有五个参数配置项：dtype、activation_fake_quant、weight_fake_quant、activation_qkwargs、weig\nht_qkwargs。\n\n * dtype：量化比特类型，支持 \"qint8\", 缺省时使用默认值 \"qint8\"。\n\n * activation_fake_quant：指定activation的量化器，支持 \"fake_quant\"、\"lsq\"、\"pact\", 缺省时使用默认值\n   \"fake_quant\"。\n\n * weight_fake_quant：指定weight的量化器。支持及使用方式同 activation_fake_quant。\n\n * activation_qkwargs：指定activation量化器的参数。\n   \n   * activation_fake_quant 是 \"fake_quant\" 时, activation_qkwargs 可设置参数：\n     \n     \n   \n   * activation_fake_quant 是 \"lsq\" 时, activation_qkwargs 可设置参数：\n     \n     \n     \n     * activation_fake_quant 是 \"pact\" 时, activation_qkwargs 可设置参数：\n     \n     \n\n * weight_qkwargs：指 weight量化器的参数。除了weight_qkwargs 的默认 observer 是\n   MovingAveragePerChannelMinMaxObserver外, 其他参数配置及用法，同 activation_qkwargs。\n\n注意\n\nactivation_qkwargs 和 weight_qkwargs 一般是不需要进行设置的，缺省使用默认配置即可。但当使用 calibration 后进行\nQAT 训练时，可能需要修改 averaging_constant。","routePath":"/guide/advanced_content/hat/tutorials/custom_qconfig","lang":"zh","toc":[{"text":"set_qconfig 方法书写规范","id":"set_qconfig-方法书写规范","depth":2,"charIndex":3},{"text":"自定义 QAT QConfig 参数","id":"自定义-qat-qconfig-参数","depth":2,"charIndex":181}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":360,"title":"FCOS-EfficientNetB0的config构造详细说明","content":"#\n\n为了帮助您对一个完整的 config 文件有个更加清楚的了解，这篇文档以 FCOS-EfficientNetB0\n模型为例，对它的每一个模块增加一个简短的注释供您参考，如下所示：\n\n","routePath":"/guide/advanced_content/hat/tutorials/detailed_config","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":361,"title":"启动方式","content":"#\n\ntools+configs\n是HAT目前最基础的训练方式。但是很多情况下，我们需要处理多卡或者分布式的环境。这些环境需要依赖一些第三方库的方式，才能够在多个环境中把基础的训练方式有效的组织\n起来。\n\n在多卡或者分布式的环境里，常用的启动方式有 torchrun 等，这里我们从命令行的形式简单讲一下这些启动方式的区别。\n\n下面以resnet18的float训练为例，把所有目前HAT支持的启动方式都列出来，作为参考。\n\n\n最简单的模式#\n\n这种最简单的模式，支持的范围包括单机单卡和单机多卡两种模式，不支持多机多卡，配置的方式只需要修改configs中的device_ids 的索引即可。\n\n需要说明的是，这里单机多卡的支持方式，其实是借助 torch.multiprocess\n来把单机内部的所有进程有效的管理起来，当然这也是这种模式不支持多机的原因。\n\n\n\n\ntorchrun#\n\n首先需要注意的是，torchrun要求torch的版本大于等于1.10.0。对于小于1.10.0使用的torch.distributed.launch在HAT中\n则没有单独支持，也不建议使用了。\n\ntorchrun是torch框架为了方便快捷地处理分布式环境里面的各种环境变量而提供的启动工具。\n\ntorchrun的细节可以参考Pytorch社区文档。\n\n\n\n最后，需要说明的是，不管是python多进程，还是torchrun其实都是进程管理程序，而进程间的通信方法依赖于torch内部 process group\n的初始化方法，因此不同的管理程序对于训练效率没有影响。\n\n而不同的管理程序最大的区别其实就是进程管理方法的区别：比如异常退出的时候，是否能在主进程端拿到所有节点的报错信息。单个进程异常时，能否保证所有进程完整退出。其他\n的像内部开发模式等其实整体上区别也不大。","routePath":"/guide/advanced_content/hat/tutorials/launcher","lang":"zh","toc":[{"text":"最简单的模式","id":"最简单的模式","depth":2,"charIndex":213},{"text":"torchrun","id":"torchrun","depth":2,"charIndex":384}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":362,"title":"通过命令行覆盖config参数","content":"#\n\nHAT支持通过命令行修改config参数，使用方式，在 --opts 后面加上 (key, value) 键值对即可，如下：\n\n\n\n其中， key 是要修改的参数名， value 为想要传入的值。\n\n例如配置文件 config.py ：\n\n\n\n注解\n\n本文档中的示例，仅为向您说明使用 --opts 时的不同类型参数的正确书写格式，示例 config 中的参数、字段配置等，可能与项目中实际 config\n略有不同。\n\n * key 可以支持多级参数的修改，如下所示：\n   \n   \n   \n   上面这条命令，可以把 config 中 model.backbone 字段的值修改成\"resnet50\" 。\n\n * value 可以是数值、字符串（str）、列表（list）、元组（tuple）形式，但不支持 dict 形式。\n   \n   如下命令，执行后可以将 config 中 model.num_classes 字段的值修改成10。\n   \n   \n   \n   但是由于解析机制的原因，在要传入 value 是 tuple 或 list 形式时，需要对 tuple 或 list 加上引号：\n   \n   \n   \n   如果要传入的 value 是 str 形式的，需要额外加上引号，例如：\n   \n   ","routePath":"/guide/advanced_content/hat/tutorials/opts","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":363,"title":"如何进行量化训练","content":"#\n\n本文档仅说明在HAT中进行量化训练时需要的操作，关于量化的基本原理和在训练框架中的实现方式请参阅 horizon_plugin_pytorch 的相关文档。\n\n在量化训练中，由浮点模型到定点模型的转换流程如下：\n\n其中大部分步骤都已集成在HAT的训练pipeline中，只需注意在添加自定义模型时实现 fuse_model 方法来完成模型融合，且实现 set_qconfig\n方法对量化方式进行配置即可。在编写模型时需要注意以下几点：\n\n * HAT只会调用最外层模块的 fuse_model 方法，因此在 fuse_model 的实现中要负责所有子模块的fuse。\n\n * 优先使用 hat.models.base_modules 中提供的基础模块，这些基础模块已实现 fuse_model 方法，可减少工作量和开发难度。\n\n * 模型注册，HAT中的各种模块全部采用了注册机制，只有将定义的模型在对应的注册项中进行注册，才可以在config文件中以\n   dict(type={$class_name}, ...) 的形式使用模型。\n\n * 需要在最外层模块实现 set_qconfig 方法，如果子模块中有特殊layer需单独设置 QConfig，也需要在该子模块中实现 set_qconfig\n   方法，此部分细节可见 set_qconfig 书写规范和自定义 qconfig 介绍 章节。\n\n此外，为使模型可转为量化模型，需要满足一些条件，具体见horizon_plugin_pytorch 的相关文档。\n\n\n量化训练流程简介#\n\n\n添加自定义模型#\n\n\n\n\n添加 config 文件#\n\n\n\n\n训练#\n\n只需在使用 tools/train.py 脚本时按顺序指定训练阶段即可，会自动根据训练阶段调用相应的 solver 来执行训练过程：\n\n\n\n * float：正常的浮点训练。\n\n * qat：QAT训练（量化感知训练），首先初始化一个浮点模型，加载训练好的浮点模型权重，再将此模点模型转为QAT模型进行训练。\n\n * int_infer：定点转化预测，此阶段首先初始化一浮点模型，将此浮点模型先转为QAT模型并加载训练好的QAT模型权重，再将\n   QAT模型转为定点模型。转出的定点模型无法进行训练，只能执行validation得到最终的定点模型精度。\n\n\n恢复训练#\n\n可以通过在 config 的 {stage}_trainer 中配置 resume_optimizer 和 resume_epoch_or_step\n字段来恢复意外中断的训练，或仅恢复optimizer来进行fine-tune。例如：\n\n\n\n恢复训练有三种使用场景：\n\n 1. 完全恢复： 该场景为恢复意外中断的训练，会恢复上一个checkpoint的所有状态，包括optimizer、LR、epoch、step\n    等。该场景只需配置 resume_optimizer 字段即可；\n\n 2. 恢复optimizer用于fine-tune： 该场景只会恢复optimizer和LR的状态，但epoch、step都会从0开始，用于某些任务的\n    fine-tune。该场景需要配置 resume_optimizer，并且需要配置resume_epoch_or_step=False。\n\n 3. 只加载模型参数： 该场景只会加载模型参数，不会恢复其他任何状态(optimizer、epoch、step、LR)。该场景只需要在\n    model_convert_pipeline 中配置 LoadCheckpoint ，并且需要配置 resume_optimizer=False 和\n    resume_epoch_or_step=False。\n\n\nqat_mode#\n\n\n作用#\n\nqat_mode 用于设置QAT阶段是否带BN进行量化训练，配合HAT提供的 FuseBN 接口还可以控制量化训练全程带BN或是中途逐步吸收BN。\n\n\n可选项定义#\n\nqat_mode可选的设置有如下三种：\n\n\n\n\n原理介绍#\n\nfuse_bn#\n\nQAT阶段没有BN，HAT默认的量化训练方式。\n\n通过将qat_mode设置为 fuse_bn\n，在浮点模型op融合的过程中，BN的weight和bias均被吸收到Conv的weight和bias中，原来的Conv + BN的组合将只剩下\nConv，这一吸收过程理论上是没有误差的。\n\nwith_bn#\n\nQAT 阶段带 BN 进行训练。\n\n通过设置qat_mode为 with_bn ，浮点模型转为QAT模型的时候BN不会吸收进Conv，而是在QAT阶段以 Conv + BN + 输出量化节点\n的形式作为一个被融合的量化op存在于量化模型中。最终在量化训练结束转为quantized(也称int infer)\n模型的步骤中，BN的weight和bias将自动吸收进conv的量化参数中，吸收之后得到的quantized op和原来的QAT op计算结果保持一致。\n\n在这一模式下，您还可以选择在QAT中途将BN吸收进Conv。手动吸收BN前后QAT模型的forward结果不一致，原因是BN weight吸收至Conv\nweight之后，在之前量化训练中统计出来的量化参数conv_weight_scale不再适用于当前的conv_weight，在对conv_weight的量化中\n将产生较大误差，需要继续进行量化训练调整量化参数。\n\nwith_bn_reverse_fold#\n\nQAT 阶段带 BN 进行训练。\n\n本模式与 with_bn\n的不同之处在于在BN吸收之前，量化训练阶段计算conv_weight_scale时会考虑BN的weight(具体的计算方式不在此详述)，目的是为了吸收BN\nweight之后conv_weight_scale仍然适用于新的conv_weight。\n\n该模式用意是为分步吸收BN提供一种无损的吸收方式：在量化训练中途吸收BN，吸收前后模型forward结果理论上完全一致，您可以在量化训练结束前逐步吸收模型中所有\n的BN并且保证每次吸收之后loss不会有太大的波动。\n\n在该模式下如果有BN在量化训练结束时仍未被吸收，在QAT模型转quantized模型的过程中剩余的BN将自动被吸收，这一吸收操作理论上是无损的。\n\n\n用法#\n\n设置 qat_mode#\n\n只需要在 model_convert_pipeline 中设置 qat_mode 即可。\n\n例如：\n\n\n\n查看当前 qat_mode#\n\n\n\n设置逐步吸收 BN#\n\n在 with_bn 和 with_bn_reverse_fold 两种模式下，可以将 FuseBN\n设置为回调函数用于在指定的epoch或是step吸收指定module中的BN。\n\nFuseBN定义：\n\n\n\n在config文件中使用FuseBN Example：\n\n\n\n\nqat_mode 总结#\n\n一般训练流程是浮点训练到理想精度然后量化训练，该流程只需要使用 fuse_bn\n即可。如果是没有浮点训练一开始就是量化训练，为了确保模型能收敛，才需要使用带BN的量化训练模式。\n\n注解\n\n本文中之所以说“理论上吸收前后无损”或“无变化”，是由于在实际计算中吸收前后两次浮点计算的结果有较低的概率会在小数点较靠后的数位上不一致，微小的变化加上量化操作\n导致吸收BN后Conv的输出相比吸收前Conv + BN的输出在部分数值上可能会产生一个输出scale的绝对误差。","routePath":"/guide/advanced_content/hat/tutorials/quantization","lang":"zh","toc":[{"text":"量化训练流程简介","id":"量化训练流程简介","depth":2,"charIndex":663},{"text":"添加自定义模型","id":"添加自定义模型","depth":3,"charIndex":675},{"text":"添加 config 文件","id":"添加-config-文件","depth":3,"charIndex":688},{"text":"训练","id":"训练","depth":2,"charIndex":706},{"text":"恢复训练","id":"恢复训练","depth":2,"charIndex":991},{"text":"qat_mode","id":"qat_mode","depth":2,"charIndex":1576},{"text":"作用","id":"作用","depth":3,"charIndex":1588},{"text":"可选项定义","id":"可选项定义","depth":3,"charIndex":1669},{"text":"原理介绍","id":"原理介绍","depth":3,"charIndex":1701},{"text":"fuse_bn","id":"fuse_bn","depth":4,"charIndex":1708},{"text":"with_bn","id":"with_bn","depth":4,"charIndex":1861},{"text":"with_bn_reverse_fold","id":"with_bn_reverse_fold","depth":4,"charIndex":2288},{"text":"用法","id":"用法","depth":3,"charIndex":2650},{"text":"设置 qat_mode","id":"设置-qat_mode","depth":4,"charIndex":2655},{"text":"查看当前 qat_mode","id":"查看当前-qat_mode","depth":4,"charIndex":2722},{"text":"设置逐步吸收 BN","id":"设置逐步吸收-bn","depth":4,"charIndex":2740},{"text":"qat_mode 总结","id":"qat_mode-总结","depth":3,"charIndex":2888}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":364,"title":"注册机制","content":"#\n\n注册机制是辅助构建 config 的重要模块，也是HAT的重要组成部分。\n\n本小节通过自定义模块的例子，为您说明如何在注册机制下在增加新的模块并在 config 中正常使用。\n\n\n自定义模块#\n\n以 backbone 为例，这里展示一下如何开发以 mobilenet 为例的新模块。\n\n\n定义一个新的backbone（如MobileNet）：#\n\n新建一个新文件：hat/models/backbones/mobilenet.py。\n\n\n\n\n导入新定义的模块#\n\n可以在 hat/models/backbones/__init__.py 中增加导入模块的行。\n\n\n\n\n在config中使用新的backbone#\n\n\n\n以此类推，其他任何可注册的模块，都可以使用这种方法来完成开发和使用。","routePath":"/guide/advanced_content/hat/tutorials/registry","lang":"zh","toc":[{"text":"自定义模块","id":"自定义模块","depth":2,"charIndex":92},{"text":"定义一个新的backbone（如MobileNet）：","id":"定义一个新的backbone如mobilenet","depth":3,"charIndex":146},{"text":"导入新定义的模块","id":"导入新定义的模块","depth":3,"charIndex":223},{"text":"在`config`中使用新的`backbone`","id":"在config中使用新的backbone","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":365,"title":"HBDK Tool API Reference","content":"#\n\n\nhbdk api introduction#\n\nGeneral restrictions:\n\nThe maximum number of model inputs/outputs should not exceed 256.\n\nThe dimensions of all tensors in the model should not exceed 10.\n\nThe name of the input and output of the model must be unique.\n\nThe data types we support are ui8/si8/si16/ui32/si32/si64/float/bool. For\nspecific data types supported by a certain operator, please refer to the\noperator constraint document.\n\n\nModule: hbdk4.compiler.onnx#\n\nexport(proto onnx.ModelProto, *, name Optional[str] = None) -> Module#\n\n\n\nstatistics(proto onnx.ModelProto)#\n\n\n\n\nModule: hbdk4.compiler.torch#\n\nexport( jit torch.jit.ScriptModule, example_input Any, *, name Optional[str] =\nNone, input_names List[str] = None, output_names List[str] = None,\nlower_non_tensor bool = True) -> Module#\n\n\n\nstatistics(jit torch.jit.ScriptModule, example_input Any)#\n\n\n\n\nModule: hbdk4.compiler.apis#\n\nload(path str) -> Module#\n\n\n\nsave(m Module, path str) -> None#\n\n\n\nconvert( m Module, march Union[MarchBase, str], advice=False, advice_path=\"\",\n**kwargs) -> Module#\n\n\n\nstatistics(m Module) -> list#\n\n\n\nlink(hbo_list List[Hbo], output_path str, desc Optional[str] = None)#\n\n\n\ncompile( m Module, path str, march Union[MarchBase, str], opt int = 2, jobs int\n= 4, max_time_per_fc float = 0.0, debug bool = False, hbdk3_compatible_mode bool\n= False, progress_bar bool = False, advice float = 0.0, balance int = 100,\ninput_no_padding bool = False, output_no_padding bool = False) -> Union[Hbm,\nHbo]#\n\n\n\nvisualize( m Module, onnx_file Optional[str] = None, use_netron Optional[bool] =\nFalse)#\n\n\n\n\nModule: hbdk4.compiler.hbm_tools#\n\nhbm_extract_desc(model str) -> dict#\n\n\n\nhbm_update_desc(model str, desc_dict dict)#\n\n\n\nhbm_perf(model str, output_dir str = None)#\n\n\n\n\nClass: hbdk4.compiler.overlay.Argument#\n\nis_removable(self) -> Tuple#\n\n\n\nget_attached_op(self) -> List[Operation]#\n\n\n\nremove_attached_op(self)#\n\n\n\nerase(self)#\n\n\n\ninsert_transpose(self, permutes List[int])#\n\n\n\ninsert_image_convert(self, mode str = \"nv12\")#\n\n\n\ninsert_image_preprocess( self, mode str, divisor int, mean List[float], std\nList[float], is_signed bool = True)#\n\n\n\ninsert_roi_resize( self, mode str, interp_mode=\"bilinear\", pad_mode=\"constant\",\npad_value Optional[tuple] = (0, -128))#\n\n\n\ninsert_split(self, dim int)#\n\n\n\n\nClass: hbdk4.compiler.overlay.Module#\n\nfunctions(self) -> List[Function]#\n\n\n\ngraphs(self) -> List[Function]#\n\n\n\n\nClass: hbdk4.compiler.overlay.Function#\n\nremove_io_op(self, op_types=None, op_names=None)#\n\n\n\n\nhbdk api example usage#\n\n\nexport model and view model information#\n\nonnx#\n\n\n\n\nserialize model#\n\n\n\n\nfixed point model#\n\n\n\n\ncompile model#\n\ncompile the model exported using PTQ/QAT#\n\n\n\n\npackage multiple models into one HBM#\n\n\n\n\nmodel static perf#\n\n\n\n\nmodel inference#\n\n\n\n\nHBIR model modify#\n\ntensor name modify#\n\n\n\nmodel desc modify#\n\n\n\ninsert nodes#\n\nNote: To avoid the new insertion operator not running in some conversion passes,\nit is recommended to call the insert_xxx api before the convert stage\n\ninsert pyramid input#\n\n\n\ninsert image preprocess#\n\n\n\ninsert roi resize#\n\n\n\ninsert transpose#\n\n\n\ninsert split#\n\n\n\nremove adjacent nodes in the input and output of the model#\n\n\n\n\nHBM modify#\n\n","routePath":"/guide/advanced_content/hbdk_api_reference","lang":"zh","toc":[{"text":"hbdk api introduction","id":"hbdk-api-introduction","depth":2,"charIndex":3},{"text":"Module: hbdk4.compiler.onnx","id":"module-hbdk4compileronnx","depth":3,"charIndex":425},{"text":"`export`(proto onnx.ModelProto, *, name Optional[str] = None) -> Module","id":"exportproto-onnxmodelproto--name-optionalstr--none---module","depth":4,"charIndex":-1},{"text":"`statistics`(proto onnx.ModelProto)","id":"statisticsproto-onnxmodelproto","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.torch","id":"module-hbdk4compilertorch","depth":3,"charIndex":568},{"text":"`export`( jit torch.jit.ScriptModule, example_input Any, *, name Optional[str] = None, input_names List[str] = None, output_names List[str] = None, lower_non_tensor bool = True) -> Module","id":"export-jit-torchjitscriptmodule-example_input-any--name-optionalstr--none-input_names-liststr--none-output_names-liststr--none-lower_non_tensor-bool--true---module","depth":4,"charIndex":-1},{"text":"`statistics`(jit torch.jit.ScriptModule, example_input Any)","id":"statisticsjit-torchjitscriptmodule-example_input-any","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.apis","id":"module-hbdk4compilerapis","depth":3,"charIndex":852},{"text":"`load`(path str) -> Module","id":"loadpath-str---module","depth":4,"charIndex":-1},{"text":"`save`(m Module, path str) -> None","id":"savem-module-path-str---none","depth":4,"charIndex":-1},{"text":"`convert`( m Module, march Union[MarchBase, str], advice=False, advice_path=\"\", **kwargs) -> Module","id":"convert-m-module-march-unionmarchbase-str-advicefalse-advice_path-kwargs---module","depth":4,"charIndex":-1},{"text":"`statistics`(m Module) -> list","id":"statisticsm-module---list","depth":4,"charIndex":-1},{"text":"`link`(hbo_list List[Hbo], output_path str, desc Optional[str] = None)","id":"linkhbo_list-listhbo-output_path-str-desc-optionalstr--none","depth":4,"charIndex":-1},{"text":"`compile`( m Module, path str, march Union[MarchBase, str], opt int = 2, jobs int = 4, max_time_per_fc float = 0.0, debug bool = False, hbdk3_compatible_mode bool = False, progress_bar bool = False, advice float = 0.0, balance int = 100, input_no_padding bool = False, output_no_padding bool = False) -> Union[Hbm, Hbo]","id":"compile-m-module-path-str-march-unionmarchbase-str-opt-int--2-jobs-int--4-max_time_per_fc-float--00-debug-bool--false-hbdk3_compatible_mode-bool--false-progress_bar-bool--false-advice-float--00-balance-int--100-input_no_padding-bool--false-output_no_padding-bool--false---unionhbm-hbo","depth":4,"charIndex":-1},{"text":"`visualize`( m Module, onnx_file Optional[str] = None, use_netron Optional[bool] = False)","id":"visualize-m-module-onnx_file-optionalstr--none-use_netron-optionalbool--false","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.hbm_tools","id":"module-hbdk4compilerhbm_tools","depth":3,"charIndex":1571},{"text":"`hbm_extract_desc`(model str) -> dict","id":"hbm_extract_descmodel-str---dict","depth":4,"charIndex":-1},{"text":"`hbm_update_desc`(model str, desc_dict dict)","id":"hbm_update_descmodel-str-desc_dict-dict","depth":4,"charIndex":-1},{"text":"`hbm_perf`(model str, output_dir str = None)","id":"hbm_perfmodel-str-output_dir-str--none","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Argument","id":"class-hbdk4compileroverlayargument","depth":3,"charIndex":1741},{"text":"`is_removable`(self) -> Tuple","id":"is_removableself---tuple","depth":4,"charIndex":-1},{"text":"`get_attached_op`(self) -> List[Operation]","id":"get_attached_opself---listoperation","depth":4,"charIndex":-1},{"text":"`remove_attached_op`(self)","id":"remove_attached_opself","depth":4,"charIndex":-1},{"text":"`erase`(self)","id":"eraseself","depth":4,"charIndex":-1},{"text":"`insert_transpose`(self, permutes List[int])","id":"insert_transposeself-permutes-listint","depth":4,"charIndex":-1},{"text":"`insert_image_convert`(self, mode str = \"nv12\")","id":"insert_image_convertself-mode-str--nv12","depth":4,"charIndex":-1},{"text":"`insert_image_preprocess`( self, mode str, divisor int, mean List[float], std List[float], is_signed bool = True)","id":"insert_image_preprocess-self-mode-str-divisor-int-mean-listfloat-std-listfloat-is_signed-bool--true","depth":4,"charIndex":-1},{"text":"`insert_roi_resize`( self, mode str, interp_mode=\"bilinear\", pad_mode=\"constant\", pad_value Optional[tuple] = (0, -128))","id":"insert_roi_resize-self-mode-str-interp_modebilinear-pad_modeconstant-pad_value-optionaltuple--0--128","depth":4,"charIndex":-1},{"text":"`insert_split`(self, dim int)","id":"insert_splitself-dim-int","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Module","id":"class-hbdk4compileroverlaymodule","depth":3,"charIndex":2273},{"text":"`functions`(self) -> List[Function]","id":"functionsself---listfunction","depth":4,"charIndex":-1},{"text":"`graphs`(self) -> List[Function]","id":"graphsself---listfunction","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Function","id":"class-hbdk4compileroverlayfunction","depth":3,"charIndex":2386},{"text":"`remove_io_op`(self, op_types=None, op_names=None)","id":"remove_io_opself-op_typesnone-op_namesnone","depth":4,"charIndex":-1},{"text":"hbdk api example usage","id":"hbdk-api-example-usage","depth":2,"charIndex":2481},{"text":"export model and view model information","id":"export-model-and-view-model-information","depth":3,"charIndex":2507},{"text":"onnx","id":"onnx","depth":4,"charIndex":2549},{"text":"serialize model","id":"serialize-model","depth":3,"charIndex":2559},{"text":"fixed point model","id":"fixed-point-model","depth":3,"charIndex":2580},{"text":"compile model","id":"compile-model","depth":3,"charIndex":2603},{"text":"compile the model exported using PTQ/QAT","id":"compile-the-model-exported-using-ptqqat","depth":4,"charIndex":2619},{"text":"package multiple models into one HBM","id":"package-multiple-models-into-one-hbm","depth":3,"charIndex":2665},{"text":"model static perf","id":"model-static-perf","depth":3,"charIndex":2707},{"text":"model inference","id":"model-inference","depth":3,"charIndex":2730},{"text":"HBIR model modify","id":"hbir-model-modify","depth":3,"charIndex":2751},{"text":"tensor name modify","id":"tensor-name-modify","depth":4,"charIndex":2771},{"text":"model desc modify","id":"model-desc-modify","depth":4,"charIndex":2794},{"text":"insert nodes","id":"insert-nodes","depth":4,"charIndex":2816},{"text":"remove adjacent nodes in the input and output of the model","id":"remove-adjacent-nodes-in-the-input-and-output-of-the-model","depth":4,"charIndex":3096},{"text":"HBM modify","id":"hbm-modify","depth":3,"charIndex":3160}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":366,"title":"hmct.api.build_model","content":"#\n\n\n接口说明#\n\nHMCT提供的模型转换功能，输入一个onnx模型，经过模型转换和量化输出量化后的模型。\n\n\n接口形式#\n\n\n\n\n返回值说明#\n\n输出一个量化后的onnx模型，可以用于量化模型精度评测，以及通过hbdk编译成部署模型。\n\n\n参数说明#\n\n\n生成物说明#\n\n生成物名称                        生成物说明\noriginal_float_model.onnx    原始模型转换之后的输出，该阶段的转换包括：opset、ir version转换，input_shape修改操作。\noptimized_float_model.onnx   模型优化阶段的输出，该阶段的转换包括：常量折叠、算子融合、无用算子删除、算子替换、算子拆分。\ncalibrated_model.onnx        模型校准阶段的输出，该阶段的量化包括：插入校准节点，统计数据分布，计算量化参数。\nptq_model.onnx               模型量化阶段的输出，该阶段的量化包括：基于指定march的量化参数调整和转换。","routePath":"/guide/advanced_content/hmct_api_reference/build_model","lang":"zh","toc":[{"text":"接口说明","id":"接口说明","depth":2,"charIndex":3},{"text":"接口形式","id":"接口形式","depth":3,"charIndex":56},{"text":"返回值说明","id":"返回值说明","depth":3,"charIndex":66},{"text":"参数说明","id":"参数说明","depth":2,"charIndex":121},{"text":"生成物说明","id":"生成物说明","depth":2,"charIndex":129}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":367,"title":"hmct.api.check_model","content":"#\n\n\n接口说明#\n\nHMCT提供对模型的转换流程的快速检查功能，对输入的onnx模型，用一些随机的量化参数检查模型的转换和过程是否成功。\n\n\n接口形式#\n\n\n\n\n返回值说明#\n\n生成一个随机参数量化的onnx模型。\n\n\n参数说明#\n\ncheck_model中与build_model相同参数的取值范围和定义完全相同。","routePath":"/guide/advanced_content/hmct_api_reference/check_model","lang":"zh","toc":[{"text":"接口说明","id":"接口说明","depth":2,"charIndex":3},{"text":"接口形式","id":"接口形式","depth":3,"charIndex":71},{"text":"返回值说明","id":"返回值说明","depth":3,"charIndex":81},{"text":"参数说明","id":"参数说明","depth":2,"charIndex":110}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":368,"title":"hmct.api.load_model","content":"#\n\nHMCT提供的模型加载功能，加载一个指定路径的onnx模型，用于后续的模型转换。\n\n\n接口形式#\n\n\n\n\n参数说明#\n\nonnx_model_file，字符串数据类型，必选，onnx模型文件路径。\n\n\n返回值说明#\n\nonnx ModelProto对象。","routePath":"/guide/advanced_content/hmct_api_reference/load_model","lang":"zh","toc":[{"text":"接口形式","id":"接口形式","depth":2,"charIndex":45},{"text":"参数说明","id":"参数说明","depth":2,"charIndex":55},{"text":"返回值说明","id":"返回值说明","depth":2,"charIndex":103}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":369,"title":"hmct.api.ORTExecutor","content":"#\n\n\n接口说明#\n\nHMCT提供的模型推理功能，用于支持对模型转换过程中产生的中间模型进行推理。\n\n\n接口形式#\n\n\n\n\n成员函数#","routePath":"/guide/advanced_content/hmct_api_reference/ortexecutor","lang":"zh","toc":[{"text":"接口说明","id":"接口说明","depth":2,"charIndex":3},{"text":"接口形式","id":"接口形式","depth":3,"charIndex":51},{"text":"成员函数","id":"成员函数","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":370,"title":"常用缩略语","content":"#\n\nA\n\nB\n\nC\n\nD\n\nF\n\nH\n\nI\n\nL\n\nP\n\nQ\n\nR\n\nS","routePath":"/guide/appendix/common_abbreviations","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":371,"title":"常见图像格式","content":"#\n\n\n简介#\n\n随着人工智能的发展，深度神经网络在视觉领域“百花齐放”，为了满足不同场景的需求，我们会接触到多种图像数据格式，\n本小节为大家详细介绍深度学习场景中常用的几种图像数据格式：RGB、BGR、YUV、NV12 和 Gray。\n\n\nRGB#\n\nRGB，一种常见的彩色图像格式。图像的每一个像素点都会存储红（Red）、绿（Green）、蓝（Blue）三个颜色通道的亮度值（0 ~ 255，UINT8）。\n\n基于此，如果按（R，G，B）的方式记录，那么（255,0,0）、（0,255,0）、（0,0,255）可以分别表示最纯粹的 红、绿、蓝。而若 RGB\n三个通道的数值均为 0，综合得到黑色；若三个通道的数值均取最大值 255，综合得到白色。\n\nRGB可表示的色彩数量可达256x256x256≈1677 万，远超人眼的感知范围（约1000万种），因此，RGB\n被广泛应用于各种显示领域，与日常生活息息相关。\n\n然而，RGB在表示颜色时有一个特点，每个像素点必须同时存储 R、G、B\n三个通道数值，即每个像素点需要3个字节的存储空间，而这个特点针对视频场景的存储与传输是非常不友好的，会占用大量的空间和带宽。\n\n\nBGR#\n\nBGR图像格式与RGB类似，只是红、绿、蓝三个通道的排列顺序不同。RGB格式中，像素点的通道顺序是红、绿、蓝，而在BGR格式中，像素点的通道顺序是蓝、绿、红。\n\nBGR格式常用于OpenCV等计算机视觉库中，是一些软件和硬件的默认图像格式，与这些软件和硬件的兼容性更好。\n\nBGR与RGB一样，数据量较大，不适合视频场景的存储与传输。因此，我们还需要其他的图像格式来替代 RGB/BGR用于视频领域。\n\n\nYUV#\n\n\n简介#\n\nYUV，一种彩色图像格式，其中Y表示亮度（Luminance），用于指定一个像素的亮度（可以理解为是黑白程度），U和V表示色度（Chrominance或Chro\nma），用于指定像素的颜色，每个数值都采用UINT8表示，如下图所示。\n\nYUV格式采用亮度-色度分离的方式，也就是说只有U、V参与颜色的表示，这一点与RGB是不同的。\n\n即使没有U、V分量，仅凭Y分量我们也能 “识别”\n出一幅图像的基本内容，只不过此时呈现的是一张黑白图像。而U、V分量为这些基本内容赋予了色彩，黑白图像演变为了彩色图像。\n这意味着，我们可以在保留Y分量信息的情况下，尽可能地减少U、V两个分量的采样，以实现最大限度地减少数据量，这对于视频数据的存储和传输是有极大裨益的，这也是YUV\n相比于RGB更适合视频处理领域的原因。\n\n\nYUV常见格式#\n\n据研究表明，人眼对亮度信息比色彩信息更加敏感。YUV下采样就是根据人眼的特点，将人眼相对不敏感的色彩信息进行压缩采样，得到相对小的文件进行播放和传输。根据Y和U\nV的占比，常用的YUV格式有：YUV444，YUV422，YUV420三种。\n\n用三个图来直观地表示不同采集方式下Y和UV的占比。\n\n\n\n * YUV444：每一个Y分量对应一对UV分量，每像素占用3字节（Y + U + V = 8 + 8 + 8 = 24bits）。\n * YUV422：每两个Y分量共用一对UV分量，每像素占用2字节（Y + 0.5U + 0.5V = 8 + 4 + 4 = 16bits）。\n * YUV420：每四个Y分量共用一对UV分量，每像素占用1.5字节（Y + 0.25U + 0.25V = 8 + 2 + 2 = 12bits）。\n\n此时来理解YUV4xx中的4，这个4，实际上表达了最大的共享单位！也就是最多4个Y共享一对UV。\n\n\nYUV420详解#\n\n在YUV420中，一个像素点对应一个Y，一个4X4的小方块对应一个U和V，每个像素占用1.5个字节。依据不同的UV分量排列方式，还可以将YUV420分为YUV4\n20P和YUV420SP两种格式。\n\nYUV420P是先把U存放完，再存放V，排列方式如下图：\n\n\n\nYUV420SP是UV、UV交替存放的，排列方式如下图：\n\n\n\n此时，相信大家就可以理解为什么YUV420 数据在内存中的长度是width * height * 3 / 2了。\n\n\nNV12#\n\nNV12图像格式属于YUV颜色空间中的YUV420SP格式，每四个Y分量共用一组U分量和V分量，Y连续存放，U与V交叉存放。\n\nNV12在保持图像亮度信息的同时，数据量是RGB/BGR等格式的一半，可以减少模型加载输入数据的时间，因此，嵌入式端通常选用NV12图像作为部署时的图像数据输入\n。\n\n\nGray#\n\nGray图像格式，也称灰度图像格式，是一种单通道图像格式。在Gray图像中，每个像素只包含一个亮度值，每个数值都采用UINT8类型表示，即0~255之间的整数。\n这个亮度值表示图像中每个像素的明暗程度，取值越大表示像素越亮，取值越小表示像素越暗。\n\nGray图像格式也是其他彩色图像格式（如RGB、YUV等）转换为单通道图像时的一种常见格式，只包含图像的亮度信息，图像数据相对较小，因此针对一些对图像色彩信息不\n太敏感的场景，仍然具有重要的应用价值。\n\n\n图像格式之间的转换#\n\n在图像采集、显示方面，主要使用 RGB，但是在图像存储、处理、传输方面，又要选择\nYUV，在一个完整的应用场景中，可能会需要用到不同的图像格式，这时就需要进行图像格式的转换。\n\n那如何实现图像格式之间的转换呢？可以简单地理解为，有一个“标准”，基于这个标准，通过一定的数学运算即可完成不同图像格式之间的转换。下面以计算机视觉库opencv\n封装好的函数为例，看一下如何实现图片格式转换：\n\n\n\n\n\n我们在OE包内中提供了常见图像格式之间的转换源码（例如：RGB2NV12、BGR2RGB等），图片处理常用transformer说明文档请参考用户手册\n图片处理transformer说明\n章节，对应的源码位于OE开发包的samples/ai_toolchain/horizon_model_convert_sample/01_common/pyth\non/data路径下。\n\n不同的图像格式具有不同的性能和优缺点，实际使用时，可以根据自己的需求，个性化选择图像格式。\n\n\n参考链接#\n\n * https://blog.csdn.net/onion2007/article/details/46805335\n\n * https://zhuanlan.zhihu.com/p/538058910\n\n * https://zhuanlan.zhihu.com/p/248116694\n\n * https://blog.csdn.net/zego_0616/article/details/126658494\n\n * https://blog.csdn.net/luoyingxing/article/details/108516163","routePath":"/guide/appendix/community_articles/common_image_format","lang":"zh","toc":[{"text":"简介","id":"简介","depth":2,"charIndex":3},{"text":"RGB","id":"rgb","depth":2,"charIndex":120},{"text":"BGR","id":"bgr","depth":2,"charIndex":511},{"text":"YUV","id":"yuv","depth":2,"charIndex":720},{"text":"简介","id":"简介-1","depth":3,"charIndex":727},{"text":"YUV常见格式","id":"yuv常见格式","depth":3,"charIndex":1087},{"text":"YUV420详解","id":"yuv420详解","depth":3,"charIndex":1516},{"text":"NV12","id":"nv12","depth":2,"charIndex":1750},{"text":"Gray","id":"gray","depth":2,"charIndex":1906},{"text":"图像格式之间的转换","id":"图像格式之间的转换","depth":2,"charIndex":2141},{"text":"参考链接","id":"参考链接","depth":2,"charIndex":2588}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":372,"title":"OE开发包示例介绍","content":"#\n\n算法工具链覆盖了模型训练（浮点训练和量化训练，可选）、转换、性能/精度验证、部署和推理等关键步骤。为了方便您快速体验和学习，OE开发包中提供了丰富、全面的示例。\n为了方便了解和使用这些示例，本文将对这些示例进行详细介绍。\n\n首先，在获取OE开发包后，解压后的示例包目录结构如下所示：\n\n\n\nsamples目录中提供了模型训练示例、浮点模型转定点模型示例以及统一计算平台UCP的相关示例。如下为工具链各阶段使用的一般流程和示例的分布：\n\n在上图工具链的使用流程中，这些示例对应着不同的使用阶段：\n\n * 模型转换示例包含了模型PTQ转换阶段的模型检查、校准数据处理、模型量化编译、单张图片推理和精度评估等过程。\n * 模型训练示例包含了浮点模型训练（可选）和QAT量化训练（可选）阶段的数据集打包脚本、模型config文件、训练脚本以及其它工具脚本，当应用模型精度调优手段\n   后仍未达到预期精度时，可以尝试QAT量化训练策略。\n * UCP板端部署示例，包含视觉处理、模型推理、高性能算子库、自定义算子及目标检测全流程等示例，模型推理示例中提供了模型部署阶段的编译脚本、运行脚本和源代码。\n\n\n模型转换示例（horizon_model_convert_sample）#\n\n工具链在samples/ai_toolchain/horizon_model_convert_sample文件夹下提供了模型转换示例，示例包目录结构如下所示：\n\n\n\nOE包内不仅提供了PTQ模型转换示例，另外还包含了模型检查、校准数据预处理、转换编译、推理等一键运行脚本。\n\n以horizon_model_convert_sample/03_classification目录下的03_resnet50为例，介绍相关脚本的作用：\n\n\n\nPTQ模型转换示例的使用教程请参考PTQ模型转换示例章节的介绍。\n\n\n模型训练示例（horizon_model_train_sample）#\n\n工具链在samples/ai_toolchain/horizon_model_train_sample目录下提供了模型训练示例，示例包结构如下所示：\n\n\n\n通常只需要使用以下命令就可以实现模型的训练：\n\n\n\n * $ 需要配置为configs文件夹中模型训练对应的 config\n   文件路径，它定义了模型结构、数据集加载、以及整套的训练流程，示例中提供了包括分类、检测、分割和光流估计任务等模型。\n\n * tools文件夹提供了包含数据集处理、模型训练、转换编译、计算量统计等执行脚本，相关脚本的功能如下:\n   \n   * calops.py：网络模型计算量统计工具。\n   \n   * compile_perf_hbir.py：编译和perf工具。\n   \n   * create_data.py：数据集预处理工具，用于预处理Kitti3D雷达数据集。\n   \n   * dataset_converters：文件夹下提供了不同数据集格式转换脚本。\n   \n   * datasets：文件夹下提供了数据集打包和数据可视化脚本。\n   \n   * export_hbir.py：HBIR模型导出工具。\n   \n   * export_onnx.py：ONNX模型导出工具，导出的ONNX模型只可用于可视化，不支持推理。\n   \n   * gen_camera_param_nusc.py：从nuscenes中获取相机内外参的脚本。\n   \n   * homography_generator.py：计算ego到图像的转换矩阵的脚本。\n   \n   * reference_points_generator.py：从单应性矩阵计算模型输入参考点的脚本。\n   \n   * gen_reference_points_nusc.py：从nuscenes中获取模型输入参考点的脚本。\n   \n   * infer_hbir.py：单图预测工具。\n   \n   * model_checker.py：模型检查工具。\n   \n   * predict.py：预测工具。\n   \n   * quant_analysis.py：精度debug工具。\n   \n   * train.py：模型训练脚本，支持浮点模型训练、量化训练功能。\n   \n   * validation_hbir.py：精度验证工具，提供上板完全对齐的结果。\n\n模型训练示例的使用教程请参考Horizon Torch Samples章节的介绍。\n\n\n统一计算平台示例（ucp_tutorial）#\n\n工具链在samples/ai_toolchain/ucp_tutorial目录下提供了统一计算平台UCP的示例源代码和运行脚本，示例包结构如下所示：\n\n\n\n * all-round：目标检测全流程示例，详细介绍及使用教程请参考 目标检测全流程示例 章节的介绍。\n\n * custom_operator：自定义算子示例，包含DSP示例和基于OpenCL接口调用板端GPU的开发示例，详细介绍及使用教程请参考 UCP-自定义算子\n   章节的介绍。\n\n * deps_aarch64：AArch64公共依赖目录，包含UCP依赖库及头文件等内容。\n\n * deps_x86：X86仿真公共依赖目录。\n\n * dnn：DNN示例，包括：\n   \n   * ai_benchmark，提供了嵌入式应用开发常见模型的性能和精度评测示例，详细介绍及使用教程请参考 AI Benchmark使用说明 章节的介绍。\n   \n   * basic_samples，提供了模型推理相关由浅及深的使用示例，旨在帮助您熟悉和学习模型推理相关的接口以及各种进阶功能，详细介绍及使用教程请参考\n     基础示例包使用说明 章节的介绍。\n\n * hpl：HPL示例，包括示例源码及最小可执行环境，详细介绍及使用教程请参考 高性能算子库-示例 章节的介绍。\n\n * vp：VP示例，包括示例源码及最小可执行环境，详细介绍及使用教程请参考 视觉处理开发-示例 章节的介绍。\n\n * tools：统一计算平台UCP提供的工具。","routePath":"/guide/appendix/community_articles/oe_package_sample","lang":"zh","toc":[{"text":"模型转换示例（horizon_model_convert_sample）","id":"模型转换示例horizon_model_convert_sample","depth":2,"charIndex":500},{"text":"模型训练示例（horizon_model_train_sample）","id":"模型训练示例horizon_model_train_sample","depth":2,"charIndex":791},{"text":"统一计算平台示例（ucp_tutorial）","id":"统一计算平台示例ucp_tutorial","depth":2,"charIndex":1884}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":373,"title":"数据集下载","content":"#\n\n示例模型使用的数据集可以从以下链接进行获取：\n\n数据集            下载地址                                                           下载结构\nImageNet       https://www.image-net.org/download.php                         下载结构请您参考 数据集参考结构 中的相关描述\nCOCO           https://cocodataset.org/                                       下载结构请您参考 数据集参考结构 中的相关描述\nVOC            http://host.robots.ox.ac.uk/pascal/VOC/                        需要下载2007和2012两个版本， 下载结构请您参考 数据集参考结构 中的相关描述\nCityscapes     https://github.com/mcordts/cityscapesScripts                   下载结构请您参考 数据集参考结构 中的相关描述\nCIFAR-10       http://www.cs.toronto.edu/~kriz/cifar.html                     下载结构请您参考 数据集参考结构 中的相关描述\nFlyingChairs   https://lmb.informatik.uni-freiburg.de/resources/datasets/Fl   下载结构请您参考 数据预处理 中的相关描述\n               yingChairs.en.html\nKITTI3D        https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_be   下载结构请您参考 数据预处理 中的相关描述\n               nchmark=3d\nCULane         https://xingangpan.github.io/projects/CULane.html              下载结构请您参考 数据预处理 中的相关描述\nNuscenes       https://www.nuscenes.org/nuscenes                              下载结构请您参考 数据预处理 中的相关描述\nMot17          https://opendatalab.com/MOT17                                  下载结构请您参考 数据预处理 中的相关描述\nCarfusion      http://www.cs.cmu.edu/~ILIM/projects/IM/CarFusion/cvpr2018/i   下载结构请您参考 数据预处理 中的相关描述\n               ndex.html\nArgoverse 1    https://www.argoverse.org/av1.html                             下载结构请您参考 数据预处理 中的相关描述\nSceneFlow      https://lmb.informatik.uni-freiburg.de/resources/datasets/Sc   下载结构请您参考 数据预处理 中的相关描述\n               eneFlowDatasets.en.html\n\n如果您在数据准备过程中有遇到问题，可以前往 地平线智能汽车开发者社区 发帖进行求助。","routePath":"/guide/appendix/dataset_link","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":374,"title":"HBIR Operator Definition","content":"#\n\n\nhbir.abs (::mlir::hbdk::hbir::AbsOp)#\n\nHBIR tensor abs.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.acos (::mlir::hbdk::hbir::AcosOp)#\n\nHBIR tensor acos.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.acosh (::mlir::hbdk::hbir::AcoshOp)#\n\nHBIR tensor acosh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.add (::mlir::hbdk::hbir::AddOp)#\n\nHBIR tensor addition.\n\nApplies addition operator element-wise, $y_i=lhs_i+rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch add.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.asin (::mlir::hbdk::hbir::AsinOp)#\n\nHBIR tensor asin.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.asinh (::mlir::hbdk::hbir::AsinhOp)#\n\nHBIR tensor asinh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.atan (::mlir::hbdk::hbir::AtanOp)#\n\nHBIR tensor atan.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.atanh (::mlir::hbdk::hbir::AtanhOp)#\n\nHBIR tensor atanh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.avg_pool (::mlir::hbdk::hbir::AvgPoolOp)#\n\nHBIR n-D average pooling(only support 1d and 2d currently).\n\nApplies a nD average pooling over input.\n\nIn the simplest case, the output value of the operator with input size $(N, H,\nW, C)$, output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h}, ker_{w})$\ncan be precisely described as:\n\n$ out(N_i, h, w, C_j) = \\frac{1} { ker_h *ker_w } \\sum_{m = 0} ^ { ker_h - 1\n}\\sum_{n = 0} ^ { ker_w - 1 } input(N_i, stride[0]\\times h + m, stride[1] \\times\nw + n, C_j) $\n\nwhere $h,w$ respectively represent the size of H and W.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * parameters has the same manner as the Conv2D operator, the same goes for the\n   output size.\n\n * ceilMode controls output's compute is mode of floor or ceil, it's default\n   value is false.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in},\n   W_{in})$\n\n * Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(, H_{out},\n   W_{out}, C)$, where $$ represents any number of dimension.\n\n$ H_{out} = \\lfloor {\\frac{H_{in} + padding[0] + padding[2] - kernel[0]}\n{stride[0]} + 1}\\rfloor $\n\n$ W_{out} = \\lfloor {\\frac{W_{in} + padding[1] + padding[3] - kernel[1]}\n{stride[1]} + 1}\\rfloor $\n\nif ceilMode = true, please use ceil replace floor in the above output formula.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch avg_pool.\n\nTraits: CommonVerifier, PoolLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nkernel      ::mlir::ArrayAttr   64-bit integer array attribute\nstride      ::mlir::ArrayAttr   64-bit integer array attribute\npad         ::mlir::ArrayAttr   64-bit integer array attribute\ndilation    ::mlir::ArrayAttr   64-bit integer array attribute\nceilMode    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.batchnorm (::mlir::hbdk::hbir::BatchNormOp)#\n\nHbir Batch Normalize\n\nApplies Batch Normalization over each dimension of input. This compute can be\nprecisely described as:\n\n$ y = \\frac{x-mean[x]}{\\sqrt{Var[x]+\\epsilon}}*weight+bias $\n\nThis mean and standard-deviation are calculated per-dimension over the batches\nand weight and bias are learnable parameter vectors of the input size.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * eps - a value added to the denominator for numerical stability.\n * $mean(x)$ and $Var[x]$'s shape are $(C)$.\n * weight and bias are learnable scalar.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H,W,C)$ or $(N,M,H,W,C)$ or $(H,W,C)$ or $(,H,W,C)$, where $$\n   reprensent any number of dimension.\n * Output: same shape as input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch BatchNorm.\n\nTraits: CommonVerifier, Misc, SameVariadicOperandSize\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\neps         ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nmean      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nvar       1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nweight    1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.bev_pool_v2 (::mlir::hbdk::hbir::BevPoolV2Op)#\n\nHBIR bev_pool_v2 op, cpu operator, from mmdet3d, no corresponding operator in\ntorch/onnx\n\nConvert several planar image inputs into bev image outputs, thus providing\nsupport for data processing under bird's eye view.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * depth: the depth tensor.\n * feat: the feat tensor.\n * ranks_depth: Stores the index value of depth.\n * ranks_feat: Stores the index value of feat.\n * ranks_bev: Stores the Voxel index value of the valid bev space.\n * interval_starts: Each element marks the starting point of each \"continuation\n   segment\" of the ranks_bev feat.\n * interval_lengths: Each element identifies the length of each \"continuous\n   segment\" of the ranks_bev feat.\n * bev_feat_shape: output's shape. Aligned with the public version of cudu\n   kernel, no permute(0, 4, 1, 2, 3) operation is performed in the kernel. And\n   can support rank>=4.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * depth: (B, N, D, fH, fW)\n\n * feat: (B, N, fH, fW, C)\n\n * ranks_depth: (N_points, )\n\n * ranks_feat: (N_points, )\n\n * ranks_bev: (N_points, )\n\n * interval_starts: (N_pillar, )\n\n * interval_lengths: (N_pillar, )\n\n * output: shape same as bev_feat_shape, (B, D_Z, D_Y, D_X, C)\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE        MLIR TYPE           DESCRIPTION\nbev_feat_shape   ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND            DESCRIPTION\ndepth              tensor of 64-bit float or 32-bit float or 16-bit float or\n                   bfloat16 type or 8-bit signed integer or 16-bit signed\n                   integer or 32-bit signed integer or 64-bit signed integer or\n                   8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer or values or\n                   none type\nfeat               tensor of 64-bit float or 32-bit float or 16-bit float or\n                   bfloat16 type or 8-bit signed integer or 16-bit signed\n                   integer or 32-bit signed integer or 64-bit signed integer or\n                   8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer or values or\n                   none type\nranks_depth        tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\nranks_feat         tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\nranks_bev          tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\ninterval_starts    tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\ninterval_lengths   tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cast_type (::mlir::hbdk::hbir::CastTypeOp)#\n\nelemental type cast operation\n\nData are actually moved. Traits: CommonVerifier, Misc, NaiveRoiInfer,\nNaiveTiling, SameOperandsAndResultShape\n\nInterfaces: CastOpInterface, HBTLExecutable, HbdkExecutorInterface,\nHbdkInferType, MoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE          DESCRIPTION\nforceSaturate   ::mlir::BoolAttr   bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.ceil (::mlir::hbdk::hbir::CeilOp)#\n\nHBIR tensor ceil.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.clip (::mlir::hbdk::hbir::ClipOp)#\n\nHBIR Clip Op.\n\nClamps all elements in input into the range $[min, max] $.Assume min_value and\nmax_value be min and max, respectively, this performs:\n\n$ y_i = min(max(x_i, min_value_i), max_value_i) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * min(min_value): lower-bound of the range to be clamped to\n * max(max_value): upper-bound of the range to be clamped to\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch hardtanh.\n\nTraits: CommonVerifier, LutLike, MoveF16CastLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nmin         ::mlir::Attribute   64-bit float attribute or 64-bit signless integer attribute\nmax         ::mlir::Attribute   64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.concat (::mlir::hbdk::hbir::ConcatOp)#\n\nConcatenates tensors along one dimension\n\nConcatenates the given sequence of seq tensors in the given dimension. No\nelemental type conversion.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * dim - the dimension over which the tensors are concatenated.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch cat.\n\nTraits: CommonVerifier, MoveLike, NaiveTiling, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, PortAccess, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninputs    variadic of tensor of 64-bit float or 32-bit float or 16-bit\n          float or bfloat16 type or 8-bit signed integer or 16-bit\n          signed integer or 32-bit signed integer or 64-bit signed\n          integer or 8-bit unsigned integer or 16-bit unsigned integer\n          or 32-bit unsigned integer or 64-bit unsigned integer or or\n          values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.constant (::mlir::hbdk::hbir::ConstantOp)#\n\nHBIR constant generation op.\n\nGenerate a constant with specified type and value Traits: CommonVerifier,\nConstant, NoFuseFp16TypeLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, NoMemoryEffect\n(MemoryEffectOpInterface), NonBatchAxesInfer, Perf, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE              DESCRIPTION\nvalues      ::mlir::ElementsAttr   constant vector/tensor attribute\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.conv2d (::mlir::hbdk::hbir::Conv2dOp)#\n\nHBIR 2-D convolution.\n\nApplies a 2D convolution over an input signal composed of several input\nchannels.\n\nIn the simplest case, the output value of the layer with input size $(N, H_{in},\nW_{in}, C_{in})$ and output $(N, H_{out}, W_{out}, C_{out})$ can be precisely\ndescibed as:\n\n$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in} -\n1}weight(C_{out_j},k) \\star input(N_i,k) $\n\nwhere $\\star$ is the valid 2D cross-correlation operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with 2\n   elements, default value is (1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end),\n   default value is (0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 2\n   elements, default value is (0,0). It's harder to describe, but this link has\n   a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH\n   represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or\n   $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where * represent\n   any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[0] + padding[2] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, NoFuseFp16TypeLike, StencilLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nNoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable, SchedInterface,\nSchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nstride      ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation    ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv2dtranspose (::mlir::hbdk::hbir::Conv2dTransposeOp)#\n\nHBIR transposed conv2d op.\n\nInverse operation of Conv2dTranspose in shape.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor\n * weight: the deconvolution kernel\n * stride: same as conv2d's stride, [s_h, s_w]\n * pad: pad information for clipping output [h_top,w_left,h_bottom,w_right]\n * dilation: same as conv2d's dilation, [d_h, d_w]\n * group: same as conv2d's group\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*, h, w, in_channel)$\n * weight: $(in_channel, kh, kw, out_channel / group)$\n * output: $(*, ho, wo, out_channel)$\n * bias: $out_channel$\n\nwhere:\n\n$ ho = (h - 1) * stride[0] - (pad[0] + pad[2]) + dilation[0] * (kh - 1) + 1 $ $\nwo = (w - 1) * stride[1] - (pad[1] + pad[3]) + dilation[1] * (kw - 1) + 1 $\n\nTraits: CommonVerifier, ConvLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE             DESCRIPTION\nstride          ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad             ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation        ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum        ::mlir::IntegerAttr   64-bit signless integer attribute\nillegalWeight   ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv3d (::mlir::hbdk::hbir::Conv3dOp)#\n\nHBIR 3-D convolution.\n\nApplies a 3D convolution over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size $(N, C_{in},\nD, H, W)$ and output $(N, C_{out}, D_{out}, H_{out}, W_{out})$ can be precisely\ndescribed as:\n\n$ out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1}\nweight(C_{out_j}, k) \\star input(N_i, k) $\n\nwhere $\\star$ is the valid 3D [cross-correlation] operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $D$, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with 3\n   elements, default value is (1,1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 5 elements, the padding sequences is\n   (d_begin,h_begin,w_begin,d_end,h_end,w_end), default value is (0,0,0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 3\n   elements, default value is (0,0,0). It's harder to describe, but this link\n   has a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight: $(C_{out}, KD, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and\n   KH represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,D_{in},H_{in},W_{in},C_{in})$ or $(D_{in},H_{in},W_{in},C_{in})$\n   or $(*,D_{in},H_{in},W_{in},C_{in})$, where * represent any number of\n   dimension.\n\n * Output: $(N,D_{out},H_{out},W_{out},C_{out})$ or\n   $(D_{out},H_{out},W{out},C_{out})$ or $(*,D_{out},H_{out},W_{out},C_{out})$.\n\n$ D_{out}=\\lfloor \\frac{D_{in} + padding[0] + padding[3] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[1] + padding[4] -\ndilation[1]\\times(kernel[1]-1)-1}{stride[1]}+1\\rfloor $\n\n$ W_{out}=\\lfloor \\frac{W_{in} + padding[2] + padding[5] -\ndilation[2]\\times(kernel[2]-1)-1}{stride[2]}+1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nstride      ::mlir::ArrayAttr     64-bit integer array attribute with exactly 3 elements\npad         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 6 elements\ndilation    ::mlir::ArrayAttr     64-bit integer array attribute with exactly 3 elements\ngroupNum    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    5D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv (::mlir::hbdk::hbir::ConvOp)#\n\nHBIR convolution.\n\nApplies a convolution over an input signal composed of several input planes.\n\nrank in convolution is an integer greater than or equal to 1.\n\nIn the simplest case, for 2D convolution, rank=2 the output value of the layer\nwith input size $(N, H_{in}, W_{in}, C_{in})$ and output $(N, H_{out}, W_{out},\nC_{out})$ can be precisely descibed as:\n\n$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in} -\n1}weight(C_{out_j},k) \\star input(N_i,k) $\n\nwhere $\\star$ is the valid 2D cross-correlation operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with\n   rank elements, default value is (1,1,...).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 2*rank elements, when rank=2, the padding sequences is\n   (h_begin,w_begin,h_end,w_end), default value is (0,0,0,0), when n=3, the\n   padding sequences is (d_begin,h_begin,w_begin,d_end,h_end,w_end), default\n   value is (0,0,0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with\n   rank elements, default value is (0,0,...). It's harder to describe, but this\n   link has a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight for 2D: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW\n   and KH represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape for 2D:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or\n   $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where * represent\n   any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[0] + padding[2] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, NoFuseFp16TypeLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE     MLIR TYPE             DESCRIPTION\nstride        ::mlir::ArrayAttr     64-bit integer array attribute\npad           ::mlir::ArrayAttr     64-bit integer array attribute\ndilation      ::mlir::ArrayAttr     64-bit integer array attribute\ngroupNum      ::mlir::IntegerAttr   64-bit signless integer attribute\nchannelLast   ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.cos (::mlir::hbdk::hbir::CosOp)#\n\nHBIR tensor cos.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cosh (::mlir::hbdk::hbir::CoshOp)#\n\nHBIR tensor cosh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cumsum (::mlir::hbdk::hbir::CumSumOp)#\n\nHBIR cumsum.\n\nPerforms cumulative sum of the input elements along the given axis. By default,\nit will do the sum inclusively meaning the first element is copied as is.\nThrough an exclusive attribute, this behavior can change to exclude the first\nelement. It can also perform summation in the opposite direction of the axis.\nFor that, set reverse attribute to 1.\n\nArgs: input (Tensor): the input tensor. output (Tensor): Output tensor of the\nsame type as input with cumulative sums of the input elements Attribute: axis\n(int): Must be in the range [-rank(input), rank(input)-1]. Negative value means\ncounting dimensions from the back. exclusive (int): Must be 0 or 1, defaut is 0.\n0 means the first element is copied to output, 1 will not. reverse (int): Must\nbe 0 or 1, defaut is 0. 1 means performing summation in the opposite direction\nof the axis.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch cumsum.\n\nTraits: CommonVerifier, Misc, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\naxis        ::mlir::IntegerAttr   64-bit signless integer attribute\nexclusive   ::mlir::IntegerAttr   64-bit signless integer attribute\nreverse     ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.deform_conv2d (::mlir::hbdk::hbir::DeformConv2dOp)#\n\nHBIR deformable 2-D convolution.\n\nApplies a deformable 2D convolution over an input signal composed of several\ninput channels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * offset controls the offset for the sampling locations in the convolution\n   kernel.\n\n * mask controls different weights to different positions in the convolution\n   kernel.\n\n * stride controls the stride for the cross-correlation, an integer array with 2\n   elements, default value is (1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end),\n   default value is (0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 2\n   elements, default value is (0,0). It's harder to describe, but this link has\n   a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * offsetGroup controls the connections between inputs and offset, an integer\n   variable, default value is 1.\n\n * weight: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH\n   represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where *\n   represent any number of dimension.\n\n * Offset:\n   $(N,H_{out},W_{out},2\\timesoffset_groups\\timeskernel[0]\\timeskernel[1])$ or\n   $(*,H_{out},W_{out},2\\timesoffset_groups\\timeskernel[0]\\timeskernel[1])$,\n   where * represent any number of dimension.\n\n * Mask: $(N,H_{out},W_{out},offset_groups\\timeskernel[0]\\timeskernel[1])$ or\n   $(*,H_{out},W_{out},offset_groups\\timeskernel[0]\\timeskernel[1])$, where *\n   represent any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor\n\\frac{H_{in}+padding[0]+padding[2]-dilation[0]\\times(kernel[0]-1)-1}{stride[0]}+\n1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, Misc, NoFuseFp16TypeLike, SameVariadicOperandSize,\nStencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE        MLIR TYPE             DESCRIPTION\nstride           ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad              ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum         ::mlir::IntegerAttr   64-bit signless integer attribute\noffsetGroupNum   ::mlir::IntegerAttr   64-bit signless integer attribute\nuseMask          ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\noffset    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nmask      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.div (::mlir::hbdk::hbir::DivOp)#\n\nHBIR tensor division.\n\nApplies division operator element-wise, $y_i=lhs_i\\div rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch div.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.equal (::mlir::hbdk::hbir::EqualOp)#\n\nHBIR tensor equal.\n\nDetermines whether two tensors are equal element by element - wise, $y_i =\n(lhs_i == rhs_i) $. Traits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.exp (::mlir::hbdk::hbir::ExpOp)#\n\nHBIR tensor exp.\n\nApplies exponential operator element - wise, $y=e^{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the exponential of the elements of the input tensor\n   input.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.fake_cast (::mlir::hbdk::hbir::FakeCastOp)#\n\nfake elemental type cast operation\n\nCast float input to specified dtype, and then cast back to the same float type.\nTraits: CommonVerifier, Misc, SameElementType, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE          DESCRIPTION\ndtype       ::mlir::TypeAttr   any type attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 32-bit float or 16-bit float values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 32-bit float or 16-bit float values\n\n\nhbir.flip (::mlir::hbdk::hbir::FlipOp)#\n\nHBIR flip\n\nReverse the order of the input tensor along given axis in dims.\n\n--------------------------------------------------------------------------------\n\nParametes:\n\n * input: the input tensor.\n * dims: axis need to reverse.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: same as the input.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.floor (::mlir::hbdk::hbir::FloorOp)#\n\nHBIR tensor floor.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.gelu (::mlir::hbdk::hbir::GELUOp)#\n\nHBIR GELU activation.\n\nApplies gelu funciton element-wise. Gelu functon defined as:\n\n$ Gelu(x) = xP(X\\leq x) = x*\\phi(x) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Gelu is shorthand for Gaussian Error Linear Unit.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch gelu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.gather_elements (::mlir::hbdk::hbir::GatherElementsOp)#\n\nHBIR gather op for onnx GatherElements.\n\nHBIR gather op for onnx GatherElements. Traits: CommonVerifier, Misc,\nMoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer,\nPerf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.gather_nd (::mlir::hbdk::hbir::GatherNdOp)#\n\nHBIR gather_nd op for onnx gather_nd.\n\nHBIR gather_nd op for onnx gather_nd. Traits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer,\nPerf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nbatchDim    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.greater_equal (::mlir::hbdk::hbir::GreaterEqualOp)#\n\nHBIR tensor greater_equal.\n\nApplies greater_equal operator element - wise, $y_i = lhs_i > = rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.greater (::mlir::hbdk::hbir::GreaterOp)#\n\nHBIR tensor greater.\n\nApplies greater operator element - wise, $y_i = lhs_i > rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.grid_sample (::mlir::hbdk::hbir::GridSampleOp)#\n\nHBIR grid_sample.\n\nFrom the input and a flow-field grid, computes the output using input values and\npixel locations from the grid.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: input of shape $(*, H_{in}, W_{in}, C_{in}) $, where * represent any\n   number of dimension.\n * grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$\n * output: $(*, H_{out}, W_{out}, C_{in})$\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\nalignCorner     ::mlir::BoolAttr                      bool attribute\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\ngrid      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.index (::mlir::hbdk::hbir::IndexOp)#\n\nHBIR index op for aten::index_select\n\nReturns a new tensor which indexes the :attr:input tensor along dimension\n:attr:dim using the entries in :attr:index which is a LongTensor.\n\nThe returned tensor has the same number of dimensions as the original tensor\n(:attr:input). The :attr:dim\\ th dimension has the same size as the length of\n:attr:index; other dimensions have the same size as in the original tensor.\n\nArgs: input (Tensor): the input tensor. dim (int): the dimension in which we\nindex index (IntTensor or LongTensor): the tensor containing the indices to\nindex\n\nTraits: CommonVerifier, Misc, MoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindex     tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.layernorm (::mlir::hbdk::hbir::LayerNormOp)#\n\nHbir Layer Normalize\n\nApplies Layer Normalization over a mini - batch of inputs. This compute can be\nprecisely described as:\n\n$ y = \\frac{x - mean[x]} {\\sqrt{Var[x] +\\epsilon}} * weight + bias $\n\nThe Mean and standard-deviation are calculated over the last D dimensions, where\nD is the dimension of normalized_shape(dims).\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Unlike Batch Normalization, which compute mean and standard - deviation,\n   LayerNormalization compute these in single sample's different dimension.\n * dims controls normalized_shape.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, *)$\n * Output: $(N, *)$ (same shape as input)\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch layerNorm.\n\nTraits: CommonVerifier, Misc, SameVariadicOperandSize\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nNoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer, Perf, Quantizable,\nRoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\neps         ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nweight    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nbias      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.leaky_relu (::mlir::hbdk::hbir::LeakyReLUOp)#\n\nHBIR Leaky ReLU Op.\n\nApplies LeakyRelu funciton element - wise.LeakyRelu function defined as:\n\n$ LeakyRelu(x) = max(0, x) + slope * min(0, x) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * slope - Controls the angle of the negative slope.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch leaky_relu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling, SameElementType,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nslop        ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.less_equal (::mlir::hbdk::hbir::LessEqualOp)#\n\nHBIR tensor less_equal.\n\nApplies less_equal operator element - wise, $y_i = lhs_i < = rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.less (::mlir::hbdk::hbir::LessOp)#\n\nHBIR tensor less.\n\nApplies less operator element - wise, $y_i = lhs_i < rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.linear (::mlir::hbdk::hbir::LinearOp)#\n\nHBIR Linear.\n\nApplies a linear transformation to the incoming data: $y = xW ^ T + b$\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Weight's shape is $(C_{in},C_{out})$.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(, C_{in})$ where $$ represents any number of dimensions and $C_{in}$\n   = in_features.\n * Output: $(*, C_{out})$ where all but the last dimension are the same shape as\n   the input and $C_{out}$ = out_features.\n\n--------------------------------------------------------------------------------\n\nPrototype: pytorch linear.\n\nTraits: CommonVerifier, LinearLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    2D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values\n\n\nhbir.log (::mlir::hbdk::hbir::LogOp)#\n\nHBIR tensor log.\n\nApplies natural logarithm operator element - wise, $y = \\log_e{(x)}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the natural logarithm of the elements of input.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.log_softmax (::mlir::hbdk::hbir::LogSoftmaxOp)#\n\nHBIR LogSoftmax Op.\n\nApplies the LogSoftmax function to an n - dimensional input Tensor rescaling\nthem so that the elements of the n-dimensional output. The output tensor has the\nsame dimension and shape as the input with values in the range [-inf, 0)\n\nLogSoftmax function is defined as:\n\n$ LogSoftmax(x_i) = log(\\frac{exp(x_i)} {\\sum_jexp(x_j)}) $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(*), where * means, any number of additional dimensions.\n * Output(*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch LogSoftmax.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.logical_and (::mlir::hbdk::hbir::LogicalAndOp)#\n\nHBIR tensor and.\n\nApplies 'logical and' operator element - wise, $y_i = lhs_i && rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike, SameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\nrhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.logical_not (::mlir::hbdk::hbir::LogicalNotOp)#\n\nHBIR tensor logical not, output bool.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape,\nSameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 8-bit signed integer or 16-bit signed integer or\n          16-bit float or or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.logical_or (::mlir::hbdk::hbir::LogicalOrOp)#\n\nHBIR tensor or.\n\nApplies 'logical or' operator element - wise, $y_i = lhs_i || rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike, SameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\nrhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.matmul (::mlir::hbdk::hbir::MatMulOp)#\n\nHBIR Matrix Multiplication.\n\nApplies matrix multiplication between two inputs: $C = A \\times B$, where\n$\\times$ means matrix multiplication.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * If both tensors are 1-dimensional, the dot product (scalar) is returned\n * If both arguments are 2-dimensional, the matrix-matrix product is returned\n * If the first argument is 1-dimensional and the second argument is\n   2-dimensional, the matrix-vector product is returned\n * If the first argument is 2-dimensional and the second argument is\n   1-dimensional, the matrix-vector product is returned\n * If both arguments are at least 1-dimensional and at least one argument is\n   N-dimensional (where N > 2), then a batched matrix multiply is returned\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * lhs: $(B_{l}, M, C)$, where $B_{l}$ represent any number of dimension.\n * rhs: $(B_{r}, C, N)$, where $B_{r}$ represent any number of dimension.\n * output: $(B_{o}, M, N)$, where $B_{o}$ represent represent the result of\n   broadcast between $B_{l}$ and $V_{r}$.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch Matmul.\n\nTraits: CommonVerifier, MatmulLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values\n\n\nhbir.max (::mlir::hbdk::hbir::MaxOp)#\n\nHBIR tensor max.\n\nApplies maximum operator element-wise, $y_i=max(lhs_i,rhs_i)$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.max_pool (::mlir::hbdk::hbir::MaxPoolOp)#\n\nHBIR n-D max pooling(only support 1d and 2d currently).\n\nApplies a n-D max Pooling over an input.\n\nIn the 2d case, for example, the output value of the operator with input size\n$(N, H, W, C)$, output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h},\nker_{w})$ can be precisely described as:\n\n$ out(N_i, h, w, C_j) = \\frac{1} { ker_h *ker_w }\\max_{m = 0} ^ { ker_h - 1\n}\\max_{n = 0} ^ { ker_w - 1 } input(N_i, stride[0] \\times h + m, stride[1]\n\\times w + n, C_j) $\n\nwhere $h,w$ respectively represent the size of H and W.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * parameters has the same manner as the Conv2D operator, the same goes for the\n   output size.\n * ceilMode controls output 's compute is mode of floor or ceil, it' s default\n   value is false.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in},\n   W_{in})$\n\n * Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(, H_{out},\n   W_{out}, C)$, where $$ represents any number of dimension.\n\n$ H_{out} = \\lfloor {\\frac{H_{in} + padding[0] + padding[2] - kernel[0]}\n{stride[0]} + 1}\\rfloor $\n\n$ W_{out} = \\lfloor {\\frac{W_{in} + padding[1] + padding[3] - kernel[1]}\n{stride[1]} + 1}\\rfloor $\n\nif ceilMode = true, please use ceil replace floor in the above ouput formula.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch max_pool.\n\nTraits: CommonVerifier, MoveF16CastLike, PoolLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nkernel      ::mlir::ArrayAttr   64-bit integer array attribute\nstride      ::mlir::ArrayAttr   64-bit integer array attribute\npad         ::mlir::ArrayAttr   64-bit integer array attribute\ndilation    ::mlir::ArrayAttr   64-bit integer array attribute\nceilMode    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.min (::mlir::hbdk::hbir::MinOp)#\n\nHBIR tensor min.\n\nApplies minimum operator element-wise, $y_i=min(lhs_i,rhs_i)$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.mod (::mlir::hbdk::hbir::ModOp)#\n\nHbir get modulo of two dividing tensors\n\nComputes the modulo function. It is equivalent to the operator x1 % x2\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * sameSignAsDividend: result has the same sign as dividend or divisor. Default\n   true means same sign as dividend.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE            MLIR TYPE          DESCRIPTION\nsameSignAsDividend   ::mlir::BoolAttr   bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.mul (::mlir::hbdk::hbir::MulOp)#\n\nHBIR tensor multiplication.\n\nApplies multiplication operator element-wise, $y_i=lhs_i\\times rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch mul.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.nan_to_num (::mlir::hbdk::hbir::NanToNumOp)#\n\nHBIR tensor nan_to_num.\n\nReplaces NaN, positive infinity, and negative infinity values in input with the\nvalues specified by nan, posinf, and neginf.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the replace value of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nnan         ::mlir::FloatAttr   64-bit float attribute\nposinf      ::mlir::FloatAttr   64-bit float attribute\nneginf      ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type values or none type\n\n\nhbir.neg (::mlir::hbdk::hbir::NegOp)#\n\nHBIR tensor neg.\n\nApplies negation operator element - wise, $y = x\\times - 1$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the negative of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.nonzero (::mlir::hbdk::hbir::NonZeroOp)#\n\nHBIR nonzero op for torch.nonzero\n\nFind all indices in tensor that are not 0\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer values or\n         none type\n\n\nhbir.pad (::mlir::hbdk::hbir::PadOp)#\n\nPad at both edges of Tensor.\n\nPadding at the begin and end position with constant / border value. Traits:\nCommonVerifier, Expansion, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                         DESCRIPTION\nbegin           ::mlir::ArrayAttr                 64-bit integer array attribute\nend             ::mlir::ArrayAttr                 64-bit integer array attribute\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr   mode to expand input feature on H/W\npadValue        ::mlir::Attribute                 64-bit float attribute or 64-bit signless integer attribute\nfoldable        ::mlir::BoolAttr                  bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.point_pillar_preprocess (::mlir::hbdk::hbir::PointPillarPreProcessOp)#\n\nHBIR point pillar preprocess op.\n\nHBIR point pillar preprocess.Voxelization and Normalization Traits:\nCommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE             DESCRIPTION\npcRanges            ::mlir::ArrayAttr     64-bit float array attribute\nnormRanges          ::mlir::ArrayAttr     64-bit float array attribute\nvoxelSizes          ::mlir::ArrayAttr     64-bit float array attribute\nmaxVoxelNum         ::mlir::IntegerAttr   64-bit signless integer attribute\nmaxPointsPerVoxel   ::mlir::IntegerAttr   64-bit signless integer attribute\nnormDims            ::mlir::ArrayAttr     64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\npoints    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\nvoxels   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\ncoords   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.pow (::mlir::hbdk::hbir::PowOp)#\n\nHBIR tensor pow.\n\nPow takes input data(lhs) and exponent Tensor(rhs), and produces one output data\nwhere the function $f(x) = x ^ {exponent}$, is applied to the data tensor\nelement - wise Traits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.prelu (::mlir::hbdk::hbir::PreluOp)#\n\nHBIR tensor prelu\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nslope     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.relu (::mlir::hbdk::hbir::ReLUOp)#\n\nHBIR ReLU activation.\n\nApplies the rectified linear unit function element - wise.Relu function is\ndefined as:\n\n$ ReLU(x) = (x) ^ + = max(0, x) $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(*), where * means any number of dimensions.\n * Output(*), same shapes as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch Relu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling, SameElementType,\nSameOperandsAndResultShape\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reciprocal (::mlir::hbdk::hbir::ReciprocalOp)#\n\nHBIR tensor reciprocal.\n\nApplies reciprocal operator element - wise, $y = \\frac{1}{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the reciprocal of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_all (::mlir::hbdk::hbir::ReduceAllOp)#\n\nTests if all elements in input evaluate to True.\n\nReturn True if all elements in the row evaluate to True and False otherwise.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce all on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.reduce_argmax (::mlir::hbdk::hbir::ReduceArgmaxOp)#\n\nCalculate max on multiple axes and return its index.\n\nReturn the indices of the max elements of the input tensor's element along the\nprovided axis.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimension to perform reduce argmax on. Accepted range is [-r, r - 1]\n   where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output\n   shape will be 1x3x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceArgMax.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, MoveTransposeInterface, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.reduce_argmin (::mlir::hbdk::hbir::ReduceArgminOp)#\n\nCalculate min on multiple axes and return its index.\n\nReturn the indices of the min elements of the input tensor's element along the\nprovided axis.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimension to perform reduce argmin on. Accepted range is [-r, r - 1]\n   where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output\n   shape will be 1x3x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceArgMin.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, MoveTransposeInterface, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.reduce_max (::mlir::hbdk::hbir::ReduceMaxOp)#\n\nCalculate max on multiple axes.\n\nReturn the max value of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce max on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMax.\n\nTraits: CommonVerifier, MoveF16CastLike, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_mean (::mlir::hbdk::hbir::ReduceMeanOp)#\n\nCalculate mean on multiple axes.\n\nReturn the mean of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce mean on. If it's a list, reduce over all\n   of them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMean.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_min (::mlir::hbdk::hbir::ReduceMinOp)#\n\nCalculate min on multiple axes.\n\nReturn the min value of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce min on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMin\n\nTraits: CommonVerifier, MoveF16CastLike, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_sum (::mlir::hbdk::hbir::ReduceSumOp)#\n\nCalculate sum on multiple axes.\n\nReturn the sum of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce sum on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceSum.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reshape (::mlir::hbdk::hbir::ReshapeOp)#\n\nView a tensor as another shape\n\nReturns a tensor with the same data and number of elements as input, but with\nthe specified shape.When possible, the returned tensor will be a view of input.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * shape - the new shape.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch reshape.\n\nTraits: CommonVerifier, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nshape       ::mlir::ArrayAttr   64-bit integer array attribute\nfoldable    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.resize2d (::mlir::hbdk::hbir::Resize2dOp)#\n\nHBIR 2-D resizing.\n\nScale the input proportionally.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * ratio controls zoom size.*mode controls interpolation type, it's default\n   value is nearest.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(*, H_{in}, W_{in}, C)$\n * Output: $(*, H_{out}, W_{out}, C)$, where\n\n$ H_{out} = \\lfloor{H_{in} * ratio}\\rfloor $\n\n$ W_{out} = \\lfloor{W_{in} * ratio}\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch upsample_nearest2d.\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nratio           ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\nsize            ::mlir::ArrayAttr                     64-bit integer array attribute with exactly 2 elements\nstep            ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\ninitialOffset   ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.roll (::mlir::hbdk::hbir::RollOp)#\n\nRoll the tensor along the given dimensions\n\nRoll the tensor along the given dimension.Elements that are shifted beyond the\nlast position are re-introduced at the first position.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * shifts - The number of places by which the elements of the tensor are\n   shifted.\n * dims -Axis along which to roll.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch roll.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nshifts      ::mlir::ArrayAttr   64-bit integer array attribute\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.round (::mlir::hbdk::hbir::RoundOp)#\n\nHBIR tensor round.\n\nRounds elements of input to the nearest integer.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * This function implements the round half to even.\n\n * Returns a new tensor with the round of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndecimals    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.rsqrt (::mlir::hbdk::hbir::RsqrtOp)#\n\nHBIR tensor rsqrt.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.scatter_elements (::mlir::hbdk::hbir::ScatterElementsOp)#\n\nHBIR scatter elements op. Same semantics as scatter elements in onnx. In\naddition, it supports the mean mode of torch scatter_reduce\n\nHBIR scatter elements scatter.The ONNX op link: https: //\ngithub.com/onnx/onnx/blob/main/docs/Operators.md#ScatterElements\n\nCopy the data to output, the specify a direction axis, use the values in updates\nto update the values in output at specific location according to indices.\n\nIn addition, it supports the mean mode of torch scatter_reduce.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE                             DESCRIPTION\naxis                ::mlir::IntegerAttr                   64-bit signless integer attribute\nscatterReduceMode   ::mlir::hbdk::ScatterReduceModeAttr   scatter reduce mode\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ndata      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\nupdates   tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.scatter_nd (::mlir::hbdk::hbir::ScatterNDOp)#\n\nHBIR scatterND op. Same semantics as scatterND in onnx.\n\nHBIR scatterDN.The ONNX op link: https: //\ngithub.com/onnx/onnx/blob/main/docs/Operators.md#ScatterND\n\nCopy the data to output, then use the values in updates to update the values in\nthe output at some directions given by the indices.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE                             DESCRIPTION\nscatterReduceMode   ::mlir::hbdk::ScatterReduceModeAttr   scatter reduce mode\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ndata      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values\nupdates   tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.select (::mlir::hbdk::hbir::SelectOp)#\n\nselect a tensor from a bigger tensor on a specific dim and index\n\nSlices the input tensor along the selected dimension at the given index.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * dim - the dimension to slice.\n * index - the index to select with.\n * Select operator is equivalent to slicing.\n\n--------------------------------------------------------------------------------\n\nPrototype:Pytorch select.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nindex       ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.sigmoid (::mlir::hbdk::hbir::SigmoidOp)#\n\nHBIR Sigmoid activation.\n\nApplies the element - wise function.Sigmoid function is defined as:\n\n$ Sigmoid(x) = \\frac{1} {1 + exp(-x)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch sigmoid.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sign (::mlir::hbdk::hbir::SignOp)#\n\nHBIR tensor sign.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sin (::mlir::hbdk::hbir::SinOp)#\n\nHBIR tensor sin.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sinh (::mlir::hbdk::hbir::SinhOp)#\n\nHBIR tensor sinh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.slice (::mlir::hbdk::hbir::SliceOp)#\n\nSlice a tensor out of a tensor\n\nSlicing like python's style means taking elements from one given index to\nanother given index.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * begin -the index start to pick(inclusive).\n * end -the index end to pick(exclusive).\n * step -the step interval of picking.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch slice.\n\nTraits: CommonVerifier, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nbegin       ::mlir::ArrayAttr   64-bit integer array attribute\nend         ::mlir::ArrayAttr   64-bit integer array attribute\nstep        ::mlir::ArrayAttr   64-bit integer array attribute\nfoldable    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.slice_scatter (::mlir::hbdk::hbir::SliceScatterOp)#\n\nEmbeds the values of the src tensor into input at the given dimension\n\nEmbeds the values of the src tensor into input at the given dimension. This\nfunction returns a tensor with fresh storage; it does not create a view.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * src (Tensor) -the tensor to embed into input.\n * dim (int) -the dimension to insert the slice into.\n * start (int) -the start index of where to insert the slice.\n * end (int) -the end index of where to insert the slice.\n * step (int) -the how many elements to skip in.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch slice_scatter.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nstart       ::mlir::IntegerAttr   64-bit signless integer attribute\nend         ::mlir::IntegerAttr   64-bit signless integer attribute\nstep        ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nsrc       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.softmax (::mlir::hbdk::hbir::SoftmaxOp)#\n\nHBIR Softmax Op.\n\nApplies the Softmax function to an n - dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output. Tensor lie in the range $[0,\n1] $ and sum to 1.\n\nSoftmax function is defined as:\n\n$ Softmax(x_i) = \\frac{exp(x_i)} {\\sum_jexp(x_j)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(), where $$ means, any number of additional dimensions.Output(), same\n   shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch softmax.\n\nTraits: CommonVerifier, Misc, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.softplus (::mlir::hbdk::hbir::SoftplusOp)#\n\nHBIR Softplus Op.\n\nApplies the SoftPlus function element-wise. SoftPlus function defined as:\n\n$ SoftPlus(x) = \\frac{1}{\\beta}*log(1+exp(\\beta * x)) $\n\nSoftPlus is a smooth approximation to the ReLU function.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * beta - the $\\beta$ value for the Softplus formulation.\n * max - values is for numerical stability, when $\\beta *x > max,\n   SoftPlus(x)=x$.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch softplus.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nbeta        ::mlir::FloatAttr   64-bit float attribute\nthreshold   ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sort (::mlir::hbdk::hbir::SortOp)#\n\nHBIR tensor sort\n\nSorts the elements of the input tensor along a given dimension in ascending\norder by value.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dim: the dimension to sort along.\n * descending: controls the sorting order (ascending or descending).\n * stable: makes the sorting routine stable, which guarantees that the order of\n   equivalent elements is preserved.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, *)$\n * Values: $(N, *)$ (same shape as input)\n * Indices: $(N, *)$ (same shape as input)\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch sort.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE    MLIR TYPE             DESCRIPTION\ndim          ::mlir::IntegerAttr   64-bit signless integer attribute\ndescending   ::mlir::BoolAttr      bool attribute\nstable       ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT    DESCRIPTION\nvalues    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer values or\n          none type\n\n\nhbir.sqrt (::mlir::hbdk::hbir::SqrtOp)#\n\nHBIR tensor sqrt.\n\nApplies square root operator element - wise, $y = \\sqrt{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the square root of the elements of input. If input\n   is negative, then it will return NaN.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.stack (::mlir::hbdk::hbir::StackOp)#\n\nStack multiple tensors along one extra dimension\n\nConcatenates a sequence of tensors along a new dimension.No elemental type\nconversion.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * All tensors need to be of the same size.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch stack.\n\nTraits: CommonVerifier, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninputs    variadic of tensor of 64-bit float or 32-bit float or 16-bit\n          float or bfloat16 type or 8-bit signed integer or 16-bit\n          signed integer or 32-bit signed integer or 64-bit signed\n          integer or 8-bit unsigned integer or 16-bit unsigned integer\n          or 32-bit unsigned integer or 64-bit unsigned integer or or\n          values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.sub (::mlir::hbdk::hbir::SubOp)#\n\nHBIR tensor substraction.\n\nApplies substraction operator element-wise, $y_i=lhs_i-rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.tan (::mlir::hbdk::hbir::TanOp)#\n\nHBIR tensor tan.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.tanh (::mlir::hbdk::hbir::TanhOp)#\n\nHBIR Tanh activation.\n\nApplies the tanh function element - wise.Tanh function is defined as:\n\n$ Tanh(x) =\\frac{exp(x) - exp(-x)} {exp(x) + exp(-x)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch tanh.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.tile (::mlir::hbdk::hbir::TileOp)#\n\nConstructs a tensor by tiling a given tensor.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE    MLIR TYPE           DESCRIPTION\nmultiplies   ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.topk (::mlir::hbdk::hbir::TopkOp)#\n\nHBIR tensor topk\n\nReturns the k largest elements of the given input tensor along a given\ndimension.\n\nIf dim is not given, the last dimension of the input is chosen.\n\nIf largest is False then the k smallest elements are returned.\n\nvalues, indices are returned in separate tensors, where the indices are the\nindices of the elements in the original input tensor.\n\nThe boolean option sorted if True, will make sure that the returned k elements\nare themselves sorted.\n\nTraits: CommonVerifier, Misc, MoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nk           ::mlir::IntegerAttr   64-bit signless integer attribute\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nlargest     ::mlir::BoolAttr      bool attribute\nsorted      ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT    DESCRIPTION\nvalues    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer values or\n          none type\n\n\nhbir.transpose (::mlir::hbdk::hbir::TransposeOp)#\n\nReverse or permute the dims of an array; returns the modified array.\n\nReturns a tensor that is a view of the original tensor input with its dimesions\npermuted.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * input: the input tensor.\n * dims: the desired ordering of dimensions.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch permute.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nNoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer, Perf, Quantizable,\nRoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.warp (::mlir::hbdk::hbir::WarpOp)#\n\nHBIR warp.\n\nFrom the input, sample(bi - linear interpolation) pixels specified by grid.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: input of shape $(*, H_{in}, W_{in}, C_{in})$, where * represent any\n   number of dimension.\n * grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$\n * output: $(*, H_{out}, W_{out}, C_{in})$\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nmove      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.where (::mlir::hbdk::hbir::WhereOp)#\n\nHBIR where op\n\nReturn a tensor of elements selected from either lhs or rhs, depending on\ncondition.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND     DESCRIPTION\ncondition   tensor of values or none type\nlhs         tensor of 64-bit float or 32-bit float or 16-bit float or\n            bfloat16 type or 8-bit signed integer or 16-bit signed\n            integer or 32-bit signed integer or 64-bit signed integer or\n            8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n            unsigned integer or 64-bit unsigned integer or or values or\n            none type\nrhs         tensor of 64-bit float or 32-bit float or 16-bit float or\n            bfloat16 type or 8-bit signed integer or 16-bit signed\n            integer or 32-bit signed integer or 64-bit signed integer or\n            8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n            unsigned integer or 64-bit unsigned integer or or values or\n            none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type","routePath":"/guide/appendix/hbir_op","lang":"zh","toc":[{"text":"`hbir.abs` (::mlir::hbdk::hbir::AbsOp)","id":"hbirabs-mlirhbdkhbirabsop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands","depth":3,"charIndex":445},{"text":"Results:","id":"results","depth":3,"charIndex":842},{"text":"`hbir.acos` (::mlir::hbdk::hbir::AcosOp)","id":"hbiracos-mlirhbdkhbiracosop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-1","depth":3,"charIndex":1655},{"text":"Results:","id":"results-1","depth":3,"charIndex":2052},{"text":"`hbir.acosh` (::mlir::hbdk::hbir::AcoshOp)","id":"hbiracosh-mlirhbdkhbiracoshop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-2","depth":3,"charIndex":2868},{"text":"Results:","id":"results-2","depth":3,"charIndex":3265},{"text":"`hbir.add` (::mlir::hbdk::hbir::AddOp)","id":"hbiradd-mlirhbdkhbiraddop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-3","depth":3,"charIndex":4309},{"text":"Results:","id":"results-3","depth":3,"charIndex":5073},{"text":"`hbir.asin` (::mlir::hbdk::hbir::AsinOp)","id":"hbirasin-mlirhbdkhbirasinop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-4","depth":3,"charIndex":5889},{"text":"Results:","id":"results-4","depth":3,"charIndex":6286},{"text":"`hbir.asinh` (::mlir::hbdk::hbir::AsinhOp)","id":"hbirasinh-mlirhbdkhbirasinhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-5","depth":3,"charIndex":7102},{"text":"Results:","id":"results-5","depth":3,"charIndex":7499},{"text":"`hbir.atan` (::mlir::hbdk::hbir::AtanOp)","id":"hbiratan-mlirhbdkhbiratanop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-6","depth":3,"charIndex":8312},{"text":"Results:","id":"results-6","depth":3,"charIndex":8709},{"text":"`hbir.atanh` (::mlir::hbdk::hbir::AtanhOp)","id":"hbiratanh-mlirhbdkhbiratanhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-7","depth":3,"charIndex":9525},{"text":"Results:","id":"results-7","depth":3,"charIndex":9922},{"text":"`hbir.avg_pool` (::mlir::hbdk::hbir::AvgPoolOp)","id":"hbiravg_pool-mlirhbdkhbiravgpoolop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes","depth":3,"charIndex":12132},{"text":"Operands:","id":"operands-8","depth":3,"charIndex":12491},{"text":"Results:","id":"results-8","depth":3,"charIndex":12642},{"text":"`hbir.batchnorm` (::mlir::hbdk::hbir::BatchNormOp)","id":"hbirbatchnorm-mlirhbdkhbirbatchnormop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-1","depth":3,"charIndex":14050},{"text":"Operands:","id":"operands-9","depth":3,"charIndex":14165},{"text":"Results:","id":"results-9","depth":3,"charIndex":16018},{"text":"`hbir.bev_pool_v2` (::mlir::hbdk::hbir::BevPoolV2Op)","id":"hbirbev_pool_v2-mlirhbdkhbirbevpoolv2op","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-2","depth":3,"charIndex":17999},{"text":"Operands:","id":"operands-10","depth":3,"charIndex":18132},{"text":"Results:","id":"results-10","depth":3,"charIndex":20647},{"text":"`hbir.cast_type` (::mlir::hbdk::hbir::CastTypeOp)","id":"hbircast_type-mlirhbdkhbircasttypeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-3","depth":3,"charIndex":21493},{"text":"Operands:","id":"operands-11","depth":3,"charIndex":21606},{"text":"Results:","id":"results-11","depth":3,"charIndex":22006},{"text":"`hbir.ceil` (::mlir::hbdk::hbir::CeilOp)","id":"hbirceil-mlirhbdkhbirceilop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-12","depth":3,"charIndex":22804},{"text":"Results:","id":"results-12","depth":3,"charIndex":23201},{"text":"`hbir.clip` (::mlir::hbdk::hbir::ClipOp)","id":"hbirclip-mlirhbdkhbirclipop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-4","depth":3,"charIndex":24678},{"text":"Operands:","id":"operands-13","depth":3,"charIndex":24922},{"text":"Results:","id":"results-13","depth":3,"charIndex":25296},{"text":"`hbir.concat` (::mlir::hbdk::hbir::ConcatOp)","id":"hbirconcat-mlirhbdkhbirconcatop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-5","depth":3,"charIndex":26477},{"text":"Operands:","id":"operands-14","depth":3,"charIndex":26607},{"text":"Results:","id":"results-14","depth":3,"charIndex":27019},{"text":"`hbir.constant` (::mlir::hbdk::hbir::ConstantOp)","id":"hbirconstant-mlirhbdkhbirconstantop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-6","depth":3,"charIndex":27790},{"text":"Results:","id":"results-15","depth":3,"charIndex":27921},{"text":"`hbir.conv2d` (::mlir::hbdk::hbir::Conv2dOp)","id":"hbirconv2d-mlirhbdkhbirconv2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-7","depth":3,"charIndex":30754},{"text":"Operands:","id":"operands-15","depth":3,"charIndex":31151},{"text":"Results:","id":"results-16","depth":3,"charIndex":31535},{"text":"`hbir.conv2dtranspose` (::mlir::hbdk::hbir::Conv2dTransposeOp)","id":"hbirconv2dtranspose-mlirhbdkhbirconv2dtransposeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-8","depth":3,"charIndex":32845},{"text":"Operands:","id":"operands-16","depth":3,"charIndex":33315},{"text":"Results:","id":"results-17","depth":3,"charIndex":33686},{"text":"`hbir.conv3d` (::mlir::hbdk::hbir::Conv3dOp)","id":"hbirconv3d-mlirhbdkhbirconv3dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-9","depth":3,"charIndex":36408},{"text":"Operands:","id":"operands-17","depth":3,"charIndex":36805},{"text":"Results:","id":"results-18","depth":3,"charIndex":37189},{"text":"`hbir.conv` (::mlir::hbdk::hbir::ConvOp)","id":"hbirconv-mlirhbdkhbirconvop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-10","depth":3,"charIndex":40031},{"text":"Operands:","id":"operands-18","depth":3,"charIndex":40417},{"text":"Results:","id":"results-19","depth":3,"charIndex":40798},{"text":"`hbir.cos` (::mlir::hbdk::hbir::CosOp)","id":"hbircos-mlirhbdkhbircosop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-19","depth":3,"charIndex":41366},{"text":"Results:","id":"results-20","depth":3,"charIndex":41763},{"text":"`hbir.cosh` (::mlir::hbdk::hbir::CoshOp)","id":"hbircosh-mlirhbdkhbircoshop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-20","depth":3,"charIndex":42576},{"text":"Results:","id":"results-21","depth":3,"charIndex":42973},{"text":"`hbir.cumsum` (::mlir::hbdk::hbir::CumSumOp)","id":"hbircumsum-mlirhbdkhbircumsumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-11","depth":3,"charIndex":44656},{"text":"Operands:","id":"operands-21","depth":3,"charIndex":44922},{"text":"Results:","id":"results-22","depth":3,"charIndex":45322},{"text":"`hbir.deform_conv2d` (::mlir::hbdk::hbir::DeformConv2dOp)","id":"hbirdeform_conv2d-mlirhbdkhbirdeformconv2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-12","depth":3,"charIndex":48387},{"text":"Operands:","id":"operands-22","depth":3,"charIndex":48936},{"text":"Results:","id":"results-23","depth":3,"charIndex":49550},{"text":"`hbir.div` (::mlir::hbdk::hbir::DivOp)","id":"hbirdiv-mlirhbdkhbirdivop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-23","depth":3,"charIndex":50347},{"text":"Results:","id":"results-24","depth":3,"charIndex":51111},{"text":"`hbir.equal` (::mlir::hbdk::hbir::EqualOp)","id":"hbirequal-mlirhbdkhbirequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-24","depth":3,"charIndex":51942},{"text":"Results:","id":"results-25","depth":3,"charIndex":52706},{"text":"`hbir.exp` (::mlir::hbdk::hbir::ExpOp)","id":"hbirexp-mlirhbdkhbirexpop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-25","depth":3,"charIndex":53364},{"text":"Results:","id":"results-26","depth":3,"charIndex":53761},{"text":"`hbir.fake_cast` (::mlir::hbdk::hbir::FakeCastOp)","id":"hbirfake_cast-mlirhbdkhbirfakecastop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-13","depth":3,"charIndex":54607},{"text":"Operands:","id":"operands-26","depth":3,"charIndex":54716},{"text":"Results:","id":"results-27","depth":3,"charIndex":54821},{"text":"`hbir.flip` (::mlir::hbdk::hbir::FlipOp)","id":"hbirflip-mlirhbdkhbirflipop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-14","depth":3,"charIndex":55633},{"text":"Operands:","id":"operands-27","depth":3,"charIndex":55756},{"text":"Results:","id":"results-28","depth":3,"charIndex":56156},{"text":"`hbir.floor` (::mlir::hbdk::hbir::FloorOp)","id":"hbirfloor-mlirhbdkhbirfloorop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-28","depth":3,"charIndex":56979},{"text":"Results:","id":"results-29","depth":3,"charIndex":57376},{"text":"`hbir.gelu` (::mlir::hbdk::hbir::GELUOp)","id":"hbirgelu-mlirhbdkhbirgeluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-29","depth":3,"charIndex":58686},{"text":"Results:","id":"results-30","depth":3,"charIndex":59083},{"text":"`hbir.gather_elements` (::mlir::hbdk::hbir::GatherElementsOp)","id":"hbirgather_elements-mlirhbdkhbirgatherelementsop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-15","depth":3,"charIndex":59904},{"text":"Operands:","id":"operands-30","depth":3,"charIndex":60034},{"text":"Results:","id":"results-31","depth":3,"charIndex":60717},{"text":"`hbir.gather_nd` (::mlir::hbdk::hbir::GatherNdOp)","id":"hbirgather_nd-mlirhbdkhbirgatherndop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-16","depth":3,"charIndex":61508},{"text":"Operands:","id":"operands-31","depth":3,"charIndex":61638},{"text":"Results:","id":"results-32","depth":3,"charIndex":62321},{"text":"`hbir.greater_equal` (::mlir::hbdk::hbir::GreaterEqualOp)","id":"hbirgreater_equal-mlirhbdkhbirgreaterequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-32","depth":3,"charIndex":63130},{"text":"Results:","id":"results-33","depth":3,"charIndex":63888},{"text":"`hbir.greater` (::mlir::hbdk::hbir::GreaterOp)","id":"hbirgreater-mlirhbdkhbirgreaterop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-33","depth":3,"charIndex":64375},{"text":"Results:","id":"results-34","depth":3,"charIndex":65133},{"text":"`hbir.grid_sample` (::mlir::hbdk::hbir::GridSampleOp)","id":"hbirgrid_sample-mlirhbdkhbirgridsampleop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-17","depth":3,"charIndex":65946},{"text":"Operands:","id":"operands-34","depth":3,"charIndex":66388},{"text":"Results:","id":"results-35","depth":3,"charIndex":67146},{"text":"`hbir.index` (::mlir::hbdk::hbir::IndexOp)","id":"hbirindex-mlirhbdkhbirindexop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-18","depth":3,"charIndex":68419},{"text":"Operands:","id":"operands-35","depth":3,"charIndex":68549},{"text":"Results:","id":"results-36","depth":3,"charIndex":69232},{"text":"`hbir.layernorm` (::mlir::hbdk::hbir::LayerNormOp)","id":"hbirlayernorm-mlirhbdkhbirlayernormop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-19","depth":3,"charIndex":70846},{"text":"Operands:","id":"operands-36","depth":3,"charIndex":71024},{"text":"Results:","id":"results-37","depth":3,"charIndex":72143},{"text":"`hbir.leaky_relu` (::mlir::hbdk::hbir::LeakyReLUOp)","id":"hbirleaky_relu-mlirhbdkhbirleakyreluop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-20","depth":3,"charIndex":73508},{"text":"Operands:","id":"operands-37","depth":3,"charIndex":73623},{"text":"Results:","id":"results-38","depth":3,"charIndex":73997},{"text":"`hbir.less_equal` (::mlir::hbdk::hbir::LessEqualOp)","id":"hbirless_equal-mlirhbdkhbirlessequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-38","depth":3,"charIndex":74813},{"text":"Results:","id":"results-39","depth":3,"charIndex":75571},{"text":"`hbir.less` (::mlir::hbdk::hbir::LessOp)","id":"hbirless-mlirhbdkhbirlessop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-39","depth":3,"charIndex":76046},{"text":"Results:","id":"results-40","depth":3,"charIndex":76804},{"text":"`hbir.linear` (::mlir::hbdk::hbir::LinearOp)","id":"hbirlinear-mlirhbdkhbirlinearop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-40","depth":3,"charIndex":77804},{"text":"Results:","id":"results-41","depth":3,"charIndex":78188},{"text":"`hbir.log` (::mlir::hbdk::hbir::LogOp)","id":"hbirlog-mlirhbdkhbirlogop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-41","depth":3,"charIndex":78907},{"text":"Results:","id":"results-42","depth":3,"charIndex":79304},{"text":"`hbir.log_softmax` (::mlir::hbdk::hbir::LogSoftmaxOp)","id":"hbirlog_softmax-mlirhbdkhbirlogsoftmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-21","depth":3,"charIndex":80724},{"text":"Operands:","id":"operands-42","depth":3,"charIndex":80854},{"text":"Results:","id":"results-43","depth":3,"charIndex":81228},{"text":"`hbir.logical_and` (::mlir::hbdk::hbir::LogicalAndOp)","id":"hbirlogical_and-mlirhbdkhbirlogicalandop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-43","depth":3,"charIndex":82066},{"text":"Results:","id":"results-44","depth":3,"charIndex":82338},{"text":"`hbir.logical_not` (::mlir::hbdk::hbir::LogicalNotOp)","id":"hbirlogical_not-mlirhbdkhbirlogicalnotop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-44","depth":3,"charIndex":82823},{"text":"Results:","id":"results-45","depth":3,"charIndex":82977},{"text":"`hbir.logical_or` (::mlir::hbdk::hbir::LogicalOrOp)","id":"hbirlogical_or-mlirhbdkhbirlogicalorop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-45","depth":3,"charIndex":83495},{"text":"Results:","id":"results-46","depth":3,"charIndex":83767},{"text":"`hbir.matmul` (::mlir::hbdk::hbir::MatMulOp)","id":"hbirmatmul-mlirhbdkhbirmatmulop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-46","depth":3,"charIndex":85380},{"text":"Results:","id":"results-47","depth":3,"charIndex":85646},{"text":"`hbir.max` (::mlir::hbdk::hbir::MaxOp)","id":"hbirmax-mlirhbdkhbirmaxop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-47","depth":3,"charIndex":86335},{"text":"Results:","id":"results-48","depth":3,"charIndex":87099},{"text":"`hbir.max_pool` (::mlir::hbdk::hbir::MaxPoolOp)","id":"hbirmax_pool-mlirhbdkhbirmaxpoolop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-22","depth":3,"charIndex":89332},{"text":"Operands:","id":"operands-48","depth":3,"charIndex":89691},{"text":"Results:","id":"results-49","depth":3,"charIndex":89842},{"text":"`hbir.min` (::mlir::hbdk::hbir::MinOp)","id":"hbirmin-mlirhbdkhbirminop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-49","depth":3,"charIndex":90544},{"text":"Results:","id":"results-50","depth":3,"charIndex":91308},{"text":"`hbir.mod` (::mlir::hbdk::hbir::ModOp)","id":"hbirmod-mlirhbdkhbirmodop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-23","depth":3,"charIndex":92346},{"text":"Operands:","id":"operands-50","depth":3,"charIndex":92469},{"text":"Results:","id":"results-51","depth":3,"charIndex":93227},{"text":"`hbir.mul` (::mlir::hbdk::hbir::MulOp)","id":"hbirmul-mlirhbdkhbirmulop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-51","depth":3,"charIndex":94292},{"text":"Results:","id":"results-52","depth":3,"charIndex":95056},{"text":"`hbir.nan_to_num` (::mlir::hbdk::hbir::NanToNumOp)","id":"hbirnan_to_num-mlirhbdkhbirnantonumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-24","depth":3,"charIndex":96107},{"text":"Operands:","id":"operands-52","depth":3,"charIndex":96332},{"text":"Results:","id":"results-53","depth":3,"charIndex":96480},{"text":"`hbir.neg` (::mlir::hbdk::hbir::NegOp)","id":"hbirneg-mlirhbdkhbirnegop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-53","depth":3,"charIndex":97212},{"text":"Results:","id":"results-54","depth":3,"charIndex":97609},{"text":"`hbir.nonzero` (::mlir::hbdk::hbir::NonZeroOp)","id":"hbirnonzero-mlirhbdkhbirnonzeroop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-54","depth":3,"charIndex":98358},{"text":"Results:","id":"results-55","depth":3,"charIndex":98758},{"text":"`hbir.pad` (::mlir::hbdk::hbir::PadOp)","id":"hbirpad-mlirhbdkhbirpadop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-25","depth":3,"charIndex":99430},{"text":"Operands:","id":"operands-55","depth":3,"charIndex":99931},{"text":"Results:","id":"results-56","depth":3,"charIndex":100308},{"text":"`hbir.point_pillar_preprocess` (::mlir::hbdk::hbir::PointPillarPreProcessOp)","id":"hbirpoint_pillar_preprocess-mlirhbdkhbirpointpillarpreprocessop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-26","depth":3,"charIndex":101102},{"text":"Operands:","id":"operands-56","depth":3,"charIndex":101610},{"text":"Results:","id":"results-57","depth":3,"charIndex":102010},{"text":"`hbir.pow` (::mlir::hbdk::hbir::PowOp)","id":"hbirpow-mlirhbdkhbirpowop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-57","depth":3,"charIndex":103188},{"text":"Results:","id":"results-58","depth":3,"charIndex":103952},{"text":"`hbir.prelu` (::mlir::hbdk::hbir::PreluOp)","id":"hbirprelu-mlirhbdkhbirpreluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-58","depth":3,"charIndex":104715},{"text":"Results:","id":"results-59","depth":3,"charIndex":105473},{"text":"`hbir.relu` (::mlir::hbdk::hbir::ReLUOp)","id":"hbirrelu-mlirhbdkhbirreluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-59","depth":3,"charIndex":106704},{"text":"Results:","id":"results-60","depth":3,"charIndex":107101},{"text":"`hbir.reciprocal` (::mlir::hbdk::hbir::ReciprocalOp)","id":"hbirreciprocal-mlirhbdkhbirreciprocalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-60","depth":3,"charIndex":108086},{"text":"Results:","id":"results-61","depth":3,"charIndex":108483},{"text":"`hbir.reduce_all` (::mlir::hbdk::hbir::ReduceAllOp)","id":"hbirreduce_all-mlirhbdkhbirreduceallop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-27","depth":3,"charIndex":110021},{"text":"Operands:","id":"operands-61","depth":3,"charIndex":110191},{"text":"Results:","id":"results-62","depth":3,"charIndex":110464},{"text":"`hbir.reduce_argmax` (::mlir::hbdk::hbir::ReduceArgmaxOp)","id":"hbirreduce_argmax-mlirhbdkhbirreduceargmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-28","depth":3,"charIndex":111803},{"text":"Operands:","id":"operands-62","depth":3,"charIndex":111973},{"text":"Results:","id":"results-63","depth":3,"charIndex":112370},{"text":"`hbir.reduce_argmin` (::mlir::hbdk::hbir::ReduceArgminOp)","id":"hbirreduce_argmin-mlirhbdkhbirreduceargminop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-29","depth":3,"charIndex":113948},{"text":"Operands:","id":"operands-63","depth":3,"charIndex":114118},{"text":"Results:","id":"results-64","depth":3,"charIndex":114515},{"text":"`hbir.reduce_max` (::mlir::hbdk::hbir::ReduceMaxOp)","id":"hbirreduce_max-mlirhbdkhbirreducemaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-30","depth":3,"charIndex":116087},{"text":"Operands:","id":"operands-64","depth":3,"charIndex":116257},{"text":"Results:","id":"results-65","depth":3,"charIndex":116654},{"text":"`hbir.reduce_mean` (::mlir::hbdk::hbir::ReduceMeanOp)","id":"hbirreduce_mean-mlirhbdkhbirreducemeanop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-31","depth":3,"charIndex":118286},{"text":"Operands:","id":"operands-65","depth":3,"charIndex":118456},{"text":"Results:","id":"results-66","depth":3,"charIndex":118853},{"text":"`hbir.reduce_min` (::mlir::hbdk::hbir::ReduceMinOp)","id":"hbirreduce_min-mlirhbdkhbirreduceminop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-32","depth":3,"charIndex":120501},{"text":"Operands:","id":"operands-66","depth":3,"charIndex":120671},{"text":"Results:","id":"results-67","depth":3,"charIndex":121068},{"text":"`hbir.reduce_sum` (::mlir::hbdk::hbir::ReduceSumOp)","id":"hbirreduce_sum-mlirhbdkhbirreducesumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-33","depth":3,"charIndex":122694},{"text":"Operands:","id":"operands-67","depth":3,"charIndex":122864},{"text":"Results:","id":"results-68","depth":3,"charIndex":123261},{"text":"`hbir.reshape` (::mlir::hbdk::hbir::ReshapeOp)","id":"hbirreshape-mlirhbdkhbirreshapeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-34","depth":3,"charIndex":124449},{"text":"Operands:","id":"operands-68","depth":3,"charIndex":124619},{"text":"Results:","id":"results-69","depth":3,"charIndex":125019},{"text":"`hbir.resize2d` (::mlir::hbdk::hbir::Resize2dOp)","id":"hbirresize2d-mlirhbdkhbirresize2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-35","depth":3,"charIndex":126344},{"text":"Operands:","id":"operands-69","depth":3,"charIndex":127147},{"text":"Results:","id":"results-70","depth":3,"charIndex":127544},{"text":"`hbir.roll` (::mlir::hbdk::hbir::RollOp)","id":"hbirroll-mlirhbdkhbirrollop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-36","depth":3,"charIndex":128789},{"text":"Operands:","id":"operands-70","depth":3,"charIndex":128975},{"text":"Results:","id":"results-71","depth":3,"charIndex":129375},{"text":"`hbir.round` (::mlir::hbdk::hbir::RoundOp)","id":"hbirround-mlirhbdkhbirroundop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-37","depth":3,"charIndex":130382},{"text":"Operands:","id":"operands-71","depth":3,"charIndex":130512},{"text":"Results:","id":"results-72","depth":3,"charIndex":130909},{"text":"`hbir.rsqrt` (::mlir::hbdk::hbir::RsqrtOp)","id":"hbirrsqrt-mlirhbdkhbirrsqrtop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-72","depth":3,"charIndex":131725},{"text":"Results:","id":"results-73","depth":3,"charIndex":132122},{"text":"`hbir.scatter_elements` (::mlir::hbdk::hbir::ScatterElementsOp)","id":"hbirscatter_elements-mlirhbdkhbirscatterelementsop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-38","depth":3,"charIndex":133307},{"text":"Operands:","id":"operands-73","depth":3,"charIndex":133563},{"text":"Results:","id":"results-74","depth":3,"charIndex":134610},{"text":"`hbir.scatter_nd` (::mlir::hbdk::hbir::ScatterNDOp)","id":"hbirscatter_nd-mlirhbdkhbirscatterndop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-39","depth":3,"charIndex":135600},{"text":"Operands:","id":"operands-74","depth":3,"charIndex":135764},{"text":"Results:","id":"results-75","depth":3,"charIndex":136742},{"text":"`hbir.select` (::mlir::hbdk::hbir::SelectOp)","id":"hbirselect-mlirhbdkhbirselectop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-40","depth":3,"charIndex":137926},{"text":"Operands:","id":"operands-75","depth":3,"charIndex":138124},{"text":"Results:","id":"results-76","depth":3,"charIndex":138524},{"text":"`hbir.sigmoid` (::mlir::hbdk::hbir::SigmoidOp)","id":"hbirsigmoid-mlirhbdkhbirsigmoidop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-76","depth":3,"charIndex":139715},{"text":"Results:","id":"results-77","depth":3,"charIndex":140112},{"text":"`hbir.sign` (::mlir::hbdk::hbir::SignOp)","id":"hbirsign-mlirhbdkhbirsignop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-77","depth":3,"charIndex":140946},{"text":"Results:","id":"results-78","depth":3,"charIndex":141343},{"text":"`hbir.sin` (::mlir::hbdk::hbir::SinOp)","id":"hbirsin-mlirhbdkhbirsinop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-78","depth":3,"charIndex":142153},{"text":"Results:","id":"results-79","depth":3,"charIndex":142550},{"text":"`hbir.sinh` (::mlir::hbdk::hbir::SinhOp)","id":"hbirsinh-mlirhbdkhbirsinhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-79","depth":3,"charIndex":143363},{"text":"Results:","id":"results-80","depth":3,"charIndex":143760},{"text":"`hbir.slice` (::mlir::hbdk::hbir::SliceOp)","id":"hbirslice-mlirhbdkhbirsliceop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-41","depth":3,"charIndex":144998},{"text":"Operands:","id":"operands-80","depth":3,"charIndex":145294},{"text":"Results:","id":"results-81","depth":3,"charIndex":145694},{"text":"`hbir.slice_scatter` (::mlir::hbdk::hbir::SliceScatterOp)","id":"hbirslice_scatter-mlirhbdkhbirslicescatterop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-42","depth":3,"charIndex":147079},{"text":"Operands:","id":"operands-81","depth":3,"charIndex":147413},{"text":"Results:","id":"results-82","depth":3,"charIndex":148177},{"text":"`hbir.softmax` (::mlir::hbdk::hbir::SoftmaxOp)","id":"hbirsoftmax-mlirhbdkhbirsoftmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-43","depth":3,"charIndex":149526},{"text":"Operands:","id":"operands-82","depth":3,"charIndex":149656},{"text":"Results:","id":"results-83","depth":3,"charIndex":150053},{"text":"`hbir.softplus` (::mlir::hbdk::hbir::SoftplusOp)","id":"hbirsoftplus-mlirhbdkhbirsoftplusop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-44","depth":3,"charIndex":151550},{"text":"Operands:","id":"operands-83","depth":3,"charIndex":151720},{"text":"Results:","id":"results-84","depth":3,"charIndex":152117},{"text":"`hbir.sort` (::mlir::hbdk::hbir::SortOp)","id":"hbirsort-mlirhbdkhbirsortop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-45","depth":3,"charIndex":153561},{"text":"Operands:","id":"operands-84","depth":3,"charIndex":153793},{"text":"Results:","id":"results-85","depth":3,"charIndex":154190},{"text":"`hbir.sqrt` (::mlir::hbdk::hbir::SqrtOp)","id":"hbirsqrt-mlirhbdkhbirsqrtop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-85","depth":3,"charIndex":155365},{"text":"Results:","id":"results-86","depth":3,"charIndex":155762},{"text":"`hbir.stack` (::mlir::hbdk::hbir::StackOp)","id":"hbirstack-mlirhbdkhbirstackop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-46","depth":3,"charIndex":156855},{"text":"Operands:","id":"operands-86","depth":3,"charIndex":156985},{"text":"Results:","id":"results-87","depth":3,"charIndex":157397},{"text":"`hbir.sub` (::mlir::hbdk::hbir::SubOp)","id":"hbirsub-mlirhbdkhbirsubop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-87","depth":3,"charIndex":158345},{"text":"Results:","id":"results-88","depth":3,"charIndex":159109},{"text":"`hbir.tan` (::mlir::hbdk::hbir::TanOp)","id":"hbirtan-mlirhbdkhbirtanop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-88","depth":3,"charIndex":159922},{"text":"Results:","id":"results-89","depth":3,"charIndex":160319},{"text":"`hbir.tanh` (::mlir::hbdk::hbir::TanhOp)","id":"hbirtanh-mlirhbdkhbirtanhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-89","depth":3,"charIndex":161513},{"text":"Results:","id":"results-90","depth":3,"charIndex":161910},{"text":"`hbir.tile` (::mlir::hbdk::hbir::TileOp)","id":"hbirtile-mlirhbdkhbirtileop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-47","depth":3,"charIndex":162660},{"text":"Operands:","id":"operands-90","depth":3,"charIndex":162785},{"text":"Results:","id":"results-91","depth":3,"charIndex":163185},{"text":"`hbir.topk` (::mlir::hbdk::hbir::TopkOp)","id":"hbirtopk-mlirhbdkhbirtopkop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-48","depth":3,"charIndex":164352},{"text":"Operands:","id":"operands-91","depth":3,"charIndex":164648},{"text":"Results:","id":"results-92","depth":3,"charIndex":165045},{"text":"`hbir.transpose` (::mlir::hbdk::hbir::TransposeOp)","id":"hbirtranspose-mlirhbdkhbirtransposeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-49","depth":3,"charIndex":166401},{"text":"Operands:","id":"operands-92","depth":3,"charIndex":166524},{"text":"Results:","id":"results-93","depth":3,"charIndex":166901},{"text":"`hbir.warp` (::mlir::hbdk::hbir::WarpOp)","id":"hbirwarp-mlirhbdkhbirwarpop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-50","depth":3,"charIndex":167954},{"text":"Operands:","id":"operands-93","depth":3,"charIndex":168327},{"text":"Results:","id":"results-94","depth":3,"charIndex":169085},{"text":"`hbir.where` (::mlir::hbdk::hbir::WhereOp)","id":"hbirwhere-mlirhbdkhbirwhereop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-94","depth":3,"charIndex":169875},{"text":"Results:","id":"results-95","depth":3,"charIndex":170707}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":375,"title":"ONNX算子BPU约束列表","content":"#\n\n下方表格中：\n\nlhs：left-hand side，指运算中的左操作数。\n\nrhs：right-hand side，指运算中的右操作数。","routePath":"/guide/appendix/supported_op_list/onnx_operator_support_list","lang":"zh","toc":[],"domain":"","frontmatter":{"outline":false},"version":"3.0.22"},{"id":376,"title":"","content":"","routePath":"/guide/appendix/supported_op_list/onnx_operator_support_list_html","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":377,"title":"","content":"下方表格中：\n\nlhs：left-hand side，指运算中的左操作数。\n\nrhs：right-hand side，指运算中的右操作数。","routePath":"/guide/appendix/supported_op_list/onnx_operator_support_list_remark","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":378,"title":"Torch算子BPU约束列表","content":"#\n\n注意\n\n下方默认进行了如下别名替换：\n\n\n\n下方表格中：\n\nlhs：left-hand side，指运算中的左操作数。\n\nrhs：right-hand side，指运算中的右操作数。","routePath":"/guide/appendix/supported_op_list/torch_operator_support_list","lang":"zh","toc":[],"domain":"","frontmatter":{"outline":false},"version":"3.0.22"},{"id":379,"title":"","content":"","routePath":"/guide/appendix/supported_op_list/torch_operator_support_list_html","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":380,"title":"","content":"注意\n\n下方默认进行了如下别名替换：\n\n\n\n下方表格中：\n\nlhs：left-hand side，指运算中的左操作数。\n\nrhs：right-hand side，指运算中的右操作数。","routePath":"/guide/appendix/supported_op_list/torch_operator_support_list_remark","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":381,"title":"模型性能Benchmark","content":"#\n\n\n说明#\n\n * 测试条件：\n   \n   * 测试开发板：J6E。\n   \n   * 测试核心数：latency单核，fps双核。\n   \n   * 性能数据获取频率设置为：5分钟时间内性能参数的平均值。\n   \n   * Python版本：Python3.10。\n   \n   * 模型来源：OE包内 samples/ucp_tutorial/dnn/ai_benchmark/j6 路径下的模型。\n\n * 缩写说明：\n   \n   * C = 计算量，单位为GOPs（十亿次运算/秒）。此数据通过调用 hbm_perf 接口获得。\n   \n   * FPS = 每秒帧率。此数据在开发板单线程运行ai_benchmark示例包/script路径下各模型子文件夹的 fps.sh 脚本获取，包含后处理。\n   \n   * ITC = 推理耗时，单位为ms（毫秒）。此数据在开发板单线程运行ai_benchmark示例包/script路径下各模型子文件夹的\n     latency.sh 脚本获取，不含后处理。\n   \n   * TCPP = 后处理耗时，单位为ms（毫秒）。此数据在开发板单线程运行ai_benchmark示例包/script路径下各模型子文件夹的\n     latency.sh 脚本获取。\n   \n   * RV = 单次推理读取数据量，单位为mb（兆比特）。此数据通过调用 hbm_perf 接口获得。\n   \n   * WV = 单次推理写入数据量，单位为mb（兆比特）。此数据通过调用 hbm_perf 接口获得。\n\n\n模型主要性能数据#\n\n\n模型全部性能数据#\n\n\nMobileNetv1#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 1.14\n\n * FPS: 5087.50\n\n * ITC(ms): 0.455\n\n * TCPP(ms): 0.034\n\n * RV(mb): 4.56\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7374(FLOAT)/0.7298(INT8)\n\n\nMobileNetv2#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 0.63\n\n * FPS: 4662.30\n\n * ITC(ms): 0.480\n\n * TCPP(ms): 0.034\n\n * RV(mb): 3.95\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7217(FLOAT)/0.7147(INT8)\n\n\nResNet50#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 7.72\n\n * FPS: 1115.10\n\n * ITC(ms): 1.165\n\n * TCPP(ms): 0.034\n\n * RV(mb): 26.08\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7703(FLOAT)/0.7678(INT8)\n\n\nGoogleNet#\n\n * INPUT SIZE: 1x3x224x224\n * C(GOPs): 3.00\n * FPS: 2627.10\n * ITC(ms): 0.687\n * TCPP(ms): 0.034\n * RV(mb): 6.93\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7018(FLOAT)/0.6992(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/GoogleNet\n\n\nEfficientNet_Lite0#\n\n * INPUT SIZE: 1x224x224x3\n * C(GOPs): 0.77\n * FPS: 3959.00\n * ITC(ms): 0.562\n * TCPP(ms): 0.034\n * RV(mb): 5.21\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7479(FLOAT)/0.7452(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite1#\n\n * INPUT SIZE: 1x240x240x3\n * C(GOPs): 1.20\n * FPS: 3057.00\n * ITC(ms): 0.637\n * TCPP(ms): 0.034\n * RV(mb): 6.11\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7652(FLOAT)/0.7612(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite2#\n\n * INPUT SIZE: 1x260x260x3\n * C(GOPs): 1.72\n * FPS: 2327.70\n * ITC(ms): 0.736\n * TCPP(ms): 0.034\n * RV(mb): 6.95\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7734(FLOAT)/0.7696(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite3#\n\n * INPUT SIZE: 1x280x280x3\n * C(GOPs): 2.77\n * FPS: 1755.20\n * ITC(ms): 0.883\n * TCPP(ms): 0.033\n * RV(mb): 9.39\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7917(FLOAT)/0.7891(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite4#\n\n * INPUT SIZE: 1x300x300x3\n * C(GOPs): 5.11\n * FPS: 1234.70\n * ITC(ms): 1.121\n * TCPP(ms): 0.034\n * RV(mb): 14.47\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.8063(FLOAT)/0.8047(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nVargconvnet#\n\n * INPUT SIZE: 1x3x224x224\n * C(GOPs): 9.06\n * FPS: 1438.00\n * ITC(ms): 0.996\n * TCPP(ms): 0.034\n * RV(mb): 10.44\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7793(FLOAT)/0.7764(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/VargConvNet\n\n\nEfficientnasnet_m#\n\n * INPUT SIZE: 1x3x300x300\n * C(GOPs): 4.53\n * FPS: 1418.00\n * ITC(ms): 0.999\n * TCPP(ms): 0.034\n * RV(mb): 13.76\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7935(FLOAT)/0.7920(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientnas\n   Net\n\n\nEfficientnasnet_s#\n\n * INPUT SIZE: 1x3x280x280\n * C(GOPs): 1.44\n * FPS: 3250.80\n * ITC(ms): 0.598\n * TCPP(ms): 0.034\n * RV(mb): 5.45\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7441(FLOAT)/0.7528(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientnas\n   Net\n\n\nResNet18#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 3.63\n\n * FPS: 2350.10\n\n * ITC(ms): 0.695\n\n * TCPP(ms): 0.034\n\n * RV(mb): 11.87\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7169(FLOAT)/0.7164(INT8)\n\n\nYOLOv2_Darknet19#\n\n * INPUT SIZE: 1x3x608x608\n * C(GOPs): 62.94\n * FPS: 224.52\n * ITC(ms): 4.746\n * TCPP(ms): 0.299\n * RV(mb): 51.86\n * WV(mb): 0.77\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.2760(FLOAT)/0.2700(INT8)\n * LINKS: https://pjreddie.com/darknet/yolo\n\n\nYOLOv3_Darknet53#\n\n * INPUT SIZE: 1x3x416x416\n * C(GOPs): 65.87\n * FPS: 203.94\n * ITC(ms): 5.238\n * TCPP(ms): 1.753\n * RV(mb): 69.24\n * WV(mb): 9.64\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.3370(FLOAT)/0.3360(INT8)\n * LINKS: https://github.com/ChenYingpeng/caffe-yolov3/\n\n\nyolov5x_v2.0#\n\n * INPUT SIZE: 1x3x672x672\n * C(GOPs): 243.86\n * FPS: 59.90\n * ITC(ms): 17.092\n * TCPP(ms): 5.873\n * RV(mb): 145.54\n * WV(mb): 51.90\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.4810(FLOAT)/0.4670(INT8)\n * LINKS: https://github.com/ultralytics/yolov5/releases/tag/v2.0\n\n\nSSD_MobileNetv1#\n\n * INPUT SIZE: 1x3x300x300\n * C(GOPs): 2.30\n * FPS: 2296.80\n * ITC(ms): 0.843\n * TCPP(ms): 0.199\n * RV(mb): 6.28\n * WV(mb): 0.25\n * Dataset: VOC\n * ACCURACY: mAP: 0.7345(FLOAT)/0.7262(INT8)\n * LINKS: https://github.com/chuanqi305/MobileNet-SSD\n\n\nYOLOv3_VargDarknet#\n\n * INPUT SIZE: 1x3x416x416\n * C(GOPs): 42.82\n * FPS: 295.50\n * ITC(ms): 3.725\n * TCPP(ms): 1.765\n * RV(mb): 46.53\n * WV(mb): 4.91\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.3280(FLOAT)/0.3270(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Yolov3_VargD\n   arknet\n\n\nDeeplabv3plus_efficientnetb0#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 30.80\n * FPS: 148.04\n * ITC(ms): 7.106\n * TCPP(ms): 0.312\n * RV(mb): 18.86\n * WV(mb): 10.49\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7630(FLOAT)/0.7570(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nFastscnn_efficientnetb0#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 12.52\n * FPS: 247.44\n * ITC(ms): 4.384\n * TCPP(ms): 0.309\n * RV(mb): 14.20\n * WV(mb): 9.57\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.6997(FLOAT)/0.6911(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/FastSCNN\n\n\nDeeplabv3plus_efficientnetm1#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 77.08\n * FPS: 91.03\n * ITC(ms): 11.356\n * TCPP(ms): 0.303\n * RV(mb): 74.71\n * WV(mb): 54.00\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7794(FLOAT)/0.7754(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nDeeplabv3plus_efficientnetm2#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 124.19\n * FPS: 64.83\n * ITC(ms): 15.794\n * TCPP(ms): 0.305\n * RV(mb): 142.30\n * WV(mb): 81.26\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7882(FLOAT)/0.7854(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nBev_gkt_mixvargenet_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points(0-8): 6x64x64x2\n\n * C(GOPs): 206.95\n\n * FPS: 60.16\n\n * ITC(ms): 17.934\n\n * TCPP(ms): 5.596\n\n * RV(mb): 150.79\n\n * WV(mb): 118.67\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.2809(FLOAT)/0.2790(INT8) MeanIOU: 0.4852(FLOAT)/0.4835(INT8)\n   mAP: 0.1990(FLOAT)/0.2000(INT8)\n\n\nBev_ipm_4d_efficientnetb0_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points: 6x128x128x2 prev_feat: 1x164x28x128\n   prev_point: 1x128x128x2\n\n * C(GOPs): 53.58\n\n * FPS: 106.12\n\n * ITC(ms): 10.929\n\n * TCPP(ms): 5.721\n\n * RV(mb): 68.76\n\n * WV(mb): 55.37\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3721(FLOAT)/0.3736(INT8) MeanIOU: 0.5287(FLOAT)/0.5387(INT8)\n   mAP: 0.2200(FLOAT)/0.2218(INT8)\n\n\nBev_ipm_efficientnetb0_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points: 6x128x128x2\n\n * C(GOPs): 52.97\n\n * FPS: 109.29\n\n * ITC(ms): 10.242\n\n * TCPP(ms): 5.626\n\n * RV(mb): 65.45\n\n * WV(mb): 53.27\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3055(FLOAT)/0.3029(INT8) MeanIOU: 0.5145(FLOAT)/0.5098(INT8)\n   mAP: 0.2170(FLOAT)/0.2163(INT8)\n\n\nDetr3d_efficientnetb3#\n\n * INPUT SIZE: coords(0-3): 6x4x256x2 image: 6x3x512x1408 masks: 1x4x256x24\n\n * C(GOPs): 225.28\n\n * FPS: 26.27\n\n * ITC(ms): 38.796\n\n * TCPP(ms): 1.114\n\n * RV(mb): 477.43\n\n * WV(mb): 308.98\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3304(FLOAT)/0.3281(INT8) mAP: 0.2752(FLOAT)/0.2705(INT8)\n\n\nPetr_efficientnetb3#\n\n * INPUT SIZE: image: 6x3x512x1408 pos_embed: 1x96x44x256\n\n * C(GOPs): 217.42\n\n * FPS: 13.06\n\n * ITC(ms): 77.263\n\n * TCPP(ms): 1.136\n\n * RV(mb): 1272.47\n\n * WV(mb): 1151.16\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3765(FLOAT)/0.3737(INT8) mAP: 0.3038(FLOAT)/0.2935(INT8)\n\n\nBevformer_tiny_resnet50_detection#\n\n * INPUT SIZE: img: 6x3x480x800 prev_bev: 1x2500x256 prev_bev_ref: 1x50x50x2\n   queries_rebatch_grid: 6x20x32x2 restore_bev_grid: 1x100x50x2\n   reference_points_rebatch: 6x640x4x2 bev_pillar_counts: 1x2500x1\n\n * C(GOPs): 385.94\n\n * FPS: 27.57\n\n * ITC(ms): 46.385\n\n * TCPP(ms): 1.404\n\n * RV(mb): 308.11\n\n * WV(mb): 214.28\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3713(FLOAT)/0.3668(INT8) mAP: 0.2673(FLOAT)/0.2608(INT8)\n\n\nFlashocc_henet_lss_occ3d_nuscenes#\n\n * INPUT SIZE: img: 6x3x512x960 points: 10x128x128x2 points_depth: 10x128x128x2\n\n * C(GOPs): 126.80\n\n * FPS: 88.96\n\n * ITC(ms): 12.103\n\n * TCPP(ms): 40.872\n\n * RV(mb): 110.47\n\n * WV(mb): 68.86\n\n * Dataset: Nuscenes\n\n * ACCURACY: mIoU: 0.3674(FLOAT)/0.0738(INT8)\n\n\nHorizon_swin_transformer#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 8.98\n\n * FPS: 255.81\n\n * ITC(ms): 4.292\n\n * TCPP(ms): 0.034\n\n * RV(mb): 43.32\n\n * WV(mb): 4.82\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.8024(FLOAT)/0.7953(INT8)\n\n\nMixvargenet#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 2.07\n\n * FPS: 4944.60\n\n * ITC(ms): 0.463\n\n * TCPP(ms): 0.033\n\n * RV(mb): 2.51\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7075(FLOAT)/0.7063(INT8)\n\n\nVargnetv2#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 0.72\n\n * FPS: 3853.60\n\n * ITC(ms): 0.530\n\n * TCPP(ms): 0.034\n\n * RV(mb): 4.68\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7342(FLOAT)/0.7317(INT8)\n\n\nVit_small#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 9.20\n\n * FPS: 477.07\n\n * ITC(ms): 2.370\n\n * TCPP(ms): 0.034\n\n * RV(mb): 31.35\n\n * WV(mb): 5.56\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7950(FLOAT)/0.7797(INT8)\n\n\nCenterpoint_pointpillar#\n\n * INPUT SIZE: points: 300000x5 voxel_feature: 1x5x20x40000 coors: 40000x4\n\n * C(GOPs): 127.76\n\n * FPS: 89.29\n\n * ITC(ms): 60.218\n\n * TCPP(ms): 12.967\n\n * RV(mb): 48.98\n\n * WV(mb): 24.78\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.5831(FLOAT)/0.5819(INT8) mAP: 0.4804(FLOAT)/0.4780(INT8)\n\n\nDetr_efficientnetb3#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 67.31\n\n * FPS: 45.88\n\n * ITC(ms): 22.138\n\n * TCPP(ms): 0.341\n\n * RV(mb): 270.02\n\n * WV(mb): 146.61\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.3721(FLOAT)/0.3591(INT8)\n\n\nDetr_resnet50#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 202.99\n\n * FPS: 35.99\n\n * ITC(ms): 28.139\n\n * TCPP(ms): 0.342\n\n * RV(mb): 399.17\n\n * WV(mb): 263.34\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.3569(FLOAT)/0.3140(INT8)\n\n\nFCOS3D_efficientnetb0#\n\n * INPUT SIZE: 1x3x512x896\n\n * C(GOPs): 19.94\n\n * FPS: 410.89\n\n * ITC(ms): 3.517\n\n * TCPP(ms): 2.721\n\n * RV(mb): 11.45\n\n * WV(mb): 4.14\n\n * Dataset: nuscenes\n\n * ACCURACY: NDS: 0.3060(FLOAT)/0.3029(INT8) mAP: 0.2133(FLOAT)/0.2079(INT8)\n\n\nGanet_mixvargenet#\n\n * INPUT SIZE: 1x3x320x800\n\n * C(GOPs): 10.74\n\n * FPS: 1436.60\n\n * ITC(ms): 0.998\n\n * TCPP(ms): 0.218\n\n * RV(mb): 2.18\n\n * WV(mb): 0.53\n\n * Dataset: CuLane\n\n * ACCURACY: F1Score: 0.7948(FLOAT)/0.7878(INT8)\n\n\nKeypoint_efficientnetb0#\n\n * INPUT SIZE: 1x3x128x128\n\n * C(GOPs): 0.58\n\n * FPS: 4734.00\n\n * ITC(ms): 0.491\n\n * TCPP(ms): 0.076\n\n * RV(mb): 4.62\n\n * WV(mb): 0.01\n\n * Dataset: Carfusion\n\n * ACCURACY: PCK(alpha=0.1): 0.9433(FLOAT)/0.9432(INT8)\n\n\nPointpillars_kitti_car#\n\n * INPUT SIZE: 150000x4\n\n * C(GOPs): 67.23\n\n * FPS: 24.49\n\n * ITC(ms): 221.340\n\n * TCPP(ms): 0.534\n\n * RV(mb): 51.20\n\n * WV(mb): 30.80\n\n * Dataset: Kitti3d\n\n * ACCURACY: APDet= 0.7732(FLOAT)/0.7676(INT8)\n\n\nDeformable_detr_resnet50#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 425.65\n\n * FPS: 4.47\n\n * ITC(ms): 224.570\n\n * TCPP(ms): 15.521\n\n * RV(mb): 3663.77\n\n * WV(mb): 2724.37\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.4414(FLOAT)/0.4205(INT8)\n\n\nStereonetplus_mixvargenet#\n\n * INPUT SIZE: 2x3x544x960\n\n * C(GOPs): 48.59\n\n * FPS: 205.38\n\n * ITC(ms): 5.364\n\n * TCPP(ms): 1.956\n\n * RV(mb): 40.57\n\n * WV(mb): 34.81\n\n * Dataset: SceneFlow\n\n * ACCURACY: EPE: 1.1270(FLOAT)/1.1345(INT8)\n\n\nCenterpoint_mixvargnet_multitask#\n\n * INPUT SIZE: points: 300000x5 voxel_feature: 1x5x20x40000 coors: 40000x4\n\n * C(GOPs): 51.45\n\n * FPS: 90.71\n\n * ITC(ms): 57.852\n\n * TCPP(ms): 12.087\n\n * RV(mb): 39.20\n\n * WV(mb): 16.39\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.5809(FLOAT)/0.5754(INT8) MeanIOU: 0.9128(FLOAT)/0.9121(INT8)\n   mAP: 0.4727(FLOAT)/0.4629(INT8)\n\n\nUnet_mobilenetv1#\n\n * INPUT SIZE: 1x3x1024x2048\n\n * C(GOPs): 7.36\n\n * FPS: 751.49\n\n * ITC(ms): 1.748\n\n * TCPP(ms): 0.148\n\n * RV(mb): 16.49\n\n * WV(mb): 11.67\n\n * Dataset: Cityscapes\n\n * ACCURACY: mIoU: 0.6802(FLOAT)/0.6757(INT8)\n\n\nMotr_efficientnetb3#\n\n * INPUT SIZE: image: 1x800x1422x3 track_query: 1x2x128x156 ref_points:\n   1x2x128x4 mask_query: 1x1x256x1\n\n * C(GOPs): 82.01\n\n * FPS: 65.57\n\n * ITC(ms): 15.589\n\n * TCPP(ms): 4.947\n\n * RV(mb): 130.73\n\n * WV(mb): 52.07\n\n * Dataset: Mot17\n\n * ACCURACY: MOTA: 0.5805(FLOAT)/0.5754(INT8)\n\n\nDensetnt_vectornet#\n\n * INPUT SIZE: goals_2d: 30x1x2048x2 goals_2d_mask: 30x1x2048x1 instance_mask:\n   30x1x96x1 lane_feat: 30x9x64x11 traj_feat: 30x19x32x9\n\n * C(GOPs): 18.67\n\n * FPS: 144.18\n\n * ITC(ms): 11.123\n\n * TCPP(ms): 2.339\n\n * RV(mb): 79.04\n\n * WV(mb): 59.54\n\n * Dataset: Argoverse 1\n\n * ACCURACY: minFDA: 1.2975(FLOAT)/1.3044(INT8)\n\n\nMaptroe_henet-tinym_bevformer#\n\n * INPUT SIZE: img: 6x3x480x800 osm_mask: 1x1x50x100 queries_rebatch_grid:\n   6x20x100x2 restore_bev_grid: 1x100x100x2 reference_points_rebatch: 6x2000x4x2\n   bev_pillar_counts: 1x5000x1\n * C(GOPs): 133.06\n * FPS: 67.20\n * ITC(ms): 15.626\n * TCPP(ms): 0.258\n * RV(mb): 148.08\n * WV(mb): 46.42\n * Dataset: Nuscenes\n * ACCURACY: mAP: 0.6633(FLOAT)/0.6340(INT8)","routePath":"/guide/benchmark","lang":"zh","toc":[{"text":"说明","id":"说明","depth":2,"charIndex":3},{"text":"模型主要性能数据","id":"模型主要性能数据","depth":2,"charIndex":678},{"text":"模型全部性能数据","id":"模型全部性能数据","depth":2,"charIndex":690},{"text":"MobileNetv1","id":"mobilenetv1","depth":3,"charIndex":702},{"text":"MobileNetv2","id":"mobilenetv2","depth":3,"charIndex":924},{"text":"ResNet50","id":"resnet50","depth":3,"charIndex":1146},{"text":"GoogleNet","id":"googlenet","depth":3,"charIndex":1366},{"text":"EfficientNet_Lite0","id":"efficientnet_lite0","depth":3,"charIndex":1666},{"text":"EfficientNet_Lite1","id":"efficientnet_lite1","depth":3,"charIndex":1984},{"text":"EfficientNet_Lite2","id":"efficientnet_lite2","depth":3,"charIndex":2302},{"text":"EfficientNet_Lite3","id":"efficientnet_lite3","depth":3,"charIndex":2620},{"text":"EfficientNet_Lite4","id":"efficientnet_lite4","depth":3,"charIndex":2938},{"text":"Vargconvnet","id":"vargconvnet","depth":3,"charIndex":3257},{"text":"Efficientnasnet_m","id":"efficientnasnet_m","depth":3,"charIndex":3562},{"text":"Efficientnasnet_s","id":"efficientnasnet_s","depth":3,"charIndex":3881},{"text":"ResNet18","id":"resnet18","depth":3,"charIndex":4199},{"text":"YOLOv2_Darknet19","id":"yolov2_darknet19","depth":3,"charIndex":4419},{"text":"YOLOv3_Darknet53","id":"yolov3_darknet53","depth":3,"charIndex":4688},{"text":"yolov5x_v2.0","id":"yolov5x_v20","depth":3,"charIndex":4969},{"text":"SSD_MobileNetv1","id":"ssd_mobilenetv1","depth":3,"charIndex":5259},{"text":"YOLOv3_VargDarknet","id":"yolov3_vargdarknet","depth":3,"charIndex":5523},{"text":"Deeplabv3plus_efficientnetb0","id":"deeplabv3plus_efficientnetb0","depth":3,"charIndex":5851},{"text":"Fastscnn_efficientnetb0","id":"fastscnn_efficientnetb0","depth":3,"charIndex":6182},{"text":"Deeplabv3plus_efficientnetm1","id":"deeplabv3plus_efficientnetm1","depth":3,"charIndex":6498},{"text":"Deeplabv3plus_efficientnetm2","id":"deeplabv3plus_efficientnetm2","depth":3,"charIndex":6829},{"text":"Bev_gkt_mixvargenet_multitask","id":"bev_gkt_mixvargenet_multitask","depth":3,"charIndex":7162},{"text":"Bev_ipm_4d_efficientnetb0_multitask","id":"bev_ipm_4d_efficientnetb0_multitask","depth":3,"charIndex":7505},{"text":"Bev_ipm_efficientnetb0_multitask","id":"bev_ipm_efficientnetb0_multitask","depth":3,"charIndex":7900},{"text":"Detr3d_efficientnetb3","id":"detr3d_efficientnetb3","depth":3,"charIndex":8241},{"text":"Petr_efficientnetb3","id":"petr_efficientnetb3","depth":3,"charIndex":8556},{"text":"Bevformer_tiny_resnet50_detection","id":"bevformer_tiny_resnet50_detection","depth":3,"charIndex":8853},{"text":"Flashocc_henet_lss_occ3d_nuscenes","id":"flashocc_henet_lss_occ3d_nuscenes","depth":3,"charIndex":9312},{"text":"Horizon_swin_transformer","id":"horizon_swin_transformer","depth":3,"charIndex":9612},{"text":"Mixvargenet","id":"mixvargenet","depth":3,"charIndex":9845},{"text":"Vargnetv2","id":"vargnetv2","depth":3,"charIndex":10067},{"text":"Vit_small","id":"vit_small","depth":3,"charIndex":10287},{"text":"Centerpoint_pointpillar","id":"centerpoint_pointpillar","depth":3,"charIndex":10505},{"text":"Detr_efficientnetb3","id":"detr_efficientnetb3","depth":3,"charIndex":10820},{"text":"Detr_resnet50","id":"detr_resnet50","depth":3,"charIndex":11063},{"text":"FCOS3D_efficientnetb0","id":"fcos3d_efficientnetb0","depth":3,"charIndex":11301},{"text":"Ganet_mixvargenet","id":"ganet_mixvargenet","depth":3,"charIndex":11563},{"text":"Keypoint_efficientnetb0","id":"keypoint_efficientnetb0","depth":3,"charIndex":11791},{"text":"Pointpillars_kitti_car","id":"pointpillars_kitti_car","depth":3,"charIndex":12034},{"text":"Deformable_detr_resnet50","id":"deformable_detr_resnet50","depth":3,"charIndex":12265},{"text":"Stereonetplus_mixvargenet","id":"stereonetplus_mixvargenet","depth":3,"charIndex":12517},{"text":"Centerpoint_mixvargnet_multitask","id":"centerpoint_mixvargnet_multitask","depth":3,"charIndex":12753},{"text":"Unet_mobilenetv1","id":"unet_mobilenetv1","depth":3,"charIndex":13115},{"text":"Motr_efficientnetb3","id":"motr_efficientnetb3","depth":3,"charIndex":13345},{"text":"Densetnt_vectornet","id":"densetnt_vectornet","depth":3,"charIndex":13653},{"text":"Maptroe_henet-tinym_bevformer","id":"maptroe_henet-tinym_bevformer","depth":3,"charIndex":13997}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":382,"title":"","content":"","routePath":"/guide/doc_introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":383,"title":"环境部署","content":"#\n\n\n前言#\n\n地平线OpenExplorer目前同时提供了2套模型量化方案：\n\n * PTQ：Post-training Quantization，训练后量化。\n * QAT：Quantization aware training，量化感知训练（暂时只支持Pytorch框架）。\n\n其中：\n\n * 两套方案均不干预浮点模型的训练阶段，您需要自行负责。地平线也在\n   samples/ai_toolchain/horizon_model_train_sample\n   路径下开源了一些分类/检测/分割等场景高效模型的公版Pytorch实现以供参考，并支持在宿主机上进行训练和复现。\n\n * 对于PTQ方案，您需要在宿主机的开发环境完成模型的量化转换，再将编译生成的 .hbm 模型拷贝至开发板环境完成后续的部署工作。\n\n * 对于QAT方案，您则需要在宿主机的开发环境先完成模型的QAT训练，再进行量化转换，再将编译生成的 .hbm 模型拷贝至开发板环境完成后续的部署工作。\n\n两种量化方案以及高效模型的开发环境，地平线都提供了Docker容器和本地手动安装两种方式。\n我们强烈建议您使用不污染本地环境且使用方便的Docker容器，以下也将分别对这两种方式进行介绍。\n\n\n开发环境部署#\n\n\n开发机准备#\n\n为了顺利地使用工具链，地平线建议您选择的开发机应满足以下要求：\n\n硬件/操作系统   要求\nCPU       CPU I3以上或者同级别E3/E5的处理器\n内存        16G或以上级别\nGPU       CUDA11.8、驱动版本Linux:>=\n          510.39.01*（推荐驱动版本Linux:520.61.05）适配显卡包括但不限于：1. GeForce RTX\n          30902. GeForce RTX 2080 Ti3. NVIDIA TITAN V4. Tesla\n          V100S-PCIE-32GB5. A100\n系统        原生Ubuntu 22.04\n\n更多关于CUDA与显卡的兼容性问题请参考 NVIDIA官网信息 。\n\n\nDocker容器部署 #\n\nDocker基础环境#\n\n地平线要求的Docker基础环境如下，请提前在您的宿主机上完成安装：\n\n * Docker（20.10.10或更高版本，建议安装20.10.10版本），详见 Docker安装手册 。\n * NVIDIA Container Toolkit（1.13.5或更高版本，建议安装1.15.0），详见 NVIDIA Container\n   Toolkit安装手册。\n\n完成Docker基础环境安装后，还需要将无root权限的用户添加到Docker用户组中。参考命令如下：\n\n\n\n获取本节需要使用的Docker镜像的地址如下：\n\n * 地平线天工开物 Docker Hub GPU Docker\n * 地平线天工开物 Docker Hub CPU Docker\n\n镜像文件命名形式为：\n\n * GPU版本Docker： openexplorer/ai_toolchain_ubuntu_22_j6_gpu:{version}\n\n * CPU版本Docker： openexplorer/ai_toolchain_ubuntu_22_j6_cpu:{version}\n\n小技巧\n\n{version} 需替换为您获取到的版本号。\n\nDocker镜像使用 #\n\n为了帮助您快速使用工具链，我们提供了包含完整开发环境的Docker镜像，大大简化了开发环境的部署过程。\n\n小技巧\n\n若您已下载离线镜像，需先使用下方命令将镜像加载到本地。\n\n\n\n您可以在OE包的一级目录下直接运行以下脚本启动当前OE版本所对应的Docker容器（如果本地没有对应镜像，则脚本会自动从官方Docker hub拉取镜像）：\n\n\n\n其中， data 为评测数据集文件夹路径，请提前创建好后再运行命令，否则将导致加载问题。\n\n若您想要使用CPU版本Docker镜像则需要增加 cpu 参数：\n\n\n\nOE包示例所依赖的相关公开评测数据集的下载链接可参考 数据集下载 章节的介绍进行获取。\n\n如果您希望手动启动Docker容器，可以参考下方命令，其中 {version} 为您当前所用的OE版本号。\n\n注解\n\n为方便使用，我们为您提供了两种CPU Docker以及GPU Docker，可按需选择。\n\n注意\n 1. 由于OE Docker镜像构建过程中配置了 PATH 和 LD_LIBRARY_PATH 等环境变量的值，未使用推荐方式（如docker\n    attach）进入容器可能会导致环境变量加载不正常从而导致Cmake、GCC、CUDA等工具使用异常。\n 2. 若希望Docker容器退出后不销毁，请使用命令行 docker run -it 手动启动，不要带上 --rm 选项。\n 3. 若希望Docker容器启动后可在后台执行，请在命令行 docker run -it 后增加 -d 选项，容器启动后会返回容器ID，此时可通过\n    docker exec -it {容器ID} /bin/bash 命令再次进入容器。\n\n\n本地手动安装#\n\n本节将为您介绍本地手动安装环境方法，并分别介绍两种量化方案和地平线开源的高效模型训练的环境相关依赖及说明。\n我们推荐您在浮点模型训练完成后优先选择简单易用的PTQ量化方案，只在精度问题确实无法解决时再切换到QAT量化方案。\n\n本地手动安装环境方法#\n\n本地手动安装环境，仅需运行下方脚本，即可一键式完成环境安装。\n\n\n\n安装脚本会自动检查相应的环境，缺少相应依赖或配置会中断安装过程。\n\n根据建议补充依赖后，重新执行install脚本即可。\n\n注解\n * 如需要生成板端可执行程序，则使用交叉编译工具 aarch64-none-linux-gnu-gcc 和\n   aarch64-none-linux-gnu-g++，版本为 Arm GNU Toolchain 12.2.Rel1 。\n * 如需要生成X86仿真环境可执行程序，则使用 X86 gcc\n   ，如果修正建议指出是gcc/g++版本不符合要求，在安装完指定版本之后，您需要重新建立gcc和g++软链接为gcc-12.2.0和g++-12.2.0。\n * 对于host端（x86）依赖的库，比如 isl,gmp,mpc,mpfr\n   等，在lib/x86_64-linux-gnu下，如果编译报错，请在编译工程中通过LD_LIBRARY_PATH来指定。\n * 如果编译过程中出现glibc库版本冲突的问题，例如：xxx@GLIBC_xxx的未定义符号的错误，请在编译工程中通过-rpath-link来指定到工具链的\n   aarch64-none-linux-gnu/lib路径下，同时在编译工程中加上-lxxx,\n   例如：-lpthread。另外需要特别注意的是下图框起来的记录源文件的变量SRCS，最好放在 \\${LIBS} 链接库的前面，否则可能也会报未定义的符号。\n * 安装脚本会自动检查相应的环境，缺少相应依赖或配置会中断安装过程，可以根据提示补充依赖，并重新执行install脚本。\n * 脚本执行成功后，会在 ~/.bashrc 系统环境变量中添加Path等信息（环境变量 LD_LIBRARY_PATH\n   经常被使用，建议您检查环境变量是否符合预期），请执行 source ~/.bashrc 来使当前terminal的配置生效。\n * torch版本应满足2.3.0+cu118，torchvision版本应满足0.18.0。\n\nPTQ量化环境依赖#\n\nPTQ量化方案对于开发机操作环境的基础软件依赖如下：\n\n * 操作系统：Ubuntu22.04\n * Python3.10\n * libpython3.10\n * python3-devel\n * python3-pip\n * gcc&g++: 12.2.1\n * graphviz\n\nQAT量化环境依赖 #\n\nQAT量化环境在本地环境进行安装，需要先确保已满足以下基础环境条件。 量化训练工具能够训练起来所必需的环境依赖如下表：\n\n硬件/操作系统       GPU                        CPU\nos            Ubuntu22.04                Ubuntu22.04\ncuda          11.8                       N/A\npython        3.10                       3.10\ntorch         2.3.0+cu118                2.3.0+cpu\ntorchvision   0.18.0+cu118               0.18.0+cpu\n推荐显卡          titan v/2080ti/v100/3090   N/A\n\n在完成QAT模型的训练后，您可以在当前训练环境安装相关工具包，并直接通过接口调用的方式完成后续的模型转换工作。\n\n高效模型浮点训练环境说明#\n\n地平线在 samples/ai_toolchain/horizon_model_train_sample\n路径下开源了一些高效模型源码，其浮点及QAT训练基础环境请参考 QAT量化环境依赖 。\n\n\n运行环境部署#\n\n当模型完成量化转换后，即可将编译好的模型部署在开发板环境推理运行。运行环境的部署需要您先准备好一块烧写好系统镜像的 开发板，再将相关补充文件拷贝到开发板中即可。\n\n\n开发板准备#\n\n此阶段需要验证下开发板的可用性，将可用系统镜像烧写到开发板中。\n\n\n板端工具安装#\n\n工具链的部分补充工具未包含在系统镜像中，但可以在宿主机环境下执行OE包中的安装脚本将其拷贝至开发板，其参考命令如下：\n\n\n\n注解\n\n其中， ${board_ip} 是您为开发板设置的IP地址，请确保在开发机上可以成功访问该IP。 补充文件成功安装后，请重新启动开发板，在开发板上执行\nhrt_model_exec --help 即可验证安装是否成功。","routePath":"/guide/env_install","lang":"zh","toc":[{"text":"前言","id":"前言","depth":2,"charIndex":3},{"text":"开发环境部署","id":"开发环境部署","depth":2,"charIndex":538},{"text":"开发机准备","id":"开发机准备","depth":3,"charIndex":548},{"text":"Docker容器部署","id":"docker容器部署","depth":3,"charIndex":-1},{"text":"Docker基础环境","id":"docker基础环境","depth":4,"charIndex":926},{"text":"Docker镜像使用","id":"docker镜像使用","depth":4,"charIndex":-1},{"text":"本地手动安装","id":"本地手动安装","depth":3,"charIndex":2185},{"text":"本地手动安装环境方法","id":"本地手动安装环境方法","depth":4,"charIndex":2307},{"text":"PTQ量化环境依赖","id":"ptq量化环境依赖","depth":4,"charIndex":3207},{"text":"QAT量化环境依赖","id":"qat量化环境依赖","depth":4,"charIndex":-1},{"text":"高效模型浮点训练环境说明","id":"高效模型浮点训练环境说明","depth":4,"charIndex":3831},{"text":"运行环境部署","id":"运行环境部署","depth":2,"charIndex":3945},{"text":"开发板准备","id":"开发板准备","depth":3,"charIndex":4037},{"text":"板端工具安装","id":"板端工具安装","depth":3,"charIndex":4079}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":384,"title":"PTQ、QAT简介","content":"#\n\n依据是否要对量化后的参数进行调整，我们可以将量化方法分为训练后量化（PTQ）和量化感知训练（QAT）。\n这两种方法的操作区别如下图所示（图左为PTQ，图右为QAT）：\n\n训练后量化 PTQ 是使用一批校准数据对训练好的模型进行校准，将训练过的FP32模型直接转换为定点计算的模型，过程中无需对原始模型进行任何训练。\n只对几个超参数调整就可完成量化过程，且过程简单快速，无需训练，因此此方法已被广泛应用于大量的端侧和云侧部署场景，我们优先推荐您尝试PTQ方法来查看是否满足您的部\n署精度和性能要求 。\n\n量化训练 QAT 是将训练过的模型量化后又再进行重训练。由于定点数值无法用于反向梯度计算，实际操作过程是在某些op前插入伪量化节点（fake\nquantization nodes），用于在训练时获取流经该op的数据的截断值，\n便于在部署量化模型时对节点进行量化时使用。我们需要在训练中通过不断优化精度来获取最佳的量化参数。由于它需要对模型进行训练，对操作人员技术要求较高。","routePath":"/guide/faststart/ptq_qat_overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":385,"title":"算法模型PTQ量化+上板 快速上手","content":"#\n\n本章节中，我们将为您介绍训练后量化PTQ方案的基本使用流程，便于您实现快速上手。\n这里我们以ResNet50模型为例，为您进行使用演示，详细内容将在后续章节为您展开介绍，基本工作流程如下图所示。\n\n注意\n\n请注意，在您进行以下操作前，请确保您已经参考 环境部署 章节完成了开发机和开发板上的环境安装。\n\n\n浮点模型准备#\n\nOE包在 samples/ai_toolchain/horizon_model_convert_sample 路径下为您提供了丰富的PTQ模型示例，\n其中ResNet50模型示例位于 03_classification/03_resnet50 路径下。\n\n请先执行其中的 00_init.sh 脚本来获取示例对应的校准数据集和原始模型。示例模型的模型来源和相关说明请参考 如何准备模型 章节。\n\n如您需要转换私有模型，请参考 浮点模型准备 章节内容提前准备好 opset=10-19的onnx模型。 下表为不同框架到ONNX模型格式转换的参考方案：\n\n\n模型验证#\n\n在浮点模型准备好之后，我们建议先进行快速的模型验证，以确保其符合计算平台的支持约束。\n对于ONNX格式的ResNet50模型，我们可以在命令行中键入以下命令完成模型验证：\n\n\n\n如您的模型为多输入模型，可参考如下指令：\n\n\n\nhb_compile 工具在验证模型时需要使用到的主要参数如下，更多参数说明还请参考 验证模型 章节。\n\n以ResNet50模型为例，您可以使用 01_check.sh 脚本快速完成模型验证。\n\n01_check.sh 脚本文件中的主要内容如下所示：\n\n\n\n如果模型验证不通过，请根据终端打印或在当前路径下生成的 hb_compile.log 日志文件确认报错信息和修改建议，更多说明请参考 验证模型 章节。\n\n\n模型转换#\n\n模型验证通过后，就可以使用 hb_compile 工具进行模型转换，参考命令如下：\n\n\n\n其中， resnet50_config.yaml 为模型转换对应的配置文件，配置文件模板请参考 配置文件模板 章节。\n\n另外，PTQ方案的模型量化还需要依赖一定数量预处理后的样本进行校准，将在 校准数据预处理 中进行介绍。\n\n\nYaml配置文件 #\n\nYaml配置文件共包含4个必选参数组（ model_parameters 、 input_parameters 、 calibration_parameters\n、 compiler_parameters）。\n\n每个参数组下也区分必选和可选参数（可选参数默认隐藏），具体要求和填写方式可以参考 配置文件具体参数信息 章节。\n\n注解\n\nONNX模型仅需配置 onnx_model 参数，无需配置 model_parameters 参数组中的 caffe_model 和 prototxt 参数。\n\n * input_parameters 参数组中的 input_type_rt 和 input_type_train\n   参数分别用于指定模型在板端实际部署时会接收到的数据类型（如nv12）和本身训练时的数据类型（如rgb）。\n   \n   当两种数据类型不一致时，转换工具会自动在模型前端插入一个能BPU加速的预处理节点，以完成对应的颜色空间转换。同时，该参数组中的 mean_value 、\n   scale_value 、\n   std_value参数还能用于配置图片输入模型的数据归一化操作，配置后转换工具也会将其集成进预处理节点实现BPU加速。数据归一化的计算公式为：\n   \n   $data_norm = (data - mean_value) * scale_value$\n\n * calibration_parameters 参数组中的 cal_data_dir 参数需要配置预处理好的校准数据文件夹路径，预处理方式的说明请参考\n   校准数据预处理。\n\n\n校准数据预处理 #\n\n注意\n * 请注意，在进行此步之前，请确保您已经通过执行对应示例目录下的 00_init.sh 脚本完成了校准数据集的获取。\n * 如果当前只关注模型性能，那么无需配置 cal_data_dir 参数，并跳过本小节，工具会在模型转换时进行伪校准方便您快速验证。\n\nPTQ方案的校准数据一般是从训练集或验证集中筛选100份左右（可适当增减）的典型数据，并应避免非常少见的异常样本，如纯色图片、不含任何检测或分类目标的图片等。筛\n选出的校准数据还需进行与模型inference前一致的预处理操作，处理后保持与原始模型一样的数据类型（ input_type_train ）、layout （\ninput_layout_train ）和尺寸（ input_shape ）。\n\n对于校准数据的预处理，地平线建议直接参考示例代码进行修改使用。以ResNet50模型为例，preprocess.py文件中的calibration_transf\normers函数的包含了其校准数据的前处理transformers，处理完的校准数据与其 yaml 配置文件保持一致，即：\n\n * input_type_train ： 'rgb'\n\n * input_layout_train ：'NCHW'\n\n\n\n其中，transformers都定义在 ../../../01_common/python/data/transformer.py 文件中， 具体说明请参考\n图片处理transformer说明，您可以按需选用或者自定义修改及扩展。\n\n修改完preprocess.py文件后，即可修改02_preprocess.sh脚本并执行，以完成校准数据的预处理。\n\n\n\n02_preprocess.sh脚本文件的主要内容如下：\n\n\n\ndata_preprocess.py文件的传参说明如下：\n\n * src_dir为原始校准数据路径。\n\n * dst_dir为处理后数据的存放路径，可自定义。\n\n * pic_ext为处理后数据的文件后缀，主要用于帮助记忆数据类型，可不配置。\n\n * read_mode为图片读取方式，可配置为skimage、opencv或PIL。\n   需要注意的是，skimage读取的图片类型为RGB，数据范围为0-1，opencv读取的图片类型为BGR，数据范围为0-255，PIL读取的图片类型为RG\n   B，数据范围为0-255。\n\n * saved_data_type为处理后数据的保存类型。\n\n如果您选择自行编写python代码实现校准数据预处理，那么可以使用 numpy.save 命令将其保存为npy文件，工具链校准时会基于 numpy.load\n命令进行读取。\n\n\n转换模型 #\n\n准备完校准数据和yaml配置文件后，即可一步命令完成模型解析、图优化、校准、量化、编译的全流程转换，内部过程详解请参考 转换内部过程解读章节。\n\n转换完成后，会在yaml文件配置的 working_dir 路径下保存如下文件，详细说明请参考 转换产出物解读 章节。\n\n\n\n注解\n\n如果模型转换过程中存在移除输入/输出端的节点的情况，生成物中还会保存 *_quantized_removed_model.bc，\n此种场景下，如后续需进行一致性对比，我们建议您使用此HBIR模型与最终生成的hbm模型进行对比。\n\n\n性能快速验证#\n\n针对转换生成的 xxx.hbm\n模型文件，地平线既支持先在开发机端预估模型BPU部分的的静态性能，也在板端提供给了无需任何代码开发的可执行工具快速评测动态性能，以下将分别进行介绍。\n更详细的说明和性能调优建议请参考 模型性能分析 与 模型性能调优 章节。\n\n\n静态性能评估#\n\n如 转换模型 所述，模型成功转换后会在 working_dir 路径下生成包含模型静态性能预估的html和json文件，两者内容相同，\n但html文件的可读性更好。以下为 ResNet50 模型转换生成的html文件，其中：\n\n * Summary选项卡提供了编译器预估的模型BPU部分性能。\n   \n   \n\n * Temporal Statistics选项卡内则主要提供了模型一帧推理时间内的带宽占用情况。\n   \n   \n\n * 另外，html文件中还会包含Layer\n   Details选项卡，其中提供了每一层BPU算子的计算量、计算耗时、数据搬运耗时的信息以及编译后layer活跃时间段（不代表该layer执行时间，通常为多\n   个layer交替/并行执行）。\n   \n   \n\n\n动态性能评估#\n\n当模型的静态性能符合预期后，我们可以进一步上板实测模型的动态性能，其参考方式如下：\n\n 1. 首先请确保已按照 环境部署 章节完成开发板环境部署。\n\n 2. 将转换生成的 xxx.hbm 模型拷贝至开发板 /userdata 文件夹下任意路径。\n\n 3. 通过 hrt_model_exec perf 工具快捷评估模型的耗时和帧率。\n\n\n\nhrt_model_exec 工具的主要参数说明如下，更多说明请参考 hrt_model_exec工具介绍 章节。\n\n注解\n\n * 如果您在板端无法找到 hrt_model_exec 工具，可以再执行一次OE包中 package/board 路径下的 install.sh 脚本。\n\n * 评测Latency时一般采用单线程串行的推理方式，可以指定 thread_num 为1。\n\n * 评测FPS时一般采用多线程并发的推理方式来占满BPU资源，此时可以配置 core_id 为 0，并配置 thread_num 为多线程。\n\n * 模型的输入为动态输入时，请根据实际输入填写 input_valid_shape 和 input_stride 参数。\n\n * 如果您配置了 profile_path 参数，程序需要正常运行结束才会生成 profiler.log 和 profiler.csv 日志文件，请勿使用\n   Ctrl+C 命令中断程序。\n\n当模型的动态性能不符合预期时，请参考 模型性能调优 章节进行性能调优。\n\n\n精度验证#\n\n当模型的性能验证符合预期后，即可进行后续的精度验证。请首先确保您已经准备好相关的评测数据集，并挂载在Docker容器中。 示例模型使用的数据集可以参考\n数据集下载 章节的介绍进行获取。\n\n如 转换模型 所述，模型转换会生成 xxx_quantized_model.bc 和 xxx.hbm 两个量化模型，两者输出是保持数值一致的。\n\n您也可以在开发机环境使用 hb_verifier 工具进行一致性验证，参考命令如下，详细说明请参考 hb_verifier 工具 章节。\n\n\n\n相比于 xxx.hbm ，地平线更建议优先在开发机Python环境评测 xxx_quantized_model.bc 模型的量化精度，其评测方式更加简单快捷，\n具体请见 开发机Python环境验证 。 xxx.hbm 在板端基于C++代码的评测说明请见 开发板C++环境验证， 更详细的精度验证和优化建议请参考\n模型精度分析 与 模型精度优化 章节。\n\n\n开发机Python环境验证 #\n\n以ResNet50模型为例，resnet50_224x224_nv12_quantized_model.bc量化模型的单张推理和验证集精度评测示例请参考示例目录\n中的 04_inference.sh 和 05_evaluate.sh 脚本，参考命令如下：\n\n\n\n两个脚本会分别调用 ../../cls_inference.py 和 ../../cls_evaluate.py 文件进行推理， 以\ncls_inference.py文件为例，代码中的主要接口使用逻辑如下：\n\n\n\n其中，infer_image_preprocess函数的前处理操作来源于 校准数据预处理 章节所述preprocess.py文件。\n相比于calibration_transformers函数，会额外增加数据转换为 input_type_rt 的过程（参数说明请见 Yaml配置文件 章节），\nHBIR模型在推理时的数据准备（包括 input_type_rt 到 input_type_train\n的色彩转换，mean/scale的处理等）会在内部完成，所以无需额外在外部进行归一化等处理。 具体代码如下：\n\n\n\n\n开发板C++环境验证 #\n\n在开发板端，地平线提供了统一异构计算平台UCP，来帮助您快速完成模型的部署工作，并提供了相关示例。 您可以首先参考 模型部署\n章节学习模型部署和模型推理接口的基础使用， 再参考 AI-Benchmark 章节学习示例模型精度评测的完整代码框架。\n\n模型部署 #\n\nOE包提供了模型部署的基础示例，以便于您学习UCP模型推理API接口的使用方式， 示例的详细说明可以参考 基础示例包使用说明 章节。\n\n注意\n\n请注意，在您进行模型部署前，需要先获取上板使用的模型：\n\n * 在 samples/ai_toolchain/model_zoo/runtime/ai_benchmark 目录下，执行\n   resolve_ai_benchmark_ptq.sh 、 resolve_ai_benchmark_qat.sh 脚本。\n * 在 samples/ai_toolchain/model_zoo/runtime/basic_samples 目录下，执行\n   resolve_runtime_sample.sh 脚本。\n\n其中，示例目录下的 code/00_quick_start/src/main.cc\n文件提供了resnet50模型从准备数据到模型推理，再执行后处理出分类结果的完整流程代码。\n\nmain.cc中的主要代码逻辑包括以下6个步骤，代码中所涉及的API接口的具体说明可以参考 模型推理API手册 章节。\n\n 1. 加载模型，获取模型句柄。\n\n 2. 准备模型输入输出tensor，申请对应的BPU内存空间。\n\n 3. 读取模型输入数据，并放入申请好的输入tensor中。\n\n 4. 推理模型，获取模型输出。\n\n 5. 基于输出tensor中的数据实现模型后处理。\n\n 6. 释放相关资源。\n\n\n\n该示例运行的参考方式如下：\n\n\n\nAI-Benchmark #\n\nOE包在 samples/ai_toolchain/ucp_tutorial/dnn/ai_benchmark\n路径下还提供了典型分类、检测、分割、光流示例模型板端性能和精度评测的示例包，您可以基于这些示例进行进一步的应用开发。 更多说明您可参考\nAI-Benchmark使用说明 章节。\n\n\n应用开发#\n\n当模型的性能和精度验证都符合预期后，即可参考 模型推理应用开发指导 章节实现上层应用的具体开发。","routePath":"/guide/faststart/ptq_quickstart","lang":"zh","toc":[{"text":"浮点模型准备","id":"浮点模型准备","depth":2,"charIndex":155},{"text":"模型验证","id":"模型验证","depth":2,"charIndex":441},{"text":"模型转换","id":"模型转换","depth":2,"charIndex":768},{"text":"Yaml配置文件","id":"yaml配置文件","depth":3,"charIndex":-1},{"text":"校准数据预处理","id":"校准数据预处理","depth":3,"charIndex":-1},{"text":"转换模型","id":"转换模型","depth":3,"charIndex":-1},{"text":"性能快速验证","id":"性能快速验证","depth":2,"charIndex":3046},{"text":"静态性能评估","id":"静态性能评估","depth":3,"charIndex":3185},{"text":"动态性能评估","id":"动态性能评估","depth":3,"charIndex":3542},{"text":"精度验证","id":"精度验证","depth":2,"charIndex":4184},{"text":"开发机Python环境验证","id":"开发机python环境验证","depth":3,"charIndex":-1},{"text":"开发板C++环境验证","id":"开发板c环境验证","depth":3,"charIndex":-1},{"text":"模型部署","id":"模型部署","depth":4,"charIndex":-1},{"text":"AI-Benchmark","id":"ai-benchmark","depth":4,"charIndex":-1},{"text":"应用开发","id":"应用开发","depth":2,"charIndex":6070}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":386,"title":"算法模型QAT量化+部署 快速上手","content":"#\n\n除PTQ量化外，我们也为您提供了算法模型通过QAT方式进行量化到部署的快速上手介绍， 请参考 算法模型QAT量化+部署 快速上手 章节。","routePath":"/guide/faststart/qat_quickstart","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":387,"title":"关键概念","content":"#\n\n * 原始浮点模型\n   \n   指您通过TensorFlow/PyTorch等等DL框架训练得到的可用模型，这个模型的计算精度为float32。\n\n * 板端部署HBM模型\n   \n   是一种适合在地平线计算平台上运行的模型格式，能够支持模型同时在ARM CPU和BPU上执行。\n   由于在BPU上的运算速度会远大于CPU上的速度，因此会尽可能的将算子放在BPU上运算。对于BPU上暂时不支持的算子，则会放在CPU上进行运算。\n\n * 算子\n   \n   深度学习算法由计算单元组成，我们称这些计算单元为算子（Operator，也称op）。\n   算子是一个函数空间到函数空间上的映射，同一模型中算子名称是唯一的，但是同一类型的算子可以存在多个。\n   如：Conv1、Conv2，是两个算子类型相同的不同算子。\n\n * 模型转换\n   \n   指的是将原始浮点模型或符合要求的onnx模型转换为地平线板端可部署模型的过程。\n\n * 模型量化\n   \n   目前工业界最有效的模型优化方法之一，量化是指定点与浮点等数据之间建立一种数据映射关系，使得以较小的精度损失代价获得了推理性能收益，可简单理解为用\"低比特\"\n   数字表示FP32等数值，如FP32-->INT8可以实现4倍的参数压缩，在压缩内存的同时可以实现更快速的计算。\n   \n   * Quantize节点用于将模型float类型的输入数据量化至int8类型，其计算公式如下：\n     \n     \n     \n     * round(x) 实现浮点数的四舍五入。\n     * clamp(x) 函数实现将数据钳位在-128~127之间的整数数值。\n     * scale 为量化比例因子。\n     * zero_point 为非对称量化零点偏移值，对称量化时 zero_point = 0 。\n     \n     C++的参考实现如下：\n     \n     \n   \n   * Dequantize节点则用于将模型int8或int32类型的输出数据反量化回float或double类型，其计算公式如下：\n     \n     \n     \n     C++的参考实现如下：\n     \n     \n\n * PTQ\n   \n   即训练后量化方案，先训练浮点模型，然后使用校准图片计算量化参数，将浮点模型转为量化模型的量化方法。更详细的介绍可参考 PTQ、QAT简介 章节。\n\n * QAT\n   \n   即量化感知训练方案，在浮点训练的时候，就先对浮点模型结构进行干预，使得模型能够感知到量化带来的损失，减少量化损失精度的方案。更详细的介绍可参考\n   PTQ、QAT简介 章节。\n\n * 张量\n   \n   张量，也称Tensor，具备统一数据类型的多维数组，作为算子计算数据的容器，包含输入输出数据。\n   张量具体信息的载体，包含张量数据的名称、shape、数据排布、数据类型等内容。\n\n * 数据排布\n   \n   深度学习中，多维数据通过多维数组（张量）进行存储，通用的神经网络特征图通常使用四维数组（即4D）格式进行保存，即以下四个维度：\n   \n   * N：Batch数量，如图片的数量。\n   * H：Height，图片的高度。\n   * W：Width，图片的宽度。\n   * C：Channel，图片的通道数。\n   \n   但是数据只能线性存储，因此四个维度有对应的顺序，不同的数据排布（format）方式，会显著影响计算性能。 常见的数据存储格式有NCHW和NHWC两种：\n   \n   * NCHW：将同一通道的所有像素值按顺序进行存储。\n   * NHWC：将不同通道的同一位置的像素值按顺序进行存储。\n   \n   如下图所示：\n\n * 数据类型\n   \n   下文常用到的图片数据类型包括rgb、bgr、gray、yuv444、nv12、featuremap。\n   \n   * rgb、bgr和gray都是比较常见的图像格式，每个数值都采用UINT8表示。\n   * yuv444也是一种常见的图像格式，它的每个数值都采用UINT8表示。\n   * nv12是常见的yuv420图像格式，每个数值都采用UINT8表示。\n   * featuremap适用于以上列举格式不满足您需求的情况，此type每个数值采用float32表示。例如雷达和语音等模型处理就常用这个格式。\n\n * Batch和Batch Size\n   \n   模型训练过程中每一轮迭代所使用的一批训练样本集，这个样本集我们称之为Batch，而Batch Size指的就是在每一轮迭代中，模型处理的样本数量。\n\n * 余弦相似度\n   \n   精度比对算法之一，计算结果取值范围为[-1,1]，比对的结果如果越接近1，表示两者的值越相近，越接近-1意味着两者的值越相反。\n\n * Stride\n   \n   跨距（Stride）是指图像储存在内存中时，每一行所占空间的实际大小。\n   计算机的处理器大都为32位或64位，因此处理器一次读取到的完整数据量最好为4字节或8字节的倍数，若为其他数值，则计算机需要进行专门处理，从而导致运行效率的\n   降低。\n   为了能让计算机高效处理图像，通常会在原本数据的基础上，填充一些额外的数据以做到4字节或8字节对齐。对齐的操作又叫Padding，实际的对齐规则取决于具体的\n   软硬件系统。\n   \n   假设我们有一张8位深的灰度图，高（Height）为20像素，宽（Width）为30像素，那么该图像每行的有效数据为30字节，如果计算机的对齐规则是8字节，\n   那么对齐后图像的跨距为32字节，此时每行需要Padding的数据量为2字节。\n\n * 校准数据集\n   \n   训练后量化（PTQ）场景中，做前向推理使用的数据集。该数据集的分布代表着所有数据集的分布，获取校准集时应该具有代表性。\n   如果数据集不是模型匹配的数据集或者代表性不够，则根据校准集计算得到的量化因子，在全数据集上表现较差，量化损失大，量化后精度低。\n\n还有更多针对文档中缩略词的介绍，请您参考 常用缩略语 章节的介绍。","routePath":"/guide/key_concept","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":388,"title":"Pyramid输入的多Batch ResNet18模型部署实践指导","content":"#\n\n地平线OpenExplorer工具链的PTQ链路整体使用流程包括模型优化、模型校准、模型转换为定点模型、模型编译及上板等多个阶段。\n本章节以基于公版ResNet18的Pyramid输入的多Batch分类模型为例，分步骤为您进行部署实践的使用演示供您进行参考。\n\n\n准备浮点模型#\n\n准备ResNet18浮点模型，这里我们使用torchvision导出所需的浮点模型。\n\n\n\n\n校准集准备#\n\n公版ResNet18模型的相关信息可参考 Pytorch文档内对ResNet18的说明，可以看到ResNet18模型的数据前处理流程为：\n\n 1. 图像短边放缩至256。\n 2. 以中心裁剪方式将图像尺寸调整至224x224。\n 3. 数据归一化处理，mean取值[0.485, 0.456, 0.406]，std取值[0.229, 0.224, 0.225]。\n\n数据前处理代码示例如下：\n\n\n\n为支持PTQ模型校准，我们需要从ImageNet数据集中取出一个小批量数据集，这里用前100张图像为例：\n\n\n\n则基于上文的数据前处理代码生成的校准集目录结构如下：\n\n\n\n\n生成板端模型 #\n\nPTQ转换链路支持命令行工具及PTQ API两种方式进行模型量化编译以生成板端模型，下方为您分别介绍两种方式的使用。\n\n\n命令行工具方式#\n\n命令行工具的方式只需要您安装horizon_tc_ui（Docker环境内已预装）并根据模型信息配置创建对应的yaml文件即可，此处我们以ResNet18模型对\n应的yaml文件（config.yaml）进行展示并说明。\n\n\n\n注解\n\n这里将 input_name 及 input_shape\n直接置空，是因为工具支持单输入且输入无动态shape的场景下自动补充这两个参数（即工具内部对ONNX模型进行解析，并获取输入的name及shape信息）。\n\n当yaml文件配置完成后，您只需要调用 hb_compile工具 执行命令即可，工具执行命令及关键log如下：\n\n\n\n命令执行完成后，在yaml文件working_dir参数配置的目录（model_output）下，将生成如下所示各阶段中间模型、最终的上板模型及模型信息文件，其\n中resnet18_224x224_nv12.hbm即为板端可推理的模型文件：\n\n\n\n\nPTQ API方式#\n\n命令行工具在提供高易用性的同时也带来了一些灵活度的降低，因此，当您有灵活性需求时，可以使用PTQ\nAPI方式来完成模型的量化编译，下方为您介绍使用API的方式生成板端模型的具体流程。\n\n注意\n\n请注意，由于部分接口存在较多参数，下方示例展示中我们仅对必要参数进行了配置以便于您进行整体的实践验证，具体接口的全量参数请参考 HMCT API\nRefernence 和 HBDK Tool API Reference。\n\n模型优化校准#\n\n首先，对浮点模型进行图优化及校准量化，这个过程我们使用 HMCT 的API，具体示例如下：\n\n\n\n正确执行完build_model后，在working_dir目录下将生成各阶段ONNX模型，目录结构如下：\n\n\n\n这里的\n*ptq_model.onnx文件即经过图优化、校准及预编译过程的ONNX模型文件，中间阶段ONNX模型的具体说明请参考训练后量化(PTQ)-PTQ转换步骤-模\n型量化与编译-转换产出物解读 章节。\n\n模型转定点及编译#\n\n接下来需要完成PTQ模型转为定点模型及模型编译操作，这个过程我们需要通过编译器的API来完成，示例如下：\n\n\n\n编译完成后，working_dir目录下将保存中间阶段和最终可用于上板的模型文件，目录结构如下：\n\n\n\n\n构建板端示例 #\n\n 1. 准备板端示例所需依赖库\n\n如需尽快完成板端示例的构建，我们建议您直接使用OE包中 samples/ucp_tutorial/deps_aarch64。\n\n目录下内容作为依赖库，板端运行示例依赖的关键头文件及动态库路径如下：\n\n\n\n 2. 板端示例开发\n\n下方示例展示了基于二进制文件输入和板端模型，完成一次板端模型推理并获取分类结果TOP1的过程。\n\n\n\n 3. 交叉编译生成板端可执行程序\n\n进行交叉编译前，您需先准备好CMakeLists.txt和示例文件。CMakeLists.txt内容如下，因示例不包含数据前处理等操作，所以依赖较少，此处主要是\n对GCC的编译参数、依赖的头文件及动态库的配置。 其中dnn板端推理库，而hbucp用于对tensor做操作。\n\n\n\n编译的环境目录结构如下：\n\n\n\n当示例文件及CMakeLists.txt准备好之后即可执行编译。编译命令的示例如下：\n\n注意\n\n请注意，编译脚本中要将CC和CXX配置为交叉编译GCC和G++的实际路径。\n\n\n\n编译完成后，即可生成可上板运行的 run_sample 二进制程序。至此，板端示例构建流程已全部完成。\n\n\n板端运行准备#\n\n当可执行程序编译完成后，需要对模型的输入进行准备。由于此篇实践教程的场景校准集可作为模型输入，所以这里我们直接用校准集数据作为模型输入即可。当然，您也可以根据校\n准集的前处理逻辑对板端程序进行修改并给到模型输入（此过程需注意，修改后的程序中需要保证对原始图片做了与校准时相同的前处理）。\n\n这里我们只需要简单将npy格式的校准集转为binary文件即可，示例如下：\n\n\n\n除了输入数据的准备，在板端运行前，还需要您确保现在已准备好如下内容：\n\n * 地平线J6开发板，用于实际执行板端程序运行。\n * 一个可用于板端推理的模型，即 生成板端模型 的产出物。\n * 板端程序，即 构建板端示例 的产出物。\n * 板端程序依赖库，为了降低部署成本，您可以直接使用OE包samples/ucp_tutorial/deps_aarch64/ucp/lib文件夹中的内容。\n * binary格式的输入文件，用于板端示例执行推理时的模型输入。\n\n\n板端运行#\n\n以上步骤完成后，我们切换到板端环境，在板端环境中将程序、模型文件、输入数据及依赖库整合到一起，参考目录结构如下：\n\n\n\n最后，对LD_LIBRARY_PATH就能行配置并运行程序即可，如下所示：\n\n\n\n可以看到，LOG中打印的 label: 65 正是对应ImageNet数据集中ILSVRC2012_val_00000001图片的label，即分类结果正确。\n\n至此，以Pyramid输入的多Batch ResNet18模型的全流程部署实践就结束了。","routePath":"/guide/model_deployment_guidance/pyramid_batch_resnet18_deployment_guidance","lang":"zh","toc":[{"text":"准备浮点模型","id":"准备浮点模型","depth":2,"charIndex":134},{"text":"校准集准备","id":"校准集准备","depth":2,"charIndex":190},{"text":"生成板端模型","id":"生成板端模型","depth":2,"charIndex":-1},{"text":"命令行工具方式","id":"命令行工具方式","depth":3,"charIndex":555},{"text":"PTQ API方式","id":"ptq-api方式","depth":3,"charIndex":975},{"text":"模型优化校准","id":"模型优化校准","depth":4,"charIndex":1196},{"text":"模型转定点及编译","id":"模型转定点及编译","depth":4,"charIndex":1416},{"text":"构建板端示例","id":"构建板端示例","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":2046},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":2474}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":389,"title":"Pyramid输入的ResNet18模型部署实践指导","content":"#\n\n地平线OpenExplorer工具链的PTQ链路整体使用流程包括模型优化、模型校准、模型转换为定点模型、模型编译及上板等多个阶段。\n本章节以基于公版ResNet18的Pyramid输入分类模型为例，分步骤为您进行部署实践的使用演示供您进行参考。\n\n\n准备浮点模型#\n\n准备ResNet18浮点模型，这里我们使用torchvision导出所需的浮点模型。\n\n\n\n\n校准集准备#\n\n公版ResNet18模型的相关信息可参考 Pytorch文档内对ResNet18的说明，可以看到ResNet18模型的数据前处理流程为：\n\n 1. 图像短边放缩至256。\n 2. 以中心裁剪方式将图像尺寸调整至224x224。\n 3. 数据归一化处理，mean取值[0.485, 0.456, 0.406]，std取值[0.229, 0.224, 0.225]。\n\n数据前处理代码示例如下：\n\n\n\n为支持PTQ模型校准，我们需要从ImageNet数据集中取出一个小批量数据集，这里用前100张图像为例：\n\n\n\n则基于上文的数据前处理代码生成的校准集目录结构如下：\n\n\n\n\n生成板端模型 #\n\nPTQ转换链路支持命令行工具及PTQ API两种方式进行模型量化编译以生成板端模型，下方为您分别介绍两种方式的使用。\n\n\n命令行工具方式#\n\n命令行工具的方式只需要您安装horizon_tc_ui（Docker环境内已预装）并根据模型信息配置创建对应的yaml文件即可，此处我们以ResNet18模型对\n应的yaml文件（config.yaml）进行展示并说明。\n\n\n\n注解\n\n这里将 input_name 及 input_shape\n直接置空，是因为工具支持单输入且输入无动态shape的场景下自动补充这两个参数（即工具内部对ONNX模型进行解析，并获取输入的name及shape信息）。\n\n当yaml文件配置完成后，您只需要调用 hb_compile工具 执行命令即可，工具执行命令及关键log如下：\n\n\n\n命令执行完成后，在yaml文件working_dir参数配置的目录（model_output）下，将生成如下所示各阶段中间模型、最终的上板模型及模型信息文件，其\n中resnet18_224x224_nv12.hbm即为板端可推理的模型文件：\n\n\n\n\nPTQ API方式#\n\n命令行工具在提供高易用性的同时也带来了一些灵活度的降低，因此，当您有灵活性需求时，可以使用PTQ\nAPI方式来完成模型的量化编译，下方为您介绍使用API的方式生成板端模型的具体流程。\n\n注意\n\n请注意，由于部分接口存在较多参数，下方示例展示中我们仅对必要参数进行了配置以便于您进行整体的实践验证，具体接口的全量参数请参考 HMCT API\nRefernence 和 HBDK Tool API Reference。\n\n模型优化校准#\n\n首先，对浮点模型进行图优化及校准量化，这个过程我们使用 HMCT 的API，具体示例如下：\n\n\n\n正确执行完build_model后，在working_dir目录下将生成各阶段ONNX模型，目录结构如下：\n\n\n\n这里的\n*ptq_model.onnx文件即经过图优化、校准及预编译过程的ONNX模型文件，中间阶段ONNX模型的具体说明请参考训练后量化(PTQ)-PTQ转换步骤-模\n型量化与编译-转换产出物解读 章节。\n\n模型转定点及编译#\n\n接下来需要完成PTQ模型转为定点模型及模型编译操作，这个过程我们需要通过编译器的API来完成，示例如下：\n\n\n\n编译完成后，working_dir目录下将保存中间阶段和最终可用于上板的模型文件，目录结构如下：\n\n\n\n\n构建板端示例 #\n\n 1. 准备板端示例所需依赖库\n\n如需尽快完成板端示例的构建，我们建议您直接使用OE包中 samples/ucp_tutorial/deps_aarch64。\n\n目录下内容作为依赖库，板端运行示例依赖的关键头文件及动态库路径如下：\n\n\n\n 2. 板端示例开发\n\n下方示例展示了基于二进制文件输入和板端模型，完成一次板端模型推理并获取分类结果TOP1的过程。\n\n\n\n 3. 交叉编译生成板端可执行程序\n\n进行交叉编译前，您需先准备好CMakeLists.txt和示例文件。CMakeLists.txt内容如下，因示例不包含数据前处理等操作，所以依赖较少，此处主要是\n对GCC的编译参数、依赖的头文件及动态库的配置。 其中dnn板端推理库，而hbucp用于对tensor做操作。\n\n\n\n编译的环境目录结构如下：\n\n\n\n当示例文件及CMakeLists.txt准备好之后即可执行编译。编译命令的示例如下：\n\n注意\n\n请注意，编译脚本中要将CC和CXX配置为交叉编译GCC和G++的实际路径。\n\n\n\n编译完成后，即可生成可上板运行的 run_sample 二进制程序。至此，板端示例构建流程已全部完成。\n\n\n板端运行准备#\n\n当可执行程序编译完成后，需要对模型的输入进行准备。由于此篇实践教程的场景校准集可作为模型输入，所以这里我们直接用校准集数据作为模型输入即可。当然，您也可以根据校\n准集的前处理逻辑对板端程序进行修改并给到模型输入（此过程需注意，修改后的程序中需要保证对原始图片做了与校准时相同的前处理）。\n\n这里我们只需要简单将npy格式的校准集转为binary文件即可，示例如下：\n\n\n\n除了输入数据的准备，在板端运行前，还需要您确保现在已准备好如下内容：\n\n * 地平线J6开发板，用于实际执行板端程序运行。\n * 一个可用于板端推理的模型，即 生成板端模型 的产出物。\n * 板端程序，即 构建板端示例 的产出物。\n * 板端程序依赖库，为了降低部署成本，您可以直接使用OE包samples/ucp_tutorial/deps_aarch64/ucp/lib文件夹中的内容。\n * binary格式的输入文件，用于板端示例执行推理时的模型输入。\n\n\n板端运行#\n\n以上步骤完成后，我们切换到板端环境，在板端环境中将程序、模型文件、输入数据及依赖库整合到一起，参考目录结构如下：\n\n\n\n最后，对LD_LIBRARY_PATH就能行配置并运行程序即可，如下所示：\n\n\n\n可以看到，log中打印的 label: 65 正是对应ImageNet数据集中ILSVRC2012_val_00000001图片的label，即分类结果正确。\n\n至此，以Pyramid输入ResNet18模型的全流程部署实践就结束了。","routePath":"/guide/model_deployment_guidance/pyramid_resnet18_deployment_guidance","lang":"zh","toc":[{"text":"准备浮点模型","id":"准备浮点模型","depth":2,"charIndex":127},{"text":"校准集准备","id":"校准集准备","depth":2,"charIndex":183},{"text":"生成板端模型","id":"生成板端模型","depth":2,"charIndex":-1},{"text":"命令行工具方式","id":"命令行工具方式","depth":3,"charIndex":548},{"text":"PTQ API方式","id":"ptq-api方式","depth":3,"charIndex":968},{"text":"模型优化校准","id":"模型优化校准","depth":4,"charIndex":1189},{"text":"模型转定点及编译","id":"模型转定点及编译","depth":4,"charIndex":1409},{"text":"构建板端示例","id":"构建板端示例","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":2039},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":2467}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":390,"title":"Resizer输入的ResNet18模型部署实践指导","content":"#\n\n地平线OpenExplorer工具链的PTQ链路整体使用流程包括模型优化、模型校准、模型转换为定点模型、模型编译及上板等多个阶段。\n本章节以基于公版ResNet18的Resizer输入分类模型为例，分步骤为您进行部署实践的使用演示供您进行参考。\n\n\n准备浮点模型#\n\n准备ResNet18浮点模型，这里我们使用torchvision导出所需的浮点模型。\n\n\n\n\n校准集准备#\n\n公版ResNet18模型的相关信息可参考 Pytorch文档内对ResNet18的说明，可以看到ResNet18模型的数据前处理流程为：\n\n 1. 图像短边放缩至256。\n 2. 以中心裁剪方式将图像尺寸调整至224x224。\n 3. 数据归一化处理，mean取值[0.485, 0.456, 0.406]，std取值[0.229, 0.224, 0.225]。\n\n数据前处理代码示例如下：\n\n\n\n为支持PTQ模型校准，我们需要从ImageNet数据集中取出一个小批量数据集，这里用前100张图像为例：\n\n\n\n则基于上文的数据前处理代码生成的校准集目录结构如下：\n\n\n\n\n生成板端模型 #\n\nPTQ转换链路支持命令行工具及PTQ API两种方式进行模型量化编译以生成板端模型，下方为您分别介绍两种方式的使用。\n\n\n命令行工具方式#\n\n命令行工具的方式只需要您安装horizon_tc_ui(Docker环境内已预装)并根据模型信息配置创建对应的yaml文件即可，此处我们以ResNet18模型对\n应的yaml文件（config.yaml）进行展示并说明。\n\n\n\n注解\n\n这里将 input_name 及 input_shape\n直接置空，是因为工具支持单输入且输入无动态shape的场景下自动补充这两个参数（即工具内部对ONNX模型进行解析，并获取输入的name及shape信息）。\n\n当yaml文件配置完成后，您只需要调用 hb_compile工具 执行命令即可，工具执行命令及关键log如下：\n\n\n\n命令执行完成后，在yaml文件working_dir参数配置的目录（model_output）下，将生成如下所示各阶段中间模型、最终的上板模型及模型信息文件，其\n中resnet18_224x224_nv12_resizer.hbm即为板端可推理的模型文件：\n\n\n\n\nPTQ API方式#\n\n命令行工具在提供高易用性的同时也带来了一些灵活度的降低，因此，当您有灵活性需求时，可以使用PTQ\nAPI方式来完成模型的量化编译，下方为您介绍使用API的方式生成板端模型的具体流程。\n\n注意\n\n请注意，由于部分接口存在较多参数，下方示例展示中我们仅对必要参数进行了配置以便于您进行整体的实践验证，具体接口的全量参数请参考 HMCT API\nRefernence 和 HBDK Tool API Reference。\n\n模型优化校准#\n\n首先，对浮点模型进行图优化及校准量化，这个过程我们使用 HMCT 的API，具体示例如下：\n\n\n\n正确执行完build_model后，在working_dir目录下将生成各阶段ONNX模型，目录结构如下：\n\n\n\n这里的\n*ptq_model.onnx文件即经过图优化、校准及预编译过程的ONNX模型文件，中间阶段ONNX模型的具体说明请参考训练后量化(PTQ)-PTQ转换步骤-模\n型量化与编译-转换产出物解读 章节。\n\n模型转定点及编译#\n\n接下来需要完成PTQ模型转为定点模型及模型编译操作，这个过程我们需要通过编译器的API来完成，示例如下：\n\n\n\n编译完成后，working_dir目录下将保存中间阶段和最终可用于上板的模型文件，目录结构如下：\n\n\n\n\n构建板端示例 #\n\n 1. 准备板端示例所需依赖库\n\n如需尽快完成板端示例的构建，我们建议您直接使用OE包中 samples/ucp_tutorial/deps_aarch64。\n\n目录下内容作为依赖库，板端运行示例依赖的关键头文件及动态库路径如下：\n\n\n\n 2. 板端示例开发\n\n下方示例展示了基于二进制文件输入和板端模型，完成一次板端模型推理并获取分类结果TOP1的过程。\n\n\n\n 3. 交叉编译生成板端可执行程序\n\n进行交叉编译前，您需先准备好CMakeLists.txt和示例文件。CMakeLists.txt内容如下，因示例不包含数据前处理等操作，所以依赖较少，此处主要是\n对GCC的编译参数、依赖的头文件及动态库的配置。 其中dnn板端推理库，而hbucp用于对tensor做操作。\n\n\n\n编译的环境目录结构如下：\n\n\n\n当示例文件及CMakeLists.txt准备好之后即可执行编译。编译命令的示例如下：\n\n注意\n\n请注意，编译脚本中要将CC和CXX配置为交叉编译GCC和G++的实际路径。\n\n\n\n编译完成后，即可生成可上板运行的 run_sample 二进制程序。至此，板端示例构建流程已全部完成。\n\n\n板端运行准备#\n\n当可执行程序编译完成后，需要对模型的输入进行准备。由于此篇实践教程的场景校准集可作为模型输入，所以这里我们直接用校准集数据作为模型输入即可。当然，您也可以根据校\n准集的前处理逻辑对板端程序进行修改并给到模型输入（此过程需注意，修改后的程序中需要保证对原始图片做了与校准时相同的前处理）。\n\n这里我们只需要简单将npy格式的校准集转为binary文件即可，示例如下：\n\n\n\n除了输入数据的准备，在板端运行前，还需要您确保现在已准备好如下内容：\n\n * 地平线J6开发板，用于实际执行板端程序运行。\n * 一个可用于板端推理的模型，即 生成板端模型 的产出物。\n * 板端程序，即 构建板端示例 的产出物。\n * 板端程序依赖库，为了降低部署成本，您可以直接使用OE包samples/ucp_tutorial/deps_aarch64/ucp/lib文件夹中的内容。\n * binary格式的输入文件，用于板端示例执行推理时的模型输入。\n\n\n板端运行#\n\n以上步骤完成后，我们切换到板端环境，在板端环境中将程序、模型文件、输入数据及依赖库整合到一起，参考目录结构如下：\n\n\n\n最后，对LD_LIBRARY_PATH就能行配置并运行程序即可，如下所示：\n\n\n\n可以看到，log中打印的 label: 65 正是对应ImageNet数据集中ILSVRC2012_val_00000001图片的label，即分类结果正确。\n\n至此，以Resizer输入ResNet18模型的全流程部署实践就结束了。","routePath":"/guide/model_deployment_guidance/resizer_resnet18_deployment_guidance","lang":"zh","toc":[{"text":"准备浮点模型","id":"准备浮点模型","depth":2,"charIndex":127},{"text":"校准集准备","id":"校准集准备","depth":2,"charIndex":183},{"text":"生成板端模型","id":"生成板端模型","depth":2,"charIndex":-1},{"text":"命令行工具方式","id":"命令行工具方式","depth":3,"charIndex":548},{"text":"PTQ API方式","id":"ptq-api方式","depth":3,"charIndex":976},{"text":"模型优化校准","id":"模型优化校准","depth":4,"charIndex":1197},{"text":"模型转定点及编译","id":"模型转定点及编译","depth":4,"charIndex":1417},{"text":"构建板端示例","id":"构建板端示例","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":2047},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":2475}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":391,"title":"移除Dequantize节点的ResNet18模型部署实践指导","content":"#\n\n地平线OpenExplorer工具链的PTQ链路整体使用流程包括模型优化、模型校准、模型转换为定点模型、模型编译及上板等多个阶段。\n本章节以基于公版ResNet18的RGB输入分类模型移除Dequantize节点为例，分步骤进行部署实践的使用演示供您参考。\n\n\n准备浮点模型#\n\n准备ResNet18浮点模型，这里我们使用torchvision导出所需的浮点模型。\n\n\n\n\n校准集准备#\n\n公版ResNet18模型的相关信息可参考 Pytorch文档内对ResNet18的说明，可以看到ResNet18模型的数据前处理流程为：\n\n 1. 图像短边放缩至256。\n 2. 以中心裁剪方式将图像尺寸调整至224x224。\n 3. 数据归一化处理，mean取值[0.485, 0.456, 0.406]，std取值[0.229, 0.224, 0.225]。\n\n数据前处理代码示例如下：\n\n\n\n为支持PTQ模型校准，我们需要从ImageNet数据集中取出一个小批量数据集，这里用前100张图像为例：\n\n\n\n则基于上文的数据前处理代码生成的校准集目录结构如下：\n\n\n\n\n生成板端模型 #\n\nPTQ转换链路支持命令行工具及PTQ API两种方式进行模型量化编译以生成板端模型，下方为您分别介绍两种方式的使用。\n\n\n命令行工具方式#\n\n命令行工具的方式只需要您安装horizon_tc_ui（Docker环境内已预装）并根据模型信息配置创建对应的yaml文件即可，此处我们以ResNet18模型对\n应的yaml文件（config.yaml）进行展示并说明。\n\n\n\n注解\n\n这里将 input_name 及 input_shape\n直接置空，是因为工具支持单输入且输入无动态shape的场景下自动补充这两个参数（即工具内部对ONNX模型进行解析，并获取输入的name及shape信息）。\n\n当yaml文件配置完成后，您只需要调用 hb_compile工具 执行命令即可，工具执行命令及关键log如下：\n\n\n\n命令执行完成后，在yaml文件working_dir参数配置的目录（model_output）下，将生成如下所示各阶段中间模型、最终的上板模型及模型信息文件，其\n中resnet18_224x224_rgb_removed.hbm即为板端可推理的模型文件：\n\n\n\n\nPTQ API方式#\n\n命令行工具在提供高易用性的同时也带来了一些灵活度的降低，因此，当您有灵活性需求时，可以使用PTQ\nAPI方式来完成模型的量化编译，下方为您介绍使用API的方式生成板端模型的具体流程。\n\n注意\n\n请注意，由于部分接口存在较多参数，下方示例展示中我们仅对必要参数进行了配置以便于您进行整体的实践验证，具体接口的全量参数请参考 HMCT API\nRefernence 和 HBDK Tool API Reference。\n\n模型优化校准#\n\n首先，对浮点模型进行图优化及校准量化，这个过程我们使用 HMCT 的API，具体示例如下：\n\n\n\n正确执行完build_model后，在working_dir目录下将生成各阶段ONNX模型，目录结构如下：\n\n\n\n这里的\n*ptq_model.onnx文件即经过图优化、校准及预编译过程的ONNX模型文件，中间阶段ONNX模型的具体说明请参考训练后量化(PTQ)-PTQ转换步骤-模\n型量化与编译-转换产出物解读 章节。\n\n模型转定点及编译#\n\n接下来需要完成PTQ模型转为定点模型及模型编译操作，这个过程我们需要通过编译器的API来完成，示例如下：\n\n\n\n编译完成后，working_dir目录下将保存中间阶段和最终可用于上板的模型文件，目录结构如下：\n\n\n\n\n构建板端示例 #\n\n 1. 准备板端示例所需依赖库\n\n如需尽快完成板端示例的构建，我们建议您直接使用OE包中 samples/ucp_tutorial/deps_aarch64。\n\n目录下内容作为依赖库，板端运行示例依赖的关键头文件及动态库路径如下：\n\n\n\n 2. 板端示例开发\n\n下方示例展示了基于二进制文件输入和板端模型，完成一次板端模型推理并获取分类结果TOP1的过程。\n\n\n\n 3. 交叉编译生成板端可执行程序\n\n进行交叉编译前，您需先准备好CMakeLists.txt和示例文件。CMakeLists.txt内容如下，因示例不包含数据前处理等操作，所以依赖较少，此处主要是\n对GCC的编译参数、依赖的头文件及动态库的配置。 其中dnn板端推理库，而hbucp用于对tensor做操作。\n\n\n\n编译的环境目录结构如下：\n\n\n\n当示例文件及CMakeLists.txt准备好之后即可执行编译。编译命令的示例如下：\n\n注意\n\n请注意，编译脚本中要将CC和CXX配置为交叉编译GCC和G++的实际路径。\n\n\n\n编译完成后，即可生成可上板运行的 run_sample 二进制程序。至此，板端示例构建流程已全部完成。\n\n\n板端运行准备#\n\n当可执行程序编译完成后，需要对模型的输入进行准备。由于此篇实践教程的场景校准集可作为模型输入，所以这里我们直接用校准集数据作为模型输入即可。当然，您也可以根据校\n准集的前处理逻辑对板端程序进行修改并给到模型输入（此过程需注意，修改后的程序中需要保证对原始图片做了与校准时相同的前处理）。\n\n这里我们只需要简单将npy格式的校准集转为binary文件即可，示例如下：\n\n\n\n除了输入数据的准备，在板端运行前，还需要您确保现在已准备好如下内容：\n\n * 地平线J6开发板，用于实际执行板端程序运行。\n * 一个可用于板端推理的模型，即 生成板端模型 的产出物。\n * 板端程序，即 构建板端示例 的产出物。\n * 板端程序依赖库，为了降低部署成本，您可以直接使用OE包samples/ucp_tutorial/deps_aarch64/ucp/lib文件夹中的内容。\n * binary格式的输入文件，用于板端示例执行推理时的模型输入。\n\n\n板端运行#\n\n以上步骤完成后，我们切换到板端环境，在板端环境中将程序、模型文件、输入数据及依赖库整合到一起，参考目录结构如下：\n\n\n\n最后，对LD_LIBRARY_PATH就能行配置并运行程序即可，如下所示：\n\n\n\n可以看到，log中打印的 label: 65 正是对应ImageNet数据集中ILSVRC2012_val_00000001图片的label，即分类结果正确。\n\n至此，以RGB输入的ResNet18模型移除Dequantize节点全流程部署实践就结束了。","routePath":"/guide/model_deployment_guidance/rgb_remove_resnet18_deployment_guidance","lang":"zh","toc":[{"text":"准备浮点模型","id":"准备浮点模型","depth":2,"charIndex":133},{"text":"校准集准备","id":"校准集准备","depth":2,"charIndex":189},{"text":"生成板端模型","id":"生成板端模型","depth":2,"charIndex":-1},{"text":"命令行工具方式","id":"命令行工具方式","depth":3,"charIndex":554},{"text":"PTQ API方式","id":"ptq-api方式","depth":3,"charIndex":981},{"text":"模型优化校准","id":"模型优化校准","depth":4,"charIndex":1202},{"text":"模型转定点及编译","id":"模型转定点及编译","depth":4,"charIndex":1422},{"text":"构建板端示例","id":"构建板端示例","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":2052},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":2480}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":392,"title":"RGB输入的ResNet18模型部署实践指导","content":"#\n\n地平线OpenExplorer工具链的PTQ链路整体使用流程包括模型优化、模型校准、模型转换为定点模型、模型编译及上板等多个阶段。\n本章节以基于公版ResNet18的RGB输入分类模型为例，分步骤进行部署实践的使用演示供您参考。\n\n\n准备浮点模型#\n\n准备ResNet18浮点模型，这里我们使用torchvision导出所需的浮点模型。\n\n\n\n\n校准集准备#\n\n公版ResNet18模型的相关信息可参考 Pytorch文档内对ResNet18的说明，可以看到ResNet18模型的数据前处理流程为：\n\n 1. 图像短边放缩至256。\n 2. 以中心裁剪方式将图像尺寸调整至224x224。\n 3. 数据归一化处理，mean取值[0.485, 0.456, 0.406]，std取值[0.229, 0.224, 0.225]。\n\n数据前处理代码示例如下：\n\n\n\n为支持PTQ模型校准，我们需要从ImageNet数据集中取出一个小批量数据集，这里用前100张图像为例：\n\n\n\n则基于上文的数据前处理代码生成的校准集目录结构如下：\n\n\n\n\n生成板端模型 #\n\nPTQ转换链路支持命令行工具及PTQ API两种方式进行模型量化编译以生成板端模型，下方为您分别介绍两种方式的使用。\n\n\n命令行工具方式#\n\n命令行工具的方式只需要您安装horizon_tc_ui（Docker环境内已预装）并根据模型信息配置创建对应的yaml文件即可，此处我们以ResNet18模型对\n应的yaml文件（config.yaml）进行展示并说明。\n\n\n\n注解\n\n这里将 input_name 及 input_shape\n直接置空，是因为工具支持单输入且输入无动态shape的场景下自动补充这两个参数（即工具内部对ONNX模型进行解析，并获取输入的name及shape信息）。\n\n当yaml文件配置完成后，您只需要调用 hb_compile工具 执行命令即可，工具执行命令及关键log如下：\n\n\n\n命令执行完成后，在yaml文件working_dir参数配置的目录（model_output）下，将生成如下所示各阶段中间模型、最终的上板模型及模型信息文件，其\n中resnet18_224x224_rgb.hbm即为板端可推理的模型文件：\n\n\n\n\nPTQ API方式#\n\n命令行工具在提供高易用性的同时也带来了一些灵活度的降低，因此，当您有灵活性需求时，可以使用PTQ\nAPI方式来完成模型的量化编译，下方为您介绍使用API的方式生成板端模型的具体流程。\n\n注意\n\n请注意，由于部分接口存在较多参数，下方示例展示中我们仅对必要参数进行了配置以便于您进行整体的实践验证，具体接口的全量参数请参考 HMCT API\nRefernence 和 HBDK Tool API Reference。\n\n模型优化校准#\n\n首先，对浮点模型进行图优化及校准量化，这个过程我们使用 HMCT 的API，具体示例如下：\n\n\n\n正确执行完build_model后，在working_dir目录下将生成各阶段ONNX模型，目录结构如下：\n\n\n\n这里的\n*ptq_model.onnx文件即经过图优化、校准及预编译过程的ONNX模型文件，中间阶段ONNX模型的具体说明请参考训练后量化(PTQ)-PTQ转换步骤-模\n型量化与编译-转换产出物解读 章节。\n\n模型转定点及编译#\n\n接下来需要完成PTQ模型转为定点模型及模型编译操作，这个过程我们需要通过编译器的API来完成，示例如下：\n\n\n\n编译完成后，working_dir目录下将保存中间阶段和最终可用于上板的模型文件，目录结构如下：\n\n\n\n\n构建板端示例 #\n\n 1. 准备板端示例所需依赖库\n\n如需尽快完成板端示例的构建，我们建议您直接使用OE包中 samples/ucp_tutorial/deps_aarch64。\n\n目录下内容作为依赖库，板端运行示例依赖的关键头文件及动态库路径如下：\n\n\n\n 2. 板端示例开发\n\n下方示例展示了基于二进制文件输入和板端模型，完成一次板端模型推理并获取分类结果TOP1的过程。\n\n\n\n 3. 交叉编译生成板端可执行程序\n\n进行交叉编译前，您需先准备好CMakeLists.txt和示例文件。CMakeLists.txt内容如下，因示例不包含数据前处理等操作，所以依赖较少，此处主要是\n对GCC的编译参数、依赖的头文件及动态库的配置。 其中dnn板端推理库，而hbucp用于对tensor做操作。\n\n\n\n编译的环境目录结构如下：\n\n\n\n当示例文件及CMakeLists.txt准备好之后即可执行编译。编译命令的示例如下：\n\n注意\n\n请注意，编译脚本中要将CC和CXX配置为交叉编译GCC和G++的实际路径。\n\n\n\n编译完成后，即可生成可上板运行的 run_sample 二进制程序。至此，板端示例构建流程已全部完成。\n\n\n板端运行准备#\n\n当可执行程序编译完成后，需要对模型的输入进行准备。由于此篇实践教程的场景校准集可作为模型输入，所以这里我们直接用校准集数据作为模型输入即可。当然，您也可以根据校\n准集的前处理逻辑对板端程序进行修改并给到模型输入（此过程需注意，修改后的程序中需要保证对原始图片做了与校准时相同的前处理）。\n\n这里我们只需要简单将npy格式的校准集转为binary文件即可，示例如下：\n\n\n\n除了输入数据的准备，在板端运行前，还需要您确保现在已准备好如下内容：\n\n * 地平线J6开发板，用于实际执行板端程序运行。\n * 一个可用于板端推理的模型，即 生成板端模型 的产出物。\n * 板端程序，即 构建板端示例 的产出物。\n * 板端程序依赖库，为了降低部署成本，您可以直接使用OE包samples/ucp_tutorial/deps_aarch64/ucp/lib文件夹中的内容。\n * binary格式的输入文件，用于板端示例执行推理时的模型输入。\n\n\n板端运行#\n\n以上步骤完成后，我们切换到板端环境，在板端环境中将程序、模型文件、输入数据及依赖库整合到一起，参考目录结构如下：\n\n\n\n最后，对LD_LIBRARY_PATH就能行配置并运行程序即可，如下所示：\n\n\n\n可以看到，log中打印的 label: 65 正是对应ImageNet数据集中ILSVRC2012_val_00000001图片的label，即分类结果正确。\n\n至此，以RGB输入ResNet18模型的全流程部署实践就结束了。","routePath":"/guide/model_deployment_guidance/rgb_resnet18_deployment_guidance","lang":"zh","toc":[{"text":"准备浮点模型","id":"准备浮点模型","depth":2,"charIndex":119},{"text":"校准集准备","id":"校准集准备","depth":2,"charIndex":175},{"text":"生成板端模型","id":"生成板端模型","depth":2,"charIndex":-1},{"text":"命令行工具方式","id":"命令行工具方式","depth":3,"charIndex":540},{"text":"PTQ API方式","id":"ptq-api方式","depth":3,"charIndex":959},{"text":"模型优化校准","id":"模型优化校准","depth":4,"charIndex":1180},{"text":"模型转定点及编译","id":"模型转定点及编译","depth":4,"charIndex":1400},{"text":"构建板端示例","id":"构建板端示例","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":2030},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":2458}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":393,"title":"模型性能调优","content":"#\n\n基于前文的性能分析工作，您可能发现性能结果不及预期，本章节内容介绍了地平线对提升模型性能的建议与措施，包括：检查yaml配置参数、处理CPU算子等内容。\n\n注解\n\n检查yaml配置参数、处理CPU算子仅适用于使用 hb_compile 进行模型转换编译的使用场景。\n\n\n检查影响模型性能的yaml参数#\n\n在模型转换的yaml配置文件中，部分参数会实际影响模型的最终性能，可以先检查下是否已正确按照预期配置，各参数的具体含义和作用请参考 配置文件具体参数信息\n小节的内容。\n\n * debug_mode：该参数用于精度调试分析。如果配置了dump_all_layers_output，则会为每个卷积和矩阵乘算子增加一个反量化输出节点以输\n   出模型的中间结果，它会显著的降低模型上板后的性能。所以在性能评测时，务必要将dump_all_layers_output参数从debug_mode中移除。\n\n * compile_mode：该参数用于选择模型编译时的优化方向为带宽还是时延，关注性能时请配置为 latency。\n\n * optimize_level：该参数用于选择编译器的优化等级，O0不做任何优化,\n   编译速度最快，优化程度最低，O1-O2随着优化等级提高，预期编译后的模型的执行速度会更快，但是所需编译时间也会变长。\n\n * max_time_per_fc：该参数用于控制编译后的模型数据指令的function-call的执行时长，从而实现模型优先级抢占功能。设置此参数更改被抢占\n   模型的function-call执行时长会影响该模型的上板性能。\n\n\n处理CPU算子#\n\n如果根据hrt_model_exec perf的评估，确认了突出的性能瓶颈是由于当前算子在CPU上运行导致的。那么此种情况下，我们建议您先查看\n工具链算子支持约束列表-ONNX Operator Support List 章节，确认当前运行在CPU上的算子是否具备BPU支持的能力。\n\n如果所使用的算子参数超出了BPU支持的约束范围，建议先将原始浮点模型计算参数调整到BPU约束范围内。 为了方便您快速知晓超出约束的具体参数，建议您再使用\n验证模型 部分介绍的方法做一遍检查，工具将会直接给出超出BPU支持范围的参数提示。\n\n注释\n\n修改原始浮点模型参数对模型计算精度的影响需要您自己把控，例如Convolution的 input_channel 或 output_channel\n超出范围就是一种较典型的情况，减少channel快速使得该算子被BPU支持，单单只做这一处修改也预计会对模型精度产生影响。\n\n如果算子并不具备BPU支持能力，就需要您根据以下情况做出对应优化操作：\n\n * CPU算子处于模型中部\n   \n   对于CPU算子处于模型中部的情况，建议您优先尝试参数调整、算子替换或修改模型。\n\n * CPU算子处于模型首尾部\n   \n   对于CPU算子处于模型首尾部的情况，请参考以下示例，下面以量化/反量化节点为例：\n   \n   * 对于与模型输入输出相连的节点，可以在yaml文件model_parameters配置组（模型参数组）中增加 remove_node_type\n     参数，并重新编译模型。\n     \n     ","routePath":"/guide/performance_tune","lang":"zh","toc":[{"text":"检查影响模型性能的yaml参数","id":"检查影响模型性能的yaml参数","depth":2,"charIndex":137},{"text":"处理CPU算子","id":"处理cpu算子","depth":2,"charIndex":690}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":394,"title":"Eager 模式","content":"#\n\nhorizon_plugin_pytorch 目前支持采用 eager 模式进行量化，但是，我们已经不推荐再使用该模式。 Eager 模式的整体流程同样参考了\nPyTorch 官方的量化接口和思路，因此，建议您先阅读 PyTorch 官方文档中 Eager 模式相关部分。\n\n\n与 fx 模式的区别#\n\n在 horizon_plugin_pytorch 中使用 eager 模式，和 fx 模式的主要区别在于：\n\n * eager 模式仅支持 module 形式的算子。在执行量化流程前，您需要手动将浮点模型中的函数形式的算子替换为 PyTorch 中 Module\n   类型的算子或者是 horizon_plugin_pytorch 中定义的专有算子，包括但不限于：\n\n * 您必须手动定义需要融合的算子，并在执行量化流程前显式调用融合函数执行算子融合，调用时也需指定使用 horizon_plugin_pytorch 中提供的\n   fuser_func。如下所示：\n\n","routePath":"/guide/plugin/advanced_tutorial/eager","lang":"zh","toc":[{"text":"与 fx 模式的区别","id":"与-fx-模式的区别","depth":2,"charIndex":141}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":395,"title":"FX Quantization 原理介绍","content":"#\n\n阅读此文档前，建议先阅读 torch.fx — PyTorch documentation，以对 torch 的 FX 机制有初步的了解。\n\nFX 采用符号执行的方式，可以在 nn.Module 或 function 的层面对模型建图，从而实现自动化的 fuse 以及其他基于图的优化。\n\n\n量化流程#\n\n\nFuse（可选）#\n\nFX 可以感知计算图，所以可以实现自动化的算子融合，您不再需要手动指定需要融合的算子，直接调用接口即可。\n\n\n\n * 注意 fuse_fx 没有 inplace 参数，因为内部需要对模型做 symbolic trace 生成一个 GraphModule，所以无法做到\n   inplace 的修改。\n * fused_model 和 model 会共享几乎所有属性（包括子模块、算子等），因此在 fuse 之后请不要对 model 做任何修改，否则可能影响到\n   fused_model。\n * 不必显式调用 fuse_fx 接口，因为后续的 prepare 接口内部集成了 fuse 的过程。\n\n\nPrepare#\n\n在调用 prepare 接口之前必须根据目标硬件平台设置全局的 march。接口内部会先执行 fuse 过程（即使模型已经 fuse\n过了），再将模型中符合条件的算子替换为 horizon.nn.qat 中的实现。\n\n * 可以根据需要选择合适的 qconfig（Calibtaion 或 QAT，注意两种 qconfig 不能混用）。\n * 和 fuse_fx 类似，此接口不支持 inplace 参数，且在 prepare 之后请不要对输入的模型做任何修改。\n\n\n\n\nEager Mode 兼容性#\n\n大部分情况下，FX 量化的接口可以直接替换 eager mode 量化的接口（prepare_qat -> prepare），但是不能和 eager mode\n的接口混用。部分模型在以下情况下需要对代码结构做一定的修改。\n\n * FX 不支持的操作：torch 的 symbolic trace 支持的操作是有限的，例如不支持将非静态变量作为判断条件、默认不支持 torch 以外的\n   pkg（如 numpy）等，且未执行到的条件分支将被丢弃。\n * 不想被 FX 处理的操作：如果模型的前后处理中使用了 torch 的 op，FX 在 trace 时会将他们视为模型的一部分，产生不符合预期的行为（例如将\n   torch 的某些 function 调用替换为 FloatFunctional）。\n\n以上两种情况，都可以采用 wrap 的方法来避免，下面以 RetinaNet 为例进行说明。\n\n","routePath":"/guide/plugin/advanced_tutorial/fx_quantization_explain","lang":"zh","toc":[{"text":"量化流程","id":"量化流程","depth":2,"charIndex":148},{"text":"Fuse（可选）","id":"fuse可选","depth":3,"charIndex":156},{"text":"Prepare","id":"prepare","depth":3,"charIndex":468},{"text":"Eager Mode 兼容性","id":"eager-mode-兼容性","depth":3,"charIndex":714}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":396,"title":"算子融合","content":"#\n\n训练工具支持的算子融合可分为两大类：1. 吸收 BN；2. 融合 Add、ReLU(6)。\n\n\n吸收 BN#\n\n吸收 BN 的目的是为了减少模型的计算量。因为 BN 是线性变换过程，因此，当 BN 和 Conv 一起出现的时候，可以把 BN 的参数吸收到 Conv\n的参数中，从而在部署的模型中消除 BN 的计算。\n\n吸收的计算过程如下：\n\n通过吸收 BN ，可以把 Conv2d + BN2d 简化为 Conv2d：\n\n\n融合 Add、ReLU(6)#\n\n和 CUDA Kernel Fusion 中将 CUDA Kernel 融合以提高计算速度不同，训练工具支持的融合更加偏重量化层面。\n\nBPU 硬件针对常见的模型基本结构做了优化，在计算 Conv -> Add -> ReLU\n这种算子组合时，可使算子间的数据传递保留高精度的状态，提高模型整体的数值精度。因此在对模型进行量化时，我们可以将 Conv -> Add -> ReLU\n视为一个整体。\n\n由于训练工具对模型进行量化改造时以 torch.nn.Module 为单位，为了在量化时将 Conv -> Add -> ReLU\n视为一个整体，需要将它们合并为一个 Module。\n\n算子融合除了可以使中间结果保留高精度状态之外，也可以省去将中间结果转化为低精度表示的过程，因此执行速度和不融合相比也会更快。\n\n由于算子融合既可以提高模型精度，又可以提高模型速度，一般应该对所有可融合的部分进行融合。\n\n\n实现原理#\n\n得益于 FX 可以获取计算图的优势，训练工具可以自动化地对模型的计算图进行分析，根据预定义的 fusion pattern 对可融合部分进行匹配，并通过\nsubmodule 替换实现融合的操作。下面举例进行说明：\n\n吸收 BN 和融合 Add、ReLU(6) 可以通过相同的机制完成，因此在融合时不需要进行区分。\n\n\n\n可以看到，对模型执行算子融合操作后，BN 被吸收进 Conv 中，且 Conv、Add、ReLU 被融合进一个 Module\n中（_generated_add_0）。原本的 submodule 被替换为 Identity，且不在 forward 代码中调用。\n\nFX 自动地将模型中 x = x + y 的加号替换为了名为 _generated_add_0 的 Module 形式，以支持算子融合和量化的相关操作。\n\n\n可以融合的算子#\n\n目前支持的可融合的算子组合见以下函数定义：\n\n","routePath":"/guide/plugin/advanced_tutorial/op_fusion","lang":"zh","toc":[{"text":"吸收 BN","id":"吸收-bn","depth":2,"charIndex":50},{"text":"融合 Add、ReLU(6)","id":"融合-addrelu6","depth":2,"charIndex":214},{"text":"实现原理","id":"实现原理","depth":2,"charIndex":635},{"text":"可以融合的算子","id":"可以融合的算子","depth":2,"charIndex":1012}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":397,"title":"","content":"class horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int =\n512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int =\n-1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nKL observer.\n\nKL observer based on histogram. Histogram is calculated online and won’t be\nsaved.\n\n * Parameters:\n * bins (int) – Number of histograms bins.\n * update_interval (int) – Interval of computing KL entropy and update min/max.\n   KLObserver will constantly collect histograms of activations, but only\n   perform KL calculation when update_interval is satisfied. if it is set to 1,\n   KL entropy will be computed every forward step. Larger interval guarantees\n   less time and does no harm to calibration accuracy. Set it to the total\n   calibration steps can achieve best performance. update_interval must be no\n   greater than total calibration steps, otherwise no min/max will be computed.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int =\n1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype |\nQuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min:\nint | None = None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMSE observer.\n\nObserver module for computing the quantization parameters based on the Mean\nSquare Error (MSE) between the original tensor and the quantized one.\n\nThis observer linear searches the quantization scales that minimize MSE.\n\n * Parameters:\n * stride (int) – Searching stride. Larger value gives smaller search space,\n   which means less computing time but possibly poorer accuracy. Default is 1.\n   Suggests no greater than 20.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMin max observer.\n\nThis observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average minimum\nand maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MixObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMix observer.\n\nThis observer computes the quantization parameters based on multiple calibration\nmethods and selects the quantization parameters with the smallest quantization\nerror.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver\n(percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01,\nch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nPercentile observer.\n\nPercentile observer based on histogram. Histogram is calculated online and won’t\nbe saved. The minimum and maximum are moving averaged to compute the\nquantization parameters.\n\n * Parameters:\n * percentile (float) – Index percentile of histrogram\n * bins (int) – Number of histograms bins.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\n\n\nclass horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver\n(averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric,\nquant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)\n\nMovingAverageMinMax Observer.\n\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving averages\nof minimums and maximums of the incoming tensors. The module records the average\nminimum and maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, only support per_tensor_symmetric\n   scheme\n * reduce_range – Reduces the range of the quantized data type by 1 bit\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – Whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver\n(averaging_constant=0.01, ch_axis=0, dtype=torch.qint8,\nqscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None,\nis_sync_quantize=False, factory_kwargs=None)\n\nMovingAveragePerChannelMinMax Observer.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * ch_axis – Channel axis\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, Only support per_channel_symmetric\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.","routePath":"/guide/plugin/api_processed/calibration_api","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":398,"title":"环境依赖","content":"#\n\n依赖            GPU            CPU\nos            Ubuntu22.04    Ubuntu22.04\ncuda          11.8           N/A\npython        3.10           3.10\ntorch         2.3.0+cu118    2.3.0+cpu\ntorchvision   0.18.0+cu118   0.18.0+cpu","routePath":"/guide/plugin/installation","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":399,"title":"简介","content":"#\n\n量化是指以低于浮点精度的比特宽度执行计算和存储张量的技术。量化模型使用整数而不是浮点值对张量执行部分或全部操作。 与典型的 FP32\n模型相比，量化之后的模型可以节省计算资源，以 INT8 量化为例，模型大小减少 4 倍，内存带宽需求减少 4 倍。对 INT8 计算的硬件支持通常比 FP32\n计算快 2 到 4 倍。量化主要是一种加速推理的技术，量化运算只支持前向计算。\n\nhorizon_plugin_pytorch 提供了适配 BPU\n的量化操作，支持量化感知训练，该训练使用伪量化模块对前向计算和反向传播中的量化误差进行建模。请注意，量化感知训练的整个计算过程是使用浮点运算执行的。\n在量化感知训练结束时，horizon_plugin_pytorch 提供转换函数，将训练后的模型转换为定点模型，在 BPU\n上使用更紧凑的模型表示和高性能矢量化操作。\n\n本章内容为您详细介绍地平线基于 PyTorch 开发的 horizon_plugin_pytorch 的量化感知训练工具。\n\nhorizon_plugin_pytorch 基于 PyTorch 开发，为了降低学习成本，horizon_plugin_pytorch 参考了 PyTorch\n关于量化感知训练的设计。 本文档对于 PyTorch 文档已经包含的内容不再赘述，如果想了解工具细节推荐您阅读 官方代码 或者本工具的 Python 源码。\n为保证使用的流畅性，请您首先阅读 PyTorch 的相关文档，熟悉 PyTorch 提供的量化感知训练和部署工具的使用方法。\n\n为行文简洁，文档中代码默认进行了如下别名替换：\n\n","routePath":"/guide/plugin/introduce","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":400,"title":"quantization.hbdk4.export","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.export (model: Module, example_inputs:\nAny, *, name: str = 'forward', input_names: Any | None = None, output_names: Any\n| None = None, input_descs: Any | None = None, output_descs: Any | None = None)\n\nExport nn.Module to hbir model.\n\n * Parameters:\n * model (Module) – Input model.\n * example_inputs (Any) – Example input for tracing.\n * name (str) – The name of func in exported module. Users can get the func by\n   getattr(hbir_module, name).\n * input_names (Optional[Any]) – Set hbir inputs with given names, should have\n   the same structure with example_inputs.\n * output_names (Optional[Any]) – Set hbir outputs with given names, should have\n   the same structure with model output.\n * input_descs (Optional[Any]) – Set hbir inputs with given descriptions, should\n   have the same structure with example_inputs.\n * output_descs (Optional[Any]) – Set hbir outputs with given descriptions,\n   should have the same structure with model output.\n * Return type: Module\n * Returns: Hbir model wrapped with Module.","routePath":"/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_export","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":401,"title":"quantization.hbdk4.get_hbir_input_flattener","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.get_hbir_input_flattener (model:\nModule)\n\nGet an callable func to flatten model input into a flat tuple of Tensor.\n\n * Parameters: model (Module) – Hbir model.\n * Return type: callable","routePath":"/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_get_hbir_input_flattener","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":402,"title":"quantization.hbdk4.get_hbir_output_unflattener","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.get_hbir_output_unflattener (model:\nModule)\n\nGet an callable func to unflatten model output into origin format.\n\n * Parameters: model (Module) – Hbir model.\n * Return type: callable","routePath":"/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_get_hbir_output_unflattener","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":403,"title":"伪量化算子","content":"#\n\n\n\nclass horizon_plugin_pytorch.quantization.FakeQuantize (observer: type = ,\nsaturate: bool | None = None, in_place: bool = False, compat_mask: bool = True,\nchannel_len: int = 1, fast_training=True, **observer_kwargs)\n\nSimulate the quantize and dequantize operations in training time.\n\nThe output of this module is given by\n\nfake_quant_x = clamp(floor(x / scale + 0.5), quant_min, quant_max) * scale\n\n * scale defines the scale factor used for quantization.\n\n * zero_point specifies the quantized value to which 0 in floating point maps to\n\n * quant_min specifies the minimum allowable quantized value.\n\n * quant_max specifies the maximum allowable quantized value.\n\n * fake_quant_enabled controls the application of fake quantization on tensors,\n   note that statistics can still be updated.\n\n * observer_enabled controls statistics collection on tensors\n\n * dtype specifies the quantized dtype that is being emulated with\n   fake-quantization, the allowable values is qint8 and qint16. The values of\n   quant_min and quant_max should be chosen to be consistent with the dtype\n\n * Parameters:\n\n * observer (type) – Module for observing statistics on input tensors and\n   calculating scale and zero-point.\n\n * saturate (Optional[bool]) – Whether zero out the grad for value out of quanti\n   range.\n\n * in_place (bool) – Whether use in place fake quantize.\n\n * compat_mask (bool) – Whether pack the bool mask into bitfield when saturate =\n   True.\n\n * channel_len (int) – Size of data at channel dim.\n\n * fast_training – Whether use fast training mode. If True, computing scale and\n   fake quantization will be done in one step.\n\n * observer_kwargs – Arguments for the observer module\n\nobserver\n\nUser provided module that collects statistics on the input tensor and provides a\nmethod to calculate scale and zero-point.\n\nextra_repr()\n\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\nforward (x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nset_qparams (scale: Tensor | Sequence | float, zero_point: Tensor | Sequence |\nint | None = None)\n\nSet qparams, default symmetric.\n\n * Parameters:\n * scale ( Tensor | Sequence | float)\n * zero_point ( Tensor | Sequence | int | None)\n\nclassmethod with_args (**kwargs)\n\nWrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample:\n\n","routePath":"/guide/plugin/plugin_api_reference/fake_quantize","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":404,"title":"bgr2centered_gray","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2centered_gray (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to centered gray\n\n * Parameters: input (Tensor) – input image in BGR format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: centered gray image of shape [N, 1, H, W], ranging -128 to 127\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2centered_gray","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":405,"title":"bgr2centered_yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2centered_yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from BGR format to centered YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in BGR format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: -112 to 107, U, V: -112 to\n   112). “full” for YUV full swing (Y, U, V: -128 to 127). default is “studio”\n * Returns: centered YUV image\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2centered_yuv","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":406,"title":"bgr2gray","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2gray (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to gray\n\n * Parameters: input (Tensor) – input image in BGR format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: gray image of shape [N, 1, H, W], ranging 0 to 255\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2gray","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":407,"title":"bgr2rgb","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2rgb (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to RGB\n\n * Parameters: input (Tensor) – image in BGR format with shape [N, 3, H, W]\n * Returns: image in RGB format with shape [N, 3, H, W]\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2rgb","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":408,"title":"bgr2yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from BGR format to YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in BGR format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: 16 to 235, U, V: 16 to 240).\n   “full” for YUV full swing (Y, U, V: 0 to 255). default is “studio”\n * Returns: YUV image\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2yuv","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":409,"title":"nn.BgrToYuv444","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.BgrToYuv444 (channel_reversal: bool = False)\n\nConvert image color format from bgr to yuv444.\n\n * Parameters: channel_reversal (bool) – Color channel order, set to True when\n   used on RGB input. Defaults to False.\n\nforward (input: Tensor)\n\nForward pass of BgrToYuv444.\n\n * Parameters: input ( Tensor)","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_BgrToYuv444","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":410,"title":"nn.Correlation","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.Correlation (kernel_size: int = 1,\nmax_displacement: int = 1, stride1: int = 1, stride2: int = 1, pad_size: int =\n0, is_multiply: bool = True)\n\nPerform multiplicative patch comparisons between two feature maps.\n\nCorrelation performs multiplicative patch comparisons between two feature maps.\nGiven two multi-channel feature maps $f_{1}, f_{2}$, with $w$, $h$, and $c$\nbeing their width, height, and number of channels, the correlation layer lets\nthe network compare each patch from $f_{1}$ with each patch from $f_{2}$.\n\nFor now we consider only a single comparison of two patches. The ‘correlation’\nof two patches centered at $x_{1}$ in the first map and $x_{2}$ in the second\nmap is then defined as:\n\n$c(x_{1}, x_{2}) = \\displaystyle\\sum_{o \\in [-k,k] \\times [-k,k]} $\n\nfor a square patch of size $K:=2k+1$.\n\nNote that the equation above is identical to one step of a convolution in neural\nnetworks, but instead of convolving data with a filter, it convolves data with\nother data. For this reason, it has no training weights.\n\nComputing $c(x_{1}, x_{2})$ involves $c * K^{2}$ multiplications. Comparing all\npatch combinations involves $w^{2}*h^{2}$ such computations.\n\nGiven a maximum displacement $d$, for each location $x_{1}$ it computes\ncorrelations $c(x_{1}, x_{2})$ only in a neighborhood of size $D:=2d+1$, by\nlimiting the range of $x_{2}$. We use strides $s_{1}, s_{2}$, to quantize\n$x_{1}$ globally and to quantize $x_{2}$ within the neighborhood centered around\n$x_{1}$.\n\nThe final output is defined by the following expression:\n\n$out[n, q, i, j] = c(x_{i, j}, x_{q})$\n\nwhere $i$ and $j$ enumerate spatial locations in $f_{1}$, and $q$ denotes the\n$q^{th}$ neighborhood of $x_{i,j}$.\n\n * Parameters:\n * kernel_size (int) – kernel size for Correlation must be an odd number\n * max_displacement (int) – Max displacement of Correlation\n * stride1 (int) – stride1 quantize data1 globally\n * stride2 (int) – stride2 quantize data2 within neighborhood centered around\n   data1\n * pad_size (int) – pad for Correlation\n * is_multiply (bool) – operation type is either multiplication or subduction,\n   only support True now\n\nforward (data1: Tensor | QTensor, data2: Tensor | QTensor)\n\nForward for Horizon Correlation.\n\n * Parameters:\n * data1 (Union[Tensor, QTensor]) – shape of [N,C,H,W]\n * data2 (Union[Tensor, QTensor]) – shape of [N,C,H,W]\n * Returns: output\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_Correlation","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":411,"title":"nn.MultiScaleDeformableAttention","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.MultiScaleDeformableAttention (embed_dims: int =\n256, num_heads: int = 8, num_levels: int = 4, num_points: int = 4, im2col_step:\nint = 64, dropout: float = 0.1, batch_first: bool = False, value_proj_ratio:\nfloat = 1.0, split_weight_mul: bool = False, split_batch: bool = False)\n\nAn attention module used in Deformable-Detr.\n\nDeformable DETR: Deformable Transformers for End-to-End Object Detection..\n\n * Parameters:\n * embed_dims (int) – The embedding dimension of Attention. Default: 256.\n * num_heads (int) – Parallel attention heads. Default: 8.\n * num_levels (int) – The number of feature map used in Attention. Default: 4.\n * num_points (int) – The number of sampling points for each query in each head.\n   Default: 4.\n * im2col_step (int) – The step used in image_to_column. Default: 64.\n * dropout (float) – A Dropout layer on inp_identity. Default: 0.1.\n * batch_first (bool) – Key, Query and Value are shape of (batch, n, embed_dim)\n   or (n, batch, embed_dim). Default to False.\n * value_proj_ratio (float) – The expansion ratio of value_proj. Default: 1.0.\n * split_weight_mul (bool) – Whether split attention weight mul onto each level\n   outputs. Enable this can reduce memory usage in qat training.\n * split_batch (bool) – Whether Compute each batch at a time. Enable this can\n   reduce memory usage in qat training.\n\nforward (query: Tensor | QTensor, key: Tensor | QTensor | None = None, value:\nTensor | QTensor | None = None, identity: Tensor | QTensor | None = None,\nquery_pos: Tensor | QTensor | None = None, key_padding_mask: Tensor | None =\nNone, reference_points: Tensor | QTensor | None = None, spatial_shapes: Tensor |\nNone = None)\n\nForward Function of MultiScaleDeformAttention.\n\n * Parameters:\n * query (Union[Tensor, QTensor]) – Query of Transformer with shape (num_query,\n   bs, embed_dims).\n * key (Union[Tensor, QTensor, None]) – The key tensor with shape (num_key, bs,\n   embed_dims).\n * value (Union[Tensor, QTensor, None]) – The value tensor with shape (num_key,\n   bs, embed_dims).\n * identity (Union[Tensor, QTensor, None]) – The tensor used for addition, with\n   the same shape as query. Default None. If None, query will be used.\n * query_pos (Union[Tensor, QTensor, None]) – The positional encoding for query.\n   Default: None.\n * key_padding_mask (Optional[Tensor]) – ByteTensor for query, with shape [bs,\n   num_key].\n * reference_points (Union[Tensor, QTensor, None]) – The normalized reference\n   points with shape (bs, num_query, num_levels, 2), all elements is range in\n   [0, 1], top-left (0,0), bottom-right (1, 1), including padding area. or (bs,\n   num_query, num_levels, 4), add additional two dimensions is (w, h) to form\n   reference boxes.\n * spatial_shapes (Optional[Tensor]) – Spatial shape of features in different\n   levels. int tensor with shape (num_levels, 2), last dimension represents (h,\n   w).\n * Returns: the same shape with query.\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_MultiScaleDeformableAttention","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":412,"title":"nn.PointPillarsScatter","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.PointPillarsScatter (output_shape=None)\n\nforward (voxel_features: Tensor, coords: Tensor, output_shape: Tensor | list |\ntuple | None = None)\n\nForward of Horizon PointPillarsScatter.\n\n * Parameters:\n * voxel_features (Tensor) – [M, …], dimention after M will be flattened.\n * coords (Tensor) – [M, (n, …, y, x)], only indices on N, H and W are used.\n * output_shape (Union[Tensor, list, tuple, None]) – Expected output shape.\n   Defaults to None.\n * Returns: The NCHW pseudo image.\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_PointPillarsScatter","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":413,"title":"nn.functional.filter","content":"#\n\n\n\nhorizon_plugin_pytorch.nn.functional.filter (*inputs: Tuple[Tensor] |\nTuple[QTensor], threshold: float, idx_range: Tuple[int, int] | None = None)\n\nFilter.\n\nThe output order is different with bpu, because that the compiler do some\noptimization and slice input following complex rules, which is hard to be done\nby plugin.\n\nAll inputs are filtered along HW by the max value within a range in channel dim\nof the first input. Each NCHW input is splited, transposed and flattened to\nList[Tensor[H * W, C]] first. If input is QTensor, the output will be\ndequantized.\n\n * Parameters:\n * inputs (Union[Tuple[Tensor], Tuple[QTensor]]) – Data in NCHW format. Each\n   input shold have the same size in N, H, W. The output will be selected\n   according to the first input.\n * threshold (float) – Threshold, the lower bound of output.\n * idx_range (Optional[Tuple[int, int]]) – The index range of values counted in\n   compare of the first input. Defaults to None which means use all the values.\n * Returns: A list with same length of batch size, and each element contains:\n\n * max_value: Flattened max value within idx_range in channel dim.\n * max_idx: Flattened max value index in channel dim.\n * coord: The original coordinates of the output data in the input data in the\n   shape of [M, (h, w)].\n * (multi) data: Filtered data in the shape of [M, C].\n\n * Return type: Union[List[List[Tensor]], List[List[QTensor]]]","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_functional_filter","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":414,"title":"nn.functional.point_pillars_preprocess","content":"#\n\n\n\nhorizon_plugin_pytorch.nn.functional.point_pillars_preprocess (points_list:\nList[Tensor], pc_range: Tensor, voxel_size: Tensor, max_voxels: int,\nmax_points_per_voxel: int, use_max: bool, norm_range: Tensor, norm_dims: Tensor)\n\nPreprocess PointPillars.\n\n * Parameters:\n * points_list (List[Tensor]) – [(M1, ndim), (M2, ndim),…], List of PointCloud\n   data.\n * pc_range (Tensor) – (6,), indicate voxel range, format: [x_min, y_min, z_min,\n   x_max, y_max, z_max]\n * voxel_size (Tensor) – (3,), xyz, indicate voxel size.\n * max_voxels (int) – Indicate maximum voxels.\n * max_points_per_voxel (int) – Indicate maximum points contained in a voxel.\n * use_max (bool) – Whether to use max_voxels, for deploy should be True.\n * norm_range (Tensor) – Feature range, like [x_min, y_min, z_min, …, x_max,\n   y_max, z_max, …].\n * norm_dims (Tensor) – Dims to do normalize.\n * Returns: (features, coords), encoded feature and coordinates in (idx, z, y,\n   x) format.\n * Return type: (Tensor, Tensor)","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_functional_point_pillars_preprocess","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":415,"title":"rgb2bgr","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2bgr (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to BGR\n\n * Parameters: input (Tensor) – image in RGB format with shape [N, 3, H, W]\n * Returns: image in BGR format with shape [N, 3, H, W]\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2bgr","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":416,"title":"rgb2centered_gray","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2centered_gray (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to centered gray\n\n * Parameters: input (Tensor) – input image in RGB format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: centered gray image of shape [N, 1, H, W], ranging -128 to 127\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2centered_gray","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":417,"title":"rgb2centered_yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2centered_yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from RGB format to centered YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in RGB format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: -112 to 107, U, V: -112 to\n   112). “full” for YUV full swing (Y, U, V: -128 to 127). default is “studio”\n * Returns: centered YUV image\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2centered_yuv","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":418,"title":"rgb2gray","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2gray (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to gray\n\n * Parameters: input (Tensor) – input image in RGB format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: gray image of shape [N, 1, H, W], ranging 0 to 255\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2gray","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":419,"title":"rgb2yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from RGB format to YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in RGB format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: 16 to 235, U, V: 16 to 240).\n   “full” for YUV full swing (Y, U, V: 0 to 255). default is “studio”\n * Returns: YUV image\n * Return type: Tensor","routePath":"/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2yuv","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":420,"title":"March","content":"#\n\n\n\nclass horizon_plugin_pytorch.march.March\n\nBPU platform.","routePath":"/guide/plugin/plugin_api_reference/march","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":421,"title":"ONNX","content":"#\n\n\n\nhorizon_plugin_pytorch.utils.onnx_helper.export_to_onnx (model, args, f,\nexport_params=True, verbose=False, training=, input_names=None,\noutput_names=None, operator_export_type=, opset_version=11,\ndo_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None,\ncustom_opsets=None)\n\nExport a (float or qat)model into ONNX format.\n\n * Parameters:\n * model ( torch.nn.Module/torch.jit.ScriptModule/ScriptFunction) – the model to\n   be exported.\n * args ( tuple or torch.Tensor) –\n\n\n\n\n\n * f – a file-like object or a string containing a file name. A binary protocol\n   buffer will be written to this file.\n * export_params ( bool , default True) – if True, all parameters will be\n   exported.\n * verbose ( bool , default False) – if True, prints a description of the model\n   being exported to stdout, doc_string will be added to graph. doc_string may\n   contaion mapping of module scope to node name in future torch onnx.\n * training ( enum , default TrainingMode.EVAL) –\n\n\n\n\n\n * input_names ( list of str , default empty list) – names to assign to the\n   input nodes of the graph, in order.\n * output_names ( list of str , default empty list) – names to assign to the\n   output nodes of the graph, in order.\n * operator_export_type ( enum , default ONNX_FALLTHROUGH) – *\n   OperatorExportTypes.ONNX: Export all ops as regular ONNX ops (in the default\n   opset domain). * OperatorExportTypes.ONNX_FALLTHROUGH: Try to convert all ops\n   to standard ONNX ops in the default opset domain. *\n   OperatorExportTypes.ONNX_ATEN: All ATen ops (in the TorchScript namespace\n   “aten”) are exported as ATen ops. * OperatorExportTypes.ONNX_ATEN_FALLBACK:\n   Try to export each ATen op (in the TorchScript namespace “aten”) as a regular\n   ONNX op. If we are unable to do so,fall back to exporting an ATen op.\n * opset_version ( int , default 11) – by default we export the model to the\n   opset version of the onnx submodule.\n * do_constant_folding ( bool , default False) – Apply the constant-folding\n   optimization. Constant-folding will replace some of the ops that have all\n   constant inputs with pre-computed constant nodes.\n * dynamic_axes ( dict , list ( int ) /dict , str>> , default empty dict) –\n\n\n\n\n\n * keep_initializers_as_inputs ( bool , default None) – If True, all the\n   initializers (typically corresponding to parameters) in the exported graph\n   will also be added as inputs to the graph. If False, then initializers are\n   not added as inputs to the graph, and only the non-parameter inputs are added\n   as inputs. This may allow for better optimizations (e.g. constant folding) by\n   backends/runtimes.\n * custom_opsets ( dict , int> , default empty dict) –\n\n\n\n","routePath":"/guide/plugin/plugin_api_reference/onnx","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":422,"title":"fx.fx_helper.wrap","content":"#\n\n\n\nExtended tracer and wrap of torch.fx.\n\nThis file defines a inherit tracer of torch.fx.Tracer and a extended wrap to\nallow wrapping of user-defined Module or method, which help users do some\noptimization of their own module by torch.fx\n\nhorizon_plugin_pytorch.fx.fx_helper.wrap (skip_compile: bool = False)\n\nExtend torch.fx.wrap.\n\nThis function can be: : 1) called or used as a decorator on a string to register\na builtin function as a “leaf function”\n\n 2. called or used as a decorator on a function to register this function as a\n    “leaf function”\n\n 3. called or used as a decorator on subclass of torch.nn.Module to register\n    this module as a “leaf module”, and register all user defined method in this\n    class as “leaf method”\n\n 4. called or used as a decorator on a class method to register it as “leaf\n    method”\n\n * Parameters: skip_compile ( bool , optional) – Whether wrapped obj is skipped\n   in compile, used by\n   horizon_plugin_pytorch.quantization.fx.split_compilable_model\n   .split_compilable_model. Defaults to False.\n * Returns: The actural decorator.\n * Return type: FxWrapManager.wrap","routePath":"/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_fx_fx_helper_wrap","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":423,"title":"quantization.fuse_known_modules","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.fuse_known_modules (mod_list, is_qat=False,\nadditional_fuser_method_mapping=None)\n\nFuse modules.\n\nReturn a list of modules that fuses the operations specified in the input module\nlist.\n\nFuses only the following sequence of modules: conv, bn; conv, bn, relu; conv,\nrelu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear,\nbn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu;\nlinear, add; linear, add, relu. For these sequences, the first element in the\noutput module list performs the fused operation. The rest of the elements are\nset to nn.Identity()","routePath":"/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_fuse_known_modules","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":424,"title":"quantization.fuse_modules","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.fuse_modules (model, modules_to_fuse,\ninplace=False, fuser_func=, fuse_custom_config_dict=None)\n\nFuses a list of modules into a single module.\n\nFuses only the following sequence of modules: conv, bn; conv, bn, relu; conv,\nrelu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear,\nbn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu;\nlinear, add; linear, add, relu. For these sequences, the first element in the\noutput module list performs the fused operation. The rest of the elements are\nset to nn.Identity()\n\n * Parameters:\n * model – Model containing the modules to be fused\n * modules_to_fuse – list of list of module names to fuse. Can also be a list of\n   strings if there is only a single list of modules to fuse.\n * inplace – bool specifying if fusion happens in place on the model, by default\n   a new model is returned\n * fuser_func – Function that takes in a list of modules and outputs a list of\n   fused modules of the same length. For example, fuser_func([convModule,\n   BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to\n   torch.ao.quantization.fuse_known_modules\n * fuse_custom_config_dict – custom configuration for fusion\n\n\n\n * Returns: model with fused modules. A new copy is created if inplace=True.\n\nExamples:\n\n","routePath":"/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_fuse_modules","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":425,"title":"quantization.prepare","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.prepare (model: Module, example_inputs: Any\n| None = None, qconfig_setter: Tuple[QconfigSetterBase, ...] | QconfigSetterBase\n| None = None, method: PrepareMethod = PrepareMethod.JIT_STRIP)\n\nPrepare model.\n\nPrepare and check a copy of the model for QAT.\n\n * Parameters:\n * model (Module) – Model to be prepared.\n * example_inputs (Optional[Any]) – Model inputs. Used to trace and check model.\n * qconfig_setter (Union[Tuple[QconfigSetterBase, ...], QconfigSetterBase,\n   None]) – Qconfig setter. Used to set qconfig.\n * method (PrepareMethod) – Method used to trace model, availiable options are:\n   ‘eager’: Don’t trace. ‘symbolic’: Use symbolic trace. ‘jit’: Use jit trace.\n   ‘jit-strip’: Use jit trace and strip the graph outside QuantStub and\n   Dequantstub.\n * Return type: Module","routePath":"/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_prepare","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":426,"title":"qconfig","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.get_qconfig (observer:\n~typing.Type[~horizon_plugin_pytorch.quantization.observer_v2.ObserverBase] = ,\nin_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType | None = None,\nweight_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType | None =\n'qint8', out_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType |\nNone = 'qint8', fix_scale: bool = False)\n\nGet qconfig.\n\n * Parameters:\n * observer (Type[ObserverBase]) – observer type for input and output. Support\n   MinMaxObserver and MSEObserver\n * in_dtype (Union[dtype, QuantDType, None]) – input dtype.\n * weight_dtype (Union[dtype, QuantDType, None]) – weight dtype.\n * out_dtype (Union[dtype, QuantDType, None]) – output dtype.\n * fix_scale (bool) – Whether fix input/output scale.","routePath":"/guide/plugin/plugin_api_reference/qconfig","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":427,"title":"常见故障","content":"#\n\n\nimport 出错#\n\n错误一：Cannot find the extension library(_C.so)\n\n解决方法：\n\n * 确定 horizon_plugin_pytorch 版本和 cuda 版本是对应的。\n * 在 python3 中，找到 horizon_plugin_pytorch 的执行路径，检测该目录下是否有 .so 文件。可能同时存在多个\n   horizon_plugin_pytorch 的版本，需要卸载只保留一个需要的版本。\n\n--------------------------------------------------------------------------------\n\n错误二：RuntimeError: Cannot load custom ops. Please rebuild the\nhorizon_plugin_pytorch\n\n解决方法：确认本地 CUDA 环境是否正常，如路径、版本等。\n\n\n无法正常 prepare_calibration/qat#\n\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support\nthe deepcopy protocol at the moment\n\n解决方法：一般是模型中包含 non-leaf tensor 才会出现这样的错误，尝试以下方法：\n\n * 将 prepare_calibration/qat 的 inplace 设为 True。\n * 正常 horizon_plugin_pytorch 定义的算子不会出现这种错误，检查模型中自定义的算子是否有 non-leaf tensor 的定义。\n\n\nprepare_qat 后 forward 报错#\n\nTypeError: when calling function\n\n解决方法：自定义算子继承了某个 torch 的 Module 算子，导致 prepare_qat 没有被成功转成 qat module。建议使用\nsubmodule 的方式调用 conv2d。\n\n\n量化精度异常#\n\nQAT/Quantized 精度不符合预期、出现 NAN 或 QAT 初始 loss 相对 float 明显异常。 解决方法：请参考 精度调优工具使用指南。\n\n\nCannot find the extension library(_C.so)#\n\n解决方法：主要发生在 horizon_plugin_pytorch 安装成功但 import 失败，解决方案如下：\n\n 1. 确定 horizon_plugin_pytorch 版本和 cuda 版本是对应的；\n 2. 在 python3 中，找到 horizon-plugin-pytorch 的执行路径，检测该目录下是否有 .so 文件。可能同时存在多个\n    horizon-plugin-pytorch 的版本，需要卸载只保留一个需要的版本。\n\n\nRuntimeError: Cannot load custom ops. Please rebuild the\nhorizon_plugin_pytorch.#\n\n解决方法：请确认本地 CUDA 环境是否正常，如路径、版本是否符合预期。\n\n\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support\nthe deepcopy protocol at the moment#\n\n解决方法：主要发生在无法正常 prepare 阶段，一般是由于模型中包含 non-leaf tensor 导致的，请将\nprepare_calibration/qat 的 inplace 配置为 True。\n\n\ntorch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with\nsignal SIGKILL#\n\n解决方法：可能是多线程，python 程序没有完全杀干净导致的。\n\n\nAttributeError: ‘NoneType’ object has no attribute ‘numel’#\n\n解决方法：该报错主要发生在插入伪量化节点阶段，是算子的输入 scale 为 None 导致。造成原因可能是输出层 conv 插入 Dequant 后又接了某个\nop，存在类似于 conv+dequant+conv 的结构；或者是配置了高精度输出的 conv 后又接了其他算子导致。此时请检查 dequant\n算子或高精度输出配置是否使用正确。\n\n\nsymbolically traced variables cannot be used as inputs to control flow#\n\n解决方法：该报错是在 fx 模式下使用了 if、循环等动态控制流导致。目前 fx 模式仅支持静态控制流，因此需要避免在 forward 中使用\nif、for、assert 等动态语句。\n\n\nNotImplementedError: function is not implemented for QTensor.#\n\n解决方法：该报错可能发生在 fx 模式下的 Calibration 阶段，是因为 fx 模式不支持 (-x) 形式的计算导致，请将 (-x) 修改为\n(-1)*(x)。\n\n\nNotimplementedError: function rsub at 0x7f5a7cdiee50> is not implemented for\nQTensor.#\n\n解决方法：该报错可能发生在 fx 模式下的 Calibration 阶段，是因为 fx\n模式下算子替换的逻辑是如果减法中的被减数是常量，就不自动进行算子替换，所以需要将减法修改为加法，例如将 (1-x) 修改为 (x+(-1))*(-1)。","routePath":"/guide/plugin/qat_faq/failure","lang":"zh","toc":[{"text":"import 出错","id":"import-出错","depth":2,"charIndex":3},{"text":"无法正常 prepare_calibration/qat","id":"无法正常-prepare_calibrationqat","depth":2,"charIndex":434},{"text":"prepare_qat 后 forward 报错","id":"prepare_qat-后-forward-报错","depth":2,"charIndex":761},{"text":"量化精度异常","id":"量化精度异常","depth":2,"charIndex":921},{"text":"Cannot find the extension library(_C.so)","id":"cannot-find-the-extension-library_cso","depth":2,"charIndex":1011},{"text":"RuntimeError: Cannot load custom ops. Please rebuild the horizon_plugin_pytorch.","id":"runtimeerror-cannot-load-custom-ops-please-rebuild-the-horizon_plugin_pytorch","depth":2,"charIndex":-1},{"text":"RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment","id":"runtimeerror-only-tensors-created-explicitly-by-the-user-graph-leaves-support-the-deepcopy-protocol-at-the-moment","depth":2,"charIndex":-1},{"text":"torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL","id":"torchmultiprocessingspawnprocessexitedexception-process-0-terminated-with-signal-sigkill","depth":2,"charIndex":-1},{"text":"AttributeError: ‘NoneType’ object has no attribute ‘numel’","id":"attributeerror-nonetype-object-has-no-attribute-numel","depth":2,"charIndex":1760},{"text":"symbolically traced variables cannot be used as inputs to control flow","id":"symbolically-traced-variables-cannot-be-used-as-inputs-to-control-flow","depth":2,"charIndex":1995},{"text":"NotImplementedError: function <method ‘neg’ of ‘torch._C._TensorBase’ objects> is not implemented for QTensor.","id":"notimplementederror-function-method-neg-of-torch_c_tensorbase-objects-is-not-implemented-for-qtensor","depth":2,"charIndex":-1},{"text":"NotimplementedError: function <function Tensor.**rsub** at 0x7f5a7cdiee50> is not implemented for QTensor.","id":"notimplementederror-function-function-tensorrsub-at-0x7f5a7cdiee50-is-not-implemented-for-qtensor","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":428,"title":"常见问题","content":"#\n\n\n训练环境#\n\n\nDocker 容器无法使用 Nvidia 资源？#\n\n解决方法：建议参考文档中 环境部署 章节中 GPU Docker 部分的描述。\n\n\nQAT 量化训练#\n\n\n量化精度异常#\n\n解决方法：QAT/Quantized 精度不符合预期、出现 NAN 或 QAT 初始 loss 相对 float 明显异常。请参考 精度调优工具使用指南。\n\n\n为什么开启多机训练后精度表现变差？#\n\n解决方法：开启多机训练后 batchsize 成倍增大，此时需要同步调整 LR 等超参来进行平衡。\n\n\nQconfig 是否需要用户干预？#\n\n解决方法：地平线提供的 Qconfig 定义了 activation 和 weight 如何进行量化，目前支持 FakeQuantize、LSQ、PACT\n等量化算法。\n\n\n如何导出各阶段的 ONNX 模型？#\n\n解决方法： 请参考如下代码实现：\n\n\n\n\n为什么 QAT 训练时存在 nan？#\n\n解决方法： 该问题的影响因素较多，建议从以下几个方面检查：\n\n 1. 检查输入数据是否含 nan；\n 2. 检查浮点模型是否收敛。未收敛的浮点模型针对某些微量化误差可能会导致很大波动；\n 3. 检查是否开启 calib。建议开启，可以给模型更好的初始系数；\n 4. 检查训练策略是否适合。不适合的训练策略也 会导致出现 NAN 值，例如学习率 lr 过大（可通过调低学习率或使用梯度截断方式）等。在训练策略上，默认\n    QAT 与浮点保持一致，如果浮点训练采用的是 OneCycle 等会影响 LR 设置的优化器，建议使用 SGD 替换。\n\n\n配置 int16 节点或高精度输出节点无效#\n\n该现象可能是因为错误配置 module_name 导致，module_name 字段只支持 string，不支持按 index 索引进行配置。\n\n\n如何查看某一层是否开启了高精度输出？#\n\n解决方法：可以打印 qat_model 的所在层，查看该层是否有 (activation_post_process):\nFakeQuantize，若没有，则说明其为高精度输出。例如 int32 高精度 conv 打印如下：\n\n\n\nint8 低精度 conv 打印如下：\n\n\n\n\n辅助分支能否插入伪量化节点？#\n\n解决方法：建议仅对部署上板的模型部分插入伪量化节点。由于 QAT\n训练为全局训练，辅助分支的存在会导致训练难度增加，若辅助分支处的数据分布与其他分支差异较大还会加大精度风险，建议去除。\n\n\n如何将地平线 gridsample 算子改写为 torch 公版实现？#\n\n解决方法：horizon_plugin_pytorch 中的 gridsample 算子非公版实现，其 grid 输入（输入 2）是 int16\n类型的绝对坐标，而 torch 公版是 float32 类型的归一化坐标，范围是 [-1, 1]。\n\n因此，从 torch.nn.functional.grid_sample 路径导入 grid_sample 算子后，可以通过如下方式归一化 grid：\n\n\n\n\nload calibration 后的 QAT 训练为什么权重参数不更新？#\n\n解决方法： 可以依次检查如下：\n\n 1. prepare 是否在 optimizer 定义之前。因为 prepare_qat 会进行算子融合，导致模型结构发生变化；\n 2. fake_quant_enabled 和 observe_enabled 是否为 1；\n 3. module 中的 training 变量是否为 True。\n\n\n如何处理 prepare_fx 不支持的 float 类型常量 +-*/ 操作？#\n\n解决方法：由于模型在转换为静态图时，float 操作无法被 fx 记录，所以会触发 add 不支持，float 未转换为 qtensor 等报错。\n此时需要将常量修改为 tensor，并将输入的 tensor 做量化（插入 quantstub），即可将符号运算转换为\nFloatFunction。以下提供一个参考示例。\n\n修改前：\n\n\n\n修改后：\n\n\n\n\n设置类错误#\n\n\n无需量化的模块设置了非 None 的 qconfig，例如 前后处理，loss function 等。#\n\n解决方法：只对需要量化的模块设置 qconfig。\n\n\n没有正确设置 march，这样可能导致模型编译失败或部署精度不一致。#\n\n解决方法：根据要部署的处理器选择正确的 BPU 架构，如 J6 需要使用 Nash：\n\n\n\n\n模型输出节点没有设置成高精度输出，导致量化精度不符合预期。#\n\n错误示例如下：\n\n假设模型定义如下：\n\n\n\n解决方法：为了提高模型精度，模型输出节点设置成高精度，示例如下：\n\n\n\n\n方法类错误#\n\n\nCalibration 过程使用多卡。#\n\n解决方法：由于底层限制，Calibration 目前不支持多卡，请使用单卡进行 Calibration 操作。\n\n\n模型输入图像数据采用数据格式为 RGB 等非 centered YUV444 格式，这样可能导致模型部署精度不一致。#\n\n解决方法：由于 Horizon 硬件支持的图像格式为 centered YUV444，因此建议您从模型训练开始就直接使用 YUV444\n格式作为网络输入进行训练。\n\n\n量化感知训练中使用 qat 模型进行模型精测评测和监控，导致不能及时发现部署时精度异常的问题。#\n\n解决方法：导致 QAT 与 Quantized 误差的原因是 QAT 阶段不能完全模拟 Quantized 中纯定点计算逻辑，建议使用 quantized\n模型进行模型精度评测和监控。\n\n\n\n\n网络类错误#\n\n\n多次调用同一个通过 FloatFunctional() 定义的成员。#\n\n错误示例如下：\n\n\n\n解决方法：禁止在 forward 中多次调用同一个通过 FloatFunctional() 定义的变量。\n\n\n\n\n算子类错误#\n\n\nQuantized 模型中部分算子没有经过前期的 calibration 或 QAT，如某后处理算子想要在 BPU\n上加速，但是没有经过量化阶段，这时候会导致量化 Inference 失败或部署时的精度异常。#\n\n解决方法：Quantized 阶段并非完全不能直接添加算子，如颜色空间转换算子等，具体添加指南详见文档。但是并非所有算子都可以直接添加，比如\ncat，这种算子必须在 calibration 或 QAT 阶段统计获得的真实量化参数才能不影响最终精度，有类似需求需要调整网络结构，可以咨询框架研发。\n\n\n模型类错误#\n\n\n浮点模型过拟合。#\n\n模型过拟合常见判定方法：\n\n * 对输入数据稍加变换之后，输出结果变化较大。\n * 模型参数赋值较大。\n * 模型 activation 较大。\n\n解决方法：自行解决浮点模型过拟合问题。","routePath":"/guide/plugin/qat_faq/faq","lang":"zh","toc":[{"text":"训练环境","id":"训练环境","depth":2,"charIndex":3},{"text":"Docker 容器无法使用 Nvidia 资源？","id":"docker-容器无法使用-nvidia-资源","depth":3,"charIndex":11},{"text":"QAT 量化训练","id":"qat-量化训练","depth":2,"charIndex":80},{"text":"量化精度异常","id":"量化精度异常","depth":3,"charIndex":92},{"text":"为什么开启多机训练后精度表现变差？","id":"为什么开启多机训练后精度表现变差","depth":3,"charIndex":181},{"text":"Qconfig 是否需要用户干预？","id":"qconfig-是否需要用户干预","depth":3,"charIndex":253},{"text":"如何导出各阶段的 ONNX 模型？","id":"如何导出各阶段的-onnx-模型","depth":3,"charIndex":360},{"text":"为什么 QAT 训练时存在 nan？","id":"为什么-qat-训练时存在-nan","depth":3,"charIndex":401},{"text":"配置 int16 节点或高精度输出节点无效","id":"配置-int16-节点或高精度输出节点无效","depth":3,"charIndex":696},{"text":"如何查看某一层是否开启了高精度输出？","id":"如何查看某一层是否开启了高精度输出","depth":3,"charIndex":794},{"text":"辅助分支能否插入伪量化节点？","id":"辅助分支能否插入伪量化节点","depth":3,"charIndex":954},{"text":"如何将地平线 gridsample 算子改写为 torch 公版实现？","id":"如何将地平线-gridsample-算子改写为-torch-公版实现","depth":3,"charIndex":1066},{"text":"load calibration 后的 QAT 训练为什么权重参数不更新？","id":"load-calibration-后的-qat-训练为什么权重参数不更新","depth":3,"charIndex":1308},{"text":"如何处理 prepare_fx 不支持的 float 类型常量 +-*/ 操作？","id":"如何处理-prepare_fx-不支持的-float-类型常量---操作","depth":3,"charIndex":1516},{"text":"设置类错误","id":"设置类错误","depth":2,"charIndex":1737},{"text":"无需量化的模块设置了非 None 的 qconfig，例如 前后处理，loss function 等。","id":"无需量化的模块设置了非-none-的-qconfig例如-前后处理loss-function-等","depth":3,"charIndex":1746},{"text":"没有正确设置 march，这样可能导致模型编译失败或部署精度不一致。","id":"没有正确设置-march这样可能导致模型编译失败或部署精度不一致","depth":3,"charIndex":1828},{"text":"模型输出节点没有设置成高精度输出，导致量化精度不符合预期。","id":"模型输出节点没有设置成高精度输出导致量化精度不符合预期","depth":3,"charIndex":1912},{"text":"方法类错误","id":"方法类错误","depth":2,"charIndex":2003},{"text":"Calibration 过程使用多卡。","id":"calibration-过程使用多卡","depth":3,"charIndex":2012},{"text":"模型输入图像数据采用数据格式为 RGB 等非 centered YUV444 格式，这样可能导致模型部署精度不一致。","id":"模型输入图像数据采用数据格式为-rgb-等非-centered-yuv444-格式这样可能导致模型部署精度不一致","depth":3,"charIndex":2092},{"text":"量化感知训练中使用 qat 模型进行模型精测评测和监控，导致不能及时发现部署时精度异常的问题。","id":"量化感知训练中使用-qat-模型进行模型精测评测和监控导致不能及时发现部署时精度异常的问题","depth":3,"charIndex":2237},{"text":"网络类错误","id":"网络类错误","depth":2,"charIndex":2384},{"text":"多次调用同一个通过 `FloatFunctional()` 定义的成员。","id":"多次调用同一个通过-floatfunctional-定义的成员","depth":3,"charIndex":-1},{"text":"算子类错误","id":"算子类错误","depth":2,"charIndex":2498},{"text":"Quantized 模型中部分算子没有经过前期的 calibration 或 QAT，如某后处理算子想要在 BPU 上加速，但是没有经过量化阶段，这时候会导致量化 Inference 失败或部署时的精度异常。","id":"quantized-模型中部分算子没有经过前期的-calibration-或-qat如某后处理算子想要在-bpu-上加速但是没有经过量化阶段这时候会导致量化-inference-失败或部署时的精度异常","depth":3,"charIndex":-1},{"text":"模型类错误","id":"模型类错误","depth":2,"charIndex":2764},{"text":"浮点模型过拟合。","id":"浮点模型过拟合","depth":3,"charIndex":2773}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":429,"title":"算法模型QAT量化+部署 快速上手","content":"#\n\n\n基本流程#\n\n量化感知训练工具的基本使用流程如下：\n\n\n\n下面以 torchvision 中的 MobileNetV2 模型为例，介绍流程中每个阶段的具体操作。\n\n出于流程展示的执行速度考虑，我们使用了 cifar-10 数据集，而不是 ImageNet-1K 数据集。\n\n\n\n\n\n\n获取浮点模型#\n\n首先，对浮点模型做必要的改造，以支持量化相关操作。模型改造必要的操作有：\n\n * 在模型输入前插入 QuantStub。\n * 在模型输出后插入 DequantStub。\n\n改造模型时需要注意：\n\n * 插入的 QuantStub 和 DequantStub 必须注册为模型的子模块，否则将无法正确处理它们的量化状态。\n * 多个输入仅在 scale 相同时可以共享 QuantStub，否则请为每个输入定义单独的 QuantStub。\n * 若需要将上板时输入的数据来源指定为 \"pyramid\"，请手动设置对应 QuantStub 的 scale 参数为 1/128。\n * 也可以使用 torch.quantization.QuantStub，但是仅有\n   horizon_plugin_pytorch.quantization.QuantStub 支持通过参数手动固定 scale。\n\n改造后的模型可以无缝加载改造前模型的参数，因此若已有训练好的浮点模型，直接加载即可，否则需要正常进行浮点训练。\n\n注意\n\n模型上板时的输入图像数据一般为 centered_yuv444 格式，因此模型训练时需要把图像转换成 centered_yuv444 格式（注意下面代码中对\nrgb2centered_yuv 的使用）。\n\n如果无法转换成 centered_yuv444 格式进行模型训练，请在模型部署时在输入上插入适当的颜色空间转换结点。（注意，该方法可能导致模型精度降低）\n\n本示例中浮点和 QAT 训练的 epoch 较少，仅为说明训练工具使用流程，精度不代表模型最好水平。\n\n\n\n\n\n\nCalibration#\n\n模型改造完成并完成浮点训练后，便可进行 Calibration。此过程通过在模型中插入 Observer 的方式，在 forward\n过程中统计各处的数据分布情况，从而计算出合理的量化参数：\n\n * 对于部分模型，仅通过 Calibration 便可使精度达到要求，不必进行比较耗时的量化感知训练。\n\n * 即使模型经过量化校准后无法满足精度要求，此过程也可降低后续量化感知训练的难度，缩短训练时间，提升最终的训练精度。\n\n\n\n\n\n模型经过 Calibration 后的量化精度若已满足要求，便可直接进行模型部署的步骤，否则需要进行量化感知训练进一步提升精度。\n\n\n量化感知训练#\n\n量化感知训练通过在模型中插入伪量化节点的方式，在训练过程中使模型感知到量化带来的影响，在这种情况下对模型参数进行微调，以提升量化后的精度。\n\n\n\n\n\n\n模型部署#\n\n伪量化精度达标后，便可执行模型部署的相关流程。\n\n\n导出 Hbir 模型#\n\n模型部署首先需要将伪量化模型导出为 Hbir 模型。\n\n注意\n * 模型导出时使用的 example_input 的 batch_size 决定了模型仿真和模型上板时的 batch_size，若需要在仿真和上板使用不同的\n   batch_size，请使用不同的数据分别导出 hbir 模型。\n * 也可以跳过 Calibration 和量化感知训练中的实际校准和训练过程，先直接进行模型部署的流程，以保证模型中不存在无法导出或编译的操作。\n\n\n\n\n\n\n转定点模型#\n\n伪量化精度达标后，便可将模型转为定点模型。一般认为定点模型的结果和编译后模型的结果是完全一致的。\n\n注意\n * Hbir 模型的输入仅支持单个 Tensor 或 Tuple[Tensor], 输出仅支持 Tuple[Tensor]。\n * 定点模型和伪量化模型之间无法做到完全数值一致，所以请以定点模型的精度为准。若定点精度不达标，需要继续进行量化感知训练。\n\n\n\n\n\n\n模型编译#\n\n测试定点模型精度并确认符合要求后，便可以进行模型编译、性能测试和可视化。\n\n注意\n\n模型性能测试使用的模型，请至少做一次校准（step 数不限），以保证模型中的统计量符合实际情况，否则会造成性能测试结果不准确。\n\n\n\n\n\n\n\n\n\n在模型编译前，我们也支持对模型进行板端部署相关的修改，常见操作及其API接口如下：\n\n * 在模型export之后，convert之前：\n   \n   1. batch输入拆分：使用 insert_split() 接口。\n   \n   2. 模型中插入图像前处理节点：\n      \n      a. 需要先将排布调整成NHWC排布以进行后续操作：此过程通过调用 insert_transpose() 接口进行。\n      \n      b. 图像归一化：此过程通过调用 insert_image_preprocess() 接口进行。\n      \n      c. 色彩转换（板端部署通常为nv12输入）：此过程通过调用 insert_image_convert() 接口进行。\n      \n      d. 配置为resizer输入，支持基于roi进行抠图和缩放：此过程通过调用 insert_roi_resize() 接口进行。\n   \n   3. 调整输入输出数据排布：此过程通过调用 insert_transpose() 接口进行。\n\n * 在模型convert之后，compile之前，进行算子删除（Quantize/Dequantize/Cast等算子）：此过程通过调用\n   remove_io_op() 接口进行。\n\n以上调用API接口的详细说明您可参考 HBDK Tool API Reference 章节的介绍。","routePath":"/guide/plugin/qat_quickstart/qat_quickstart","lang":"zh","toc":[{"text":"基本流程","id":"基本流程","depth":2,"charIndex":3},{"text":"获取浮点模型","id":"获取浮点模型","depth":2,"charIndex":145},{"text":"Calibration","id":"calibration","depth":2,"charIndex":851},{"text":"量化感知训练","id":"quantization_perception_training","depth":2,"charIndex":1148},{"text":"模型部署","id":"model_deploy","depth":2,"charIndex":1233},{"text":"导出 Hbir 模型","id":"导出-hbir-模型","depth":3,"charIndex":1266},{"text":"转定点模型","id":"转定点模型","depth":3,"charIndex":1506},{"text":"模型编译","id":"模型编译","depth":2,"charIndex":1700}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":430,"title":"术语约定","content":"#\n\n\nFloat 模型 / 浮点模型#\n\n符合量化感知训练要求的浮点模型。\n\n\nCalibration#\n\n使用校准数据得到量化参数。\n\n\nCalibration 模型#\n\nCalibration 之后得到的伪量化模型。\n\n\nQAT / 量化感知训练#\n\n量化感知的训练。\n\n\nQAT 模型#\n\n量化感知训练之后得到的伪量化模型。\n\n\n伪量化#\n\n将浮点数据先量化，再反量化的过程，在网络模型中一般通过伪量化节点实现。\n\n\n伪量化模型#\n\n带有伪量化节点的模型，一般通过 Calibration 或者 QAT 得到。\n\n\nQuantized 模型 / 定点模型 / 量化模型#\n\n通过参数转换把伪量化模型中的浮点参数转换成定点参数，并且把浮点算子转换成定点算子，该转换后的模型称为 Quantized 模型 / 定点模型 / 量化模型。\n\n\nHbir 模型#\n\n以部署为目的导出的模型，一般由 QAT 模型导出，可用于精度仿真和编译上板。\n\n\nNash#\n\nBPU 架构的名称。\n\n\nJ6#\n\n处理器的名称。\n\n\nBPU 架构和处理器的对应关系#\n\n处理器    J6E            J6M\nBPU    Nash-e         Nash-m\nenum   March.NASH_E   March.NASH_M","routePath":"/guide/plugin/terminology","lang":"zh","toc":[{"text":"Float 模型 / 浮点模型","id":"float-模型--浮点模型","depth":2,"charIndex":3},{"text":"Calibration","id":"calibration","depth":2,"charIndex":40},{"text":"Calibration 模型","id":"calibration-模型","depth":2,"charIndex":70},{"text":"QAT / 量化感知训练","id":"qat--量化感知训练","depth":2,"charIndex":113},{"text":"QAT 模型","id":"qat-模型","depth":2,"charIndex":139},{"text":"伪量化","id":"伪量化","depth":2,"charIndex":168},{"text":"伪量化模型","id":"伪量化模型","depth":2,"charIndex":212},{"text":"Quantized 模型 / 定点模型 / 量化模型","id":"quantized-模型--定点模型--量化模型","depth":2,"charIndex":261},{"text":"Hbir 模型","id":"hbir-模型","depth":2,"charIndex":371},{"text":"Nash","id":"nash","depth":2,"charIndex":422},{"text":"J6","id":"j6","depth":2,"charIndex":442},{"text":"BPU 架构和处理器的对应关系","id":"bpu-架构和处理器的对应关系","depth":2,"charIndex":457}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":431,"title":"Calibration 指南","content":"#\n\n在量化中，一个重要的步骤是确定量化参数，合理的初始量化参数能够显著提升模型精度并加快模型的收敛速度。Calibration 就是在浮点模型中插入\nObserver，使用少量训练数据，在模型 forward 过程中统计各处的数据分布，以确定合理的量化参数的过程。虽然不做 Calibration\n也可以进行量化感知训练，但一般来说，它对量化感知训练有益无害，所以推荐您将此步骤作为必选项。\n\n\n流程和示例#\n\nCalibration 与 QAT 的整体流程如下图所示：\n\n下面分别介绍各个步骤：\n\n 1. 构建并训练浮点模型。参考 horizon_plugin_pytorch 快速入门章节中的 获取浮点模型 小节内容。\n\n 2. 在浮点模型上插入 Observer 节点，参考 horizon_plugin_pytorch 快速入门章节中的 Calibration 小节内容。使用\n    prepare 方法转化浮点模型前，需要为模型配置 qconfig，参考 QConfig 详解 小节的内容。\n\n 3. 设置 fake quantize 状态为 CALIBRATION 。\n    \n    \n    \n    fake quantize 一共有三种状态，分别需要在 QAT 、 calibration 、 validation 前将模型的 fake\n    quantize 设置为对应的状态。在 calibration 状态下，仅观测各算子输入输出的统计量。在 QAT\n    状态下，除观测统计量外还会进行伪量化操作。而在 validation 状态下，不会观测统计量，仅进行伪量化操作。\n    \n    \n\n 4. calibration。把准备好的校准数据喂给模型，模型在 forward 过程中由 observer 观测相关统计量。\n\n 5. 设置模型状态为 eval 并设置 fake quantize 状态为 VALIDATION 。\n    \n    \n\n 6. 验证 calibration 效果。如果效果满意，则可以直接将模型转为定点或在此基础上进行量化感知训练，不满意则调整 calibration\n    qconfig 中的参数继续 calibration。\n\n\n常用算法介绍#\n\n注解\n\n有关每个算子的参数说明，请参考文末 API 文档。\n\n算法           速度排名   精度排名   易用性排名\nmin_max      1      5      1\npercentile   2      4      4\nmse          4      1      2\nkl           5      2      3\nmix          3      2      1\n\n常用的几种校准方法性能如上表所示，数字越小越好，速度表示相同数据校准耗时，精度表示该方法在大多数模型上的校准效果，易用性表示该方法的调参复杂度。\n\n对于同一模型而言，不同方法不同参数的精度/速度会存在较大差别，最新的一些研究工作也表明，没有一种方法可以在所有模型上都取得最好的精度，需要针对地调整其参数。所以\n推荐您对这几种校准方法都进行尝试。\n\n 1. min_max。此方法仅统计最大值最小值的滑动平均，用于快速确定 Batch size、average_constant 等通用参数，没有太多技巧。\n\n 2. percentile。此方法是所有方法中精度上限最高的，但也是调整起来最麻烦的，如果通过其他方法或本方法的默认参数就可以满足精度要求，那么不建议在调参上\n    花太多时间。percentile 可调的参数一共有两个 bins、percentile。bins 越多，max\n    的候选项间隔越小，可供调整的粒度越细，但也意味着更高的计算耗时。建议先确定 percentile 再调整\n    bins，两者交替迭代缩小调参范围直至达到满意的效果。绝大部分情况下 bins 取 2048\n    提供的调整粒度完全足够，不需要单独调整这个参数。以下是一个模型的调参路径：\n    \n    顺序   PERCENTILE   BINS   精度\n    1    99.99        2048   53.75\n    2    99.99        4096   54.38\n    3    99.995       4096   16.25\n    4    99.985       4096   32.67\n    5    99.9875      4096   57.06\n    6    99.9875      8192   62.84\n    7    99.98875     8192   57.62\n    8    99.988125    8192   63.15\n    \n    在这个例子中，可以看到仔细调整后，精度提升了大约 10%。 模型中不同 op 的输入输出之间存在很大差异，一组全局的 percentile\n    参数可能很难满足所有 op 的需求，对精度要求较高时，可以先通过上面的方法找到较好的全局参数，再通过 debug 工具找到误差较大的几个\n    op，单独为这几个 op 设置 percentile 参数，设置方式参照 qconfig 设置。下面列举几种常见的容易导致误差较大的数据分布：\n    \n    \n    \n    超长尾分布，percentile 的取值应当小一些，图中 99.9 是较好的取值。\n    \n    \n    \n    值域过大，且分布并不集中在一处，这种情况无论是保留尾部还是忽略尾部都会带来较大的精度损失，应该在训练浮点模型时通过调整 weight decay\n    等参数避免这种情况的出现。\n    \n    \n    \n    layernorm 的输出分布会呈现出若干集中度非常高的区域，此时 percentile 按照正常方法调整对于量化结果不会有任何影响，需要将\n    percentile 调整幅度增加。\n\n 3. mse。可调整的参数只有 stride，默认 stride 为 1，会逐步尝试最大值的 100 分位并选出量化反量化前后误差最小（L2\n    距离）的分位对应的值。此方法对大模型耗时较高，在合理范围内调大 stride 可以在保证精度的前提下减少耗时，stride\n    调整过大会影响精度。注意，调整此方法的参数只能优化耗时，并不能显著提升精度。\n\n 4. kl。可调的参数一共有两个，bin 和 update_interval。由于此方法耗时过长，不建议调整默认 bin，update_interval\n    默认为 1，调大可以减少耗时，但需要保证 update_interval 小于总的 calibration step，否则无法得到正常的量化参数。\n\n 5. mix。此方法为混合校准，对于每一个需要统计的地方，都会尝试 percentile 方法的不同参数，选出量化反量化前后误差最小（L2\n    距离）的方法。自动化程度较高，没有需要调整的参数。\n\n\n调参技巧#\n\n 1. calibration 数据越多越好，但因为边际效应的存在，当数据量大到一定程度后，对精度的提升将非常有限。如果训练集较小，可以全部用来\n    calibration，如果训练集较大，可以结合 calibration 耗时挑选大小合适的子集，建议至少进行 10 - 100 个 step 的校准。\n\n 2. 数据可以做水平翻转这类 augmentation，不要做马赛克这种 augmentation。尽量使用 infer 阶段的前处理 + 训练数据进行校准。\n\n 3. Batch size 尽可能大，如果数据噪声较大或模型离群点较多，可以适当减小。此参数应当在尝试 min max 方法时确定。\n\n 4. average_constant 表示每个 step 对最大值最小值的影响，average_constant 越小，当前 step\n    的影响越小，历史滑动均值的影响越大。该参数需要结合数据量在 0.01 ~ 0.5 之间调整。当数据量充足时（step >\n    100），average_constant 取 0.01，数据量不足时，average_constant 酌情增加，极端情况下，只有 2 个 step\n    的数据，average_constant 取 0.5。此参数应当在尝试 min max 方法时确定，之后其他方法都沿用此参数。\n\n 5. calibration 模型精度较好时，固定 feature map 的量化参数进行 QAT 训练可以取得更好的效果，精度较差时，则不能固定\n    calibration 得到的量化参数。关于精度是好还是坏，没有明确的标准，需要去尝试。比如：某模型精度为 100，如果 calibration 精度为\n    50，那么精度肯定称不上好，但如果 calibration 精度为 95，那么这个精度是否可以达到固定 feature map\n    量化参数的程度就需要尝试了，通常做法是固定与不固定都做实验进行对比。\n\n 6. 优先尝试 min max 方法，该方法是速度最快的，用来跑通 calibration 流程，调整并确定 batch size 和\n    average_constant 两个参数，接着分别尝试 percentile、kl、mse 和 mix 四种方法并选取效果最好的方法。\n\n\nObserver 参数文档#\n\nclass horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int =\n512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int =\n-1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nKL observer.\n\nKL observer based on histogram. Histogram is calculated online and won’t be\nsaved.\n\n * Parameters:\n * bins (int) – Number of histograms bins.\n * update_interval (int) – Interval of computing KL entropy and update min/max.\n   KLObserver will constantly collect histograms of activations, but only\n   perform KL calculation when update_interval is satisfied. if it is set to 1,\n   KL entropy will be computed every forward step. Larger interval guarantees\n   less time and does no harm to calibration accuracy. Set it to the total\n   calibration steps can achieve best performance. update_interval must be no\n   greater than total calibration steps, otherwise no min/max will be computed.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int =\n1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype |\nQuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min:\nint | None = None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMSE observer.\n\nObserver module for computing the quantization parameters based on the Mean\nSquare Error (MSE) between the original tensor and the quantized one.\n\nThis observer linear searches the quantization scales that minimize MSE.\n\n * Parameters:\n * stride (int) – Searching stride. Larger value gives smaller search space,\n   which means less computing time but possibly poorer accuracy. Default is 1.\n   Suggests no greater than 20.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMin max observer.\n\nThis observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average minimum\nand maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MixObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMix observer.\n\nThis observer computes the quantization parameters based on multiple calibration\nmethods and selects the quantization parameters with the smallest quantization\nerror.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver\n(percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01,\nch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nPercentile observer.\n\nPercentile observer based on histogram. Histogram is calculated online and won’t\nbe saved. The minimum and maximum are moving averaged to compute the\nquantization parameters.\n\n * Parameters:\n * percentile (float) – Index percentile of histrogram\n * bins (int) – Number of histograms bins.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver\n(averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric,\nquant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)\n\nMovingAverageMinMax Observer.\n\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving averages\nof minimums and maximums of the incoming tensors. The module records the average\nminimum and maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, only support per_tensor_symmetric\n   scheme\n * reduce_range – Reduces the range of the quantized data type by 1 bit\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – Whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver\n(averaging_constant=0.01, ch_axis=0, dtype=torch.qint8,\nqscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None,\nis_sync_quantize=False, factory_kwargs=None)\n\nMovingAveragePerChannelMinMax Observer.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * ch_axis – Channel axis\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, Only support per_channel_symmetric\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.","routePath":"/guide/plugin/user_guide/calibration","lang":"zh","toc":[{"text":"流程和示例","id":"流程和示例","depth":2,"charIndex":198},{"text":"常用算法介绍","id":"常用算法介绍","depth":2,"charIndex":952},{"text":"调参技巧","id":"调参技巧","depth":2,"charIndex":2988},{"text":"Observer 参数文档","id":"observer-参数文档","depth":2,"charIndex":3993}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":432,"title":"浮点模型的要求","content":"#\n\n\nsymbolic_trace#\n\n和 PyTorch 的量化感知训练类似，horizon_plugin_pytorch 基于 fx 设计和开发，因此，要求浮点模型必须是可以正确的完成\nsymbolic_trace 的。\n\n\n仅支持部分算子#\n\n由于 BPU 只支持数量有限的算子，因此，horizon_plugin_pytorch 只支持算子列表中的算子和基于 BPU 限制而内部特殊定义的特殊算子。\n\n\n构建量化友好模型#\n\n浮点模型变为定点模型的过程存在一定的精度误差，越是量化友好的浮点模型，qat\n精度提升越容易，量化后的精度也越高。一般而言，有以下几种情况会导致模型变得量化不友好：\n\n 1. 使用有精度风险的算子。例如：softmax，layernorm 等（详见 op 文档），这类算子一般底层由查表或多个 op 拼接实现，容易发生掉点问题。\n\n 2. 一次 forward\n    中多次调用同一算子。同一算子多次调用，对应的输出分布存在差异，但只会统计一组量化参数，当多次调用的输出分布差异过大时，量化误差会变大。\n\n 3. add，cat 等多输入算子的不同输入差异过大，可能造成较大误差。\n\n 4. 数据分布不合理。plugin 采用的是均匀对称量化，所以 0 均值的均匀分布最好，应尽量避免长尾和离群点。同时，数值范围需要与量化 bit\n    相匹配，如果使用 int8 量化分布为 [-1000, 1000] 均匀分布的数据，那么精度显然也是不够的。\n    例如，下面三个分布图，从左到右对量化的友好性依次递减，模型中大部分数值的分布应当为中间这种分布。在实际使用中，可以用 debug 工具查看模型\n    weight 和 feature map 的分布是否量化友好。 因为模型冗余性的存在，有些看起来分布非常量化不友好的 op\n    并不会显著降低模型的最终精度，需要结合实际的 qat 训练难度和最后达到的量化精度综合考虑。\n    \n    \n\n那么如何使得模型更加量化友好呢？具体来说：\n\n 1. 尽量少使用精度风险过大的算子，详见 op 文档。\n\n 2. 保证多次调用的共享算子每次调用的输出分布差异不要太大，或者将共享算子拆开分别单独使用。\n\n 3. 避免多输入算子不同输入的数值范围差异过大。\n\n 4. 使用 int16 量化数值范围和误差都非常大的 op。可通过 debug 工具找到这类 op。\n\n 5. 通过调大 weight decay，增加数据增强等方式防止模型过拟合。过拟合模型容易出现较大数值，且对输入非常敏感，轻微的误差可能导致输出完全错误。\n\n 6. 使用 BN。\n\n 7. 对模型输入做关于 0 对称的归一化。\n\n需要注意的是，qat\n自身具有一定的调整能力，量化不友好并不代表不能量化，很多情况下，即使出现上面的不适合量化的现象，仍然可以量化得很好。因为上述建议也可能会导致浮点模型精度下降，所\n以应当在 qat 精度无法达标时再尝试上述建议，尤其是 1 - 5 条建议，最后应当是在浮点模型精度和量化模型精度中找一个平衡点。","routePath":"/guide/plugin/user_guide/float_model_requirements","lang":"zh","toc":[{"text":"symbolic_trace","id":"symbolic_trace","depth":2,"charIndex":3},{"text":"仅支持部分算子","id":"仅支持部分算子","depth":2,"charIndex":115},{"text":"构建量化友好模型","id":"构建量化友好模型","depth":2,"charIndex":206}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":433,"title":"量化精度调优指南","content":"#\n\n量化精度调优包括两个方面：\n\n 1. 模型结构和量化配置检查。检查主要目的是避免非调优类的问题影响量化精度，比如：qconfig 配置错误，使用了量化不友好的共享模块等。\n\n 2. 混合精度调优。先使用高精度算子为主的模型快速迭代出精度达标的模型，获取精度上限和性能下限，再使用精度调优工具分析并调整量化配置，得到兼顾精度与性能的量化\n    模型。\n\n需要重点说明的是，在进行如下精度调优之前需要先验证自己的 pipeline 的正确性。\n\n在模型完成 prepare 之后，首先需要检查量化配置错误和模型结构对量化不友好的情况。可以使用 debug 工具中的 check_qat_model\n接口进行检查，接口使用方式可参考精度调优工具使用指南章节。\n\n注意\n\nprepare 接口中已集成 check_qat_model，可以直接在运行目录下查看 model_check_result.txt。\n\n\n算子融合#\n\n检查模型中是否存在可以融合但没有融合的模块。模型在 BPU 上部署时会将 conv / bn / add / relu 等算子融合，在 qat\n模型中，这些算子会被替换为一个\nModule，避免在中间插入伪量化节点。如果这些算子没有融合，那么中间会产生额外的量化节点，对精度和性能可能产生轻微影响。下面的例子表明 conv 和 relu\n没有 fuse。\n\n\n\n在 prepare 的不同 method 中，可能导致产生算子融合错误的原因和解决方法有：\n\n 1. PrepareMethod.JIT 和 PrepareMethod.JIT_STRIP：\n    \n    a. 动态代码块中包含算子融合，需要使用 dynamic_block 进行标注。\n    \n    b. 调用次数变化的部分在 trace 时仅执行了一次，需要使用能够让调用次数变化的部分执行多次的输入作为 example_inputs。\n\n 2. PrepareMethod.EAGER：未进行 fuse 操作或 fuse_modules 写错，需要检查并修复手写的 fuse 逻辑。\n\n 3. PrepareMethod.SYMBOLIC：可 fuse 的模块被包含在了 fx.wrap 中，需要将这些模块移出\n    fx.wrap，保证图中存在这些模块或使用 PrepareMethod.EAGER 的方式手动 fuse。\n\n\n共享模块#\n\n由于 horizon_plugin_pytorch\n采用模块替换的方式插入量化节点，所以对于一个模块仅能统计一组量化信息。当一个模块对象被多次调用且多次调用的输出数据分布差异较大时，使用同一组量化参数将产生较大误\n差，需要将共享模块拆开。如果多次调用的输出数据分布差异不大，那么就不需要拆开共享模块。这里先对共享模块的概念做一个说明，后面逐层比较的时候可以结合这里的结果决定\n是否需要拆开共享模块。\n\n\n\n三种常见理解中的“共享”与这里所说的“共享”的异同：\n\nA. 一个 module 后接多个 module，module A 被认为是共享，但这里 module A\n仅被调用一次，输出数据分布不存在差异，不会影响量化精度，在调用次数检查中也不会体现出来。\n\nB. 一个 module 被反复调用多次，但多次输出数据分布差异较小。虽然在调用次数检查中可以看出来，但对量化精度影响较小，不需要修改。\n\nC. 一个 module 被反复调用多次，多次输出分布差异较大。在调用次数检查中可以看出来，且对量化精度影响较大，需要手动将其复制拆分。\n\n在 model_check_result.txt 中，可以查看每个 module 的调用次数。正常每个 op 仅调用 1 次，0 表示未被调用，超过 1\n次则表示调用了多次。下面的例子中，conv 为共享模块。\n\n\n\n\nQConfig 配置错误#\n\nqconfig 使用错误可能导致模型未按照预期的方式量化，从而产生精度损失（比如：混用模板和 qconfig\n属性两种设置方法）。这里主要检查每个算子的输入输出是否符合预期，在 model_check_result.txt 中查看：\n\n 1. dtype 是否与设置一致。\n\n 2. 是否开启高精度输出。\n\n\n\n除此以外，model_check_result.txt 中还会有异常 qconfig 提示（如果有的话）。这里为工具识别到的需要您再检查一下的 qconfig\n配置，具体看是否符合预期，不一定就是错误的。\n\n 1. Weight int16。J6M 不支持输入和 weight 双 int16，如果发现 weight int16 需要检查 input 是否为\n    int16。\n\n 2. Fixed scale。检查 fixed scale 设置是否符合预期。\n\n\n\n\n混合精度调优#\n\n\n调优流程#\n\n整个流程首先进行全 int16 精度调优，此阶段用于确认模型的精度上限，排查工具使用问题和量化不友好模块。\n\n 1. 在确认全 int16 精度满足需求后，进行全 int8 精度调优。如果精度不达标则进行 int8 / int16 混合精度调优，在全 int8\n    的基础上逐步增加 int16 的比例，该阶段需要您在精度和性能间做权衡，在满足精度的前提下，找出性能尽可能好的量化配置。\n\n 2. 如果全 int16 精度不满足需求，则进行 int16 / fp16 混合精度调优。理想情况下 int16 / fp16\n    混合精度调优可以解决所有精度问题。在此基础上进行 int8 / int16 / fp16 混合精度调优，固定所有 fp16 算子的配置，按照 1 中\n    int8 / int16 混合精度调优的方法调整 int16 算子比例。\n\n\n基本调优手段#\n\n基本调优手段的目标是快速迭代，在全 int16 精度调优和全 int8\n精度调优中，模型只是一个快速迭代的中间状态，一般仅使用基本调优手段。而在混合精度调优中追求使用更少的高精度算子，需要按精度需求决定是否使用更加复杂的高级调优手段\n，这会带来更多的试错和迭代成本，但模型的精度和性能可以调得更好。\n\nCalibration#\n\n 1. 调整校准 step。校准数据越多越好，但因为边际效应的存在，当数据量大到一定程度后，对精度的提升将非常有限。如果训练集较小，可以全部用来\n    calibration，如果训练集较大，可以结合 calibration 耗时挑选大小合适的子集，建议至少进行 10 - 100 个 step 的校准。\n\n 2. 调整 batch size。一般 batch size 要尽可能大，如果数据噪声较大或模型离群点较多，可以适当减小。\n\n 3. 使用推理阶段的前处理 + 训练数据进行校准。校准数据应接近真实分布，可以使用翻转这类数据增强，不要使用旋转，马赛克等会改变真实分布的数据增强方法。\n\nQAT#\n\n 1. 调整学习率。\n    \n    a. 初始学习率：取消 warmup，取消 learning rate decay 策略，使用不同的固定学习率（1e-3, 1e-4,\n    ...）finetune 少量 step，并观察评测指标，取效果最好的作为初始学习率。如果浮点模型不同模块学习率不同，那么这里也要做对应尝试。\n    \n    b. Scheduler：学习率（learning decay）decay 策略与浮点对齐，但需要确保没有 warmup 类的策略，例如浮点 lr\n    decay 策略为 cosine，那么 qat 也应该使用 cosine。\n\n 2. 尝试固定和更新 input / output scale 两种策略。一般来说，校准模型精度较好时，固定 input / output scale 进行\n    QAT 训练可以取得更好的效果，精度较差时，则不能固定。具体使用哪种策略，没有明确指标可以参考，需要分别进行尝试。\n\n 3. 训练 step 数一般不超过浮点训练的 20%，可结合训练的 loss 和评测结果酌情调整。\n\n特别需要注意的点：\n\n 1. 除上述需要调整的内容以外，其余 qat 训练配置与浮点训练对齐。\n\n 2. 如果在浮点训练中使用了 freeze bn 的技巧，那么 qat 训练中需要将 qat mode 设置为 withbn。\n\n\n\n 3. 在 qat 调参的过程中，你可能会遇到无论怎么调参精度都不达标、nan 或 qat loss 明显异常的情况。这时，需要先以相同配置 finetune\n    浮点排除训练策略未对齐浮点或过拟合的可能。\n\n\n高级调优手段#\n\n高级调优手段一般需要花费较多时间不断尝试，在对精度有较高要求时使用。\n\n设置 Fixed Scale#\n\n模型中的某些地方很难依靠统计的方式获得最佳的量化 scale。常见的需要设置 fixed scale 的情况：算子的输出值域确定时需要设置 fixed\nscale。\n\n比如：输入数据为速度 km / h，值域为 [0, 200], 对于 quantstub 而言，输出值域是固定的，需要将 scale 设置为 200 /\n量化数值范围。之所以这么做是因为量化 scale\n值是基于统计方法获取的，在正常的校准数据中，很难保证每一个样本都达到边界情况，统计方法为了消除离群点会使用滑动平均，导致得到的量化范围小于实际值。在上面输入数据\n为速度的例子中，如果不设置 fixed\nscale，那么统计出来的最大速度可能是大多数车辆的平均速度，导致所有超过这个速度的样本在输入时就产生较大的精度损失。您在这里可能很难识别所有需要设置\nfixed scale 的情况，但在精度 debug 的逐层比较步骤中，将很容易发现此类问题。\n\n下图中，输入 a 和 b 值域确定，输入 c 值域不确定，除 quantstub_c 和后一个 add 以外，其余算子均需要设置 fixed scale。\n\nCalibration#\n\n尝试不同的校准方法。Plugin 支持多种校准方法，推荐尝试 min max / percentile / kl / mse / mix\n这几种方法，调参经验见Calibration 指南。\n\nQAT#\n\n 1. 调整 weight decay。weight decay 会影响模型中权重的数值范围，更小的数值范围更加量化友好。有时，只调整 qat 阶段的\n    weight decay 还不足以解决问题，需要调整浮点训练阶段的 weight decay。\n\n 2. 调整数据增强。量化模型比浮点模型的学习能力更差，太强的数据增强会影响 qat 模型收敛，一般需要适当减弱数据增强。\n\n\nINT8 / INT16 混合精度调优 & INT16 / FP16 混合精度调优#\n\n混合精度调优的基本思路是在某一精度的基础上逐步增加更高精度算子的比例，直到达到精度需求。在 int8 / int16 混合精度调优中，以全 int8\n模型为基础，逐步增加 int16 算子的数量。而在 int16 / fp16 混合精度调优中，则以全 int16 模型为基础，逐步增加 fp16 算子的数量。\n\n上图中的校准和 qat 调优参考基本调优手段和高级调优手段章节，增加 int16 / fp16 的高精度算子数量依赖于精度 debug 工具产出的一系列\ndebug 结果。\n\nqat 精度 debug 全部基于浮点模型和校准模型的对比，一般来说，不推荐您将浮点模型和 qat 模型进行对比，经过 qat 训练，浮点模型和 qat\n模型已失去可比性，请先阅读精度调优工具使用指南章节了解相关背景。首先，提供数据集查找校准模型损失较大的 badcase，在 badcase\n的基础上进行逐层比较并计算量化敏感度。\n\n查找 Badcase#\n\n精度 debug 的全部操作都围绕 badcase\n展开。您需要提供一个量化精度较差的样本集，此过程会遍历样本集，对比每一个样本在浮点模型和校准模型输出上的误差。一般情况下，不需要您提供误差的度量函数。\n\n注意\n\n从查找 badcase 开始，对比模型需要带上部分后处理逻辑（从原始后处理逻辑中删除或替换会使得模型输出完全失去可比性的操作）。举两个例子：\n\n 1. sigmoid 不能删除。落在 sigmoid 饱和域的数值对误差不敏感，但 0 附近的数值对误差非常敏感，删除 sigmoid\n    将不能正常反应不同定义域内的量化误差对精度的影响。\n\n 2. nms 需要删除。微小误差会导致 nms 结果完全不同，使得输出不能直接反映量化误差对精度的影响。\n\ndebug 工具已支持自动替换 sort / topk /\nargmax，除了这些算子，您需要检查模型和带上的后处理中是否还有类似的算子，将此算子之后的部分全部删除。\n\n使用 debug 工具中的 auto_find_bad_case 接口查找 badcase。\n\n\n\n查找 badcase 完成后，查看结果文件，对于模型的每一个输出，在每一种误差度量下，debug 工具都会找一个最差的样本。在下面这个例子中，模型一共有 3\n个输出，第一个表格表示每一个输出在每一种度量下的最差样本 index，第二个表格表示对应的误差是多少，第三个表格表示当前度量下，模型所有输出中误差最大的\nbadcase index。\n\n\n\n注意\n\n在后续 debug 的过程中，我们可以针对与精度掉点相关的输出进行 debug。不同的模型输出需要使用不同的误差 metric，通常情况下，L1 和\nCosine 可以反应绝大部分问题。L1 适用于 bbox 回归等需要反应绝对误差的任务，Cosine 适用于分类等需要反应整体分布误差的任务。\n\n逐层比较#\n\n逐层比较会使用指定的 badcase 分别跑浮点模型和校准模型，并对比每一层的输出，对其进行统计并计算误差，可以使用 debug 工具中的\ncompare_per_layer 接口进行对比。compare_per_layer\n适用于极为细致的精度问题分析，如果精度损失不明显，可以先跳过这一步，后面再结合敏感度结果做分析。\n\n\n\n逐层比较的结果可以通过生成的文本文件来查看。在文本文件中，可以从上至下查看从哪个算子开始，误差被放大。\n\n\n\n注意\n\n当发现掉点的算子后，首先查看 base_model_min / base_model_max / analy_model_min /\nanaly_model_max，确认极值是否产生较大误差。\n\n 1. min / max 产生较大误差：此时浮点模型该算子的输出范围应该大幅超过了校准得到的范围。查看该算子的 scale，以 dtype 和 scale\n    算出校准得到的最大值，比如：scale 为 0.0078，dtype 为 int8，那么最大值应为 0.0078 * 128 = 0.9984，再与\n    base_model_max 和 analy_model_max 进行对比。统计出的 scale\n    太小原因可能有：校准数据不合理（校准数据太少，分布偏差，产生的输出范围过小），未设置 fixed scale 导致，共享模块等。\n\n 2. min / max 未产生较大误差：同样以 1 中的方法计算校准最大值并与 base_model_max 和 analy_model_max\n    进行对比，确认此时浮点模型该算子的输出范围没有大幅超过校准得到的范围。此类精度问题可能由量化类型分辨率不足或数值范围过大导致。\n    \n    a. 观察当前的量化 dtype 与数值范围是否匹配，一般最大值超过 10 则不建议使用 int8 量化。\n    \n    b. 定位是什么原因导致校准统计出了较大的数值范围，可能是离群点或设置了不合理的数值。\n\n计算量化敏感度#\n\n这一步会评估模型中的量化节点对精度的影响，可以使用 debug 工具中的 sensitivity 接口评估量化敏感度。具体的评估方法为，以 badcase\n作为模型输入，分别将每一个量化节点打开，对比量化模型与浮点模型输出的误差，误差的度量标准与查找 badcase 时使用的一致。\n\n\n\n量化敏感度的结果中，敏感度排名越高的算子对模型精度影响越大，需要将其设置为更高的精度类型。\n\nsensitive_type 列有 weight / activation 两种，分别表示只打开该算子的 weight 量化节点/输出量化节点的情况。\n\n\n\n注意\n\n我们认为即使非常难量化的模型，也应当存在一些算子的量化敏感度是较低的，所以在正常的敏感度表中，敏感度应当是有高有低的，且最后几个算子的量化敏感度应当接近于\n0。如果发现最后几个算子的误差仍然较大，那么考虑模型中是否存在没有去除干净的后处理，nms 等。\n\n设置需要用到敏感度模板，用法详见 qconfig\n章节。若模型有多个输出，每个输出都会生成一个对应的敏感度表，您可以选取若干指标相差较大的输出所对应的敏感度表，设置敏感度模版。下面是 int8 / int16\n混合精度调优中，设置 2 个输出敏感度表 int8 敏感度 top 20% int16 的例子。总的 int16 个数为两个表中 top 20% int16\n算子的并集。之后不断调整 int16 的比例，直到找到满足精度需求的最少 int16 比例。\n\n\n\n目前暂不提供根据敏感度批量设置 fp16 的接口，需要根据 int16 的敏感度结果，使用 ModuleNameQconfigSetter 设置少量\nfp16.下面是 int16 / fp16 混合精度调优中，设置 int16 敏感度 top1 fp16 的例子。\n\n\n\nJ6M 浮点算力有限，如果没有必须使用 fp16 的情况，尽量使用 int8 / int16 混合精度调优。当全 int16\n模型无论如何也不能达到精度要求时，需要在全 int16 模型中引入少量 fp16 算子。\n\n造成全 int16 模型精度不达标的两种情况：\n\n 1. 需要使用双 int16：表现为量化敏感度表中某些算子在 activation 和 weight sensitive_type 下的敏感度都较高，设置\n    weight 和 activation 为 int16 后精度可以达标。由于 J6M 不支持 activation 和 weight 同时使用\n    int16，所以只能通过调整浮点模型的方式使两者其一变得更加量化友好。常用的方法有增大 weight decay，添加 norm 类算子等。\n\n 2. 不需要使用双 int16：表现为量化敏感度表中某些算子在 activation 或 weight sensitive_type 下的敏感度较高，一般是\n    plugin 使用问题，或部分算子需要设置 fixed scale，通过精度 debug 可以发现具体问题。\n\n\nINT8 / INT16 / FP16 混合精度调优#\n\n在进行 int8 / int16 / fp16 混合精度调优之前，您应该已经完成了 int16 / fp16 混合精度调优。复用 int16 / fp16\n混合精度调优中 fp16 的配置，在 int8 / fp16 混合校准模型的基础上进行精度 debug。参考上一节的精度 debug 方法，不断调整 int16\n的比例，直到找到满足精度需求的最少 int16 比例即可。\n\n下面是 int8 / int16 / fp16 混合精度调优中，设置 int16 敏感度 top 1 fp16，int8 敏感度 top 20% int16\n的例子。\n\n","routePath":"/guide/plugin/user_guide/precision_tuning","lang":"zh","toc":[{"text":"算子融合","id":"算子融合","depth":3,"charIndex":405},{"text":"共享模块","id":"共享模块","depth":3,"charIndex":1016},{"text":"QConfig 配置错误","id":"qconfig-配置错误","depth":3,"charIndex":1604},{"text":"混合精度调优","id":"混合精度调优","depth":2,"charIndex":2012},{"text":"调优流程","id":"调优流程","depth":3,"charIndex":2022},{"text":"基本调优手段","id":"基本调优手段","depth":3,"charIndex":2413},{"text":"Calibration","id":"calibration","depth":4,"charIndex":2573},{"text":"QAT","id":"qat","depth":4,"charIndex":2885},{"text":"高级调优手段","id":"高级调优手段","depth":3,"charIndex":3601},{"text":"设置 Fixed Scale","id":"设置-fixed-scale","depth":4,"charIndex":3646},{"text":"Calibration","id":"calibration-1","depth":4,"charIndex":4154},{"text":"QAT","id":"qat-1","depth":4,"charIndex":4265},{"text":"INT8 / INT16 混合精度调优 & INT16 / FP16 混合精度调优","id":"int8--int16-混合精度调优--int16--fp16-混合精度调优","depth":3,"charIndex":4464},{"text":"查找 Badcase","id":"查找-badcase","depth":4,"charIndex":4920},{"text":"逐层比较","id":"逐层比较","depth":4,"charIndex":5729},{"text":"计算量化敏感度","id":"计算量化敏感度","depth":4,"charIndex":6599},{"text":"INT8 / INT16 / FP16 混合精度调优","id":"int8--int16--fp16-混合精度调优","depth":3,"charIndex":7888}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":434,"title":"prepare 详解","content":"#\n\n\nprepare 的定义#\n\nprepare 是将浮点模型转换为伪量化模型的过程。这个过程会做以下几件事情：\n\n 1. 算子替换：部分 torch function 类型的算子（例如 F.interpolate）在量化时需要插入伪量化节点，因此需要将算子替换为对应的\n    Module 类型实现（horizon_plugin_pytorch.nn.Interpolate），以将伪量化节点放在此 Module\n    内部。替换前后的模型是等价的。\n\n 2. 算子融合：BPU 支持将特定的计算 pattern 进行融合，融合后算子中间结果用高精度表示，因此我们将被融合的多个算子替换为一个\n    Module，以阻止中间结果的量化。融合前后的模型也是等价的。\n\n 3. 算子转换：将浮点算子替换为 qat 算子。按照设置的 qconfig，qat 算子会在输入/输出/权重处添加伪量化/伪转换节点。\n\n 4. 模型结构检查：检查 qat 模型，生成检查结果文件。\n\n注意\n\n因历史原因，Plugin 中有 prepare_qat 和 prepare_qat_fx 两个早期的接口，后面将逐步废弃，我们只推荐您使用此文档中介绍的\nprepare 用法。\n\nprepare 接口的用法如下：\n\n\n\n\nPrepareMethod#\n\nprepare 有四种 method，他们的对比如下：\n\nMETHOD                                        原理                                                           优点                              缺点\nPrepareMethod.JIT & PrepareMethod.JIT_STRIP   使用 hook 和 subclass tensor 的方式感知图结构，在原有 forward               全自动，代码修改少，屏蔽了很多细节问题，便于 debug。   动态代码块需要特殊处理。\n                                              上做算子替换/算子融合等操作。\nPrepareMethod.SYMBOLIC                        使用 torch 官方的 symbolic trace 感知图结构，在 recomplie 生成的新 forward   全自动，屏蔽了很多细节问题。                  不支持动态控制流，不支持部分数据类型和 python 操作，不便于 debug。\n                                              上做算子替换/算子融合等操作。\nPrepareMethod.EAGER                           不感知图结构，算子替换/算子融合需手动进行。                                       用法灵活，过程可控，便于 debug 和处理各类特殊需求。   手动操作较多，代码修改多，上手成本高。\n\n目前，JIT 和 JIT_STRIP 为我们推荐的 method，两者的区别在于 JIT_STRIP 会根据模型中 QuantStub 和\nDequantStub 的位置识别并跳过前后处理，因此当模型中存在不需要量化的前后处理时，请使用\nJIT_STRIP，否则它们将被量化，除此以外，两者完全一致。SYMBOLIC 和 EAGER 为早期方案，存在较多易用性问题，我们建议您不要使用这两种方案。\n\n\n使用示例#\n\n\n\n注意\n 1. 动态代码块涉及到算子替换或算子融合时，必须使用 Tracer.dynamic_block 进行标注，否则将导致量化信息错乱或 forward 报错。\n 2. 模型中调用次数变化的部分（子 module 或 dynamic_block），若在 trace 时仅执行了一次，则有可能和非动态部分产生算子融合，导致\n    forward 报错。\n\n\n模型检查#\n\n在提供 example_inputs 的情况下，prepare 默认会对模型结构进行检查。如果检查完成，可以在运行目录下找到\nmodel_check_result.txt 文件，如果检查失败，则需要根据警告提示修改模型或单独调用\nhorizon_plugin_pytorch.utils.check_model.check_qat_model 检查模型。检查流程和 debug 工具中的\ncheck_qat_model 一致，结果文件的分析详见 check_qat_model 相关文档。","routePath":"/guide/plugin/user_guide/prepare","lang":"zh","toc":[{"text":"prepare 的定义","id":"prepare-的定义","depth":2,"charIndex":3},{"text":"PrepareMethod","id":"preparemethod","depth":2,"charIndex":554},{"text":"使用示例","id":"使用示例","depth":2,"charIndex":1558},{"text":"模型检查","id":"模型检查","depth":2,"charIndex":1747}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":435,"title":"量化感知训练指南","content":"#\n\n量化感知训练通过在模型中插入一些伪量化节点，从而使得通过量化感知训练得到的模型转换成定点模型时尽可能减少精度损失。\n量化感知训练和传统的模型训练无异，可以从零开始，搭建一个伪量化模型，然后对该伪量化模型进行训练。\n由于部署的硬件平台有诸多限制，搞清这些限制，并且根据这些限制搭建伪量化模型门槛较高。\n量化感知训练工具通过在您提供的浮点模型上根据部署平台的限制自动插入伪量化量化算子的方法，降低开发量化模型的门槛。\n\n量化感知训练由于施加了各种限制，因此，一般来说，量化感知训练比纯浮点模型的训练更加困难。量化感知训练工具的目标是降低量化感知训练的难度，降低量化模型部署的工程难\n度。\n\n\n流程和示例#\n\n虽然量化感知训练工具不强制要求从一个预训练的浮点模型开始，但是，经验表明，通常从预训练的高精度浮点模型开始量化感知训练能大大降低量化感知训练的难度。\n\n\n\n注意\n\n由于部署平台的底层限制，QAT 模型无法完全代表最终上板精度，请务必监控 quantized 模型精度，确保 quantized\n模型精度正常，否则可能出现模型上板掉点问题。\n\n由上述示例代码可以看到，与传统的纯浮点模型训练相比，量化感知训练多了两个步骤：\n\n 1. prepare。\n 2. 加载 Calibration 模型参数。\n\n\nprepare#\n\n这一步骤的目标是对浮点网络进行变换，插入伪量化节点。\n\n\n加载 Calibration 模型参数#\n\n通过加载 Calibration 得到的伪量化参数，来获得一个较好的初始化。\n\n\n训练迭代#\n\n至此，完成了伪量化模型的搭建和参数的初始化，然后就可以进行常规的训练迭代和模型参数更新，并且监控 quantized 模型精度。\n\n\n伪量化算子#\n\n量化感知训练和传统的浮点模型的训练主要区别在于插入了伪量化算子，并且，不同量化感知训练算法也是通过伪量化算子来体现的，因此，这里介绍一下伪量化算子。\n\n注解\n\n由于 BPU 只支持对称量化，因此，这里以对称量化为例介绍。\n\n\n伪量化过程#\n\n以 int8 量化感知训练为例，一般来说，伪量化算子的计算过程如下：\n\nfake_quant_x = clip(round(x / scale)，-128, 127) * scale\n\n和 Conv2d 通过训练来优化 weight, bias 参数类似，伪量化算子要通过训练来优化 scale 参数。 然而，由于 round\n作为阶梯函数，其梯度为 0，从而导致了伪量化算子无法直接通过梯度反向传播的方式进行训练。 解决这一问题，通常有两种方案：基于统计的方法和基于学习的方法。\n\n\n基于统计的方法#\n\n量化的目标是把 Tensor 中的浮点数通过 scale 参数均匀地映射到 int8 表示的 [-128, 127] 的范围上。既然是均匀映射，那么很容易得到\nscale 的计算方法：\n\n\n\n由于 Tensor 中数据分布不均匀以及外点问题，又衍生了不同的计算 xmin 和 xmax 的方法。可以参考 MinMaxObserver 等。\n\n在工具中的使用方法请参考 QConfig 详解。\n\n\n基于学习的方法#\n\n虽然 round 的梯度为 0，研究者通过实验发现，在该场景下，如果直接设置其梯度为 1 也可以使得模型收敛到预期的精度。\n\n\n\n在工具中的使用方法请参考 FakeQuantize 的定义。\n\n如您有兴趣进一步了解，可以参考如下论文：Learned Step Size Quantization。","routePath":"/guide/plugin/user_guide/qat_guide","lang":"zh","toc":[{"text":"流程和示例","id":"流程和示例","depth":2,"charIndex":296},{"text":"prepare","id":"prepare","depth":3,"charIndex":556},{"text":"加载 Calibration 模型参数","id":"加载-calibration-模型参数","depth":3,"charIndex":595},{"text":"训练迭代","id":"训练迭代","depth":3,"charIndex":658},{"text":"伪量化算子","id":"伪量化算子","depth":2,"charIndex":732},{"text":"伪量化过程","id":"伪量化过程","depth":3,"charIndex":853},{"text":"基于统计的方法","id":"基于统计的方法","depth":3,"charIndex":1104},{"text":"基于学习的方法","id":"基于学习的方法","depth":3,"charIndex":1312}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":436,"title":"QConfig 详解","content":"#\n\n\nQConfig 的定义#\n\n模型的量化方式由 qconfig 决定，在准备 qat / calibration 模型之前，需要先给模型设置 qconfig。\n\n注意\n\n因历史原因，Plugin 中有不同 qconfig 的定义和用法，早期版本的 qconfig 将在不久的将来被废弃，我们只推荐您使用此文档中介绍的\nqconfig 用法。\n\n一个 qconfig 对象可以设置 input / weight / output 三个关键字，分别表示算子输入/权重/输出的量化配置，prepare\n模型时会根据这些配置决定是否要在对应位置插入 FakeQuantize / FakeCast 节点，None 表示不插入任何节点。\n\n\n\n\nFakeQuantize 的定义#\n\nFakeQuantize\n是伪量化节点，会对输入进行量化反量化操作，插入伪量化可以在浮点模型的前向中模拟量化产生的误差。horizon_plugin_pytorch 支持\nFakeQuantize / PACTFakeQuantize / _LearnableFakeQuantize 三种伪量化，我们只推荐您使用基于统计的\nFakeQuantize，可以满足绝大部分需求。标准流程不对 PACTFakeQuantize 和 _LearnableFakeQuantize\n两种方法做详细说明，如果一定有需求，请在阅读相关论文后再使用。\n\n\n\n可以调用 FakeQuantize 的 with_args 方法得到构造器，并按上一节的代码示例用它构造 qconfig。with_args 的参数包括\nFakeQuantize 和 observer 支持配置的参数，理论上可以配置所有 FakeQuantize 和 observer 类 init\n方法声明中的参数，但为了屏蔽无关紧要的细节，我们只推荐您配置 observer 相关参数。\n\n不同 observer 的参数不同，下面列出常用 observer 构造 FakeQuantize 的例子，其他 observer 的具体用法见校准章节。\n\n\n\n\nFakeCast 的定义#\n\nFakeCast 是伪转换节点，会将输入转换为 float32 类型，如果数据类型是 float16，那么还会在中间模拟转 float16\n产生的截断误差，此节点主要用于标志需要浮点计算的算子。\n\n使用 FakeCast 构造 qconfig 的方法与 FakeQuantize 类似，但只有 dtype 一个参数。\n\n\n\n\n构造 QConfig#\n\n 1. 按照上文介绍的方法，直接构造 QConfig 对象。这种方法比较灵活，可以配置任何可配置的参数，需要您对 QConfig 有一定的理解。\n\n 2. 使用 get_qconfig 接口。此接口较直接构造 QConfig 对象的方法更简单易用，但不够灵活，高级用法和需求无法使用此接口实现。\n\n\n\n\n使用 QConfig#\n\n 1. 直接设置 qconfig 属性。此方法优先级最高，其余方法不会覆盖直接设置的 qconfig。\n\n\n\n 2. qconfig 模板。在 prepare 接口上指定 qconfig setter 和 example_inputs，自动为模型设置 qconfig。\n\n\n\n\nQConfig 模板#\n\nqconfig 模板基于 subclass trace 方案感知模型的图结构，并按设定的规则自动设置 qconfig，是我们最推荐的设置 qconfig\n方法。用法如下：\n\n\n\n注意\n\n模板的优先级低于直接给模型设置 qconfig 属性，如果模型在 prepare 之前已经使用 model.qconfig = xxx\n进行了配置，那么模板将不会生效。如果没有特殊需求，我们不推荐将两者混合使用，这很容易引发低级错误。绝大多数情况下，使用模板和 model.qconfig =\nxxx 两种设置方式中的一种即可满足需求。\n\n模板可分为三类：\n\n 1. 固定模板。固定模板中 calibration / qat / qat_fixed_act_scale 区别在于使用的 observer 类型和\n    scale 更新逻辑，分别用于校准，qat 训练，固定 activation scale qat 训练。default 模板 (\n    default_calibration_qconfig_setter / default_qat_qconfig_setter /\n    default_qat_fixed_act_qconfig_setter )\n    会做三件事：首先，将可以设置的高精度输出都设置上，对于不支持高精度的输出将给出提示；然后，从 grid sample 算子的 grid\n    输入向前搜索，直到出现第一个 gemm 类算子或者 QuantStub，将中间的所有算子都设置为 int16。根据经验这里的 grid\n    一般表达范围较宽，int8 有较大可能不满足精度需求；最后，将其余算子设置为 int8。int16 模板 (\n    qat_8bit_weight_16bit_act_qconfig_setter /\n    qat_8bit_weight_16bit_fixed_act_qconfig_setter /\n    calibration_8bit_weight_16bit_act_qconfig_setter )\n    会做两件事：首先，将可以设置的高精度输出都设置上，对于不支持高精度的输出将给出提示；其次，将其余算子设置为 int16。\n\n\n\n 2. 敏感度模板。敏感度模板有\n    sensitive_op_calibration_8bit_weight_16bit_act_qconfig_setter，sensitive_op_q\n    at_8bit_weight_16bit_act_qconfig_setter，sensitive_op_qat_8bit_weight_16bit_f\n    ixed_act_qconfig_setter，三者的区别和固定模板中三者的区别一致，也是分别用于校准，qat 训练，固定 activation\n    scale qat 训练。 敏感度模板的第一个输入是精度 debug 工具产生的敏感度结果，第二个参数可以指定 ratio 或\n    topk，敏感度模板会将量化敏感度最高的 topk 个算子设置为\n    int16。搭配固定模板，可以轻松实现混合精度调优。若模型有多个输出，每个输出都会产生一个敏感度表，您可以设置多个敏感度模版。\n\n\n\n 3. 自定义模板。自定义模板只有 ModuleNameQconfigSetter，需要传入模块名和对应 qconfig 的字典，一般用于设置 fixed\n    scale 等特殊需求，可以和固定模板，敏感度模板搭配使用。\n\n","routePath":"/guide/plugin/user_guide/qconfig","lang":"zh","toc":[{"text":"QConfig 的定义","id":"qconfig-的定义","depth":2,"charIndex":3},{"text":"FakeQuantize 的定义","id":"fakequantize-的定义","depth":2,"charIndex":320},{"text":"FakeCast 的定义","id":"fakecast-的定义","depth":2,"charIndex":889},{"text":"构造 QConfig","id":"构造-qconfig","depth":2,"charIndex":1067},{"text":"使用 QConfig","id":"使用-qconfig","depth":2,"charIndex":1232},{"text":"QConfig 模板","id":"qconfig-模板","depth":2,"charIndex":1384}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":437,"title":"精度调优工具使用指南","content":"#\n\n由于浮点转定点过程中存在误差，当您在使用量化训练工具时，难免会碰到量化模型精度掉点问题。通常来说，造成掉点的原因有：\n\n 1. 原有浮点模型不利于量化，如存在共享 op 或共享结构。\n\n 2. QAT 网络结构或配置异常，如模型中存在没有 fuse 的 pattern，没有设置高精度输出等。\n\n 3. 某些算子对量化比较敏感，该算子的量化误差在前向传播过程中逐层累积，最终导致模型输出误差较大。\n\n针对上述情况，量化训练工具提供了精度调优工具来帮助您快速定位并解决精度问题，主要包括如下模块：\n\n * 模型结构检查：检查模型中是否存在共享 op、没有 fuse 的 pattern 或者不符合预期的量化配置。\n\n * QuantAnalysis 类：自动比对分析两个模型，定位到量化模型中异常算子或者量化敏感 op。\n\n * ModelProfiler 类 和 HbirModelProfiler 类：获得模型中每一个 op\n   的数值特征信息，如输入输出的最大最小值等。这两个类的功能完全一致，区别在于 HbirModelProfiler 仅接受 qat hbir\n   模型作为输入。通常您无需手动调用该模块，可以直接通过 QuantAnalysis.run 来获得两个模型的数值信息。\n\n\n快速上手#\n\n当碰到量化模型精度掉点问题时，我们推荐按照如下的流程使用精度调优工具。\n\n 1. 检查模型中是否存在不利于量化的结构或者异常配置。\n\n 2. 使用 QuantAnalysis 模块进行分析，具体步骤如下：\n    \n    1). 找到一个 bad case 作为模型的输入。bad case 是指基准模型和待分析模型输出相差最大的那个输入。\n    \n    2). 进行量化敏感度分析，目前的经验是 L1 敏感度排序前 n 个通常为量化敏感 op（不同的模型 n\n    的数值不一样，暂无自动确定的方法，需要手动尝试，如前 10 个，20 个...）。将量化敏感 op 设置高精度量化（如 int16\n    量化），重新进行量化流程。\n    \n    3). 或者逐层比较两个模型的输入输出等信息，检查是否存在数据范围过大或者 scale 不合理等量化异常的 op，如某些具有物理含义的 op 应设置固定\n    scale。\n\n整体的流程图如下：\n\n一个完整的例子如下：\n\n\n\n\nAPI Reference#\n\n\n模型结构检查 #\n\n\n\n检查 calibration/qat 模型中是否存在不利于量化的结构以及量化 qconfig 配置是否符合预期。\n\n参数\n\n * model: 待检查模型。\n\n * example_inputs: 模型输入。\n\n * save_results: 是否将检查结果保存到 txt 文件。默认 False。\n\n * out_dir: 结果文件 'model_check_result.txt' 的保存路径。默认空，保存到当前路径下。\n\n输出\n\n * 屏幕输出：检查出的异常层。\n\n * model_check_result.txt：在 save_results = True 时生成。主要由 5 部分组成：\n   \n   1). 未 fuse 的 pattern。\n   \n   2). 每个 module 的调用次数。正常每个 op 仅调用 1 次，0 表示未被调用，超过 1\n   次则表示被共享了多次。未调用或者共享多次的会有异常提示。\n   \n   3). 每个 op 输出的 qconfig 配置。\n   \n   4). 每个 op weight（如果有的话）的 qconfig 配置。\n   \n   5). 异常 qconfig 提示（如果有的话）。\n\n\n\n注解\n\nprepare 接口已集成该检查。请您关注此接口输出的检查结果，并根据检查结果对模型做针对性的调整。\n\n\nQuantAnalysis 类 #\n\nQuantAnalysis 类可以自动寻找两个模型输出最大的 bad case，并以此作为输入，逐层比较两个模型的输出。此外，QuantAnalysis\n类还提供计算敏感度功能，您可以尝试将敏感度排名 topk 的节点设置高精度，如 int16 量化，来提升量化模型精度。\n\n\n\n参数\n\n * baseline_model: 基准模型（高精度）。\n\n * analysis_model：待分析的模型（精度掉点）。\n\n * analysis_model_type: 待分析的模型类型。支持输入：\n   \n   * fake_quant：待分析的模型可以是精度掉点的 calibration 模型，此时基准模型可以是原始浮点模型或者一个精度达标的\n     int8/int16 混合配置的 calibration 模型。\n\n * device_ids：对比分析时模型运行的 GPU 设备 index。\n\n * post_process：模型后处理。\n\n * out_dir：指定比较结果的输出目录。\n\n注意\n\n由于 QAT 训练会改变模型 weight 分布，通常情况下，我们不建议您将浮点或 calibration 模型和 qat 模型做对比。\n\n该类中各个 method 如下：\n\nauto_find_bad_case#\n\n\n\n自动寻找导致两个模型输出最差的 badcase。\n\n参数\n\n * data_generator：dataloader 或者一个自定义的迭代器，每次迭代产生一个数据。\n\n * num_steps：迭代 steps 次数。\n\n * metric：指定何种 metric 作为 badcase 的 metric。默认使用 L1 最差的结果。支持\n   Cosine/MSE/L1/KL/SQNR/custom。若为 custom，表示使用自定义的 metric 计算方法，此时\n   custom_metric_func 和 custom_metric_order_seq 两个参数必须不为 None。\n\n * device：指定模型运行 device。\n\n * custom_metric_func：自定义模型输出比较函数。\n\n * custom_metric_order_seq：自定义模型输出比较函数的排序规则，仅支持\n   \"ascending\"/\"descending\"，表示升序/降序。\n\n * cached_attrs：作为模型输入的某些属性。通常在时序模型中使用，如运行第二帧时，将第一帧的某些结果作为输入。\n\n注解\n\nauto_find_bad_case 函数遍历传入的 data_generator，运行基准模型和待分析模型，计算每个输出在\nCosine/MSE/L1/KL/SQNR 5 种 metric 上的比较结果，并找到在各个 metric 上比较结果最差的 badcase 输入。\n\n输出\n\n * badcase.txt：包含 3 部分内容\n   \n   * 各个输出在不同 metric 下找到的 badcase 输入。（在 data_generator 中的索引值）。\n   \n   * 各个输出在不同 metric 下找到的 badcase 输入对应的 metric 最差值。\n   \n   * 相同 metric 指标下，输出最差的 badcase 输入。\n   \n   \n\n * badcase.pt：保存的输入数据，为参数 metric 指定的 metric 指标最差的输入。作为 run 函数的默认输入。\n\nset_bad_case#\n\n\n\n手动设置 badcase。\n\n注意\n\n通常情况下，我们建议您通过 auto_find_bad_case 函数寻找 badcase。若手动设置的 badcase 非真正的\nbadcase，分析工具很难找出量化敏感层。\n\n参数\n\n * data: badcase 输入。\n\n * baseline_model_cached_attr：基准模型的 cached_attrs。\n\n * analysis_model_cached_attr：待分析模型的 cached_attrs。\n\nload_bad_case#\n\n\n\n从指定的文件中加载 badcase。\n\n参数\n\n * filename：指定的文件路径。默认从初始化时指定的 out_dir 目录中加载 auto_find_bad_case 函数保存的 badcase\n   相关文件。\n\nsave_bad_case#\n\n\n\n将 badcase 保存到文件。\n\n注意\n\n和 set_bad_case 搭配使用。通常情况下，您无需手动调用此函数。\n\nset_model_profiler_dir#\n\n\n\n手动指定 model_profiler 的输出保存路径。\n\n某些情况下，在 QuantAnalysis 初始化之前，ModelProfiler 就已定义并运行，此时可以直接指定已有的 ModelProfiler\n路径，跳过 QuantAnalysis 的 run 步骤，直接比较两个模型的输出。\n\n参数\n\n * baseline_model_profiler_path：基准模型的 profiler 路径。\n\n * analysis_model_profiler_path：待分析模型的 profiler 路径。\n\nrun#\n\n\n\n运行两个模型并分别保存模型中每一层的结果。\n\n参数\n\n * device：模型运行的 device。\n\n * index：输入数据在 data_generator 中的 index。\n\n注意\n\n仅支持 auto_find_bad_case 函数找到并在 badcase.txt 中显示的 index 作为参数输入。\n\ncompare_per_layer#\n\n\n\n比较两个模型中每一层的结果。\n\n参数\n\n * prefixes：指定 op 名字的前缀。\n\n * types：op 类型。\n\n注解\n\n通常您无需指定 prefixes 和 types 参数。若您基于一些先验经验，想跳过某些确定的、量化影响较小 op\n的比较，或想节省时间，您可以通过两个参数，指定比较某些 op 或者某类 op。\n\n输出\n\n * abnormal_layer_advisor.txt: 所有异常层，包括数据范围过大/包含 inf 或 NaN/输出没有高精度等情况。\n\n * profiler.html: 可视化展示所有 metric 指标及模型中每一层的数据范围 diff。\n\n\n\n * compare_per_layer_out.txt: 以表格的形式展示模型中每层 layer 的具体信息，包括各种指标、数据范围、量化 dtype\n   等。从左到右每一列分别表示：\n   \n   * Index：op index。\n   \n   * mod_name：该 op 名字，若 op 为 module 类型，则显示该 module 在模型中的 prefix name，若为 function\n     类型，则不显示。\n   \n   * base_op_type：基准模型中该 op 的 type，可能是 module 类型或者 function 名称。\n   \n   * analy_op_type：待分析模型中该 op 的 type，可能是 module 类型或者 function 名称。\n   \n   * Shape：该 op 输出的 shape。\n   \n   * quant_dtype：该 op 输出的量化类型。\n   \n   * Qscale：该 op 输出的量化 scale。\n   \n   * Cosine：该 op 在两个模型中输出的余弦相似度。\n   \n   * L1：该 op 在两个模型中输出的 L1 距离。\n   \n   * Atol：该 op 在两个模型中输出的绝对误差。\n   \n   * max_qscale_diff：该 op 在两个模型中输出最大相差了几个 scale。\n   \n   * base_model_min：基准模型中该 op 输出的最小值。\n   \n   * analy_model_min：待分析模型中该 op 输出的最小值。\n   \n   * base_model_max：基准模型中该 op 输出的最大值。\n   \n   * analy_model_max：待分析模型中该 op 输出的最大值。\n   \n   * base_model_mean：基准模型中该 op 输出的平均值。\n   \n   * analy_model_mean：待分析模型中该 op 输出的平均值。\n     \n     \n\n * compare_per_layer_out.csv: 以 csv 的格式展示每层的具体信息。内容和 compare_per_layer_out.txt\n   完全一致，csv 文件的存储格式方便您通过 excel 等软件打开分析。\n\nsensitivity#\n\n\n\n模型中各个节点的敏感度排序。适用于 float 转 calibration 的精度掉点问题。\n\n注意\n\nsensitivity 函数不支持计算 hbir 模型的敏感度。\n\n参数\n\n * device：指定模型运行的 device。\n\n * metric：相似度排序的 metric，默认 L1，支持 Cosine/MSE/L1/KL/SQNR。\n\n * reserve：是否反序打印敏感度节点，以支持将某些 int16 算子退回 int8 来提升上板性能。\n\n输出\n\n * sensitive_ops.txt。文件中按照量化敏感度从高到低的顺序排列 op。从左到右每一列分别表示：\n   \n   * op_name：op 名字。\n   \n   * sensitive_type：计算量化敏感的类型，包括三种：\n     \n     * activation：仅量化该 op 输出的量化敏感度。\n     \n     * weight：仅量化该 op 权重的量化敏感度。\n   \n   * op_type：op 类型。\n   \n   * metric：计算敏感度的指标。按照敏感度从高到低的顺序排序。支持 Cosine/L1/MSE/KL/SQNR 五种指标。默认使用 L1。\n     \n     * L1：取值范围 [0, $+\\infty$]，数值越大则该 op 对量化越敏感（从大到小排序）。\n     \n     * Cosine：取值范围 [0,1]，越接近 0 则该 op 对量化越敏感（从小到大排序）。\n     \n     * MSE：取值范围 [0, $+\\infty$]，数值越大则该 op 对量化越敏感（从大到小排序）。\n     \n     * KL：取值范围 [0, $+\\infty$]，数值越大则该 op 对量化越敏感（从大到小排序）。\n     \n     * SQNR：取值范围 [0, $+\\infty$]，数值越小则该 op 对量化越敏感（从小到大排序）。\n   \n   * quant_dtype：该 op 输出的量化 dtype。通常为 qint8/qint16。\n   \n   * flops：该 op 的计算量以及在整个模型计算量中的占比。\n\n * sensitive_ops.pt。使用 torch.save\n   保存的敏感度排序的列表，方便您后续加载使用。若存在多个输出，则每个输出都会生成一个敏感度表。列表格式见返回值部分说明。\n\n返回值\n\n敏感度 List，List 中每个元素都是记录一个 op 敏感度信息的子 list。子 List 中从左到右每一项分别为 [op_name,\nsensitive_type, op_type, metric, quant_dtype, flops]。\n\n整个 List 示例如下：\n\n\n\n您可以将量化敏感度排名前 n 的 op 配置高精度（如 int16）来尝试提升量化模型精度。\n\n\n\nclean#\n\n\n\n清除中间结果。仅保留比较结果等文件。\n\n\nModelProfiler 类 #\n\n统计模型 forward 过程中，每一层算子的输入输出等信息。\n\n\n\n参数\n\n * model: 需要统计的模型。\n\n * out_dir: 相关文件保存的路径。\n\n注解\n\n该类仅支持通过 with 语句的方式使用。\n\n\n\n该类中其中各个 method 如下：\n\nget_info_manager#\n\n\n\n获得管理每个 op 信息的结构体。\n\n返回值\n\n管理存储的每个 op 信息的结构体 OpRunningInfoManager。其中两个重要的接口如下：\n\ntable#\n\n\n\n在一个表格中展示单个模型统计量。存储到 statistic.txt 文件中。\n\n参数\n\n * out_dir：statistic.txt 文件的存储路径，默认 None，存储到 self.out_dir。\n\n * prefixes：需要统计的模型中 op 的 prefixes。默认统计所有 op。\n\n * types：需要统计的模型中 op 的 type。默认统计所有 op。\n\n * with_stack: 是否显示每个 op 在代码中对应的位置。\n\n输出\n\nstatistic.txt 文件，从左到右每一列分别为：\n\n * Index：op index。\n\n * Op Name：op type，module 类名或者 function 名。\n\n * Mod Name：若是 module 类，则显示该 module 在模型中的 prefix name；若是 function 类型，则显示该\n   function 所在的 module prefix name。\n\n * Attr：input/output/weight/bias。\n\n * Dtype：tensor 的数据类型。\n\n * Scale：tensor 的 scale。\n\n * Min：当前 tensor 的最小值。\n\n * Max：当前 tensor 的最大值。\n\n * Mean：当前 tensor 的平均值。\n\n * Var：当前 tensor 中数值的方差。\n\n * Shape：tensor shape。\n\n\n\ntensorboard#\n\n\n\n在 tensorboard 中显示每一层输入输出直方图。\n\n参数\n\n * out_dir：tensorboard 相关文件保目录。默认保存到 self.out_dir/tensorboard 目录下。\n\n * prefixes：需要统计的模型中 op 的 prefixes。默认统计所有。\n\n * types：需要统计的模型中 op 的 type。默认统计所有。\n\n * force_per_channel：是否以 per_channel 量化的方式展示直方图。\n\n输出\n\ntensorboard 文件，打开后截图如下：\n\n\n\n\nHbirModelProfiler 类 #\n\n该类的功能和使用方式与 ModelProfiler 类完全一致。请参考 ModelProfiler 类 进行使用。\n\n注意\n\n由于 hbir 模型的特殊格式，qat hbir 模型在 forward 时需添加索引 0。\n\n","routePath":"/guide/plugin/user_guide/quant_analysis","lang":"zh","toc":[{"text":"快速上手","id":"快速上手","depth":2,"charIndex":552},{"text":"API Reference","id":"api-reference","depth":2,"charIndex":1005},{"text":"模型结构检查","id":"模型结构检查","depth":3,"charIndex":-1},{"text":"QuantAnalysis 类","id":"quantanalysis-类","depth":3,"charIndex":-1},{"text":"auto_find_bad_case","id":"auto_find_bad_case","depth":4,"charIndex":2184},{"text":"set_bad_case","id":"set_bad_case","depth":4,"charIndex":3130},{"text":"load_bad_case","id":"load_bad_case","depth":4,"charIndex":3385},{"text":"save_bad_case","id":"save_bad_case","depth":4,"charIndex":3515},{"text":"set_model_profiler_dir","id":"set_model_profiler_dir","depth":4,"charIndex":3594},{"text":"run","id":"run","depth":4,"charIndex":3879},{"text":"compare_per_layer","id":"compare_per_layer","depth":4,"charIndex":4046},{"text":"sensitivity","id":"sensitivity","depth":4,"charIndex":5375},{"text":"clean","id":"clean","depth":4,"charIndex":6632},{"text":"ModelProfiler 类","id":"modelprofiler-类","depth":3,"charIndex":-1},{"text":"get_info_manager","id":"get_info_manager","depth":4,"charIndex":6814},{"text":"HbirModelProfiler 类","id":"hbirmodelprofiler-类","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":438,"title":"认识 Open Explorer","content":"#\n\n\n什么是OE#\n\nOE是Open Explorer的缩写简称，中文名为天工开物（以下简称OE），它是基于地平线自研计算平台打造的全生命周期开发平台，\n主要包括模型编译优化工具集、算法仓库和应用开发SDK三大功能模块。基于这三大功能模块开发的应用参考解决方案，为智能驾驶、智能物联网等行业方案提供了案例支撑。\n\n\n\n 1. 模型编译优化工具集：聚焦于智能业务场景，包括完成算法模型转换与编译流程中所涉及到的一系列软件工具集，提供模型量化、优化、编译、调试等功能。\n 2. 算法仓库：贴合时下智驾领域的基础算法结构，和重点业务算法技术点，地平线提供了丰富的开源最佳实践，赋能地平线合作伙伴更快、更省地开发出自己的智能产品。\n 3. 应用开发SDK：提供丰富的基础接口与工具，支撑业务算法在征程平台的部署，可以完整支撑客户在仿真和物理环境下的部署全流程。\n\nOE可以为地平线合作伙伴提供丰富多样的算法资源、灵活高效的开发工具和简单易用的开发框架。OE的特色和优势可以概括为以下四个方面：\n\n\n\n为了方便将各种解决方案部署到地平线一系列开发板上，我们提供了集开发板系统镜像、开发环境部署、应用参考解决方案示例代码以及用户手册等文件于一体的全量开发包，称之为\nOE包。 在您获取到OE包后，可以先按照以下步骤对OE进行了解。\n\n 1. 先参考 发布物内容，了解发布包的目录结构。\n\n 2. 再参考 环境部署 章节进行开发环境和运行环境的部署。\n\n 3. 接着通过参考 训练后量化（PTQ） 、 量化感知训练（QAT） 以及 模型推理应用开发指导 完成模型转换与部署的全流程。\n\n更多更全的OE包使用教程，欢迎参考下方指导手册进行了解，相信地平线的OE包可以让您开发更高效，部署更简便。\n\n\n发布物内容 #\n\n\npackage#\n\npackage 目录下包含了发布物运行的一些基础库和组件。\n\n * package.board\n   \n   package.board 文件夹下为板端可执行程序。\n   \n   * hrt_model_exec 是一个模型执行工具 ，可直接在 开发板上 进行评测模型的推理性能、获取模型信息。工具分别提供了模型推理\n     infer、模型性能分析 perf 和查看模型信息 model_info 三类功能。\n   * install.sh 是一键安装脚本，可以一键将hrt工具安装到指定的开发板。\n\n * package.host\n   \n   package.host 文件夹下为发布物在x86开发环境下的环境依赖和工具依赖等。通过执行该目录下的脚本 install.sh\n   即可在开发机上安装程序运行的所有环境和工具依赖。 通过执行该目录下的脚本 resolve.sh 即可下载交叉编译工具、torch等依赖。\n\n\nsamples#\n\nsamples 下包含了 ai_toolchain、model_zoo和ucp_tutorial。\n\n * ai_toolchain提供了一些模型算法的一系列示例。（其中horizon_model_train_samples为浮点模型训练框架示例，horizon\n   _model_convert_sample为浮点模型转定点模型的转换示例，\n   model_zoo是一个模型库，用于放置工具链示例模型编译的源模型和runtime模型。）\n\n * model_zoo，软链接指向ai_toolchain文件夹下的model_zoo路径。\n\n * ucp_tutorial，统一计算平台UCP的示例包，提供UCP必要的依赖以及相关示例。\n\n\nresolve_all.sh#\n\n用于自动下载OE包内所有可下载的依赖项的脚本。\n\n运行该脚本将依次进行如下内容的下载：\n\n 1. 执行 package/host 路径下的 resolve.sh ，下载交叉编译工具、torch等依赖。\n\n 2. 执行 samples/ai_toolchain/model_zoo/runtime/ai_benchmark 路径下的\n    resolve_ai_benchmark_ptq.sh ，下载上板使用的hbm模型。\n\n 3. 执行 samples/ai_toolchain/model_zoo/runtime/ai_benchmark 路径下的\n    resolve_ai_benchmark_qat.sh ，下载上板使用的hbm模型。\n\n 4. 执行 samples/ai_toolchain/model_zoo/runtime/basic_samples 路径下的\n    resolve_runtime_sample.sh ，下载对应示例上板使用的hbm模型。\n\n 5. 执行 samples/ucp_tutorial/dnn/basic_samples/code/ 路径下的 resolve.sh\n    ，下载对应示例所需的数据集等文件。\n\n 6. 执行 samples/ai_toolchain/horizon_model_convert_sample 文件夹下所有的 00_init.sh\n    ，下载示例对应的校准数据集以及原始模型。\n\n\nrun_docker.sh#\n\n在评测数据集和OE包所需docker下载完成的情况下，可以使用命令 sh run_docker.sh {数据集路径} 自动挂载OE包并启动docker。","routePath":"/guide/preface/learn_oe","lang":"zh","toc":[{"text":"什么是OE","id":"什么是oe","depth":2,"charIndex":3},{"text":"发布物内容","id":"发布物内容","depth":2,"charIndex":-1},{"text":"package","id":"package","depth":3,"charIndex":752},{"text":"samples","id":"samples","depth":3,"charIndex":1177},{"text":"resolve_all.sh","id":"resolve_allsh","depth":3,"charIndex":1509},{"text":"run_docker.sh","id":"run_dockersh","depth":3,"charIndex":2161}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":439,"title":"工具链概览","content":"#\n\n地平线J6算法工具链（以下简称工具链）是一套完整的边缘计算平台算法落地解决方案，可以帮助您把浮点模型量化为定点模型，并在地平线计算平台上快速部署自研算法模型。\n\n目前在GPU上训练的模型大部分都是浮点模型，即参数使用的是float类型存储。地平线BPU架构的计算平台使用的是int8的计算精度（业内计算平台的通用精度），能\n运行定点量化模型。\n\n从训练出的浮点精度转为定点模型的过程，我们称之为量化。同时模型量化后能够有效减少模型大小，加速深度学习推理的速度，因此也在学术界和工业界被广泛研究和应用。\n\n依据是否要对量化后的参数进行调整，我们可以将量化方法分为训练后量化（PTQ）和量化感知训练（QAT）。这两种方法的操作区别如下图所示（图左为PTQ，图右为QAT\n）：\n\n训练后量化PTQ是使用一批校准数据对训练好的模型进行校准，将训练过的FP32模型直接转换为定点计算的模型，过程中无需对原始模型进行任何训练，只需要对几个超参数进\n行调整就可以完成量化过程，\n且过程简单快速，无需训练，因此该方法已被广泛地应用于大量的端侧和云侧部署场景。我们优先推荐您尝试PTQ方法来查看是否满足您的部署精度和性能要求。\n有关PTQ方案的详细信息请阅读 训练后量化（PTQ） 章节内容。\n\n量化感知训练QAT是将训练过的模型量化后又再进行重训练。由于定点数值无法用于反向梯度计算，实际操作过程是在某些OP前插入伪量化节点（fake\nquantization\nnodes），用于在训练时获取流经该OP的数据的截断值，便于在部署量化模型时对节点进行量化时进行使用。我们需要在训练中通过不断优化精度来获取最佳的量化参数。由于\n它需要对模型进行训练，因此对操作人员技术要求较高。有关QAT方案的详细信息请阅读 量化感知训练 章节内容。\n\n工具链由PTQ、QAT和嵌入式编译等部分组成，工具链组成示意图如下：\n\nRuntime SDK提供了运行库支持，运行库包含arm和x86两个部分，分别用于在地平线计算平台和X86仿真平台执行模型。 有关嵌入式应用开发请阅读\n模型推理应用开发指导 章节内容。\n\n此外，工具链提供了丰富的开发工具、示例以及内置了大量算法模型的模型发布物，以便于您上手理解，并提高开发效率。\n\n工具链的整体使用流程如下图所示，地平线推荐您先尝试PTQ方式来查看是否满足您的部署精度和性能要求。","routePath":"/guide/preface/toolchain_overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":440,"title":"数据归一化处理参数配置说明","content":"#\n\n\n参数说明解析#\n\nmean_value 参数讲解\n\n * 参数作用：此参数表示指定预处理方法的图像减去的均值。\n\n * 参数说明：\n   \n   * 当只有一个输入节点时，仅需要配置一个数值，表示所有通道都减去这个均值。\n   \n   * 当有多个节点时，提供与通道数量一致的数值（这些数值以空格分隔开），表示每个通道都会减去不同的均值。\n\n注意\n\n如果存在某个节点不需要 mean 处理，则为该节点配置 'None'。\n\nscale_value 参数讲解\n\n * 参数作用：此参数表示指定预处理方法的数值scale系数。\n\n * 参数说明：\n   \n   * 当只有一个输入节点时，仅需要配置一个数值，表示所有通道都乘以这个系数。\n   \n   * 当有多个节点时，提供与通道数量一致的数值（这些数值以空格分隔开），表示每个通道都会乘以不同的系数。\n\n注意\n\n如果存在某个节点不需要 scale 处理，则为该节点配置 'None'。\n\nstd_value 参数讲解\n\n * 参数作用：此参数表示指定预处理方法的数值std系数。\n\n * 参数说明：\n   \n   * 当只有一个输入节点时，仅需要配置一个数值，表示所有通道都除以这个系数。\n   \n   * 当有多个节点时，提供与通道数量一致的数值（这些数值以空格分隔开），表示每个通道都会除以不同的系数。\n\n注意\n\n如果存在某个节点不需要 std 处理，则为该节点配置 'None'。\n\n\n计算公式及示例说明#\n\n下方结合模型训练时的数据标准化处理计算公式为您进行介绍：\n\nyaml文件中的mean和scale参数与训练时的mean、std需要进行换算。\n\n预处理节点中数据标准化操作的计算方式为： $norm_data = ( data - mean ) * scale$ 。\n\n以yolov3为例，其训练时的预处理代码为：\n\n\n\n则计算公式为：$norm_data= (\\frac-mean) * \\frac$。\n\n改写为预处理节点的计算方式： $norm_data= (\\frac-mean) * \\frac =(data-255*mean) * \\frac$，\n\n则： $mean_yaml = 255*mean、scale_yaml = \\frac$ 。","routePath":"/guide/ptq/ptq_appendix/normalize_introduction","lang":"zh","toc":[{"text":"参数说明解析","id":"参数说明解析","depth":2,"charIndex":3},{"text":"计算公式及示例说明","id":"计算公式及示例说明","depth":2,"charIndex":624}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":441,"title":"图片处理transformer说明","content":"#\n\n本章节将对您在进行图片缩放裁剪时使用的各个transformer的概念及参数进行说明，并为您提供参考使用示例，方便您进行tranformer操作。\n\n在文档内容开始阅读前，以下内容请您注意：\n\n注意\n\n图像数据为三维数据，但地平线提供的transformer默认第一维为N维，会对数据以第一维拆分循环处理，所以如果您需要对图像做处理，请提供四维数据。\n\n\nAddTransformer#\n\n说明：\n\n对输入图片中的所有像素值做增加value的操作。该transformer会在输出时,将数据格式转为float32。\n\n参数：\n\n * value: 对每个像素做增加的数值, 注意value的取值可以为负数, 如 -128。\n\n使用举例：\n\n\n\n\nMeanTransformer#\n\n说明：\n\n对输入图片中的所有像素值做减去mean_value的操作。\n\n参数：\n\n * means：对每个像素做增加的数值, 注意value的取值可以为负数, 如 -128。\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"], 默认 \"CHW\"。\n\n使用举例：\n\n\n\n\nScaleTransformer#\n\n说明：\n\n对输入图片中的所有像素值做乘以data_scale系数的操作。\n\n参数：\n\n * scale_value: 需要乘以的系数，如0.0078125 或者1/128。\n\n使用举例：\n\n\n\n\nNormalizeTransformer#\n\n说明：\n\n用于对输入图片进行归一化的操作。该transformer会在输出时，将数据格式转为float32。\n\n参数：\n\n * std：输入的第一张图片，需要除以的数值。\n\n使用举例：\n\n\n\n\nTransposeTransformer#\n\n说明：\n\n用于做layout转换的操作。\n\n参数：\n\n * order：对输入图片做layout转换后的顺序（顺序与原有的layout顺序有关）。如：HWC的顺序为0,1,2，需要转为CHW时，order为(2,0\n   ,1)。\n\n使用举例：\n\n\n\n\nHWC2CHWTransformer#\n\n说明：\n\n用于将NHWC转换为NCHW的操作。\n\n参数：不涉及。\n\n使用举例：\n\n\n\n\nCHW2HWCTransformer#\n\n说明：\n\n用于将NCHW转换为NHWC的操作。\n\n参数：不涉及。\n\n使用举例：\n\n\n\n\nCenterCropTransformer#\n\n说明：\n\n以直接截断取值的方式从图片中心裁剪出一个正方形的图片的操作。该transformer会在输出时，将数据格式转为float32。当data_type的值为uint\n8时，输出为uint8。\n\n参数：\n\n * crop_size：中心裁剪的正方形的边长size。\n * data_type：输出结果的类型，取值范围为[\"float\", \"uint8\"]。\n\n使用举例：\n\n\n\n\nPILCenterCropTransformer#\n\n说明：\n\n使用PIL的方式从图片中心裁剪出一个正方形的图片的操作。该transformer会在输出时，将数据格式转为float32。\n\n参数：\n\n * size：中心裁剪的正方形的边长size。\n\n使用举例：\n\n\n\n\nLongSideCropTransformer#\n\n说明：\n\n用于做长边裁剪的操作。该 transformer 会在输出时, 将数据格式转为float32。\n\n当宽度比高度的数值大时，会裁剪出一个中心以高度大小为准的正方形，如宽100，高70，裁剪之后大小为70*70。\n\n当高度比宽度的数值大时，会裁剪出一个中心以宽度大小不变，高度为差值的一半+宽度的长方形，如宽70，高100，裁剪之后大小为 70 *（100-70）/2+70\n，即70* 85大小的长方形。\n\n参数：不涉及。\n\n使用举例：\n\n\n\n\nPadResizeTransformer#\n\n说明：\n\n使用填充的方式做图像放大的操作。该 transformer 会在输出时，将数据格式转为float32。\n\n参数：\n\n * target_size：目标大小，值为元组，如(240,240)。\n * pad_value：填充到数组中的值，默认值为127。\n * pad_position：填充的位置，取值范围为[\"boundary\"，\"bottom_right\"]，默认值为 \"boundary\"。\n\n使用举例：\n\n\n\n\nResizeTransformer#\n\n说明：\n\n用于调整图像大小的操作。\n\n参数：\n\n * target_size：目标大小，值为元组，如(240,240)。(240,240)：前一个240代表高度为240，后一个240代表宽度为240。\n\n * mode：图片处理模式，取值范围为(\"skimage\"，\"opencv\")，默认值为 \"skimage\"。\n\n * method：插值的方法，此参数仅在mode为skimage时生效。取值范围为0-5，默认值为1，其中：\n   \n   * 0代表Nearest-neighbor；\n   * 1代表Bi-linear(default)；\n   * 2代表Bi-quadratic;\n   * 3代表Bi-cubic;\n   * 4代表Bi-quartic;\n   * 5代表Bi-quintic。\n\n * data_type：输出的类型，取值范围为(uint8，float)，默认为float类型。当被设置为uint8时，输出类型为uint8，其他情况为flo\n   at32。\n\n * interpolation：插值的方法，此参数仅在mode为opencv时生效。默认为空，取值范围为(opencv的插值方式)，目前interpolati\n   on仅支持为空或opencv中的INTER_CUBIC两种插值方法，当interpolation为空时，默认使用INTER_LINEAR方式。\n   \n   以下为opencv中支持的插值方式及说明（目前未支持的插值方式将在后续迭代中逐步支持）：\n   \n   * INTER_NEAREST，最近邻插值；\n   * INTER_LINEAR，双线性插值，当interpolation为空时，默认使用这种方法。\n   * INTER_CUBIC，双三次插值4x4像素邻域内的双立方插值。\n   * INTER_AREA，使用像素面积关系重采样。它可能是图像抽取的首选方法，因为它可以提供无莫尔条纹的结果。但是当图像被缩放时，它类似于INTER_NE\n     AREST方法。\n   * INTER_LANCZOS4，8x8邻域的Lanczos插值。\n   * INTER_LINEAR_EXACT，位精确双线性插值。\n   * INTER_NEAREST_EXACT，位精确最近邻插值。这将产生与PIL、scikit-image或Matlab中的最近邻方法相同的结果。\n   * INTER_MAX，插值代码的掩码。\n   * WARP_FILL_OUTLIERS，标志，填充所有目标图像像素。如果其中一些对应于源图像中的异常值，则将它们设置为零。\n   * WARP_INVERSE_MAP，标志，逆变换。\n\n使用举例：\n\n\n\n\nPILResizeTransformer#\n\n说明：\n\n使用PIL库做调整图像大小的操作。\n\n参数：\n\n * size：目标大小，值为元组，如(240,240)。\n * interpolation：指定插值的方式，取值范围：(Image.NEAREST，Image.BILINEAR，Image.BICUBIC，Image.\n   LANCZOS)，默认值为Image.BILINEAR。\n   * Image.NEAREST：最近邻采样；\n   * Image.BILINEAR：线性插值；\n   * Image.BICUBIC：三次样条插值；\n   * Image.LANCZOS：高质量下采样滤波器。\n\n使用举例：\n\n\n\n\nShortLongResizeTransformer#\n\n说明：\n\n按照原比例对输入图片进行缩放的操作，新图片的大小与设置的参数有关。操作方式如下：\n\n 1. 先以short_size的大小除以原图片的宽和高里最小值，以这个值为缩放比例系数。\n 2. 当缩放比例系数乘以原图片的宽和高中的最大值，得到的结果大于long_size的数值时，缩放比例系数将变更为long_size除以原图片的宽和高中的最大值\n    。\n 3. 使用opencv中的resize方法，根据上方得到的缩放比例系数重新裁剪图片。\n\n参数：\n\n * short_size：预期裁剪后的短边的长度。\n * long_size：预期裁剪后的长边的长度。\n * include_im：默认值为True，设置为True时，会在返回时除了返回处理后的图片, 还会返回原图片。\n\n使用举例：\n\n\n\n\nPadTransformer#\n\n说明：\n\n通过用目标大小的size值除以输入图片宽或者高里的最大值为系数，然后使用这个系数乘以原有的宽高，resize图片。然后根据新图片的大小，除以size_divis\nor后向上取整后，再乘以size_divisor，为新的宽高，生成新的图片的操作。\n\n参数：\n\n * size_divisor：大小除数 ，默认值为128。\n * target_size：目标大小，默认值为512。\n\n使用举例：\n\n\n\n\nShortSideResizeTransformer#\n\n说明：\n\n根据期望的短边的长度，使用现在的长短边的比例，中心裁剪出新的图片大小的操作。\n\n参数：\n\n * short_size：预期的短边的长度。\n\n * data_type：输出结果的类型，取值范围为(\"float\",\"uint8\")，默认取值\"float32\"，以 float32\n   类型输出，设置为uint8时，输出类型将为uint8。\n\n * interpolation：指定插值的方式，取值范围为 opencv 中采用的插值方式，默认为空。\n   \n   目前interpolation仅支持为空或opencv中的INTER_CUBIC两种插值方法，当interpolation为空时，默认使用INTER_LI\n   NEAR方式。\n   \n   以下为opencv中支持的插值方式及说明（目前未支持的插值方式将在后续迭代中逐步支持）：\n   \n   * INTER_NEAREST，最近邻插值；\n   * INTER_LINEAR，双线性插值，当interpolation为空时，默认使用这种方法。\n   * INTER_CUBIC，双三次插值4x4像素邻域内的双立方插值。\n   * INTER_AREA，使用像素面积关系重采样。它可能是图像抽取的首选方法，因为它可以提供无莫尔条纹的结果。但是当图像被缩放时，它类似于INTER_NE\n     AREST方法。\n   * INTER_LANCZOS4，8x8邻域的Lanczos插值。\n   * INTER_LINEAR_EXACT，位精确双线性插值。\n   * INTER_NEAREST_EXACT，位精确最近邻插值。这将产生与PIL、scikit-image或Matlab中的最近邻方法相同的结果。\n   * INTER_MAX，插值代码的掩码。\n   * WARP_FILL_OUTLIERS，标志，填充所有目标图像像素。如果其中一些对应于源图像中的异常值，则将它们设置为零。\n   * WARP_INVERSE_MAP，标志，逆变换。\n\n使用举例：\n\n\n\n\nPaddedCenterCropTransformer#\n\n说明：\n\n使用填充的方式对图片中心进行裁剪的操作。\n\n注意\n\n仅适用于EfficientNet-lite相关实例模型。\n\n计算方式为：\n\n 1. 计算系数，int((float( image_size ) / ( image_size + crop_pad ))。\n 2. 计算中心size的大小， 系数 * np.minimum( 原始图片的高度，原始图片的宽度 )。\n 3. 根据计算出来的size大小，做中心裁剪。\n\n参数：\n\n * image_size：图片的大小，默认值为224。\n * crop_pad：中心填充的大小，默认值为32。\n\n使用举例：\n\n\n\n\nBGR2RGBTransformer#\n\n说明：\n\n将输入格式由BGR转成RGB的操作。\n\n参数：\n\n * data_format：数据格式，取值范围为(CHW,HWC)，默认值为CHW。\n\n使用举例：\n\n\n\n\nRGB2BGRTransformer#\n\n说明：\n\n将输入格式由RGB转成BGR的操作。\n\n参数：\n\n * data_format：数据格式，取值范围为(CHW,HWC)，默认值为CHW。\n\n使用举例：\n\n\n\n\nRGB2GRAYTransformer#\n\n说明：\n\n将输入格式由RGB转成GRAY的操作。\n\n参数：\n\n * data_format：输入的layout类型，取值范围(\"CHW\",\"HWC\")，默认为\"CHW\"。\n\n使用举例：\n\n\n\n\nBGR2GRAYTransformer#\n\n说明：\n\n将输入格式由 BGR 转成 GRAY 的操作。\n\n参数：\n\n * data_format：输入的layout类型，取值范围 [\"CHW\",\"HWC\"]，默认值为\"CHW\"。\n\n使用举例：\n\n\n\n\nRGB2GRAY_128Transformer#\n\n说明：\n\n输入格式由RGB转成GRAY_128的操作。GRAY_128取值范围为(-128,127)。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为\"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nRGB2YUV444Transformer#\n\n说明：\n\n将输入格式由RGB转成YUV444的操作。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\", \"HWC\"]，默认值为\"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nBGR2YUV444Transformer#\n\n说明：\n\n将输入格式由BGR转成YUV444的操作。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为 \"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nBGR2YUV444_128Transformer#\n\n说明：\n\n将输入格式由BGR转成YUV444_128的操作。YUV444_128取值范围为(-128,127)。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为 \"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nRGB2YUV444_128Transformer#\n\n说明：\n\n将输入格式由RGB转成YUV444_128的操作。YUV444_128取值范围为(-128,127)。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为\"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nBGR2YUVBT601VIDEOTransformer#\n\n说明：\n\n将输入格式由BGR转成YUV_BT601_Video_Range的操作。\n\nYUV_BT601_Video_Range，某些摄像头输入数据都是YUV BT601(Video\nRange)格式的，取值范围为16~235，该transformer就是适配这种格式的数据产生的。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为\"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nRGB2YUVBT601VIDEOTransformer#\n\n说明：\n\n将输入格式由RGB转成YUV_BT601_Video_Range的操作。\n\nYUV_BT601_Video_Range，某些摄像头输入数据都是YUV BT601(Video\nRange)格式的，取值范围为16~235，该transformer就是适配这种格式的数据产生的。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为\"CHW\"，此项为必填项。\n\n使用举例：\n\n\n\n\nYUVTransformer#\n\n说明：\n\n将输入格式转成YUV444的操作。\n\n参数：\n\n * color_sequence：颜色序列，此项为必填项。\n\n使用举例：\n\n\n\n\nReduceChannelTransformer#\n\n说明：\n\n将C通道缩减为单通道的操作。该transformer主要是针对于C通道，如shape为1*3*224*224\n改为1*1*224*224。使用时layout一定要和data_format值对齐，避免造成删错通道。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\", \"HWC\"]，默认值为\"CHW\"。\n\n使用举例：\n\n\n\n\nBGR2NV12Transformer#\n\n说明：\n\n将输入格式由BGR转成NV12的操作。\n\n参数：\n\n * data_format：输入的layout类型，取值范围为[\"CHW\",\"HWC\"]，默认值为\"CHW\"。\n * cvt_mode：cvt模式，取值范围为(rgb_calc，opencv)，默认值为rgb_calc。\n   * rgb_calc，采用mergeUV的方式处理图片；\n   * opencv，采用opencv的方式处理图片。\n\n使用举例：\n\n\n\n\nRGB2NV12Transformer#\n\n说明：\n\n将输入格式由RGB转成NV12的操作。\n\n参数：\n\n * data_format：输入的 layout 类型，取值范围 [\"CHW\", \"HWC\"]，默认值为\"CHW\"。\n * cvt_mode：cvt模式，取值范围为(rgb_calc,opencv)，默认值为rgb_calc。\n   * rgb_calc，采用mergeUV的方式处理图片；\n   * opencv，采用opencv的方式处理图片。\n\n使用举例：\n\n\n\n\nNV12ToYUV444Transformer#\n\n说明：\n\n将输入格式由NV12转成YUV444的操作。\n\n参数：\n\n * target_size：目标大小，值为元组，如(240,240)。\n * yuv444_output_layout：yuv444输出的layout，取值范围为(HWC,CHW)，默认值为\"HWC\"。\n\n使用举例：\n\n\n\n\nWarpAffineTransformer#\n\n说明：\n\n用于做图像仿射变换的操作。\n\n参数：\n\n * input_shape：输入的shape值。\n * scale：乘以的系数。\n\n使用举例：\n\n\n\n\nF32ToS8Transformer#\n\n说明：\n\n用于做输入格式从float32转换为int8的操作。\n\n参数：不涉及。\n\n使用举例：\n\n\n\n\nF32ToU8Transformer#\n\n说明：\n\n用于做输入格式从float32转换为uint8的操作。\n\n参数：不涉及。\n\n使用举例：\n\n","routePath":"/guide/ptq/ptq_appendix/transformer","lang":"zh","toc":[{"text":"AddTransformer","id":"addtransformer","depth":2,"charIndex":180},{"text":"MeanTransformer","id":"meantransformer","depth":2,"charIndex":325},{"text":"ScaleTransformer","id":"scaletransformer","depth":2,"charIndex":499},{"text":"NormalizeTransformer","id":"normalizetransformer","depth":2,"charIndex":616},{"text":"TransposeTransformer","id":"transposetransformer","depth":2,"charIndex":736},{"text":"HWC2CHWTransformer","id":"hwc2chwtransformer","depth":2,"charIndex":886},{"text":"CHW2HWCTransformer","id":"chw2hwctransformer","depth":2,"charIndex":951},{"text":"CenterCropTransformer","id":"centercroptransformer","depth":2,"charIndex":1016},{"text":"PILCenterCropTransformer","id":"pilcentercroptransformer","depth":2,"charIndex":1232},{"text":"LongSideCropTransformer","id":"longsidecroptransformer","depth":2,"charIndex":1368},{"text":"PadResizeTransformer","id":"padresizetransformer","depth":2,"charIndex":1621},{"text":"ResizeTransformer","id":"resizetransformer","depth":2,"charIndex":1858},{"text":"PILResizeTransformer","id":"pilresizetransformer","depth":2,"charIndex":3025},{"text":"ShortLongResizeTransformer","id":"shortlongresizetransformer","depth":2,"charIndex":3340},{"text":"PadTransformer","id":"padtransformer","depth":2,"charIndex":3721},{"text":"ShortSideResizeTransformer","id":"shortsideresizetransformer","depth":2,"charIndex":3943},{"text":"PaddedCenterCropTransformer","id":"paddedcentercroptransformer","depth":2,"charIndex":4841},{"text":"BGR2RGBTransformer","id":"bgr2rgbtransformer","depth":2,"charIndex":5156},{"text":"RGB2BGRTransformer","id":"rgb2bgrtransformer","depth":2,"charIndex":5262},{"text":"RGB2GRAYTransformer","id":"rgb2graytransformer","depth":2,"charIndex":5368},{"text":"BGR2GRAYTransformer","id":"bgr2graytransformer","depth":2,"charIndex":5487},{"text":"RGB2GRAY_128Transformer","id":"rgb2gray_128transformer","depth":2,"charIndex":5612},{"text":"RGB2YUV444Transformer","id":"rgb2yuv444transformer","depth":2,"charIndex":5771},{"text":"BGR2YUV444Transformer","id":"bgr2yuv444transformer","depth":2,"charIndex":5904},{"text":"BGR2YUV444_128Transformer","id":"bgr2yuv444_128transformer","depth":2,"charIndex":6037},{"text":"RGB2YUV444_128Transformer","id":"rgb2yuv444_128transformer","depth":2,"charIndex":6204},{"text":"BGR2YUVBT601VIDEOTransformer","id":"bgr2yuvbt601videotransformer","depth":2,"charIndex":6370},{"text":"RGB2YUVBT601VIDEOTransformer","id":"rgb2yuvbt601videotransformer","depth":2,"charIndex":6624},{"text":"YUVTransformer","id":"yuvtransformer","depth":2,"charIndex":6878},{"text":"ReduceChannelTransformer","id":"reducechanneltransformer","depth":2,"charIndex":6966},{"text":"BGR2NV12Transformer","id":"bgr2nv12transformer","depth":2,"charIndex":7180},{"text":"RGB2NV12Transformer","id":"rgb2nv12transformer","depth":2,"charIndex":7417},{"text":"NV12ToYUV444Transformer","id":"nv12toyuv444transformer","depth":2,"charIndex":7657},{"text":"WarpAffineTransformer","id":"warpaffinetransformer","depth":2,"charIndex":7831},{"text":"F32ToS8Transformer","id":"f32tos8transformer","depth":2,"charIndex":7933},{"text":"F32ToU8Transformer","id":"f32tou8transformer","depth":2,"charIndex":8006}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":442,"title":"训练后量化（PTQ）常见问题","content":"#\n\n\n如何理解BPU加速和CPU计算两种形式？#\n\n * BPU加速：是指模型在板端推理时，该算子可以通过BPU硬件进行量化加速。其中，大部分算子（如conv等）是硬件直接支持的。有些会被替换成其他算子实现加速（\n   如gemm会被替换成conv）；还有一些则依赖特定的上下文（如Reshape、Transpose需要前后均为BPU算子）才能被动量化。\n\n * CPU计算：对于模型中BPU硬件无法直接或间接加速的算子，工具链会将其放在CPU上计算，runtime预测库也会在模型推理时自动完成执行硬件的异构调度。\n\n\n如何理解模型分段的性能影响？#\n\n当模型在BPU算子中间存在不能加速的CPU算子时，在BPU算子和CPU算子之间切换计算时会引入一定的性能损耗，具体包括两方面：\n\n * CPU算子性能远低于BPU算子。\n\n * CPU和BPU之间的异构调度还会引入量化、反量化算子（运行在CPU上），且因为内部计算需要遍历数据，所以其耗时会与shape大小成正比。\n\n地平线建议您尽量选择BPU算子搭建模型，以获取更好的性能表现。\n\n\n如何理解模型尾部部分BPU可加速算子运行在CPU上？#\n\n首先，我们需要理解以下两个概念：\n\n * 目前只在模型尾部支持Conv算子以int32高精度输出，其他算子都只能以int8低精度输出。\n\n * 通常情况下，模型转换会在optimization阶段将Conv与其后的BN和ReLU/ReLU6融合在一起进行计算。但由于BPU硬件本身限制，在模型尾部以\n   int32高精度输出的Conv却并不支持算子融合。\n\n所以如果模型以Conv+ReLU/ReLU6结尾，那么为了保证量化模型的整体精度，Conv会默认以int32高精度输出，ReLU/ReLU6则会跑在CPU上。同\n理，其他尾部可加速算子运行在CPU上也都是默认精度优先的选择。不过，地平线支持在yaml文件通过配置 quant_config\n将这些算子运行在BPU上，从而获取更好的性能表现，但会引入一定的精度损失。\n\n\n如何理解yaml文件中的编译器优化等级参数？#\n\n在模型转换的yaml配置文件中，编译参数组提供了optimize_level参数来选择模型编译的优化等级，可选范围为 O0~O2。其中：\n\n * O0 不做任何优化，编译速度最快，适合在模型转换功能验证、调试不同校准方式时使用。\n\n * O1 - O2 随着优化等级越高，编译优化时的搜索空间就会越大。\n\n * 编译器的优化策略并不是算子粒度层面的，而是针对整个模型的全局优化。\n\n\n如何编译得到多 batch 模型？#\n\n根据原模型种类，我们将分为动态输入模型和非动态输入模型来讨论这个问题。\n\n注解\n\n * input_batch 参数仅在 input_shape 第一维为1的时候可以使用，（模型为多输入时，需要所有输入的 input_shape\n   第一维均为1），且此参数仅在原始onnx模型本身支持多batch推理时才能生效，该参数仅支持配置一个数值，模型为多输入时，配置后该值将作用于模型的所有输入\n   。\n\n * 每份校准数据shape大小，应和 input_shape 的大小保持一致。\n\n动态输入模型：如果原模型为动态输入模型时，比如，?x3x224x224（动态输入模型必须使用input_shape参数指定模型输入信息）。\n\n 1. 当配置input_shape为1x3x224x224时，如果您想编译得到多batch的模型，可以使用 input_batch\n    参数，此时每份校准数据shape大小为1x3x224x224。\n\n 2. 当配置input_shape的第一维为大于1的整数时，原模型本身将会认定为多batch模型，将无法使用 input_batch\n    参数，且需要注意每份校准数据shape大小。例如配置input_shape为4x3x224x224时，此时每份校准数据shape大小需要为4x3x224\n    x224。\n\n非动态输入模型：\n\n 1. 当输入的input shape[0]为1时，可以使用 input_batch 参数，每份校准数据shape大小与原模型shape保持一致。\n\n 2. 当输入的input shape[0]不为1时，不支持使用 input_batch 参数。\n\n\n多输入模型在转换过程中，模型输入顺序发生变化，此种情况正常么？#\n\n此种情况是正常现象，多输入模型在转换过程中，模型输入顺序是有可能发生变化的。 可能发生的情况如下例所示：\n\n * 原始浮点模型输入顺序：input1、input2、input3。\n\n * original.onnx模型输入顺序：input1、input2、input3。\n\n * quanti.bc模型输入顺序：input2、input1、input3。\n\n * hbm模型输入顺序：input3、input2、input1。\n\n注意\n\n * 当您做精度一致性对齐时，请确保输入顺序是正确的，不然有可能会影响精度结果。\n\n * 如果您想查看hbm模型输入的顺序，可以使用 hb_model_info\n   指令来查看，input_parametersinfo分组中列出的输入顺序，即为hbm模型的输入顺序。\n\n\n如何理解PTQ模型转换过程中的主动量化和被动量化？#\n\n在模型成功转换成hbm模型后，可能会出现发现仍然有个别op运行在CPU上的情况，但回头仔细对照工具链算子约束列表，明明该op是符合算子约束条件的，也就是理论上该\n算子应该成功运行在BPU上，为什么仍然是CPU计算呢？\n\n针对此问题，您可参考 地平线官方社区文章，该文章对模型转换工具链中内部的量化原理与背后的逻辑进行了介绍，并针对问题提供了几种解决方法。\n\n\n如何准备地平线支持转换的浮点模型？#\n\n地平线 PTQ 浮点转换工具链支持 Caffe1.0 和 ONNX（10≤opset_version≤19 且 ir_version≤9）两种模型格式。ONNX\n导出方式参考如下：\n\n训练框架           ONNX导出方式\nCaffe1.0       地平线原生支持，无需导出ONNX\nPytorch        Pytorch导出ONNX及模型可视化教程\nTensorFlow2    TensorFlow2导出ONNX及模型可视化教程\nPaddlePaddle   PaddlePaddle导出ONNX及模型可视化教程\n其他框架           ONNX Tutorials\n\n注意\n\n以上，模型可视化教程内的 ONNX 模型 opset_version、 ir_version 版本支持范围与当前略有差异，请以当前版本范围为准。\n\n\n如何验证原始浮点 ONNX 模型的正确性？#\n\n工具链在转换模型时，会使用基于公版 ONNXRuntime 封装的接口实现模型的解析和前向，所以我们应该在使用工具链之前先检查原始浮点 ONNX\n模型本身的合法性（即是否能够正常推理）， 以及从训练框架导出 ONNX 的过程中是否引入了精度偏差。具体测试方式可参考：HBRuntime 推理库 章节。\n\n\n如何转换 fp16 的模型？#\n\n工具链目前不支持 fp16 模型的直接转换，但可参考文章 如何将FP16的ONNX模型转为工具链支持的FP32的ONNX模型 将其先转换成 fp32\n模型，再使用地平线工具链进行量化转换。\n\n\n如何修改模型的输入 layout？#\n\nJ6\nPTQ工具不再支持转换过程中修改模型的输入layout格式，PTQ转换过程中会保持模型layout不改变。如有修改layout需求可以直接在导出onnx前修改模\n型完成layout的改变。\n\n如需要从模型层面修改，建议使用如下方式实现：\n\n 1. 在 DL 框架中增加 transpose，使其输入 layout 为 NHWC，并重新导出 ONNX 模型；\n 2. 使用 onnx 库直接修改模型，参考如下：\n\n\n\n\n如何理解 shape_inference_fail.onnx？#\n\n该模型只会在 PTQ 模型转换失败时产出，无特殊意义，可向地平线提供该模型和模型转换时的 log 文件进行 debug 分析。\n\n\n如何理解 log 中模型输入输出 shape 为 0？#\n\n模型转换日志会打印模型的输入输出节点的 name 及 shape，但有时也会出现 shape 为 0 的情况，如下图所示。 其主要发生在 batch\n维度，因为某些模型的输出 shape 是动态的（使用 dim_param\n来存储），转换工具前向推理模型（shape_inference）后就会使用?来占位，此时日志打印就会显示为 0。\n这种情况是符合预期的，不会影响模型转换，无需过分关注。\n\n\n如何理解optimized模型中算子发生变化？#\n\n为了提高模型的上板性能，模型转换有一个Optimizer模块，该模块会对模型进行一些图优化，以提升模型上板的性能。\nOptimizer模块主要包括常量折叠、算子替换、算子属性/输入更改、算子融合、算子移动等功能。\n\n注意\n\nOptimizer模块会导致模型的算子发生变化，该模块会对模型进行一些等价的图优化。\n\n\n在转换流程中，hb_compile打印日志中Calibrated Cosine列出现nan的几种情况以及导致此类问题的原因？#\n\n 1. 情况1：模型中where算子的第二个输入为-Inf\n    \n    在转化过程中的优化阶段，为了能够保证where算子可以被量化，在优化阶段会将此算子拆解成多个可量化算子的组合。当 where 的第二个输入为 -Inf\n    时，会出现转换过程中出现nan的情况。具体原因如下：\n\n 2. 情况2：模型中出现全 0 tensor\n    \n    1). 由 concat 导致出现全0 tensor\n    \n    \n    \n    以上述结构为例，concat的两个输入为全0常量，因此在后续经过slice后的输出有可能出现某个分支中流动的数据均为0的情况，并且导致HzCalibra\n    tion出现非法阈值(即：阈值为0)。\n    \n    2). 由equal导致出现全0 tensor\n    \n    以上述结构为例，模型的输入onnx::Not_3为bool型tensor，在经过cast+equal之后，导致tensor中的数据均为false，在经过\n    cast(bool -> float32)后，出现全0 tensor。\n\n注意\n\n此种情况需要客户尝试使用多组数据进行测试，进而排除由校准数据导致模型出现全0的可能。\n\n\n是否有硬件支持模型输入颜色空间转换？#\n\nBPU 支持常见模型输入格式间的转换（例如 nv12 -> rgb/bgr）和数据归一化，可以通过 yaml 文件进行配置。 具体请见\ninput_type_train、input_type_rt、mean_value、scale_value、std_value 等参数的说明。\n\n\n如何理解编译器优化等级？#\n\nyaml 文件 optimize_level 参数可配置 O0～O2\n的编译器优化等级，优化等级越高则搜索空间越大，通常耗时也会更多。优化等级并不会针对算子粒度层面进行一些确定的优化策略，大部分算子的优化与优化等级没关系（这些优化\n不耗时）。优化等级主要是对全局优化起作用，是结合整个模型进行的分析和优化。\n\n\n如何处理模型首尾部的量化/反量化算子？#\n\nPTQ 工具链默认会在 featuremap 输入模型的首部插入量化算子来实现输入数据从 float32 到 int8\n类型的映射，并在所有模型的尾部插入反量化算子来实现输出数据从 int8（若 BPU 以 conv 结尾则默认为 int32 输出） 到 float32\n的映射。而量化/反量化算子在 CPU 上的执行效率并不高，特别是在数据 shape 比较大的时候。\n\n所以我们更建议通过将量化/反量化操作融合进前后处理，此种方式最为高效，具体说明请见：反量化节点的融合实现。\n\n\n如何使用 unitconv 算子优化模型性能？#\n\n模型中的一个算子能运行在 BPU 上，除了其本身应满足 BPU 支持条件外，还需要能在校准时找到它的量化阈值。 而部分非计算密集型算子（如\nconcat，reshape 等）的量化阈值会依赖于上下游算子的 featuremap Tensor。 因此，若这些算子在模型首尾处就会默认跑在 CPU 上。\n此时若想追求更高效的性能，可以在该算子前/后插入 unitconv 来引入新的量化阈值统计，进而将其量化在 BPU 上。\n具体说明可见：unit_conv使用说明（用于优化模型性能） 。\n\n但需要注意的是，这种方法有可能会引入一定的量化损失。 以 conv+reshape+concat 结构输出的模型为例， 工具链默认 conv 会以 int32\n高精度输出，反量化至 float32 后再送给 CPU 上的 reshape 和 concat。 若在 concat 之后插入\nunitconv，则整个结构都将以 int8 低精度运行在 BPU 上， 此时虽然最后的 unitconv 还能以 int32 高精度输出，但前面 conv\n输出的精度压缩已经引入了一定的量化损失。 所以，是否插入 unitconv 来优化性能还请综合考虑。\n\n\n是否支持 int16/int32计算？#\n\n大部分算子默认使用int8计算，部分算子支持int16、fp16计算，算子支持范围持续扩充中，具体详见 工具链算子支持约束列表-ONNX Operator\nSupport List，另外：\n\n 1. 若模型中的 BPU 部分以 Conv 结尾，则该算子默认为 int32 高精度输出；\n 2. DSP 硬件也具备 int8/int16/float32 的计算能力。\n\n\n如何正确处理模型的校准数据？#\n\nPTQ 模型校准数据的准备请参考 校准数据集准备 章节；另外，对于 featuremap 输入的模型，请自行完成数据的预处理，并通过 numpy.save\n接口保存为所需输入类型的 npy文件。\n\n\n如何 dump 模型中间层输出？#\n\n在模型转换阶段，yaml 文件中的 debug_mode 参数如果配置了\ndump_all_layers_output，则会为每个卷积和矩阵乘算子增加一个反量化输出节点，它会显著的降低模型上板后的性能。\n\n其中，output_nodes 参数可以指定模型中的任意节点为输出节点，更利于我们调试调优。 此外，还可以使用 hb_verifier 工具对比定点模型\nquantized_model.bc 和上板 hbm 模型的一致性。\n\n在板端部署阶段，hrt_model_exec 工具也支持以 bin 或 txt 的格式保存节点输出（包括用 output_nodes\n参数指定的节点），具体使用方式可参考： hrt_model_exec工具介绍 。","routePath":"/guide/ptq/ptq_faq_troubleshooting/ptq_faq","lang":"zh","toc":[{"text":"如何理解BPU加速和CPU计算两种形式？","id":"如何理解bpu加速和cpu计算两种形式","depth":2,"charIndex":3},{"text":"如何理解模型分段的性能影响？","id":"如何理解模型分段的性能影响","depth":2,"charIndex":261},{"text":"如何理解模型尾部部分BPU可加速算子运行在CPU上？","id":"如何理解模型尾部部分bpu可加速算子运行在cpu上","depth":2,"charIndex":471},{"text":"如何理解yaml文件中的编译器优化等级参数？","id":"如何理解yaml文件中的编译器优化等级参数","depth":2,"charIndex":865},{"text":"如何编译得到多 batch 模型？","id":"如何编译得到多-batch-模型","depth":2,"charIndex":1082},{"text":"多输入模型在转换过程中，模型输入顺序发生变化，此种情况正常么？","id":"多输入模型在转换过程中模型输入顺序发生变化此种情况正常么","depth":2,"charIndex":1817},{"text":"如何理解PTQ模型转换过程中的主动量化和被动量化？","id":"如何理解ptq模型转换过程中的主动量化和被动量化","depth":2,"charIndex":2207},{"text":"如何准备地平线支持转换的浮点模型？","id":"如何准备地平线支持转换的浮点模型","depth":2,"charIndex":2415},{"text":"如何验证原始浮点 ONNX 模型的正确性？","id":"如何验证原始浮点-onnx-模型的正确性","depth":2,"charIndex":2813},{"text":"如何转换 fp16 的模型？","id":"如何转换-fp16-的模型","depth":2,"charIndex":2990},{"text":"如何修改模型的输入 layout？","id":"如何修改模型的输入-layout","depth":2,"charIndex":3104},{"text":"如何理解 shape_inference_fail.onnx？","id":"如何理解-shape_inference_failonnx","depth":2,"charIndex":3336},{"text":"如何理解 log 中模型输入输出 shape 为 0？","id":"如何理解-log-中模型输入输出-shape-为-0","depth":2,"charIndex":3436},{"text":"如何理解optimized模型中算子发生变化？","id":"如何理解optimized模型中算子发生变化","depth":2,"charIndex":3665},{"text":"在转换流程中，hb_compile打印日志中Calibrated Cosine列出现nan的几种情况以及导致此类问题的原因？","id":"在转换流程中hb_compile打印日志中calibrated-cosine列出现nan的几种情况以及导致此类问题的原因","depth":2,"charIndex":3848},{"text":"是否有硬件支持模型输入颜色空间转换？","id":"是否有硬件支持模型输入颜色空间转换","depth":2,"charIndex":4449},{"text":"如何理解编译器优化等级？","id":"如何理解编译器优化等级","depth":2,"charIndex":4612},{"text":"如何处理模型首尾部的量化/反量化算子？","id":"如何处理模型首尾部的量化反量化算子","depth":2,"charIndex":4783},{"text":"如何使用 unitconv 算子优化模型性能？","id":"如何使用-unitconv-算子优化模型性能","depth":2,"charIndex":5048},{"text":"是否支持 int16/int32计算？","id":"是否支持-int16int32计算","depth":2,"charIndex":5595},{"text":"如何正确处理模型的校准数据？","id":"如何正确处理模型的校准数据","depth":2,"charIndex":5802},{"text":"如何 dump 模型中间层输出？","id":"如何-dump-模型中间层输出","depth":2,"charIndex":5919}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":443,"title":"常见故障处理","content":"#\n\n\nSegmentation fault：core dumped#\n\n该报错可能是物理机内存不足，工具版本老旧，模型非法，或工具 bug 导致。请通过以下方式进行排查：\n\n 1. 确认当前物理机是否存在其他任务正在占用内存（或更换至更大内存的开发机）；\n 2. 升级工具链版本至最新，重新转换模型；\n 3. 确认原始模型本身是否合法，如 onnx 模型是否能在 onnxruntime 中正确运行；\n 4. 若以上检查后仍无法解决，请联系地平线团队。\n\n\nERROR Wrong mean_value num received#\n\n\n\n该报错是 yaml 文件中的 mean_value 参数数量与模型输入节点数量不一致导致，请对应检查。 不同输入节点的 mean_value 需要用 “;”\n号隔开； 若模型为混合多输入模型（即同时包含图像和 featuremap 输入节点），则需要与 input_name 对应，用 “NA” 给\nfeaturemap 节点的 mean_value 参数占位。\n\n\nERROR The input model is invalid#\n\n该报错是模型本身非法导致，即不满足公版 onnxruntime 的检查逻辑，请自行检查模型。","routePath":"/guide/ptq/ptq_faq_troubleshooting/troubleshooting","lang":"zh","toc":[{"text":"Segmentation fault：core dumped","id":"segmentation-faultcore-dumped","depth":2,"charIndex":3},{"text":"ERROR Wrong mean_value num received","id":"error-wrong-mean_value-num-received","depth":2,"charIndex":229},{"text":"ERROR The input model is invalid","id":"error-the-input-model-is-invalid","depth":2,"charIndex":453}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":444,"title":"常见算法模型示例","content":"#\n\n\n位置路径#\n\n常见算法模型示例位于 horizon_model_convert_sample 路径的： 03_classification/、 04_detection/ 和\n07_segmentation/ 文件夹中。\n\n\n如何准备数据集 #\n\n\n数据集下载地址#\n\n数据集的下载地址可参考下表：\n\n数据集          下载地址\nImageNet     https://www.image-net.org/download.php\nCOCO         https://cocodataset.org/\nVOC          http://host.robots.ox.ac.uk/pascal/VOC/ （需要下载2007和2012两个版本）\nCityscapes   https://github.com/mcordts/cityscapesScripts\n\n\n数据集参考结构 #\n\n为方便您进行后续步骤，在数据集下载完成后，您需要按照如下地平线建议的结构对评测数据集进行处理。\n\n\n如何准备模型 #\n\n在使用模型转换示例包时，请您先准备好对应的浮点模型。\n\n注解\n\nOE包默认不携带示例对应的校准数据集和原始模型，您需要在对应的示例文件夹内执行 00_init.sh 获取当前示例所需的模型和校准数据集。\n\n对于各原始模型的来源、修改点（如有）的准备过程，请您参考以下内容。\n\n\nMobileNetv1#\n\n 1. 模型来源：https://github.com/shicai/MobileNet-Caffe\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    3fd6889ec48bda46451d67274144e2a8   mobilenet.caffemodel\n    8922f90f629d428fecf866e798ac7c08   mobilenet_deploy.prototxt\n\n\nMobileNetv2#\n\n 1. 模型来源：https://github.com/shicai/MobileNet-Caffe\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    54aab8425ea068d472e8e4015f22360c   mobilenet_v2.caffemodel\n    13101ee86ab6d217d5fd6ed46f7a4faa   mobilenet_v2_deploy.prototxt\n\n\nResNet50#\n\n 1. 模型来源：https://pytorch.org/vision/main/models/generated/torchvision.models.res\n    net50.html\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    ebaeb70c972d1a3e6eb06c0e1541eeb4   resnet50.onnx\n\n\nGoogleNet#\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Google\n    Net\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    f107ae6806ea1016afbc718210b7a617   googlenet.onnx\n\n\nEfficientNet_Lite0/1/2/3/4#\n\n注意\n\n为了快速运行示例，避免使用第三方工具带来的风险，强烈推荐您直接使用地平线模型发布物 model_zoo/mapper/\n路径下准备好的ONNX浮点模型。如果您有兴趣复现tflite2onnx的模型转换过程，也可以尝试使用以下三方工具。但地平线无法保证第三方工具的质量和转换成功率。\n\n 1. 模型来源：可从\n    https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/l\n    ite 获取tar包。\n\n 2. 地平线模型发布物中转换后的ONNX模型md5sum码：\n    \n    MD5SUM                             FILE\n    001a329bd367fbec22b415c7a33d7bdb   efficientnet_lite0_fp32.onnx\n    1205e95aea66650c71292bde236d55a9   efficientnet_lite1_fp32.onnx\n    474741c15494b79a89fe51d89e0c43c7   efficientnet_lite2_fp32.onnx\n    550455b41848d333f8359279c89a6bae   efficientnet_lite3_fp32.onnx\n    bde7fe57eadb4a30ef76f68da622dcd5   efficientnet_lite4_fp32.onnx\n\n 3. 下载后可从tar包中得到 .tflite 文件，然后可通过tflite2onnx工具 将tflite转换为ONNX模型。\n    \n    不同版本的tflite2onnx转换出来的layout会不一样，若转换出来的ONNX模型的输入layout是NHWC排布，则build时，Efficie\n    ntNet_Lite0/1/2/3/4的 input_layout_train 均应该选择 NHWC。\n\n\nVargconvnet#\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/VargCo\n    nvNet\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    e21b8db17916f9046253bbe0bb8de3ef   vargconvnet.onnx\n\n\nEfficientnasnet_m#\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Effici\n    entnasNet\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    fc36c052c6f034c0b64a6197b91b0c62   efficientnasnet-m.onnx\n\n\nEfficientnasnet_s#\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Effici\n    entnasNet\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    e2744bd748f4265f4488676835a6ca24   efficientnasnet-s.onnx\n\n\nResNet18#\n\n 1. 模型来源：https://pytorch.org/vision/main/models/generated/torchvision.models.res\n    net18.html\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    62de4ff68317c65ab4bb6a451e719e6d   resnet18.onnx\n\n\nYOLOv2_Darknet19#\n\n注意\n\n * 为了快速运行示例，避免使用第三方工具带来的风险，强烈推荐您直接使用地平线模型发布物 model_zoo/mapper/\n   路径下准备好的Caffe浮点模型。如果您有兴趣复现darknet2caffe的模型转换过程，也可以尝试使用以下三方工具。但地平线无法保证三方工具的质量和转\n   换成功率。\n\n * 为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n   参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. YOLOv2_Darknet19模型需要首先从YOLO官网，下载YOLOv2\n    608x608的.cfg和.weight文件并使用darknet2caffe 转换工具将其转换为caffe model。\n\n注解\n\n该转换工具（darknet2caffe）是一个简化版本，使用时，需要修改该工具生成的.prototxt文件，将其中的 'Reshape' 层修改成\n'Passthrough' 层， Passthrough\n层具体修改后的参数请见提供的yolov2.prototxt例子，并在输出节点增加一个NCHW2NHWC的Permute操作。\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    7aa7a6764401cebf58e73e72fcbd2a45   yolov2.caffemodel\n    72e9a51c1e284e4b66e69f72ca9214c8   yolov2_transposed.prototxt\n\n\nYOLOv3_Darknet53#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. YOLOv3_Darknet53模型获取：\n\nURL：https://github.com/ChenYingpeng/caffe-yolov3/\n，caffemodel可以在该github的README.md提供的百度云下载路径中下载，并在输出节点增加一个NCHW2NHWC的Permute操作。\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    935af6e1530af5c0017b3674adce95e9   yolov3_transposed.prototxt\n    9a0f09c850656913ec27a6da06d9f9cc   yolov3.caffemodel\n\n\nYOLOv5x#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. YOLOv5x模型：可以从URL：https://github.com/ultralytics/yolov5/releases/tag/v2.0\n    中下载相应的pt文件。\n\n注意\n\n在clone代码时，请确认您使用的Tags是，否则将导致转换失败。\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    2e296b5e31bf1e1b6b8ea4bf36153ea5   yolov5l.pt\n    16150e35f707a2f07e7528b89c032308   yolov5m.pt\n    42c681cf466c549ff5ecfe86bcc491a0   yolov5s.pt\n    069a6baa2a741dec8a2d44a9083b6d6e   yolov5x.pt\n\n * 为了更好地适配后处理代码，我们在ONNX模型导出前对Github代码做了如下修改（代码参见：https://github.com/ultralytics/\n   yolov5/blob/v2.0/models/yolo.py ）:\n\n\n\n注解\n\n去除了每个输出分支尾部从4维到5维的reshape（即不将channel从255拆分成3x85），然后将layout从NHWC转换成NCHW再输出。\n\n以下左图为修改前的模型某一输出节点的可视化图，右图则为修改后的对应输出节点可视化图。\n\n * 下载完成后通过脚本 https://github.com/ultralytics/yolov5/blob/v2.0/models/export.py\n   进行pt文件到ONNX文件的转换。\n\n注意\n\n在使用export.py脚本时，请注意： 在clone代码时，请确认您使用的Tags是，否则将导致转换失败。\n\n 1. 由于地平线算法工具链支持的ONNX opset版本为 ~ ，请将 torch.onnx.export 的 opset_version\n    参数根据您要使用的版本进行修改。\n\n 2. 将 torch.onnx.export 部分的默认输入名称参数由 'images' 改为\n    'data'，与模型转换示例包的YOLOv5x示例脚本保持一致。\n\n 3. 将 parser.add_argument 部分中默认的数据输入尺寸640x640改为模型转换示例包YOLOv5x示例中的672x672。\n\n\nSSD_MobileNetv1#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. SSD_MobileNetv1模型：可以从URL：https://github.com/chuanqi305/MobileNet-SSD\n    获得Caffe模型。\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    bbcb3b6a0afe1ec89e1288096b5b8c66   mobilenet_iter_73000.caffemodel\n    3c230e4415195a50c6248be80c49882d   MobileNetSSD_deploy.prototxt\n\n\nEfficientdetd0#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Effici\n    entDet\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    ec4129c4b300cd04f1e8f71e0fe54ca5   efficientdet_nhwc.onnx\n\n\nCenterNet_Resnet101#\n\n注意\n * 我们将maxpool和sigmoid节点放到模型中，并指定编译为BPU节点，用于减少后处理过程中的计算量。\n * 在编译hbm模型的yaml文件中我们对 remove_node_type 参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Center\n    net\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    db195ff784792f475e573c5126401d2a   centernet_resnet101_coco_modify.onnx\n\n\nFcos_efficientnetb0#\n\n注意\n * 该模型为采用PTQ方式训练出来的模型。\n * 为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n   参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/blob/master/Fcos_E\n    fficientnetb0\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    9f9a1fe8508e2bd068e70146eb559b4f   fcos_efficientnetb0.onnx\n\n\nYolov4#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/YoloV4\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    aaa3c3e5e4c4c1d4830b6501b1720e4d   yolov4_efficientnetb0.onnx\n\n\nYOLOv3_VargDarknet#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. YOLOv3_VargDarknet模型获取：\n\nURL：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Yolov3_Varg\nDarknet ， 可以在该github的README.md提供的百度云下载路径中下载。\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    fd4e46bc7c9798b51778d3aa09c5053a   yolov3_vargdarknet53.onnx\n\n\nFcos_resnet50#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Fcos_R\n    esnet50\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    0218942777615fac2f54cefdac4fbfa7   fcos_resnet50.onnx\n\n\nFcos_resnext101#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Fcos_R\n    esnext101\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    4b80efd22448021721ac5a860909c59f   fcos_resnext101.onnx\n\n\nUnet_mobilenet#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Dequantize节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Mobile\n    netUnet\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    21c6c645ebca92befbebc8c39d385c1e   tf_unet_trained.onnx\n\n\nDeeplabV3plus_efficientnetb0#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Reshape和Cast节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Deepla\n    bV3Plus\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    cf3a683f31b4b0ebe090647729f869d9   deeplabv3plus_efficientnetb0.onnx\n\n\nFastscnn_efficientnetb0#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Reshape和Cast节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/FastSC\n    NN\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    c1ace8f08a9c7b9c91509fa68327d0c8   fastscnn_efficientnetb0.onnx\n\n\nDeeplabv3plus_dilation1248#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Transpose节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Deepla\n    bV3Plus\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    ad002e572cbb49e1e99d893aac69f3e3   deeplabv3_cityscapes_dila1248_permute.onnx\n\n\nDeeplabv3plus_efficientnetm1#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Reshape、Cast和Transpose节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Deepla\n    bV3Plus\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    0a1dfd01e173c68630d9e8dc1a6036fe   deeplabv3plus_efficientnetm1.onnx\n\n\nDeeplabv3plus_efficientnetm2#\n\n注意\n\n为保证板端性能达到最优，在编译hbm模型的yaml文件中我们对 remove_node_type\n参数进行了配置，将hbm模型中的Reshape、Cast和Transpose节点做了删除的操作。\n\n 1. 模型来源：https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Deepla\n    bV3Plus\n\n 2. md5sum码：\n    \n    MD5SUM                             FILE\n    c11a2673c4b3cf6e5d7bf1a051925d38   deeplabv3plus_efficientnetm2.onnx\n\n\n算法模型示例的使用演示#\n\n本小节以 Resnet50 模型为例，使用算法模型示例包中 03_classification/03_resnet50/\n路径下脚本分步骤演示浮点模型到定点模型转换的过程。\n\n\n进入Docker容器#\n\n首先，根据 Docker容器部署 一节内容完成Docker环境的安装和配置并进入docker容器。\n\n\n下载原始模型及校准数据集#\n\n在 03_classification/03_resnet50/ 文件夹内执行 00_init.sh 获取当前示例所需的模型和校准数据集。\n\n\n\n\n验证模型是否能够执行#\n\n 1. 如下所示，运行脚本：\n\n\n\n 2. 模型检查输出：\n\n使用 hb_compile 工具验证模型是否被地平线计算平台支持。\n\n\n准备校准用的数据集#\n\n在同一路径下继续执行 02_preprocess.sh 脚本，如下所示：\n\n\n\n注解\n\n * 我们从ImageNet数据集抽取了100张图作为校准数据集，在校准前，我们对数据进行了预处理：short size resize/crop\n   size/NHWC to NCHW/转为rgb。\n\n * hb_compile 工具会从转换得到二进制数据中读取数据，预处理过的二进制数据文件格式为 npy，dtype为uint8。\n\n\nbuild模型#\n\n在同一路径下继续执行 03_build.sh 脚本，如下所示：\n\n\n\n注解\n\n上述脚本使用 hb_compile 工具转换模型，最需要关注的是转换的配置文件，请参考 模型量化编译 章节。\n\n上述脚本的输出如下所示：\n\n\n\n注解\n\n您暂时只需要关心 resnet50_224x224_nv12.hbm 文件。\n\n\n单张图片推理#\n\n执行 04_inference.sh 脚本进行单张图片的推理过程，如下所示：\n\n\n\n注解\n\n * 因为图片推理过程时，需要对图片进行 前处理，对模型数据进行 后处理，所以我们提供了一个示例Python脚本。具体请参考 sh\n   04_inference.sh 。\n\n * 此脚本只是对单张图片进行推理，验证单张图片的推理结果是否符合预期，如果想做精度测评，可以参考 05_evaluate.sh 脚本。\n\n\n精度测试#\n\n继续执行 05_evaluate.sh 脚本进行精度评测，如下所示：\n\n\n\n注解\n\n * 因为精度评测时，需要对图片进行 前处理，对模型数据进行 后处理，所以我们提供了一个示例Python脚本。具体请参考 sh 05_evaluate.sh。\n\n * 为了加快评测速度，可以通过 -p 选项适当调整并发进程数，但需要注意内存的占用情况。当 -p 选项值不填或者设置为 0\n   时，CPU环境中的定点模型将按照10个进程数处理，其他场景均按1个进程数处理。\n\n\n常见问题#\n\n\n复现的精度为什么与文档中提供的指标有细微差异？#\n\n出现此种现象的原因可能有以下两点：\n\n 1. 在不同的服务器环境下，计算方式上可能会有细小的区别，就会导致不同的服务器环境中编译出来的定点onnx模型的精度与文档的记录值有细微数据波动。\n\n 2. 使用的第三方库如opencv、numpy等库的版本不同，导致图片经过前处理后的得到的结果不同，这种情况也会导致精度复现时与文档中的记录值有细微数据波动。\n\n出现这种情况，您可以不用过于担心，文档中提供的记录指标仅作为参考，您在复现时的精度与文档中的记录值有细微差异是正常现象，可以正常跑通精度即可。\n\n\n定点模型精度为何与ai_benchmark示例中的hbm文件上板精度无法对齐？#\n\n在标准交付中，我们在添加示例的时候，定点模型精度和ai_benchmark示例中的hbm文件上板精度是已经做了对齐处理的。\n\n如果您发现定点模型精度与ai_benchmark示例中的hbm文件上板精度无法对齐的情况，建议您优先检查模型输入是否一致。\n由于执行定点模型评估脚本时，使用到的是图片类型的数据集；而上板使用到的hbm模型，需要使用hb_eval_preprocess工具转换后的二进制数据集。\n基于此点，如果您在上板时使用的数据集并非通过上述方式生成的,我们建议您先在运行定点模型的精度的相同服务器上，使用我们的数据预处理工具（即hb_eval_prep\nrocess工具）重新生成上板需要的数据集，重跑上板精度，以保证模型输入一致。\n\n注意\n\n注意，在使用hb_eval_preprocess工具生成数据集和运行定点模型精度时，两者使用的环境需要保证一致。","routePath":"/guide/ptq/ptq_sample/algorithm_sample","lang":"zh","toc":[{"text":"位置路径","id":"位置路径","depth":2,"charIndex":3},{"text":"如何准备数据集","id":"如何准备数据集","depth":2,"charIndex":-1},{"text":"数据集下载地址","id":"数据集下载地址","depth":3,"charIndex":128},{"text":"数据集参考结构","id":"数据集参考结构","depth":3,"charIndex":-1},{"text":"如何准备模型","id":"如何准备模型","depth":2,"charIndex":-1},{"text":"MobileNetv1","id":"mobilenetv1","depth":3,"charIndex":605},{"text":"MobileNetv2","id":"mobilenetv2","depth":3,"charIndex":860},{"text":"ResNet50","id":"resnet50","depth":3,"charIndex":1121},{"text":"GoogleNet","id":"googlenet","depth":3,"charIndex":1346},{"text":"EfficientNet_Lite0/1/2/3/4","id":"efficientnet_lite01234","depth":3,"charIndex":1566},{"text":"Vargconvnet","id":"vargconvnet","depth":3,"charIndex":2482},{"text":"Efficientnasnet_m","id":"efficientnasnet_m","depth":3,"charIndex":2708},{"text":"Efficientnasnet_s","id":"efficientnasnet_s","depth":3,"charIndex":2950},{"text":"ResNet18","id":"resnet18","depth":3,"charIndex":3192},{"text":"YOLOv2_Darknet19","id":"yolov2_darknet19","depth":3,"charIndex":3417},{"text":"YOLOv3_Darknet53","id":"yolov3_darknet53","depth":3,"charIndex":4154},{"text":"YOLOv5x","id":"yolov5x","depth":3,"charIndex":4605},{"text":"SSD_MobileNetv1","id":"ssd_mobilenetv1","depth":3,"charIndex":5764},{"text":"Efficientdetd0","id":"efficientdetd0","depth":3,"charIndex":6165},{"text":"CenterNet_Resnet101","id":"centernet_resnet101","depth":3,"charIndex":6492},{"text":"Fcos_efficientnetb0","id":"fcos_efficientnetb0","depth":3,"charIndex":6882},{"text":"Yolov4","id":"yolov4","depth":3,"charIndex":7251},{"text":"YOLOv3_VargDarknet","id":"yolov3_vargdarknet","depth":3,"charIndex":7563},{"text":"Fcos_resnet50","id":"fcos_resnet50","depth":3,"charIndex":7960},{"text":"Fcos_resnext101","id":"fcos_resnext101","depth":3,"charIndex":8283},{"text":"Unet_mobilenet","id":"unet_mobilenet","depth":3,"charIndex":8612},{"text":"DeeplabV3plus_efficientnetb0","id":"deeplabv3plus_efficientnetb0","depth":3,"charIndex":8938},{"text":"Fastscnn_efficientnetb0","id":"fastscnn_efficientnetb0","depth":3,"charIndex":9293},{"text":"Deeplabv3plus_dilation1248","id":"deeplabv3plus_dilation1248","depth":3,"charIndex":9633},{"text":"Deeplabv3plus_efficientnetm1","id":"deeplabv3plus_efficientnetm1","depth":3,"charIndex":9992},{"text":"Deeplabv3plus_efficientnetm2","id":"deeplabv3plus_efficientnetm2","depth":3,"charIndex":10357},{"text":"算法模型示例的使用演示","id":"算法模型示例的使用演示","depth":2,"charIndex":10722},{"text":"进入Docker容器","id":"进入docker容器","depth":3,"charIndex":10825},{"text":"下载原始模型及校准数据集","id":"下载原始模型及校准数据集","depth":3,"charIndex":10890},{"text":"验证模型是否能够执行","id":"验证模型是否能够执行","depth":3,"charIndex":10979},{"text":"准备校准用的数据集","id":"准备校准用的数据集","depth":3,"charIndex":11059},{"text":"build模型","id":"build模型","depth":3,"charIndex":11285},{"text":"单张图片推理","id":"单张图片推理","depth":3,"charIndex":11451},{"text":"精度测试","id":"精度测试","depth":3,"charIndex":11665},{"text":"常见问题","id":"常见问题","depth":2,"charIndex":11903},{"text":"复现的精度为什么与文档中提供的指标有细微差异？","id":"复现的精度为什么与文档中提供的指标有细微差异","depth":3,"charIndex":11911},{"text":"定点模型精度为何与ai_benchmark示例中的hbm文件上板精度无法对齐？","id":"定点模型精度为何与ai_benchmark示例中的hbm文件上板精度无法对齐","depth":3,"charIndex":12188}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":445,"title":"发布物介绍","content":"#\n\nPTQ转换示例发布物包括模型量化示例 horizon_model_convert_sample和模型库 model_zoo/mapper\n路径下用于编译的源模型。\n\n\n模型量化示例包 horizon_model_convert_sample#\n\n注解\n\nOE包默认不携带示例对应的校准数据集和原始模型，您需要在对应的示例文件夹内执行 00_init.sh 获取当前示例所需的模型和校准数据集。\n\n全部示例的原始模型和数据集获取完成后的模型转换示例包的目录结构如下所示：\n\n\n\n示例包中包含的主要内容：\n\n常见算法模型示例 是指 03_classification/ 、04_detection/ 和 07_segmentation/ 文件夹中的示例。\n\n这些示例的主要目标是指导：\n\n * 快速体验模型转换的流程。\n\n * 快速评测模型转换的精度。\n\n * 体验转换的效果。\n\n注解\n\n这部分示例会不定期更新，为您常见的问题提供示例解答。\n\n\n模型库 model_zoo#\n\n模型库 model_zoo 文件夹下包含两个路径： mapper 和 runtime 。\n\n * mapper 路径下包含了PTQ和QAT方案进行模型转换时要使用的模型（ONNX或Caffe格式的浮点模型）。\n\n * runtime 路径下则包含了嵌入式运行时开发要使用的hbm模型。\n\n下一小节内容将为您介绍算法模型示例相关的内容。","routePath":"/guide/ptq/ptq_sample/general_description","lang":"zh","toc":[{"text":"模型量化示例包 horizon_model_convert_sample","id":"模型量化示例包-horizon_model_convert_sample","depth":2,"charIndex":86},{"text":"模型库 `model_zoo`","id":"模型库-model_zoo","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":446,"title":"精度debug工具","content":"#\n\n模型转换工具链会基于您提供的校准样本对模型进行校准量化并保障模型高效的部署在地平线计算平台上。而在模型转换的过程中，难免会因为浮点到定点的量化过程而引入精度损失\n，通常情况下造成精度损失的主要原因可能有以下几点：\n\n 1. 模型中的一部分节点对量化比较敏感会引入较大误差，即敏感节点量化问题。\n\n 2. 模型中各个节点的误差累积导致模型整体出现较大的校准误差，主要包含：权重量化导致的误差累积、激活量化导致的误差累积以及全量量化导致的误差累积。\n\n针对该情况，地平线提供了精度debug工具用以协助您自主定位模型量化过程中产生的精度问题。该工具能够协助您对校准模型进行节点粒度的量化误差分析，最终帮助您快速定\n位出现精度异常的节点。\n\n精度debug工具提供多种分析功能供您使用，例如：\n\n * 获取节点量化敏感度。\n\n * 获取模型累积误差曲线。\n\n * 获取指定节点的数据分布。\n\n * 获取指定节点输入数据通道间数据分布箱线图等。\n\n\n快速上手#\n\n使用精度debug工具主要有以下几个步骤：\n\n 1. 在yaml中的 模型参数组(model_parameters) 配置参数 debug_mode: \"dump_calibration_data\"\n    ，保存校准数据。\n\n 2. 导入debug模块，加载校准模型和数据。\n\n 3. 通过精度debug工具提供的API或命令行，对精度损失明显的模型进行分析。\n\n整体流程如下图所示：\n\n\n\n * 确认单独量化激活/权重的累积误差情况，详见：plot_acc_error 章节。\n\n * 激活敏感度排序，详见：get_sensitivity_of_nodes 章节。\n\n * 权重敏感度排序，详见：get_sensitivity_of_nodes 章节。\n\n * 节点敏感度排序，详见：get_sensitivity_of_nodes 章节。\n\n * 查看敏感激活层分布情况，详见：plot_distribution 章节以及 get_channelwise_data_distribution 章节。\n\n * 查看敏感权重分布情况，详见：plot_distribution 章节以及 get_channelwise_data_distribution 章节。\n\n * 敏感节点单独量化及部份量化测试，详见：sensitivity_analysis 章节。\n\n\n校准模型与数据的保存#\n\n首先需要在yaml文件中配置 debug_mode:\n\"dump_calibration_data\"，以开启精度debug功能，并保存校准数据(calibration_data)，对应的校准模型(calibrate\nd_model.onnx)为常态化保存。其中：\n\n * 校准数据(calibration_data)：在校准阶段，模型通过对这些数据进行前向推理来获取每个被量化节点的量化参数，包括：缩放因子(scale)和阈值\n   (threshold)。\n\n * 校准模型(calibrated_model.onnx)：将在校准阶段计算得到的每个被量化节点的量化参数保存在校准节点中，从而得到校准模型。\n\n注解\n\n此处保存的校准数据与02_preprocess.sh生成的校准数据的区别？\n\n在J6工具链中，02_preprocess.sh\n得到的校准数据与此处保存的校准数据相同，均为.npy格式，可直接用于debug工具测试模型精度。需要注意的是，在送入debug工具之前需要确认\n02_preprocess.sh 得到的校准数据文件夹结构与此处保存的校准数据文件夹结构需要一致。\n\n注解\n\n校准模型(calibrated_model.onnx)解读\n\n校准模型是模型转换工具链将浮点模型经过结构优化后，通过校准数据计算得到的每个节点对应的量化参数并将其保存在校准节点中得到的中间产物。校准模型的主要特点是模型中包\n含校准节点，校准节点的节点类型为HzCalibration。这些校准节点主要分为两类： 激活(activation)校准节点 和 权重(weight)校准节点\n。\n\n激活校准节点 的输入是当前节点的上一个节点的输出，并基于当前激活校准节点中保存的量化参数(scales和thresholds)对输入数据进行量化及反量化后输出。\n\n权重校准节点 的输入为模型的原始浮点权重，基于当前权重校准节点中保存的量化参数(scales和thresholds)对输入的原始浮点权重进行量化及反量化后输出。\n\n除却上述的校准节点，校准模型中的其他节点，精度debug工具将其称为 普通节点(node) 。 普通节点 的类型包括：Conv、Mul、Add等。\n\n\n\ncalibration_data的文件夹结构如下：\n\n\n\n\n精度debug模块导入与使用#\n\n接下来需要在代码中导入debug模块，并通过 get_sensitivity_of_nodes\n接口获取节点量化敏感度（默认使用模型输出的余弦相似度）。get_sensitivity_of_nodes 的详细参数说明可见\nget_sensitivity_of_nodes 章节。\n\n\n\n\n分析结果展示#\n\n下方为 verbose=True 时的打印结果：\n\n\n\n除此之外，该API会以字典(Dict)的形式将节点量化敏感度信息返回给您以供后续使用分析。\n\n\n\n更多功能详见 功能说明文档 章节。\n\n为了方便您使用，精度debug工具还支持通过命令行进行使用，可通过 hmct-debugger -h/--help\n查看每个功能对应的子命令。各个子命令的详细参数和用法详见 功能说明文档 章节。\n\n\n功能说明文档 #\n\n\nget_sensitivity_of_nodes #\n\n功能：获取节点量化敏感度。\n\n命令行格式：\n\n\n\n可通过 hmct-debugger get-sensitivity-of-nodes -h/--help 查看相关参数。\n\n参数组：\n\n参数名称               命令行参数缩写   参数配置说明                                                         可选/必选\nmodel_or_file      固定参数      参数作用：指定校准模型。 取值范围：无。 默认配置：无。 参数说明：必选，指定需要分析的校准模型。              必选\nmetrics            -m        参数作用：节点量化敏感度的度量方式。 取值范围： 'cosine-similarity' , 'mse' , 'mre'   必选\n                             , 'sqnr' , 'chebyshev' 。 默认配置： 'cosine-similarity'。\n                             参数说明：指定节点量化敏感度的计算方式，该参数可以为列表(List)，即以多种方式计算量化敏感度，但是输出结果仅以列表中\n                             第一位的计算方式进行排序，排名越靠前说明量化该节点引入的误差越大。\ncalibrated_data    固定参数      参数作用：指定校准数据。 取值范围：无。 默认配置：无。参数说明：必选，指定分析所需要的校准数据。              必选\noutput_node        -o        参数作用：指定输出节点。 取值范围：校准模型中的具有对应校准节点的普通节点。 默认配置：None。              可选\n                             参数说明：此参数支持您指定中间节点作为输出并计算节点量化敏感度。若保持默认参数，则精度debug工具会获取模型的最终输出\n                             并在此基础上计算节点的量化敏感度。\nnode_type          -n        参数作用：节点类型。 取值范围： 'node' , 'weight' ,'activation'。 默认配置：        可选\n                             'node'。\n                             参数说明：需要计算量化敏感度的节点类型，包括：node（普通节点）、weight（权重校准节点）、activation（\n                             激活校准节点）。\ndata_num           -d        参数作用：计算量化敏感度需要的数据数量。 取值范围：大于0，小于等于calibration_data中数据的总数。      可选\n                             默认配置：1\n                             参数说明：设置计算节点量化敏感度时所需要的数据数量。默认为1，此时使用calibration_data中的第一组数据进行\n                             计算。最小设置为1，最大为calibration_data中的数据数量。\nverbose            -v        参数作用：选择是否将信息打印在终端上。 取值范围： True 、 False。 默认配置： False。           可选\n                             参数说明：若为True，则将量化敏感度信息打印在终端上。若metrics包含多种度量方式，则按照第一位进行排序。\ninterested_nodes   -i        参数作用：设置感兴趣节点。 取值范围：校准模型中的所有节点。 默认配置：None。                      可选\n                             参数说明：若指定则只获取该节点的量化敏感度，其余节点不获取。同时，若该参数被指定，将忽视node_type指定的节点类型\n                             ，也就是说该参数的优先级要高于node_type。若保持默认参数None，则计算模型中所有可被量化节点的量化敏感度。\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n分析结果展示：\n\n描述：首先您通过node_type设置需要计算敏感度的节点类型，然后工具获取校准模型中所有符合node_type的节点，并获取这些节点的量化敏感度。当verbo\nse设置为True时，工具会将节点量化敏感度进行排序后打印在终端，排序越靠前，说明该节点量化引入的量化误差越大。同时对于不同的node_type，工具会显示不同\n的节点量化敏感度信息。\n\nverbose=True且node_type='node'时，打印结果如下：\n\n\n\n其中：\n\n * node：节点名。\n\n * cosine-similarity、mse：各个节点的量化敏感度数值。\n\nverbose=True且node_type='weight'时，打印结果如下：\n\n\n\n其中：\n\n * weight：权重校准节点名。\n\n * node：权重校准节点对应的普通节点名，即权重校准节点的输出为其输入。\n\n * cosine-similarity、mse：各个节点的量化敏感度数值。\n\nverbose=True且node_type='activation'时，打印结果如下：\n\n\n\n其中：\n\n * activation：激活校准节点名。\n\n * node：在模型结构中在激活校准节点后的普通节点，即激活校准节点的输出为其输入。\n\n * threshold：校准阈值，若有多个阈值则取最大值。\n\n * bit：量化比特。\n\n * cosine-similarity、mse：各个节点的量化敏感度数值。\n\nAPI返回值：\n\nAPI返回值为以字典格式（Key为节点名称，Value为节点的量化敏感度信息）保存的量化敏感度，格式如下：\n\n\n\n\nplot_acc_error #\n\n功能：只量化浮点模型中的某一个节点，并依次计算该模型与浮点模型中节点输出的误差，获得累积误差曲线。\n\n命令行格式:\n\n\n\n可通过 hmct-debugger plot-acc-error -h/--help 查看相关参数。\n\n参数组：\n\n\n\n分析结果展示\n\n1.指定节点量化累积误差测试\n\n * 指定单节点量化\n\n配置方式：quantize_node=['Conv_2','Conv_90']，quantize_node为单列表。\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n描述：当quantize_node为单列表时，针对您设置的quantize_node，分别单独量化quantize_node中的节点并保持模型中其他节点不量化，\n得到对应的模型后，对该模型中每个节点的输出计算其与浮点模型中对应节点输出的之间的误差，并得到对应的累积误差曲线。\n\naverage_mode = False时：\n\n\n\naverage_mode = True时：\n\n\n\n注解\n\naverage_mode\n\naverage_mode默认为False。对于一些模型，此时无法通过累积误差曲线判断哪种量化策略更加有效，因此需要将average_mode设置为True，此时\n会对前n个节点的累积误差求均值作为第n个节点的累积误差。\n\n具体计算方式如下，例如：\n\naverage_mode=False时，accumulate_error=[1.0, 0.9, 0.9, 0.8]。\n\n而将average_mode=True后，accumulate_error=[1.0, 0.95, 0.933, 0.9]。\n\n * 指定多个节点量化\n\n配置方式：quantize_node=[['Conv_2'], ['Conv_2','Conv_90']]，quantize_node为嵌套列表\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n描述：当quantize_node为嵌套列表时，针对您设置的quantize_node，分别量化quantize_node中的每个单列表指定的节点并保持模型中其\n他节点不量化，得到对应的模型后，对该模型中每个节点的输出计算其与浮点模型中对应节点输出的之间的误差，并得到对应的累积误差曲线。\n\n * partial_qmodel_0：只量化Conv_2节点，其余节点不量化；\n\n * partial_qmodel_1：只量化Conv_2和Conv_90节点，其余节点不量化。\n\naverage_mode=False时：\n\n\n\naverage_mode=True时：\n\n\n\n2.解除模型部分节点量化后累积误差测试\n\n * 指定单节点不量化\n\n配置方式：non_quantize_node=['Conv_2','Conv_90']，non_quantize_node为单列表。\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n描述：当non_quantize_node为单列表时，针对您设置的non_quantize_node，分别解除non_quantize_node中各个节点的量化\n同时保持其他节点全部量化，得到对应的模型后，对该模型中每个节点的输出计算其与浮点模型中对应节点输出的之间的误差，并得到对应的累积误差曲线。\n\naverage_mode = False时：\n\n\n\naverage_mode = True时：\n\n\n\n * 指定多个节点不量化\n\n配置方式：non_quantize_node=[['Conv_2'],\n['Conv_2','Conv_90']]，non_quantize_node为嵌套列表。\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n描述：当non_quantize_node为嵌套列表时，针对您设置的non_quantize_node，分别不量化non_quantize_node中的每个单列\n表指定的节点并保持模型中其他节点均量化，得到对应的模型后，对该模型中每个节点的输出计算其与浮点模型中对应节点输出的之间的误差，并得到对应的累积误差曲线。\n\n * partial_qmodel_0：不量化Conv_2节点，其余节点量化；\n\n * partial_qmodel_1：不量化Conv_2和Conv_90节点，其余节点量化。\n\naverage_mode = False时：\n\n\n\naverage_mode = True时：\n\n\n\n测试技巧：\n\n测试部分量化精度时，您可能会按照量化敏感度排序进行多组量化策略的精度对比，此时可以参考以下用法：\n\n\n\n3.激活权重分别量化\n\n配置方式：quantize_node=['weight','activation']。\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n描述：quantize_node也可直接指定'weight'或者'activation'。当：\n\n * quantize_node = ['weight']：只量化权重，不量化激活。\n\n * quantize_node = ['activation']：只量化激活，不量化权重。\n\n * quantize_node = ['weight', 'activation']：权重和激活分别量化。\n\n\n\n注解\n\n通常情况下，建议您对累积误差曲线图中靠近模型输出位置的曲线部分多加关注。当采用某种量化方法后测试得到的累计误差曲线靠近模型输出位置的累积误差较小，即相似度较高时\n，我们建议您优先测试此种量化方法。\n\n\nplot_distribution #\n\n功能：选取节点，分别获取该节点在浮点模型和校准模型中的输出，得到输出数据分布。另外，将两个输出结果做差，获取两个输出之间的误差分布。\n\n命令行格式：\n\n\n\n可通过 hmct-debugger plot-distribution -h/--help 查看相关参数。\n\n参数组：\n\n\n\n分析结果展示：\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\nnode_output：\n\n\n\nweight：\n\n\n\nactivation：\n\n\n\n注解\n\n上方三幅图中，蓝色三角表示：数据绝对值的最大值。红色虚线表示：最大的校准阈值。\n\n\nget_channelwise_data_distribution #\n\n功能：绘制指定校准节点输入数据通道间数据分布的箱线图。\n\n命令行格式：\n\n\n\n可通过 hmct-debugger get-channelwise-data-distribution -h/--help 查看相关参数。\n\n参数组：\n\n参数名称              命令行参数缩写   参数配置说明                                                         可选/必选\nsave_dir          -s        参数作用：保存路径。 取值范围：无。 默认配置：无。 参数说明：可选，指定分析结果的保存路径。                可选\nmodel_or_file     固定参数      参数作用：指定校准模型。 取值范围：无。 默认配置：无。 参数说明：必选，指定分析所需要的校准模型。             必选\ncalibrated_data   固定参数      参数作用：指定校准数据。 取值范围：无。 默认配置：无。 参数说明：必选，指定分析所需要的校准数据。             必选\nnodes_list        -n        参数作用：指定校准节点。 取值范围：校准模型中的所有权重校准节点和激活校准节点。 默认配置：无。               必选\n                            参数说明：必选，指定校准节点。\naxis              -n        参数作用：指定channel所在的维度。 取值范围：小于节点输入数据的维度。 默认配置：None。              可选\n                            参数说明：channel信息所在shape中的位置。参数默认为None，此时对于激活校准节点，默认认为节点输入数据的第二\n                            个维度表示channel信息，即axis=1；对于权重校准节点，会读取该节点属性中的axis参数作为channel信息。\n\n\n\n分析结果展示：\n\n描述：针对您设置的校准节点列表node_list，从参数axis中获取channel所在的维度，获取节点输入数据通道间的数据分布。\n其中axis默认为None，此时若节点为权重校准节点，则channel所在的维度默认为0；若节点为激活校准节点，则channel所在的维度默认为1。\n\n权重校准节点：\n\n\n\n激活校准节点：\n\n\n\n输出结果如下图所示：\n\n\n\n图中：\n\n * 横坐标表示节点输入数据的通道数，图例中输入数据有96个通道。\n\n * 纵坐标表示每个channel的数据分布范围，其中红色实线表示该channel数据的中位数，蓝色虚线表示均值。每个箱子的上下限分别表示上四分位数和下四分位数\n   ，上下限之外的离散点表示异常值，通过观察这些异常值绝对值的最大值来判断当前节点输入数据是否出现较大波动的情况。\n\n关于如何通过观察箱线图判断节点是否存在量化风险，请参考PTQ精度debug示例: MobileVit_s精度问题分析。\n\n\nsensitivity_analysis #\n\n功能：针对量化敏感节点，分别分析测试单独量化以及部分量化这些节点后的模型精度。\n\n命令行格式：\n\n\n\n可通过 hmct-debugger sensitivity-analysis -h/--help 查看相关参数。\n\n参数组：\n\n参数名称              命令行参数缩写   参数配置说明                                                      可选/必选\nmodel_or_file     固定参数      参数作用：指定校准模型。 取值范围：无。 默认配置：无。 参数说明：必选，指定需要分析的校准模型。           必选\ncalibrated_data   固定参数      参数作用：指定校准数据。 取值范围：无。 默认配置：无。 参数说明：必选，指定分析所需要的校准数据。          必选\npick_threshold    -p        参数作用：设置选取节点的敏感度阈值。 取值范围：无。 默认配置：0.999。                      可选\n                            参数说明：可选，此功能计算普通节点的量化敏感度，选择敏感度小于\n                            pick_threshold的节点作为敏感节点进行分析测试。\n                            注：当设置sensitive_nodes时，则直接对sensitive_nodes进行测试，不再\n                            另行计算节点敏感度并根据pick_threshold选择敏感节点。\ndata_num          -d        参数作用：计算量化敏感度需要的数据数量。 取值范围：大于0，小于等于calibration_data中数据的总数。   可选\n                            默认配置：1。 参数说明：设置计算节点量化敏感度时所需要的数据数量。\nsensitive_nodes   -sn       参数作用：指定需要分析的敏感节点。 取值范围：校准模型中的所有节点。 默认配置：无。                  可选\n                            参数说明：可选，指定需要分析的敏感节点。 注：当设置此参数时，则直接对此参数中的节点进行测试，不再另行计算\n                            节点敏感度并根据pick_threshold选择敏感节点。\nsave_dir          -sd       参数作用：保存路径。 取值范围：无。 默认配置：无。 参数说明：可选，指定分析结果的保存路径。             可选\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\n分析结果展示：\n\n\n\n图中：\n\n * 蓝色虚线：baseline，即浮点模型输出与自身的余弦相似度，为1。\n\n * 绿色x：只量化当前节点得到部分量化模型，计算部分量化模型与浮点模型最终输出的相似度。\n\n * 红色实线：不量化当前节点以及当前节点前的所有节点，计算部分量化模型与浮点模型最终输出的相似度。例如：上图中Conv_92对应的相似度数值大概在0.995左\n   右，表明解除Conv_2、Conv_7和Conv_92节点的量化并保持其余所有节点量化得到部分量化模型，该部分量化模型的最终输出与浮点模型的最终输出之间的\n   余弦相似度为0.995左右。横坐标第一个none，在红色实线中的含义为calibrated_model。\n\n\nrunall#\n\n功能：一键运行原本debug工具中的所有功能。\n\n命令行格式：\n\n\n\n可通过 hmct-debugger runall -h/--help 查看相关参数。\n\n参数组：\n\nAPI使用方法：\n\n\n\n命令行使用方法：\n\n\n\nrunall流程：\n\n\n\n当所有参数保持默认时，工具会依次执行以下功能：\n\n * step1和step2：分别获取权重校准节点和激活校准节点的量化敏感度。\n\n * step3：根据step1和step2的结果，分别取权重校准节点的top5和激活校准节点的top5绘制其数据分布。\n\n * step4：针对step3获取的节点，分别绘制其通道间数据分布的箱线图。\n\n * step5：绘制分别只量化权重和只量化激活的累积误差曲线。\n\n * step6：针对敏感节点进行部分量化以及单节点量化精度分析，由于图中示例并没有指定sensitive_nodes，因此需要debug工具自行计算普通节点的\n   量化敏感度并选取敏感度小于指定pick_threshold的节点进行测试分析。\n\n当指定 node_type='node' 时，工具会获取top5节点，并分别找到每个节点对应的校准节点，并获取校准节点的数据分布和箱线图。","routePath":"/guide/ptq/ptq_tool/accuracy_debug","lang":"zh","toc":[{"text":"快速上手","id":"快速上手","depth":2,"charIndex":423},{"text":"校准模型与数据的保存","id":"校准模型与数据的保存","depth":3,"charIndex":1012},{"text":"精度debug模块导入与使用","id":"精度debug模块导入与使用","depth":3,"charIndex":1991},{"text":"分析结果展示","id":"分析结果展示","depth":3,"charIndex":2152},{"text":"功能说明文档","id":"功能说明文档","depth":2,"charIndex":-1},{"text":"get_sensitivity_of_nodes","id":"get_sensitivity_of_nodes","depth":3,"charIndex":-1},{"text":"plot_acc_error","id":"plot_acc_error","depth":3,"charIndex":-1},{"text":"plot_distribution","id":"plot_distribution","depth":3,"charIndex":-1},{"text":"get_channelwise_data_distribution","id":"get_channelwise_data_distribution","depth":3,"charIndex":-1},{"text":"sensitivity_analysis","id":"sensitivity_analysis","depth":3,"charIndex":-1},{"text":"runall","id":"runall","depth":3,"charIndex":10624}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":447,"title":"模型验证","content":"#\n\n在实际工程中，由于并非所有浮点模型均能够转为量化模型，因此在转换之前需要进行一次检查，这个check过程，会走一遍模型转换的过程，但是对于比较耗时的步骤，进行了\n简化处理。该命令在完成模型的检查后，会输出检查结果和OP在设备上的部署情况。\n\n\n使用方法#\n\n注意\n\n请先使用 HBRuntime 对您的浮点onnx模型进行推理，确保模型合法，再参考下方方法使用hb_compile对模型是否可转换为量化模型进行验证。\n\n使用hb_compile验证模型时，参考指令如下：\n\n\n\n如您的模型为多输入模型，可参考如下指令：\n\n\n\n\n命令行参数#\n\n注解\n\n如您在模型验证过程中发现 The converted model node information 提示结果和在模型转换过程中得到的 The converted\nmodel node information\n提示结果不一致，可能是因为在做模型验证的过程中，实际上是有一个默认的yaml配置的，而如果您在进行转换前进行了yaml的配置，一些参数的不同可能会导致此种情况的\n发生，可能会导致此种情况发生的yaml配置参数包括：mean_value 、 scale_value 、 std_value 和 quant_config。","routePath":"/guide/ptq/ptq_tool/hb_compile/check","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":124},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":266}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":448,"title":"模型量化编译","content":"#\n\n模型量化编译过程中，hb_compile工具会根据配置文件的具体信息，生成中间阶段onnx模型文件以及仿真上板情况的runtime模型。\n\n\n使用方法#\n\n使用hb_compile工具对模型进行量化编译时，提供两种模式，快速性能评测模式（开启fast-perf）和传统模型转换编译模式（不开启fast-perf）。\n\n快速性能评测模式开启后，会在转换过程中生成可以在板端运行最高性能的hbm模型，工具内部主要进行以下操作：\n\n * 将BPU可执行算子尽可能运行在BPU上。\n * 删除模型首尾可删除的CPU算子，包括：Quantize/Dequantize、Transpose、Cast、Reshape等。\n\n如您想使用快速性能评测模式（开启fast-perf），使用方式如下：\n\n\n\n注意\n * 请注意，如您开启快速性能评测模式，由于该模式下，工具会使用内置的高性能配置，请勿对 --config 参数进行配置。\n * 在使用hb_compile做模型量化编译时， --input-shape 参数配置仅在快速性能评测模式（即开启fast-perf）时生效。\n\n如您想使用传统模型转换编译模式（不开启fast-perf），使用方式如下：\n\n\n\n\n命令行参数#\n\n编译产生的log文件会储存在命令执行路径下面，名称为 hb_compile.log。\n\n\n配置文件模板 #\n\n一份完整的配置文件模板如下：\n\n注解\n\n此处配置文件仅作展示，在实际模型配置文件中 caffe_model 与 onnx_model 两种只存在其中之一。\n\n即，要么是Caffe模型，要么是ONNX模型。即 caffe_model + prototxt 或者 onnx_model 二选一。\n\n\n\n配置文件主要包含模型参数组、输入信息参数组、校准参数组、编译参数组。在您的配置文件中，每个参数组位置都需要存在，具体参数分为可选和必选，可选参数可以不配置。\n\n以下是具体参数信息，参数会比较多，我们依照上述的参数组次序介绍。可选/必选表示该参数项在配置文件中是否必须进行配置。\n\n\n配置文件具体参数信息 #\n\n\n模型参数组 #\n\n\n输入信息参数组 #\n\ninput_type_rt/ input_type_train补充说明\n\n地平线的计算平台架构，在设计时为了提升性能，做了两点假设：\n\n 1. 假设输入的数据都是int8的量化数据。\n\n 2. 摄像头获取到的数据是nv12。\n\n因此，如果您在模型训练时使用rgb(NCHW)输入格式，但是想使这个模型能够高效处理nv12数据，只需要在模型转换时做如下配置：\n\n\n\n除了将输入数据转换为nv12，我们还支持您在训练和runtime infer时使用不同的rgb-order。工具会根据 input_type_rt 和\ninput_type_train 指定的数据格式自动添加数据转换节点，\n根据地平线的实际生产经验，并不是任意type组合都是需要的，为了避免您误用，我们只开放了一些固定的type组合如下表(Y为已支持类型，N为暂不支持类型，\n表格中第一行是 input_type_rt 中支持的类型，第一列是 input_type_train 支持的类型)：\n\nINPUT_TYPE_TRAIN \\ INPUT_TYPE_RT   NV12   YUV444   RGB   BGR   GRAY   FEATUREMAP\nyuv444                             Y      Y        N     N     N      N\nrgb                                Y      Y        Y     Y     N      N\nbgr                                Y      Y        Y     Y     N      N\ngray                               N      N        N     N     Y      N\nfeaturemap                         N      N        N     N     N      Y\n\n注解\n\n为了配合计算平台对于输入数据类型的要求（int8），减小推理开销，对于 input_type_rt 类型为\nrgb(NHWC/NCHW)/bgr(NHWC/NCHW) 的配置，转换工具转换出的模型， 其输入数据类型均为\nint8。也就是说，对于常规的图像数据，需要-128使用（该操作在API中已自动进行，无需再进行该操作）。\n\n在转换得到的最终产出hbm模型中， input_type_rt 到 input_type_train 是一个内部的过程，您只需要关注 input_type_rt\n的数据格式即可。 正确理解每种input_type_rt的要求，对于嵌入式应用准备推理数据很重要，以下是对input_type_rt每种格式的说明：\n\n * rgb、bgr和gray都是比较常见的图像格式，注意每个数值都采用UINT8表示。\n\n * yuv444是一种常见的图像格式，注意每个数值都采用UINT8表示。\n\n * nv12是常见的yuv420图像格式，每个数值都采用UINT8表示。\n\n * nv12有个比较特别的情况是 input_space_and_range 设置 bt601_video\n   时，较于常规nv12情况，它的数值范围由[0,255]变成了[16,235]，每个数值仍然采用UINT8表示。 请注意， bt601_video 仅在\n   input_type_train 为 bgr 或 rgb 时支持通过 input_space_and_range 进行配置。\n\n * featuremap适用于以上列举格式不满足您需求的情况，此type每个数值采用float32表示。例如雷达和语音等模型处理就常用这个格式。\n\n小技巧\n\n以上 input_type_rt 与 input_type_train 是固化在工具链的处理流程中，如果您非常确定不需要转换，将两个 input_type\n设置成一样就可以了， 一样的 input_type 会做直通处理，不会影响模型的实际执行性能。\n同样的，数据前处理也是固化在流程中，如果您不需要做任何前处理，不对 mean_value 、scale_value 及 std_value\n进行配置即可，不会影响模型的实际执行性能。\n\n\n校准参数组 #\n\n\n编译参数组 #\n\n\nparam_value配置 #\n\n具体参数的设置形式为：param_name: 'param_value' ，参数存在多个值时使用 ';' 符号分隔：param_name:\n'param_value1;param_value2;param_value3' 。\n\n小技巧\n\n当模型为多输入模型时，强烈建议您将 input_shape 等参数们显式的写出，以免造成参数对应顺序上的错误。\n\n注意\n\n请注意，如果设置 input_type_rt 为 nv12，则模型的输入尺寸中不能出现奇数。","routePath":"/guide/ptq/ptq_tool/hb_compile/convert","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":73},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":526},{"text":"配置文件模板","id":"配置文件模板","depth":2,"charIndex":-1},{"text":"配置文件具体参数信息","id":"配置文件具体参数信息","depth":2,"charIndex":-1},{"text":"模型参数组","id":"模型参数组","depth":3,"charIndex":-1},{"text":"输入信息参数组","id":"输入信息参数组","depth":3,"charIndex":-1},{"text":"校准参数组","id":"校准参数组","depth":3,"charIndex":-1},{"text":"编译参数组","id":"编译参数组","depth":3,"charIndex":-1},{"text":"param_value配置","id":"param_value配置","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":449,"title":"模型修改及HBIR模型编译","content":"#\n\n出于某些极大尺寸输入场景下的极致性能需求，部分输入/输出端的量化和转置操作可以融合在数据前处理中一并完成。 此时您可以选择使用 hb_compile\n工具移除这些节点，同时hb_compile工具还支持对HBIR模型的编译。\n\n\n使用方法#\n\n使用hb_compile修改模型以及对HBIR模型进行编译时，参考指令如下：\n\n\n\n\n命令行参数#","routePath":"/guide/ptq/ptq_tool/hb_compile/modify","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":117},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":450,"title":"quant_config说明","content":"#\n\n编译模型时可以通过quant_config进行量化参数的配置，支持在model_config、op_config、node_config三个层面配置模型量化参数\n：\n\n * model_config：配置模型总体的量化参数，key是自定义名称。\n\n * op_config：配置某类算子的量化参数，key是算子的类型。\n\n * node_config：配置某个具体节点的量化参数，key是节点的名字。\n\n配置时三个层面存在优先级关系，配置粒度越小优先级越高，即优先级model_config < op_config <\nnode_config，当某个节点同时被多个维度配置时，优先级高的维度最终生效。\n\n\n配置节点计算精度#\n\n支持配置 int8/int16/float16 三种计算精度数据类型，关于这三种数据类型的具体说明如下：\n\n * int8：大部分算子的默认量化类型，一般无需您主动进行配置。\n\n * int16：请参考下方 int16配置说明。\n\n * float16：配置为float16类型时，工具内部将仅配置该算子为float16计算精度类型（不会对float16计算算子上下文算子进行计算广播更新）。\n\n\n配置全局的模型计算精度#\n\n\n配置某类算子的计算精度#\n\n\n配置某个节点的计算精度#\n\n\n配置模型校准参数#\n\n支持配置多种校准方法如kl、max等校准阈值统计方法，对于每种校准方法，也支持灵活控制该方法的具体超参配置（如果没有配置则使用默认参数），此外允许您配置一些如p\ner_channel、asymmetric、bias_correction等独立量化功能。\n\n注意\n\n若quant_config不进行任何配置，则默认会尝试多种校准方法并选择其中量化损失最小的校准方法。\n\n\n配置激活校准参数#\n\n\n配置权重校准参数#\n\n\n配置校准参数搜索方法#\n\n支持两种不同粒度的校准参数搜索方法：\n\n * modelwise_search：在模型层面对量化参数进行搜索，该方法允许一次性配置多种校准方法，通过比较量化前后模型输出的量化损失metric（可配置）\n   ，找到一个量化损失最小的校准方法。\n\n * layerwise_search：在节点层面对量化参数进行搜索，该方法会根据每个节点量化前后模型输出，计算量化损失metric（可配置），为该节点分配量化\n   损失最小的校准方法。\n\n注意\n\n如果配置了多个校准方法，会默认启动modelwise搜索，从多个候选校准模型中找出最优的量化模型；并从中找出最优的校准方法如果配置了layerwise参数，则启\n动layerwise搜索方法，逐层搜索最优的量化参数。\n\n\n配置modelwise搜索方法#\n\n\n配置layerwise搜索方法#\n\n\njson模板配置示例#\n\n如下所示为quant_config的json模板配置示例，包含所有可配置选项，您可参考此模板进行配置。\n\n\n\n\nint16配置说明 #\n\n在模型转换的过程中，模型中的大部分算子都会被量化到int8进行计算，而通过配置 quant_config 参数，\n可以详细指定某个op的输入/输出数据类型为int16计算（具体支持的算子范围可参考 工具链算子支持约束列表-ONNX Operator Support\nList，基本原理如下：\n\n在您配置了某个op输入/输出数据类型为int16后，模型转换内部会自动进行op输入输出上下文（context）int16配置的更新和检查。\n例如，当配置op_1输入/输出数据类型为int16时，实际上潜在同时指定了op_1的上/下一个op需要支持以int16计算。\n对于不支持的场景，模型转换工具会打印log提示该int16配置组合暂时不被支持并回退到int8计算。","routePath":"/guide/ptq/ptq_tool/hb_compile/quant_config","lang":"zh","toc":[{"text":"配置节点计算精度","id":"配置节点计算精度","depth":2,"charIndex":305},{"text":"配置全局的模型计算精度","id":"配置全局的模型计算精度","depth":3,"charIndex":515},{"text":"配置某类算子的计算精度","id":"配置某类算子的计算精度","depth":3,"charIndex":530},{"text":"配置某个节点的计算精度","id":"配置某个节点的计算精度","depth":3,"charIndex":545},{"text":"配置模型校准参数","id":"配置模型校准参数","depth":2,"charIndex":560},{"text":"配置激活校准参数","id":"配置激活校准参数","depth":3,"charIndex":756},{"text":"配置权重校准参数","id":"配置权重校准参数","depth":3,"charIndex":768},{"text":"配置校准参数搜索方法","id":"配置校准参数搜索方法","depth":2,"charIndex":780},{"text":"配置modelwise搜索方法","id":"配置modelwise搜索方法","depth":3,"charIndex":1127},{"text":"配置layerwise搜索方法","id":"配置layerwise搜索方法","depth":3,"charIndex":1146},{"text":"json模板配置示例","id":"json模板配置示例","depth":2,"charIndex":1165},{"text":"int16配置说明","id":"int16配置说明","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":451,"title":"hb_config_generator工具","content":"#\n\nhb_config_generator 是用于支持您获取模型编译最简yaml配置文件、包含全部参数默认值的yaml配置文件的工具。其中：\n\n * 最简yaml配置文件中会包含您可以用于快速验证模型转换过程能否正常执行的最简参数配置。\n\n * 包含全部参数默认值的yaml配置文件中会包含模型编译时具有默认值的所有参数。\n\n\n使用方法#\n\n 1. 生成最简yaml配置文件：\n    \n    \n\n 2. 生成基于模型信息的最简yaml配置文件：\n    \n    \n\n 3. 包含全部参数默认值的yaml配置文件：\n    \n    \n\n 4. 生成基于模型信息的全部参数默认值yaml配置文件：\n    \n    \n\n\n命令行参数#\n\n参数名称                               参数说明\n-h, --help                         显示帮助信息。\n-s, --simple-yaml                  生成最简yaml配置文件，用于您快速验证模型转换能否正常进行。\n-f, --full-yaml                    生成包含全部参数默认值的yaml配置文件。\n-m, --model                        Caffe或ONNX模型文件。\n-p, --proto                        用于指定Caffe模型prototxt文件。\n--march                            BPU的微架构。使用J6E处理器需设置为 nash-e ，使用J6M处理器需设置为 nash-m。\n\n\n输出内容说明#\n\nhb_config_generator 命令会根据您的配置生成对应的最简yaml配置文件或包含全部参数默认值的yaml配置文件。\n\n注意\n\n请注意，在使用包含完整参数默认值的yaml文件前需要将该文件内的默认值修改为您当前实际情况的对应值。","routePath":"/guide/ptq/ptq_tool/hb_config_generator","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":164},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":313},{"text":"输出内容说明","id":"输出内容说明","depth":2,"charIndex":724}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":452,"title":"hb_eval_preprocess工具","content":"#\n\n用于对模型精度进行评估时，在x86环境下对图片数据进行预处理。所谓预处理是指图片数据在送入模型之前的特定处理操作。比如：图片resize、crop和paddin\ng等。\n\n\n使用方法#\n\n\n\n\n命令行参数#\n\n参数名称                         参数说明\n-h, --help                   显示帮助信息。\n--version                    显示版本并退出。\n-m, --model_name             设置模型名称，支持的模型范围可通过 hb_eval_preprocess --help 查看。\n-i, --image_dir              输入图片路径。\n-o, --output_dir             输出路径。\n-v, --val_txt                设置评测所需图片的文件名称，预处理生成的图片将与此文件中的图片名称对应。\n\n\n输出内容说明#\n\nhb_eval_preprocess 命令将会在 --output_dir 指定的路径下生成图片二进制文件。\n\n小技巧\n\n更多关于 hb_eval_preprocess 工具在上板模型精度评估中的应用示例请参见 数据预处理 一节内容。","routePath":"/guide/ptq/ptq_tool/hb_eval_preprocess","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":89},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":99},{"text":"输出内容说明","id":"输出内容说明","depth":2,"charIndex":433}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":453,"title":"hb_model_info工具","content":"#\n\nhb_model_info 是用于解析*.hbm和*.bc编译时的依赖及参数信息、*.onnx模型基本信息，同时支持对*.bc可删除节点进行查询的工具。\n\n\n使用方法#\n\n\n\n\n命令行参数#\n\n参数名称                           参数说明\n-h, --help                     显示帮助信息。\n--version                      显示版本并退出。\n-n, --name                     后接模型名称。在HBM_FILE为pack模型时指定所需模型，用于输出该指定模型的模型编译信息。\n-v, --visualize                启动webserver以查看模型结构，查看完毕后可使用 Ctrl+C 关闭。\n\n\n输出内容说明#\n\n输出部分将会是模型编译时的一些输入信息，如下所示：\n\n注解\n\n以下代码块中的版本号信息等内容将随发布包版本变化，此处仅为示例。\n\n","routePath":"/guide/ptq/ptq_tool/hb_model_info","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":81},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":91},{"text":"输出内容说明","id":"输出内容说明","depth":2,"charIndex":366}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":454,"title":"hb_verifier工具","content":"#\n\nhb_verifier 是一致性验证工具，支持进行onnx模型之间、onnx模型与hbir模型、hbir模型与hbir模型之间的余弦相似度对比，\nbc与Hbm模型之间的输出一致性对比。\n\n余弦相似度表示不同阶段量化模型之间的一致性，余弦相似度越接近1，说明对比的两个量化模型的输出越接近。\n\n一致性对比则会打印对比模型的输出一致性信息，包括输出名称、一致性、不一致元素数量、最大绝对误差、最大相对误差。\n\n\n支持范围#\n\n * Fbc代表浮点hbir模型。\n\n * Qbc代表定点hbir模型。\n\n\n使用方式#\n\n\n\n\n命令行参数#\n\n\n参考使用场景样例#\n\n 1. ONNX模型与ONNX模型之间进行余弦相似度对比。\n\n以模型优化阶段模型 optimized_float_model.onnx 与模型校准阶段模型 calibrated_model.onnx 为例：\n\n\n\n 2. ONNX模型与HBIR模型之间进行余弦相似度对比。\n\n以模型优化阶段模型 optimized_float_model.onnx 与模型量化阶段定点模型 quantized_model.bc 为例：\n\n\n\n 3. HBIR模型与HBM模型之间进行输出一致性对比。\n\n以模型量化阶段定点模型 quantized_model.bc 与模型编译阶段模型 googlenet.hbm为例：\n\n\n\n\n输出内容说明#\n\n\n余弦相似度对比#\n\n所对比模型的余弦相似度信息会打印在终端内，如下示例所示：\n\n\n\n其中：\n\n * NodeName代表算子名称。\n\n * TensorName代表该算子的第一个输出的Tensor名称。\n\n * ConsineSimilarity代表计算出的余弦相似度。\n\n\n输出一致性对比#\n\n\n\n其中：\n\n * OutputName代表输出名称。\n\n * Consistency代表该输出是否一致。\n\n * Mismatched Elements 代表不一致元素数量和占比。\n\n * Max Abs Diff 代表最大绝对误差。\n\n * Max Rel Diff 代表最大相对误差。","routePath":"/guide/ptq/ptq_tool/hb_verifier","lang":"zh","toc":[{"text":"支持范围","id":"支持范围","depth":2,"charIndex":206},{"text":"使用方式","id":"使用方式","depth":2,"charIndex":252},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":262},{"text":"参考使用场景样例","id":"参考使用场景样例","depth":2,"charIndex":271},{"text":"输出内容说明","id":"输出内容说明","depth":2,"charIndex":587},{"text":"余弦相似度对比","id":"余弦相似度对比","depth":3,"charIndex":597},{"text":"输出一致性对比","id":"输出一致性对比","depth":3,"charIndex":735}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":455,"title":"HBRuntime推理库","content":"#\n\nHBRuntime是地平线提供的一套x86端模型推理库，支持对常用训练框架直接导出的ONNX原始模型、地平线工具链进行PTQ转换过程中产出的各阶段ONNX模型以\n及地平线工具链转换过程中产出的HBIR(*.bc)模型和HBM(*.hbm)模型进行推理，使用流程如下图所示：\n\n\n使用方法及说明#\n\n使用HBRuntime进行模型推理时，参考使用方法如下：\n\n\n\n注解\n\n * output_names：用于配置输出名称，支持配置为None或自定义配置，如无特殊要求，这里我们推荐您设置为None。\n   \n   * 如配置为None，工具内部会读取模型内的输出节点信息并按解析的顺序给出推理结果。\n   \n   * 如自定义配置，可以配置全量或部分output_name，且支持修改输出顺序。则推理完成后，会按您配置的输出名称和顺序返回输出。\n\n * input_feed：用于配置模型运行的输入，需要按照输入类型和layout进行准备。配置格式要求为字典形式，输入名称和输入数据组成键值对。\n\n此外，HBRuntime支持您在使用时查看模型属性信息，支持查看如下模型属性信息。例如如想打印查看模型输入数量，参考方法：print(f\"input_num:\n{sess.input_num}\")。\n\nMODEL_ATTRIBUTE   说明\ninput_num         模型输入数量\noutput_num        模型输出数量\ninput_names       模型输入名称\noutput_names      模型输出名称\ninput_types       模型输入数据类型\noutput_types      模型输出数据类型\ninput_shapes      模型输入shape\nouput_shapes      模型输出shape\n\n\n使用示例参考#\n\n下方针对ONNX模型推理和HBIR模型推理两种使用场景，分别为您提供HBRuntime的使用示例参考。\n\n\nONNX模型推理#\n\n使用HBRuntime加载ONNX模型推理的基本流程如下所示，这份示例代码适用于所有ONNX模型的推理，根据不同模型的输入类型和layout要求准备数据即可：\n\n\n\n\nHBIR模型推理#\n\n使用HBRuntime加载HBIR模型推理的基本流程如下所示，这份示例代码适用于HBIR模型的推理，根据模型的输入类型和layout要求准备数据即可：\n\n\n\n\nHBM模型推理#\n\n使用HBRuntime加载HBM模型推理的基本流程如下所示，这份示例代码适用于HBM模型的推理，根据模型的输入类型和layout要求准备数据即可：\n\n","routePath":"/guide/ptq/ptq_tool/hbruntime","lang":"zh","toc":[{"text":"使用方法及说明","id":"使用方法及说明","depth":2,"charIndex":141},{"text":"使用示例参考","id":"使用示例参考","depth":2,"charIndex":784},{"text":"ONNX模型推理","id":"onnx模型推理","depth":3,"charIndex":847},{"text":"HBIR模型推理","id":"hbir模型推理","depth":3,"charIndex":942},{"text":"HBM模型推理","id":"hbm模型推理","depth":3,"charIndex":1033}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":456,"title":"PTQ转换工具阅读概览","content":"#","routePath":"/guide/ptq/ptq_tool/tool_overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":457,"title":"模型精度分析","content":"#\n\n基于几十或上百张校准数据实现浮点模型到定点模型转换的后量化方式，不可避免地会存在一定的精度损失。但经过大量实际生产经验验证，如果能筛选出最优的量化参数组合，地平\n线的转换工具在大部分情况下，都可以将精度损失保持在1%以内。\n\n本节先介绍了如何正确地进行模型精度分析，如果通过评估发现不及预期，则可以参考 模型精度调优 章节的内容尝试调优， 实在无法解决可寻求地平线的技术支持。\n\n在进入到此部分介绍前，我们希望您已经了解如何对一个模型进行精度评测。本节介绍的内容是如何使用模型转换的产出物进行推理。\n\n前文提到模型成功转换的产出物中包括以下模型产出物：\n\n * *_original_float_model.onnx\n\n * *_optimized_float_model.onnx\n\n * *_calibrated_model.onnx\n\n * *_ptq_model.onnx\n\n * *_quantized_model.bc\n\n * *.hbm\n\n虽然最后的hbm模型才是将部署到计算平台的模型，考虑到方便在Ubuntu开发机上完成精度评测，我们提供了*_quantized_model.bc完成这个精度评测\n的过程。quantized模型已经完成了量化，与最后的hbm模型具有一致的精度效果。使用地平线开发库加载模型推理的基本流程如下所示，下方示例代码不仅适用于qua\nntized模型，对original和optimized等onnx模型同样适用（替换模型文件即可），根据模型的输入类型和layout要求准备数据即可。\n\n","routePath":"/guide/ptq/ptq_usage/accuracy_evaluation","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":458,"title":"模型精度调优","content":"#\n\n基于前文的精度评估，您可能发现精度的评估结果不及预期。 本章节内容为您介绍，在您通过PTQ链路进行模型转换过程中出现量化精度损失时，\n如何通过精度调优工具和功能进行精度调优以减少量化精度损失或协助您定位量化精度损失的原因。\n\n注意\n\n下文所指量化精度调优均指前文所述的量化过程中生成的 calibrated_model.onnx 量化精度调优。\n\n\n精度调优建议#\n\n您可通过调整量化方法或者计算精度来进行模型精度的调优，方法参考如下：\n\n\n量化方法#\n\n可以通过配置不同的校准方法、量化参数搜索方法，或者尝试对独立量化功能进行配置来尝试对模型量化方法进行调整：\n\n 1. 校准方法配置\n    \n    您可以尝试调整模型校准方法，如尝试kl、max等校准方法配置，配置方法可参考 quant_config说明 章节的介绍。\n\n 2. 量化参数搜索配置\n    \n    支持两种不同粒度的校准参数搜索方法：\n    \n    * modelwise_search：在模型层面对量化参数进行搜索，该方法允许一次性配置多种校准方法，通过比较量化前后模型输出的量化损失metric（可\n      配置），找到一个量化损失最小的校准方法。\n    \n    * layerwise_search：在节点层面对量化参数进行搜索，该方法会根据每个节点量化前后模型输出，计算量化损失metric（可配置），为该节点分\n      配量化损失最小的校准方法。\n    \n    配置方法可参考 quant_config说明 章节的介绍。\n\n 3. 独立量化功能配置\n    \n    开启独立量化模式可以减少计算资源的占用，您可以尝试对per_channel，asymmetric，bias_correction参数进行配置，配置方法可\n    参考 quant_config说明 章节的介绍。\n\n\n计算精度#\n\n除量化方法的配置外，可尝试对模型算子的计算精度（dtype）进行配置来尝试精度调优，当前支持由model，op_type，op_name三个层面对算子的计算精度\n进行配置，支持的配置类型包括int8、int16、float16及float32， 配置方法可参考 quant_config说明 章节的介绍。\n\n\n精度debug工具#\n\n如您想定位量化精度损失的具体是由哪些算子导致的，我们还为您提供了精度debug工具协助您定位，工具使用方法可参考 精度debug工具 章节的介绍。\n\n\n量化精度调优流程#\n\n基于我们过往对典型模型精度调优过程的经验积累，下方我们为您提供了一个兼顾易用性和实用性的精度调优流程：\n\n调优流程图详细说明如下：","routePath":"/guide/ptq/ptq_usage/accuracy_tune","lang":"zh","toc":[{"text":"精度调优建议","id":"精度调优建议","depth":2,"charIndex":177},{"text":"量化方法","id":"量化方法","depth":3,"charIndex":223},{"text":"计算精度","id":"计算精度","depth":3,"charIndex":797},{"text":"精度debug工具","id":"精度debug工具","depth":2,"charIndex":959},{"text":"量化精度调优流程","id":"量化精度调优流程","depth":2,"charIndex":1047}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":459,"title":"验证模型","content":"#\n\n为了确保模型能顺利在地平线平台高效运行，模型中所使用的算子需要符合平台的算子约束。算子约束部分给出了我们支持的具体算子，每个算子都给出了具体的参数限制，具体详细\n信息请参考 工具链算子支持约束列表-ONNX Operator Support List 章节的内容。\n\n\n使用 hb_compile 工具验证模型#\n\n考虑到地平线支持的算子较多，为了避免人工逐条校对的麻烦，我们支持您通过 hb_compile 工具来验证模型所使用算子的支持情况。\n\n工具使用方法请参考 模型验证 章节。\n\n\n检查异常处理#\n\n如果模型检查不通过， hb_compile 工具会报出ERROR。在当前工作目录下会生成 hb_compile.log 文件，从文件中可以查看到具体的报错。","routePath":"/guide/ptq/ptq_usage/check_model","lang":"zh","toc":[{"text":"使用 `hb_compile` 工具验证模型","id":"使用-hb_compile-工具验证模型","depth":2,"charIndex":-1},{"text":"检查异常处理","id":"检查异常处理","depth":2,"charIndex":248}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":460,"title":"浮点模型准备","content":"#\n\n在您阅读本节内容前，我们建议您先阅读我们的 工具链算子支持约束列表-ONNX Operator Support List\n章节，对地平线支持的算子及约束条件进行了解。或在您导出ONNX模型后，可以先参考 验证模型 章节的内容，验证模型是否能被地平线支持完成正常转换部署。\n\n基于公开DL框架训练得到的浮点模型是转换工具的输入，目前转换工具支持的DL框架如下：\n\n框架                地平线工具链\nCaffe             支持\nPyTorch           支持(转ONNX)\nTensorFlow        支持(转ONNX)\nTensorFlow Lite   支持(转ONNX)\nPaddlePaddle      支持(转ONNX)\n其他框架              请联系地平线\n\n以上框架中，除Caffe导出的caffemodel是直接支持的，其余框架均为转到ONNX实现间接支持，ONNX目前支持的opset版本是opset10-19。\n\n * 对于Caffe模型，您可先完成模型浮点精度的评测以确保模型的权重及结构正确。\n\n * 对于ONNX模型，您需先使用HBRuntime进行推理，验证ONNX模型和原DL框架模型推理结果一致（即验证模型合法性）。\n\n对于不同框架到ONNX的转换，目前都有对应的标准化方案，参考如下：\n\n * Pytorch2Onnx：PyTorch官方API支持直接将模型导出为ONNX模型，参考链接：\n   \n   https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n   。\n\n * Tensorflow2Onnx：基于ONNX社区的 onnx/tensorflow-onnx 进行转换，参考链接：\n   \n   https://github.com/onnx/tensorflow-onnx 。\n\n * TensorflowLite2Onnx：基于开源工具 tflite2onnx 进行转换，参考链接：\n   \n   https://github.com/zhenhuaw-me/tflite2onnx 。\n\n * PaddlePaddle2Onnx：PaddlePaddle官方API支持直接将模型导出为ONNX模型，参考链接：\n   \n   https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/onnx/export_\n   cn.html 。\n\n * 更多框架的ONNX转换支持，参考链接：\n   \n   https://github.com/onnx/tutorials#converting-to-onnx-format 。\n\n注意\n\n原始模型限制：ir_version≤9，10≤opset=≤19，ir_version与onnx版本的对应关系请参考 onnx官方文档。","routePath":"/guide/ptq/ptq_usage/model_prepare","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":461,"title":"模型性能分析","content":"#\n\n本节介绍了如何使用地平线提供的 hb_compile 工具评估模型性能，同时我们也提供了性能分析API接口，您也可以调用此接口进行模型性能分析。\n\n\n使用 hb_compile 工具#\n\n地平线提供的 hb_compile 工具支持对模型进行转换，工具的使用方法及相关的具体配置、参数请参考 模型量化编译 章节。\n\n在模型转换完成后，会在yaml文件配置的 working_dir\n路径下生成编译器预估的模型BPU部分的模型静态评估文件：model.html（可读性更好）和model.json。\n\n\n调用 API 接口#\n\n您也可以调用API接口进行模型分析。参考命令如下：\n\n\n\n注意\n\n请注意，此处 model.hbm 仅做示例，实际使用时，请替换为您所使用的模型正确路径。\n\n成功执行后，会在终端内打印模型FPS等基本信息，同时，在当前调用API接口的目录下，会生成该模型的静态性能评估文件：\n\n\n\n您可选择 model.html 或 model.json 对BPU部分的静态性能数据进行查看。","routePath":"/guide/ptq/ptq_usage/performance_evaluation","lang":"zh","toc":[{"text":"使用 `hb_compile` 工具","id":"使用-hb_compile-工具","depth":2,"charIndex":-1},{"text":"调用 API 接口","id":"调用-api-接口","depth":2,"charIndex":253}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":462,"title":"校准数据集准备","content":"#\n\n注解\n\n如果本过程，您需在示例文件夹内进行，那么您需要先执行文件夹中的 00_init.sh 脚本以获取对应的原始模型和数据集。\n\n在进行模型校准时，需要20~100份的标定样本输入，每一份样本都是一个独立的数据文件。为了确保校准后模型的精度效果，我们希望这些校准样本来自于您训练模型使用的训\n练集或验证集，不要使用非常少见的异常样本，例如纯色图片、不含任何检测或分类目标的图片等。\n\n您需要把取自训练集/验证集的样本进行前处理（前处理过程与原始浮点模型数据前处理过程一致），处理完后的校准样本会与原始浮点模型具备一样的数据类型(input_ty\npe_train)、尺寸(input_shape) 和 layout(input_layout_train)， 您可以通过 numpy.save\n命令将数据保存为npy文件，工具链校准时会基于 numpy.load\n命令进行读取。例如，有一个使用ImageNet训练的用于分类的原始浮点模型，它只有一个输入节点，输入信息描述如下：\n\n * 输入类型：BGR。\n\n * 输入layout：NCHW。\n\n * 输入尺寸：1x3x224x224。\n\n原始浮点模型进行数据前处理时的步骤如下：\n\n 1. 图像长宽等比scale，短边缩放到256。\n\n 2. center_crop 方法获取224x224大小图像。\n\n 3. 对齐输入layout为模型所需的 NCHW 。\n\n 4. 转换色彩空间为模型所需的 BGR 。\n\n 5. 图像数值范围调整为模型所需的[0, 255]。\n\n 6. 按通道减mean。\n\n 7. 数据乘以scale系数。\n\n针对上述举例模型的样本处理代码如下（为避免过长代码篇幅，各种简单transformer实现代码未贴出，transformer使用方法可参考\n图片处理transformer说明 ）：\n\n\n\n注意\n\n请注意，yaml文件中input_shape参数作用为指定原始浮点模型的输入数据尺寸。若为动态输入模型则可通过这个参数设置转换后的输入大小，而校准数据的shap\ne大小应与input_shape保持一致。\n\n例如：若原始浮点模型输入节点shape为?x3x224x224（\"?\"号代表占位符，即该模型第一维为动态输入），转换配置文件中设置input_shape:\n8x3x224x224，则需要准备的每份校准数据大小为8x3x224x224。（请知悉，此类输入shape第一维不等于1的模型，不支持通过input_batch\n参数修改模型batch信息。）","routePath":"/guide/ptq/ptq_usage/prepare_calibration_data","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":463,"title":"模型量化与编译","content":"#\n\n转换模型阶段会完成浮点模型到地平线板端可部署模型的转换，经过这个阶段，您将得到一个可以在地平线计算平台上运行的模型。在进行转换之前，请确保已经顺利通过了\n验证模型 小节的过程。\n\n模型转换期间会完成模型优化和校准量化等重要过程，校准需要依照模型预处理要求准备校准数据，您可以参考 校准数据准备 章节内容对校准数据进行预先准备。\n\n\n使用hb_compile工具转换模型#\n\n模型转换过程使用 hb_compile 工具完成，工具的使用方法及相关的具体配置、参数请参考 模型量化编译 章节。\n\n\n转换内部过程解读 #\n\n模型转换完成浮点模型到地平线板端可部署模型的转换。 为了使得这个模型能快速高效地在嵌入式端运行，模型转换重点作用于 输入数据处理 和 模型优化编译\n两个阶段，本节会依次围绕这两个重点问题展开。\n\n输入数据处理 方面地平线的边缘计算平台会为某些特定类型的输入通路提供硬件级的支撑方案，但是这些方案的输出不一定符合模型输入的要求。\n例如视频通路方面就有视频处理子系统，为采集提供图像裁剪、缩放和其他图像质量优化功能，这些子系统的输出往往是yuv420格式图像，而我们的算法模型往往是基于bgr\n/rgb等一般常用图像格式训练得到的。\n地平线针对此种情况提供的固定解决方案是，每个转换模型都提供两份输入信息描述，一份用于描述原始浮点模型输入（input_type_train\n和input_layout_train），另一份则用于描述我们需要对接的边缘平台输入数据（input_type_rt）。\n\n图像数据的mean/scale也是比较常见的操作，显然yuv420等边缘平台数据格式不再适合做这样的操作，因此，我们也将这些常见图像前处理固化到了模型中。经过以\n上两种处理后，转换产出的模型的输入部分将变成如下图状态。\n\n上图中的数据排布就只有NCHW和NHWC两种数据排布格式，N代表数量、C代表channel、H代表高度、W代表宽度，两种不同的排布体现的是不同的内存访问特性。在\nTensorFlow模型中，NHWC格式较常用，而Caffe模型中就都使用NCHW格式，地平线平台不会限制使用的数据排布，但是有一条要求：input_layou\nt_train 必须与原始模型的数据排布一致，正确的数据排布指定是顺利解析数据的基础。\n\n模型优化编译 包括模型解析、模型优化、模型校准与量化、模型编译几个重要阶段，其内部工作过程如下图所示。\n\n\n\n * 模型解析阶段 对于Caffe浮点模型会完成到ONNX浮点模型的转换，此阶段会对未命名的node/tensor进行算子命名(名称唯一)， 产出一个\n   original_float_model.onnx，这个ONNX模型计算精度是float32。\n\n * 模型优化阶段 实现模型的一些适用于地平线平台的算子优化策略，例如BN融合到Conv等。 此阶段的产出是一个\n   optimized_float_model.onnx，这个ONNX模型的计算精度仍然是float32，经过优化后不会影响模型的计算结果。\n\n * 模型校准阶段 会使用您提供的校准数据来计算必要的量化参数，通过校准数据计算得到的每个节点对应的量化参数并将其保存在校准节点中， 此阶段的产出是\n   calibrated_model.onnx，校准后还会进行一些操作，此过程的产出是 ptq_model.onnx。\n\n * 模型量化阶段\n   会使用地平线模型编译器，使用模型校准阶段生成的模型（ptq_model.onnx），根据您的前处理配置（包括input_type_rt到input_typ\n   e_train的色彩转换，mean/scale的处理等）进行模型量化。 此阶段的产出是一个\n   quantized_model.bc。使用这个模型可以评估到模型量化带来的精度损失情况。\n   如果模型量化过程中存在移除输入/输出端的节点的情况，生成物中还会保存 quantized_removed_model.bc。\n   此种场景下，如后续需进行一致性对比，我们建议您使用此HBIR模型与最终生成的hbm模型进行对比。\n\n注意\n\n请注意，当input_type_rt为nv12时，对应 quantized.bc 的输入layout为NHWC。\n\n * 模型编译阶段\n   会使用地平线模型编译器，将量化模型转换为地平线平台支持的计算指令和数据，这个阶段的产出一个*.hbm模型，这个hbm模型是后续将在地平线边缘嵌入式平台运行\n   的模型，也就是模型转换的最终产出结果。\n\n\n转换结果解读#\n\n本节将依次介绍模型转换成功状态的解读、转换不成功的分析方式。 确认模型转换成功，需要您从 compile 状态信息、相似度信息和 working_dir\n产出三个方面确认。 compile 状态信息方面，转换成功后将在控制台输出模型的依赖及参数信息。\n\n相似度信息存在于 compile 的控制台输出内容中，其内容形式如下：\n\n\n\n上面列举的输出内容中：\n\n * Node、NodeType表示节点名称、类型。\n\n * ON表示节点执行的device，包括BPU、CPU及JIT（先由CPU生成BPU指令再由BPU执行计算）。\n\n * Threshold是每个层次的校准阈值，用于异常状态下向地平线技术支持反馈信息，正常状况下不需要关注。\n\n * Calibrated\n   Cosine表示Node指示的节点中，优化后模型（optimized_float_model.onnx）与校准后模型（calibrated_model.o\n   nnx）输出结果的余弦相似度。\n\n * Quantized\n   Cosine表示Node指示的节点中，优化后模型（optimized_float_model.onnx）与模型量化后生成的定点模型（quantized_m\n   odel.bc）输出结果的余弦相似度。\n\n * Output Data Type表示节点的输出数据类型，取值范围为['si8', 'si16', 'si32', 'si64', 'ui8',\n   'ui16', 'ui32', 'ui64', 'f32']。\n\n注意\n\n需要您特别注意的是，余弦相似度只是指明量化后数据稳定性的一种参考方式，对于模型精度的影响不存在明显的直接关联关系。一般情况下，输出节点的相似度低于0.8就有了较\n明显的精度损失，当然由于与精度不存在绝对的直接关联，完全准确的精度情况还需要您参考 模型精度分析 的介绍。\n\n转换产出存放在转换配置参数 working_dir 指定的路径中，成功完成模型转换后，您可以在该目录下得到以下文件（*部分是您通过转换配置参数\noutput_model_file_prefix 指定的内容）：\n\n * *_original_float_model.onnx\n\n * *_optimized_float_model.onnx\n\n * *_calibrated_model.onnx\n\n * *_ptq_model.onnx\n\n * *_quantized_model.bc\n\n * *_quantized_removed_model.bc（存在移除输入/输出端的节点的情况）\n\n * *.hbm\n\n * *_advice.json\n\n * *_quant_info.json\n\n * *_node_info.csv\n\n * *.html\n\n * *.json\n\n转换产出物解读 介绍了每个模型产出物的用途。 不过在上板运行前，我们强烈建议您完成 模型性能分析 和 模型精度分析\n章节介绍的性能&精度评测过程，避免将模型转换问题延伸到后续嵌入式端。\n\n如果以上验证模型转换成功的三个方面中，有任一个出现缺失都说明模型转换出现了错误。一般情况下，compile\n工具会在出现错误时将错误信息输出至控制台，例如我们在Caffe模型转换时不配置 prototxt 和 caffe_model 参数，工具给出如下提示。\n\n\n\n如果以上步骤不能帮助您发现问题，欢迎在 地平线智能汽车开发者社区 提出您的问题，我们将为您提供支持。\n\n\n转换产出物解读 #\n\n上文提到模型成功转换的产出物，本节将介绍每个产出物的用途：\n\n * *_original_float_model.onnx：产出过程可以参考 转换内部过程解读\n   的介绍，这个模型计算精度与转换输入的原始浮点模型是一模一样的。一般情况下，您不需要使用这个模型，在转换结果出现异常时，如果能把这个模型提供给地平线的技术支\n   持，将有助于帮助您快速解决问题。\n\n * *_optimized_float_model.onnx：产出过程可以参考 转换内部过程解读\n   的介绍，这个模型经过一些算子级别的优化操作，常见的就是算子融合。通过与original_float模型的可视化对比，您可以明显看到一些算子结构级别的变化，\n   不过这些都不影响模型的计算精度。一般情况下，您不需要使用这个模型，在转换结果出现异常时，如果能把这个模型提供给地平线的技术支持，将有助于帮助您快速解决问题\n   。\n\n * *_calibrated_model.onnx：产出过程可以参考 转换内部过程解读\n   的介绍，这个模型是模型转换工具链将浮点模型经过结构优化后，通过校准数据计算得到的每个节点对应的量化参数并将其保存在校准节点中得到的中间产物。\n\n * *_ptq_model.onnx：产出过程可以参考 转换内部过程阶段 的介绍，这个模型是模型转换工具链对校准得到的模型进行预量化后的产物。\n\n * *_quantized_model.bc：产出过程可以参考 转换内部过程解读\n   的介绍，这个模型已经完成了校准和量化过程，量化后的精度损失情况可以从这里查看。这个模型是精度验证过程中必须要使用的模型，具体使用方式请参考 模型精度分析\n   部分的介绍。\n\n * *_quantized_removed_model.bc：产出过程可以参考 转换内部过程解读\n   的介绍，模型量化过程中如存在移除输入/输出端的节点的情况，会自动保存此移除节点的HBIR模型。此种场景下，如后续需进行一致性对比，我们建议您使用此HBIR\n   模型与最终生成的hbm模型进行对比。\n\n * *.hbm：可以用于在地平线计算平台上加载运行的模型， 配合 模型推理应用开发指导\n   部分介绍的内容，您就可以将模型快速在计算平台上部署运行。不过为了确保模型的性能与精度效果是符合您的预期的，我们强烈建议完成 模型性能分析 和 模型精度分析\n   介绍的性能和精度分析过程后再进入到应用开发和部署。\n\n * *_advice.json：文件内保存了模型编译过程中，地平线模型编译器op checker打印的结果。\n\n * *_quant_info.json：文件内记录了算子的校准量化信息。\n\n * *_node_info.csv：文件内保存了成功后算子的余弦相似度等信息的结果，与hb_compile成功执行后控制台输出的相似度信息相同。\n\n * *.json：模型的静态性能评估文件。\n\n * *.html：模型的静态性能评估文件（可读性更好）。","routePath":"/guide/ptq/ptq_usage/quantize_compile","lang":"zh","toc":[{"text":"使用hb_compile工具转换模型","id":"使用hb_compile工具转换模型","depth":2,"charIndex":168},{"text":"转换内部过程解读","id":"转换内部过程解读","depth":2,"charIndex":-1},{"text":"转换结果解读","id":"转换结果解读","depth":2,"charIndex":1911},{"text":"转换产出物解读","id":"转换产出物解读","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":464,"title":"PTQ转换原理及流程","content":"#\n\n\n概览#\n\n模型从训练到转换再到在开发板运行的过程如下图所示：\n\n模型训练，指使用TensorFlow、PyTorch、Caffe等公开深度学习框架得到可用模型的过程，通过训练得到的可用模型将作为模型转换阶段的输入。\n工具链本身不会提供训练相关的库或工具，具体支持的公开学习框架可以参考 浮点模型准备 章节中的说明。\n\n模型转换，本阶段以模型训练得到的浮点模型为输入，通过模型结构优化、模型校准量化、模型编译等重要步骤，将浮点模型转换为可以在地平线计算平台高效运行的模型。\n具体使用请参考 模型量化和编译 章节中的说明。\n\n嵌入式应用开发，工具链支持了在X86仿真环境和真实嵌入式环境的应用开发能力，在不方便使用开发板的情况下，您在仿真环境完成程序的调试和计算结果的验证。\n为了降低仿真验证的代价，工具链提供的仿真库接口与嵌入式接口完全一致，只是采用了不同的编译配置。 具体使用方式请参考 模型推理应用开发指导 章节中的说明。\n\n下面为您详细介绍PTQ转换的流程与步骤相关内容。\n\n\nPTQ转换流程说明#\n\n配合地平线工具链的模型完整开发过程，需要经过 浮点模型准备 、 模型检查 、 模型转换 、 性能评估 和 精度评估 共五个重要阶段，如下图所示。\n\n浮点模型准备 阶段的产出是输入到模型转换工具的浮点模型，这些模型一般都是基于公开DL训练框架得到的，需要您注意的是将模型导出为地平线工具支持的格式。\n具体要求与建议请参考 浮点模型准备 章节。\n\n模型检查 阶段用来确保算法模型是符合计算平台要求的。地平线提供了指定工具完成此阶段检查，对于不符合要求的情况，\n检查工具会明确给出不符合要求的具体算子信息，方便您结合算子约束的说明将模型调整过来。具体使用请参考 验证模型 章节。\n\n模型转换\n阶段将完成浮点模型到地平线板端可部署模型的转换。为了模型能在地平线计算平台上高效运行，地平线转换工具内部会完成模型优化、量化和编译等关键步骤，地平线的量化方法经\n过了长期的技术与生产验证，在大部分典型模型上可以保证精度损失小于1%。 具体使用请参考 校准数据准备 及 模型量化与编译 章节。\n\n性能评估 阶段提供了系列评估模型性能的工具。在应用部署前，您可以使用这些工具验证模型性能是否达到应用要求。\n对于部分性能不及预期的情况，也可以参考地平线提供的性能优化建议进行调优。 具体评估请参考 模型性能分析 章节。\n\n精度评估\n阶段提供了系列评估模型精度的工具。大部分情况下，地平线转换后模型可以保持与原始浮点模型基本一致的精度效果，在应用部署前，您可以使用地平线工具验证模型的精度是否符\n合预期。 对于部分精度不及预期的情况，也可以参考地平线提供的性能优化建议进行调优。 具体评估请参考 模型精度分析 章节。\n\n注意\n\n * 通常在模型转换后就已经得到了可以上板的模型，但是为了确保您得到的模型性能和精度都是符合应用要求的，地平线强烈建议每次转换后都完成后续的性能评估与精度评估步\n   骤。\n\n * 模型转换过程会生成onnx模型，该模型均为中间产物，只是便于验证模型精度情况，因此不保证其在版本间的兼容性。若使用示例中的评测脚本对onnx模型单张或在测\n   试集上进行评测时，请使用当前版本工具生成的onnx模型进行操作。\n\n\n模型转换过程详解#\n\n那么如何使用浮点模型转换工具将Caffe、TensorFlow、PyTorch等开源框架训练好的浮点模型转换成地平线硬件上支持的定点模型呢？\n\n一般情况下，从官网下载或者自己训练得到的模型，其阈值、权重等均为浮点数(float32)，每个数占用4个字节。\n但在嵌入式端运行时，若能将其内部数值从浮点数转化为定点数(int8)，则每个数只占用1个字节，因此计算量可以大大减少，\n因此，若能在不损失或损失较小的情况下将浮点模型转换为定点模型，则其性能会有显著提升。\n\n模型的转换通常可分为以下几个步骤：\n\n 1. 检查待转换的模型中是否包含不支持的OP类型。\n\n 2. 准备好20-100张校准使用的图片，用于转换阶段的校准操作。\n\n 3. 使用浮点模型转换工具将原始浮点模型转换为定点模型。\n\n 4. 对转换后的模型进行性能和精度的评估，确保转换后的定点模型精度与之前相差不大。\n\n 5. 在模拟器/开发板上运行模型，对模型的性能和精度进行验证。\n\n\n模型检查#\n\n注解\n\n如果本过程，您需在示例文件夹内进行，那么您需要先执行文件夹中的 00_init.sh 脚本以获取对应的原始模型和数据集。\n\n模型在从浮点转换为定点模型之前，我们首先需要通过工具 hb_compile 检查一下，看看该模型是否包含不能在地平线硬件上运行的OP。\n如果存在，则会在该阶段提示不认识该OP， 使用 hb_compile 验证模型的具体使用方式可参考 模型验证 一节内容。\n\n小技巧\n\n有关地平线硬件可支持的OP信息，请参见 工具链算子支持约束列表-ONNX Operator Support List 章节内容。\n\n\n准备校准图片#\n\n注解\n\n如果本过程，您需在示例文件夹内进行，那么您需要先执行文件夹中的 00_init.sh 脚本以获取对应的原始模型和数据集。\n\n在进行模型转换时，校准阶段需要20-100张图片输入进行校准操作。由于模型的输入类型及layout的不同，输入的图片格式可以多种多样。\n该阶段既可以输入原始图片（*.jpg等），也可以输入处理过的，满足模型输入要求的图片。您可以直接从模型训练时的数据集中获取相应的校准图片，也可以自行处理图片生成\n校准数据集。\n\n注解\n\n校准数据集需为npy格式。\n\n推荐您自行对校准图片进行前处理，将图片的通道(BGR/RGB)，数据排布(NHWC/NCHW)，图像大小及填充(Resize&Padding)等操作调整好后，工\n具会通过npy文件的方式将图片读入后送入校准阶段。\n\n以Resnet50为例，则需要进行如下transformer的操作：\n\n\n\n\n模型转换#\n\n当通过上述模型检查过程，确定了模型能够被转换成功后，接下来可以使用 hb_compile 工具来将浮点模型转换为一个可以在地平线硬件上运行的定点模型。\n\n该过程需要传入一个包含转换要求的配置文件(*.yaml)。具体的配置文件设置及各参数的含义，请参考 配置文件模板 和 配置文件具体参数信息 章节的介绍。\n\n模型转换过程结束后，还会将原始模型与转换后的定点模型的相似程度打印在log中，可根据 Quantized Cosine 字段来判断模型转换前后的相似度。\n如模型转换后的 Quantized Cosine 非常接近1，即代表模型转换后的模型表现会与转换前的浮点模型非常相近。\n\n注解\n\nLog中的相似度为校准图片中的第一张的相似度情况，并不完全代表模型转换前后的精度。\n\n在模型转换成功后，会在生成的文件夹 (默认为 model_output )中生成各阶段的模型产出物及信息文件，其中的模型产出物文件会在后续阶段使用到。\n\n注解\n\n * 可调用 03_classification/03_resnet50/03_build.sh 脚本，来体验 hb_compile\n   对模型做量化编译时的使用效果。\n\n * 如果希望了解更多模型转换的工作流程，请阅读 模型量化与编译 一节内容。\n\n\n单张图片的模型推理#\n\n在运行浮点模型转换之后，得到了定点模型，还需对其自身的正确性进行验证。\n\n您需要对模型的输入/输出结构比较了解，并能够正确地对模型输入图片做前处理以及模型输出的后处理解析，并自行编写模型运行脚本。在此过程中可参照交付包中各示例文件夹下\n04_inference.sh 中的示例代码。代码的主要逻辑如下：\n\n\n\n使用该脚本后，便可通过输入单张图片验证其自身的准确性。例如，该脚本的输入为一张斑马的图片，在经过前处理将图片数据从rgb处理到input_type_rt配置的数\n据类型后（关于中间类型的介绍可参考 转换过程内部解读 ， 通过 HBRuntime 命令传入模型进行推理，推理后进行后处理，最后打印出其最可能的5个种类。\n\n运行后的输出如下所示，最可能的类别是 label: 340。\n\n\n\nlabel 的类别使用的是ImageNet的类别， 340 正是对应着斑马，因此该图片推理正确。\n\n\n模型精度验证#\n\n光对单张图片进行验证还不足以说明模型的精度，因此还有脚本对模型转换后的精度进行评测。\n\n您需要编写代码使模型能够循环推理图片，并将结果与标准结果进行比较，得到精度结果。\n\n因为精度评测时，需要对图片进行 前处理，对模型数据进行 后处理，所以我们提供了一个示例Python脚本。\n其原理与单张推理一致，但需要在整个数据集上面运行。使用该脚本后，便可通过读取数据集，对模型的输出结果进行评判，并输出评测结果。\n运行该脚本耗费时间较长，但可以通过设置 PARALLEL_PROCESS_NUM 环境变量来设置运行评测的线程数量。\n\n在该脚本执行结束后，即可得到转换后的定点模型精度。\n\n注解\n * 在不同的系统下，由于依赖库版本不同，转换得到的模型精度可能会略有差别。\n * 同时由于版本更迭，得到的定点模型精度可能也会略有差别。\n\n\n[参考]支持的校准方法#\n\n目前我们支持了以下的校准方法：\n\n 1. default\n\ndefault 是一个自动搜索的策略，会尝试从系列校准量化参数中获得一个相对效果较好的组合。\n\n 2. mix\n\nmix\n是一个集成多种校准方法的搜索策略，能够自动确定量化敏感节点，并在节点粒度上从不同的校准方法中挑选出最佳方法，最终构建一个融合了多种校准方法优势的组合校准方式。\n\n 3. KL\n\nKL校准方法是借鉴了 TensorRT提出的解决方案 ，\n使用KL熵值来遍历每个量化层的数据分布，通过寻找最低的KL熵值，来确定阈值。这种方法会导致较多的数据饱和和更小的数据量化粒度，在一些数据分布比较集中的模型中拥有\n着比max校准方法更好的效果。\n\n 4. max\n\nmax校准方法是在校准过程中，自动选择量化层中的最大值作为阈值。这种方法会导致数据量化粒度较大，但也会带来比KL方法更少的饱和点数量，适用于那些数据分布比较离散\n的神经网络模型。\n\n\n[参考]OP列表#\n\n若想了解更多关于当前工具链支持的算子及对应约束条件，请参见 工具链算子支持约束列表-ONNX Operator Support List 章节内容。","routePath":"/guide/ptq/ptq_workflow","lang":"zh","toc":[{"text":"概览","id":"概览","depth":2,"charIndex":3},{"text":"PTQ转换流程说明","id":"ptq转换流程说明","depth":2,"charIndex":444},{"text":"模型转换过程详解","id":"模型转换过程详解","depth":2,"charIndex":1366},{"text":"模型检查","id":"模型检查","depth":3,"charIndex":1802},{"text":"准备校准图片","id":"准备校准图片","depth":3,"charIndex":2077},{"text":"模型转换","id":"模型转换","depth":3,"charIndex":2475},{"text":"单张图片的模型推理","id":"单张图片的模型推理","depth":3,"charIndex":3031},{"text":"模型精度验证","id":"模型精度验证","depth":3,"charIndex":3443},{"text":"[参考]支持的校准方法","id":"参考支持的校准方法","depth":3,"charIndex":3817},{"text":"[参考]OP列表","id":"参考op列表","depth":3,"charIndex":4239}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":465,"title":"目标检测全流程示例","content":"#\n\nOE包在 samples/ucp_tutorial/all-round 路径下提供了目标检测全流程示例，该示例描述了如何在J6上运行一个检测模型并展示结果。\n\n\n程序原理#\n\n本示例包含了CameraInputModule、InferenceModule、PostProcessModule、CodecModule和WebDisplay\nModule， 并使用生产者+消费者模型将各个模块串连起来，其关系如下图所示：\n\n * CameraInputModule：获取从sensor传入的金字塔图像。\n * InferenceModule：选择金字塔图像一层，调用ucp模型推理接口执行检测任务。\n * PostProcessModule：对预测的结果进行后处理，获取检测结果。\n * CodecModule：获取金字塔图像，编码为jpg格式数据输出。\n * WebDisplayModule：获取智能结果和编码后的jpg数据，通过websocket发送到web展示。\n\n\n编译与部署#\n\n\n\n上方命令执行后，会生成一个deploy目录，将该目录拷贝到J6板子上。\n\n\n运行#\n\n进入deploy文件夹，执行如下命令：\n\n\n\nx表示log的debug级别，支持i/d/w/e。\n\n * i: 打印info信息，可显示各个module的耗时和FPS，默认设置为i。\n * d: 打印debug信息。\n * w: 打印warning信息。\n * e: 打印error信息。\n\n\ncamera支持#\n\n目前支持OVX8b（Fov120）摄像头。\n\n\n如何使用#\n\n进入deploy文件夹，修改后处理配置文件可调整检测效果：\n\n\n\n运行全流程示例：\n\n\n\n打开浏览器，访问 http://IP。其中，IP即J6板端地址。点击web展示端链接，即可查看检测效果。","routePath":"/guide/ucp/full_process_sample","lang":"zh","toc":[{"text":"程序原理","id":"程序原理","depth":2,"charIndex":83},{"text":"编译与部署","id":"编译与部署","depth":2,"charIndex":438},{"text":"运行","id":"运行","depth":2,"charIndex":486},{"text":"camera支持","id":"camera支持","depth":2,"charIndex":638},{"text":"如何使用","id":"如何使用","depth":2,"charIndex":673}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":466,"title":"简介","content":"#\n\nHPL模块在以NN为中心的计算方案中，主要作用于模型的前后处理环节。在地平线统一架构中，多种硬件均已搭载了常用的高性能的算子，而HPL模块将高性能算子相关的硬件\n调用进行了封装，\n通过设置backend来选择不同的硬件方案（若不指定backend，UCP会自动适配负载更低的处理单元），从而平衡开发板负载，充分对开发板算力进行挖掘，\n规避了不同硬件调用区别带来的不便，您可更多地关注软件功能。其功能架构如下图所示：\n\n上述架构图中，应用通过HPL模块提供的算子的任务构造函数，如hbFFT1D、hbIFFT1D等，生成对应算子的任务句柄。\nUCP提供了包含任务调度、会话管理、引擎管理等模块的Service，在对应算子的任务句柄生成后，通过UCP任务调度接口将算子任务提交到任务队列，\n分配到不同的底层硬件，实现算子的功能逻辑。最底层为不同处理单元中封装实现的算子功能。\n\n在阅读本章节前，请先明确如下基础概念，相关内容在下文可能会多次被提及：\n\n * 高性能算子库：也称High Performance Library，指封装好的高性能算子库。\n\n * 算子：也称Operator，指算子库中包含的高性能函数。\n\n * DSP：全称Digital Signal Processing，指数字信号处理。\n\n * Backends：指UCP框架中的可分配处理单元。\n\n * Kernel：指算子中的卷积核参数。","routePath":"/guide/ucp/hpl/hpl1_introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":467,"title":"快速上手","content":"#\n\n本章节介绍了UCP示例包 ucp_tutorial\n中HPL模块的示例使用方法，以及如何配置开发环境、编译并运行示例应用代码，帮助快速上手UCP中的HPL功能模块。其主要架构如下：\n\n\n示例包使用#\n\n示例包结构如下所示：\n\n\n\n其中 ucp_tutorial/hpl 文件夹为HPL模块的示例，包括了快速傅里叶变换示例等，可同时支持板端运行编译和x86仿真编译两种方式。\n具体的示例原理及实现流程请查阅 示例 章节。\n\n\n编译示例算子#\n\n在编译运行示例应用代码前，您需要确保环境满足要求，根据 环境部署 部分的指引，您的开发机中应该已经安装有相关环境，要求如下：\n\n * cmake >= 3.0。\n\n * 对于板端编译需要指定交叉编译工具链，对于x86仿真docker自带编译器即可。\n\n在示例的 ucp_tutorial/hpl/code 目录下有预先配置好的编译脚本 build.sh，当前支持编译选项 -a x86 和 -a aarch64\n两种编译方式， 直接执行 build.sh 脚本即可完成一键编译，生成的文件会被保存到 ucp_tutorial/hpl/hpl_samples 目录下。\n此外，目录中也包含了 build_aarch64.sh 和 build_x86.sh 两个编译脚本，分别对应了两个编译选项， 使用这两个脚本进行编译与使用\nbuild.sh 脚本等效。 编译x86仿真的HPL示例需要执行的命令如下：\n\n\n\n执行编译脚本后，会生成运行示例所需要的执行程序和依赖文件，保存在 ucp_tutorial/hpl/hpl_samples 目录下。\n\n以HPL为例，其生成物如下所示，包含图片数据、示例运行脚本、运行依赖库、可执行文件以及运行示例的脚本目录，共同组成了完整的运行环境和运行依赖。\n\n\n\n\n运行示例#\n\n正确地完成编译的全部步骤后，可执行示例会被配置完毕并保存在 hpl_samples 文件夹内。下面根据执行环境不同，介绍上板与仿真两种运行示例的方式。\n\n上板运行#\n\n将 hpl_samples 文件夹整个拷贝到开发板上，进入 hpl_samples/script\n文件夹，直接执行示例文件夹中提供的运行脚本即可看到示例的运行结果。 开发板上执行脚本的参考指令如下：\n\n\n\nx86仿真运行#\n\n进入 hpl_samples/script_x86 文件夹，直接执行示例文件夹中提供的运行脚本即可看到示例的运行结果。 执行脚本的参考指令如下：\n\n\n\n注解\n\n地平线J6 SOC使用的是Cadence公司的Tensilica Vision Q8\nDSP，因此x86仿真实例中dsp算子运行依赖Cadence提供的一套工具链，环境配置可参考 DSP工具链安装 的指引， 需要正确配置 License\n以及环境变量 XTENSA_ROOT 即可。\n\n\n输出物说明#\n\n以x86仿真运行为例，示例运行时会在控制台内打印流程日志并生成对应输出文件。日志中会包含全部算子的调用流程，输出结果会被保存在 data\n文件夹中。示例部分输出如下：\n\n\n\n生成物会被保存到 hpl_samples/data 目录下，内容如下：\n\n\n\n\nHPL算子使用实例#\n\n本节通过一个简单的算子调用展示了如何使用HPL封装的算子实现快速傅里叶变换的功能。主要步骤包含了数据载入、任务创建、任务提交、任务完成、销毁任务、保存输出等。您\n可以阅读相应源码和注释进行学习。\n\n该示例的作用为使用hbFFT1D算子对输入数据进行FFT变换，具体实现如下：\n\n","routePath":"/guide/ucp/hpl/hpl2_quick_start","lang":"zh","toc":[{"text":"示例包使用","id":"示例包使用","depth":2,"charIndex":95},{"text":"编译示例算子","id":"编译示例算子","depth":3,"charIndex":214},{"text":"运行示例","id":"运行示例","depth":3,"charIndex":771},{"text":"上板运行","id":"上板运行","depth":4,"charIndex":855},{"text":"x86仿真运行","id":"x86仿真运行","depth":4,"charIndex":965},{"text":"输出物说明","id":"输出物说明","depth":3,"charIndex":1199},{"text":"HPL算子使用实例","id":"hpl算子使用实例","depth":2,"charIndex":1334}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":468,"title":"模块架构","content":"#\n\n\nHPL模块架构#\n\n通过HPL模块中的API创建算子的任务句柄，而每种算子接口都有着至少一种硬件实现，如fft算子可执行在DSP硬件上，\n在UCP中可对应的backend就包含DSP。创建好任务句柄后，可通过调度参数指定backend、执行模式及其他参数，将任务部署到对应的处理单元上。而在整个从任\n务创建到任务提交、释放的过程中，UCP Service层对各个环节均提供接口及功能支持。\n\n\n算子执行流程#\n\n此处以FFT_1D算子为例，展示了算子实际的调用方法和流程，其他算子的使用方法基本与其流程一致。\n\n\n\n注解\n\n虚线框表示该步骤为非必须步骤，可通过复用参数、使用默认参数等方式略过。\n\n 1. 准备输入输出数据：即申请数据的内存空间并构建相关的描述信息。\n\n 2. 创建算子任务：此步骤为直接调用算子任务接口，同时传入算子执行所需的参数，执行完成后输出UCP任务句柄。\n\n 3. 提交任务：通过传入调度参数将算子任务提交到不同处理核心，任务提交支持指定backend，如不指定则系统会自动适配backend。\n\n 4. 指定接口等待任务结束：任务结束时，系统会根据不同的执行状态返回不同的返回值，此时，您可根据返回值来查看任务执行结果。\n\n 5. 销毁任务：任务成功执行后需要销毁任务，并释放申请的内存。","routePath":"/guide/ucp/hpl/hpl3_architecture","lang":"zh","toc":[{"text":"HPL模块架构","id":"hpl模块架构","depth":2,"charIndex":3},{"text":"算子执行流程","id":"算子执行流程","depth":2,"charIndex":200}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":469,"title":"性能指标","content":"#\n\n本章节为您提供HPL性能测试说明，并为您展示性能数据结果统计供参考。\n\n\n测试平台#\n\n测试条件（硬件及软件平台要求）：\n\n * 测试环境：J6开发板。\n * OpenCV版本：3.4.5。\n * OpenCV运行环境：A78单线程。\n\n\n测试方法#\n\n * 性能测试方法：因算子运行耗时会有波动，所以这里我们取用算子运行多次的平均值性能数据。\n * 使用steady_clock测试时间。\n\n\n\n * 数据说明：\n   \n   * Task时间：算子任务执行时间。\n   * Baseline时间：OpenCV执行时间。\n   * Ratio：Baseline Avg / Task Avg。\n\n\n性能对比#\n\n注解\n\n测试使用的FFT点数为1024点，FFT 1D 测试用的数据量为20480000个浮点数。\n\nALGORITHM   HIGH PERFORMANCE LIBRARY(US)   OPENCV 3.4.5 A78(US)   RATIO(DEFAULT DSP)\nfft1d       88584                          1108364                12.5\nifft1d      68325                          1154695                16.9","routePath":"/guide/ucp/hpl/hpl4_performance","lang":"zh","toc":[{"text":"测试平台","id":"测试平台","depth":2,"charIndex":39},{"text":"测试方法","id":"测试方法","depth":2,"charIndex":122},{"text":"性能对比","id":"性能对比","depth":2,"charIndex":302}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":470,"title":"Fast Fourier Transform","content":"#\n\n快速傅里叶变换（Fast Fourier Transform，以下简称FFT）是离散傅里叶变换（Discrete Fourier\nTransform，以下简称DFT）的快速算法，是时域一频域变换分析中最基本的方法之一。该算法可以实现数据从时域到频域之间的转换，为数据提供了另一维度的处理方法\n支持。在工程实践中，DFT算法由于计算量过大，无法在实际中广泛实现，FFT算法通过改进DFT的计算方法，将计算量优化到了可以实践的量级，离散傅里叶变换才真正在工\n程领域广泛应用了起来。\n\n\n算子效果#\n\n时域输入数据   参数                                                      频域输出数据\n         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n         numDimensionSize = 1\n\n\n原理 #\n\nFFT是基于DFT算法的改进方法，在计算结果上与DFT等效，优化了计算过程。若F(n)为f(n)的离散傅里叶变换，DFT计算公式表达如下：\n\n\n\n其中，旋转因子为 $W_N^=e^$ ，N为输入序列长度。\n\nIDFT为DFT的逆运算，其计算公式表达如下：\n\n\n\n另一个基础公式为欧拉公式：\n\n\n\n由欧拉公式可以推导出旋转因子具有以下特性：\n\n\n\n\n\n\n\n对于N点DFT公式，可以按照奇偶顺序将公式分解如下：\n\n\n\n\n\n\n\n分解效果如图，将整体的计算任务逐层二分。\n\n由上可知，N为2的幂次时，FFT任务都可以被分解为单点的计算任务。\n\n由欧拉公式， $W_N^$ 可以写成如下形式，可以看出 $W_N^$ 具有随N相关的周期性和对称性。\n\n\n\n根据此性质就可以得到 $W_N^=W_N^$ ，其中 $n$ 取值范围是 $[0, N/2)$ 。\n\n以N为8的FFT运算为例，具体分解过程如下：\n\n 1. 将8点拆解为4点\n\n 2. 4点拆解为2点\n\n 3. 计算单点FFT\n\n其中，节点两两交错的计算过程被称为蝶形运算，形式如下：\n\n具体计算公式为：\n\n\n\n\n\n上图中可以看出，F(n)和f(n)之间的n存在比特反的关系。即FFT运算中，输入输出参数之间的索引值是互相的比特相反数，为优化计算效率提供了更好的支持。\n\nIFFT的计算过程和FFT类似，根据IDFT公式推导即可。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbFFT1D。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/hpl/hpl5_op/op_fft","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":243},{"text":"原理","id":"原理","depth":2,"charIndex":-1},{"text":"API接口","id":"api接口","depth":2,"charIndex":1079},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":1110}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":471,"title":"Fast Fourier Transform 2D","content":"#\n\n二维快速傅里叶变换（Fast Fourier Transform\n2D，以下简称FFT2D）是基于快速傅里叶变换（以下简称FFT）开发的，对二维数据进行时域到频域转换的快速算法。\n该算法可以将二维数据从时域转换到频域，为频域上的信号处理提供基础支持，通常用来作为图像等二维数据进行频域分析的前置步骤。\n\n\n算子效果#\n\n时域输入数据   参数                                                      频域输出数据\n         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n         numDimensionSize = 2\n\n\n原理 #\n\nFFT2D是基于FFT算法开发的扩展方法，对二维数据分别从x方向和y方向执行FFT处理，实现时域数据到频域数据的映射。具体流程如下：\n\n 1. 对二维数据中每一行进行FFT计算，其中nx为接口参数中设置的x轴FFT点数。\n\n 2. 在步骤1的处理基础上，对二维数据中每一列进行FFT计算，其中ny为接口参数中设置的y轴FFT点数。\n\n 3. 返回步骤2的计算结果。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbFFT2D。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/hpl/hpl5_op/op_fft_2d","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":155},{"text":"原理","id":"原理","depth":2,"charIndex":-1},{"text":"API接口","id":"api接口","depth":2,"charIndex":586},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":617}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":472,"title":"Inverse Fast Fourier Transform","content":"#\n\n快速傅里叶逆变换（Inverse Fast Fourier Transform，以下简称IFFT）是离散傅里叶逆变换（Inverse Discrete\nFourier Transform，以下简称IDFT）的快速算法， 是时域一频域变换分析中最基本的方法之一。该算法可以实现数据从频域到时域之间的转换。\n\n\n算子效果#\n\n时域输入数据   参数                                                      频域输出数据\n         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n         numDimensionSize = 1\n\n\n原理#\n\n请参考FFT章节原理。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbIFFT1D。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/hpl/hpl5_op/op_ifft","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":156},{"text":"原理","id":"原理","depth":2,"charIndex":396},{"text":"API接口","id":"api接口","depth":2,"charIndex":415},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":447}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":473,"title":"Inverse Fast Fourier Transform 2D","content":"#\n\n二维快速傅里叶逆变换（Inverse Fast Fourier Transform\n2D，以下简称IFFT2D）是基于快速傅里叶逆变换（以下简称IFFT）开发的快速算法，\n实现了二维数据从频域到时域的快速转换，通常被用来将图像等二维频域数据复原为时域数据。\n\n\n算子效果#\n\n时域输入数据   参数                                                      频域输出数据\n         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n         numDimensionSize = 2\n\n\n原理 #\n\nIFFT2D是基于IFFT算法开发的扩展方法，对二维数据分别从y方向和x方向执行IFFT操作，实现频域数据到时域数据的映射。具体流程如下：\n\n 1. 对二维数据中每一列进行IFFT计算，其中ny为接口参数中设置的y轴FFT点数。\n\n 2. 在步骤1的基础上，对二维数据中每一行进行IFFT计算，其中nx为接口参数中设置的x轴FFT点数。\n\n 3. 返回步骤2的计算结果。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbIFFT2D。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/hpl/hpl5_op/op_ifft_2d","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":133},{"text":"原理","id":"原理","depth":2,"charIndex":-1},{"text":"API接口","id":"api接口","depth":2,"charIndex":567},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":599}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":474,"title":"总览","content":"#\n\n算子名                                 说明           DSP\nFast Fourier Transform              快速傅里叶变换      Y\nInverse Fast Fourier Transform      快速傅里叶逆变换     Y\nFast Fourier Transform 2D           二维快速傅里叶变换    Y\nInverse Fast Fourier Transform 2D   二维快速傅里叶逆变换   Y","routePath":"/guide/ucp/hpl/hpl5_op/overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":475,"title":"示例","content":"#\n\n\n总览#\n\n本章节为您介绍HPL模块的各个示例功能、实现流程及实现细节，帮助您快速了解HPL模块的使用方法，示例介绍均以板端运行为例。示例的编译及运行请参考 快速上手\n章节。\n\n示例列表，默认使用dsp backend。\n\n示例名                  说明\nfft_ifft_transform   FFT IFFT转换示例\n\n\n傅里叶变换示例 #\n\n傅里叶变换示例执行脚本位于 ucp_tutorial/hpl/hpl_samples/script/01_fft_ifft_transform 目录下，\n该示例展示了如何使用傅里叶变换、逆变换接口对图片进行滤波处理，信息在采集、传输或处理过程中可能会受到各种噪声的干扰，对某些类型的噪声来说，在频域上处理是更为有效\n和便捷的方式，此时就可以通过傅里叶变换的相关接口，在频域上去除这些噪声，再将滤波后的数据恢复到时域，达到去噪的目的，详细的实现方法请结合示例源码对比实践。\n\n此示例中，直接执行示例脚本即可启动示例的处理流程：读取带噪声的正弦波、fft变换、频域滤波、ifft变换、保存去噪数据。 执行方法如下：\n\n\n\n此外还可以通过追加 -help 命令获取示例用法介绍。\n\n算子执行后会在 hpl_samples/data 目录下保存数据处理结果，本示例的生成物内容如下：\n\n\n\n1D FFT示例的执行流程如下图：\n\n如示例的执行流程图所示，该示例对数据的处理过程可以大致分为以下四个阶段：\n\n 1. 生成正弦波数据，并添加噪声。将生成的数据以浮点类型保存在文件中，并在后续的运行中直接读取该数据。\n\n 2. 对生成的数据进行傅里叶变换。\n\n 3. 构造一个频域滤波器，对变换后的数据进行滤波。\n\n 4. 将滤波结果进行傅里叶逆变换，还原正弦波数据。\n\n\n构造噪声数据#\n\n这个环节构造了初始的输入数据，将正弦波作为原始数据，加噪后成为输入数据，并在后续的处理中去除噪声。噪声数据为频率为5HZ的正弦波叠加上高斯噪声，噪声构造代码如下\n：\n\n\n\n其中，正弦波原图像为：\n\n添加噪声后的图像为：\n\n\n傅里叶变换#\n\n傅里叶变换将数据由时域转变为频域，使得我们可以采用一些频域上的方法来处理数据。HPL模块提供了1维的快速傅里叶变换接口，请参考 Fast Fourier\nTransform 部分的介绍。具体代码如下：\n\n\n\n其中dst为频域的输出数据，src为时域的输入数据，即噪声数据。\n\n\n频域滤波#\n\n这一步骤是在频域上对数据进行处理，达到噪声滤波的效果。此处构造了一个滤波器，并将该滤波器与频域数据进行卷积， 具体实现如下：\n\n\n\n代码中将虚数结构体数据转换为了cv::Mat的数据类型，通过opencv提供的相关接口创建并应用了频域滤波器，最后将滤波结果转换为虚数结构体。\n\n\n傅里叶逆变换#\n\n傅里叶逆变换将数据由频域转变为时域，使得数据的展现、存储方式更符合平时的认知。HPL模块提供了1维的快速傅里叶逆变换接口，请参考 Inverse Fast\nFourier Transform 部分的介绍。具体代码如下：\n\n\n\n其中dst为时域的输出数据，src为频域的输入数据。\n\n消除噪声后的图像为：","routePath":"/guide/ucp/hpl/hpl6_sample","lang":"zh","toc":[{"text":"总览","id":"总览","depth":2,"charIndex":3},{"text":"傅里叶变换示例","id":"傅里叶变换示例","depth":2,"charIndex":-1},{"text":"构造噪声数据","id":"构造噪声数据","depth":3,"charIndex":764},{"text":"傅里叶变换","id":"傅里叶变换","depth":3,"charIndex":885},{"text":"频域滤波","id":"频域滤波","depth":3,"charIndex":1032},{"text":"傅里叶逆变换","id":"傅里叶逆变换","depth":3,"charIndex":1179}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":476,"title":"HB_HPL_MAX_DIMS","content":"#\n\n\n\n定义虚数最大支持的维度为3。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hb_hpl_max_dims","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":477,"title":"hbFFTPointSize","content":"#\n\n\n\nFFT运算点数的枚举，被用于FFT和IFFT接口中的计算参数。\n\n * 成员\n   \n   成员名称                     描述\n   HB_FFT_POINT_SIZE_16     16点FFT。\n   HB_FFT_POINT_SIZE_32     32点FFT。\n   HB_FFT_POINT_SIZE_64     64点FFT。\n   HB_FFT_POINT_SIZE_128    128点FFT。\n   HB_FFT_POINT_SIZE_256    256点FFT。\n   HB_FFT_POINT_SIZE_512    512点FFT。\n   HB_FFT_POINT_SIZE_1024   1024点FFT。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbfftpointsize","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":478,"title":"hbHPLDataType","content":"#\n\n\n\nHPL模块支持的数据类型。\n\n * 成员\n   \n   成员名称                   描述\n   HB_HPL_DATA_TYPE_I16   int16_t数据类型。\n   HB_HPL_DATA_TYPE_I32   int32_t数据类型。\n   HB_HPL_DATA_TYPE_F32   float32_t数据类型。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhpldatatype","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":479,"title":"hbHPLImaginaryData","content":"#\n\n\n\n虚数的结构体。\n\n * 成员\n   \n   成员名称               描述\n   realDataVirAddr    实部的虚拟地址。\n   realDataPhyAddr    实部的物理址。\n   imDataVirAddr      虚部的虚拟址。\n   imDataPhyAddr      虚部的物理址。\n   dataType           数据类型，支持 hbHPLDataType 中的类型。\n   imFormat           数据格式，支持 hbHPLImFormat 中的类型。\n   dimensionSize      数据各维度的尺寸大小。\n   numDimensionSize   支持的最大维度，最大值为 HB_HPL_MAX_DIMS 。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhplimaginarydata","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":480,"title":"hbHPLImFormat","content":"#\n\n\n\n虚数的数据排布格式枚举。\n\n * 成员\n   \n   成员名称                       描述\n   HB_IM_FORMAT_INTERLEAVED   实部虚部数据交错排布，R I R I... 保存在实部指针指向的地址中。\n   HB_IM_FORMAT_SEPARATE      实部虚部数据独立排布，R R... I I... 分别保存在实部与虚部指针指向的地址中。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhplimformat","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":481,"title":"hbFFT1D","content":"#\n\n\n\nFFT 1D算子参数。\n\n * 成员\n   \n   成员名称        描述\n   pSize       FFT点数，支持hbFFTPointSize中的类型。\n   normalize   保留参数，默认为0。是否进行规格化的标识，0为不进行规格化，其他值为进行规格化。\n\n注解\n\n规格化指的是对FFT结果进行某种形式的缩放，以便结果符合特定的标准或者便于解释。\n\n\n\n调用FFT 1D API。\n\n当虚数数据类型为 HB_HPL_DATA_TYPE_I16 或 HB_HPL_DATA_TYPE_I32时，其数据排布格式仅支持为\nHB_IM_FORMAT_SEPARATE。 当虚数数据类型为HB_HPL_DATA_TYPE_F32\n时，其数据排布格式仅支持为HB_IM_FORMAT_INTERLEAVED。\n\n * 参数\n   \n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dst 输出虚数数据，type、format和维度信息与输入一致。\n   * [in] src 输入虚数数据。\n   * [in] param 算子参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbfft1d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":482,"title":"hbFFT2D","content":"#\n\n\n\nFFT 2D算子参数。\n\n * 成员\n   \n   成员名称        描述\n   nx          x轴方向的FFT点数，支持hbFFTPointSize中的类型。\n   ny          y轴方向的FFT点数，支持hbFFTPointSize中的类型。\n   normalize   保留参数，默认为0。是否进行规格化的标识，0为不进行规格化，其他值为进行规格化。\n\n注解\n\n规格化指的是对FFT结果进行某种形式的缩放，以便结果符合特定的标准或者便于解释。\n\n\n\n调用FFT 2D API。\n\n当虚数数据类型为 HB_HPL_DATA_TYPE_I16 或 HB_HPL_DATA_TYPE_I32时，其数据排布格式仅支持为\nHB_IM_FORMAT_SEPARATE。 当虚数数据类型为HB_HPL_DATA_TYPE_F32\n时，其数据排布格式仅支持为HB_IM_FORMAT_INTERLEAVED。\n\n * 参数\n   \n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dst 输出虚数数据，type、format和维度信息与输入一致。\n   * [in] src 输入虚数数据。\n   * [in] param 算子参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbfft2d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":483,"title":"hbIFFT1D","content":"#\n\n\n\nIFFT 1D算子参数。\n\n * 成员\n   \n   成员名称        描述\n   pSize       IFFT点数，支持hbFFTPointSize中的类型。\n   normalize   保留参数，默认为0。是否进行规格化的标识，0为不进行规格化，其他值为进行规格化。\n\n\n\n调用IFFT 1D API。\n\n当虚数数据类型为HB_HPL_DATA_TYPE_I16或HB_HPL_DATA_TYPE_I32时，其数据排布格式仅支持为HB_IM_FORMAT_SEPAR\nATE。 当虚数数据类型为HB_HPL_DATA_TYPE_F32时，其数据排布格式仅支持为HB_IM_FORMAT_INTERLEAVED。\n\n * 参数\n   \n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dst 输出虚数数据，type、format和维度信息与输入一致。\n   * [in] src 输入虚数数据。\n   * [in] param 算子参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbifft1d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":484,"title":"hbIFFT2D","content":"#\n\n\n\nIFFT 2D算子参数。\n\n * 成员\n   \n   成员名称        描述\n   nx          x轴方向的IFFT点数，支持hbFFTPointSize中的类型。\n   ny          y轴方向的IFFT点数，支持hbFFTPointSize中的类型。\n   normalize   保留参数，默认为0。是否进行规格化的标识，0为不进行规格化，其他值为进行规格化。\n\n\n\n调用IFFT 2D API。\n\n当虚数数据类型为 HB_HPL_DATA_TYPE_I16 或 HB_HPL_DATA_TYPE_I32时，其数据排布格式仅支持为\nHB_IM_FORMAT_SEPARATE。 当虚数数据类型为HB_HPL_DATA_TYPE_F32\n时，其数据排布格式仅支持为HB_IM_FORMAT_INTERLEAVED。\n\n * 参数\n   \n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dst 输出虚数数据，type、format和维度信息与输入一致。\n   * [in] src 输入虚数数据。\n   * [in] param 算子参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbifft2d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":485,"title":"HPL API 概览","content":"#\n\n\n数据结构#\n\n\n功能接口#","routePath":"/guide/ucp/hpl/hpl7_api/hpl_api_overview","lang":"zh","toc":[{"text":"数据结构","id":"数据结构","depth":2,"charIndex":3},{"text":"功能接口","id":"功能接口","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":486,"title":"DSP开发流程","content":"#\n\n\n整体框架#\n\n在地平线提供的Softmax算子示例中，展示了如何通过调度UCP框架实现DSP自定义算子的功能封装，您可以在OE包中的samples/ucp_tutorial/\ncustom_operator/dsp_sample处获取示例源码进行同步阅读理解。\n\n\nSoftmax算子开发#\n\n本章节主要以DSP\nSoftmax算子开发为例为您介绍DSP算子开发的流程，对应地平线OE包中的samples/ucp_tutorial/custom_operator/dsp_s\nample/dsp_code/softmax/softmax_ivp.cc部分。\n\n完整的算子开发分为如下三个步骤：\n\n 1. UCP API的调用，主要负责任务发起及计算资源分配；\n\n 2. DSP算子开发；\n\n 3. DSP算子注册运行。\n\n\nSoftmax分析#\n\nSoftmax算子可以拆分为以下四个基础计算：\n\n 1. 计算输入元素中的最大值max。\n\n 2. 计算并更新输入的每个元素： input = exp(input - max) 。\n\n 3. 计算更新后input的和sum。\n\n 4. 计算 output = input / sum 。\n\n\nUCP API#\n\n在此部分，将为您展示如何使用UCP接口申请内存资源、发起任务。以Softmax示例为例，由于UCP接口只支持输入输出这两个参数，如您需要将其它参数传递到DSP端\n，则需要进行一层封装，将封装后的“输入”传入接口，再由DSP从封装的“输入”中获取参数。\n\n如下示例中，封装“输入”中包括三个字段，data_size为数据尺寸，示例中取100000，input字段为发送给dsp的数据地址，run_tcm_opt字段表\n示是否使用tcm优化实现。\n\n\n\n\nDSP算子实现 #\n\n本章节将对如何实现上一节中提到的四个基础运算从而实现DSP Softmax算子进行介绍。\n\nCadence实现了一些基础数学运算，方便您进行开发。您可以从Cadence的基础示例中获取源码，也可从地平线直接获取编译好的依赖库dsp_math。\n\n为了充分利用硬件性能，您需要了解DSP特性并使用好这些特性（VLIW、SIMD）。在进行开发时，可参照Cadence本身已实现的基础运算。\n\nvecmaxf SIMD实现如下：\n\n\n\nvecexpf_max SIMD实现如下：\n\n\n\nvecsum SIMD实现如下：\n\n\n\n除法运算可变为乘法运算，同时实现乘法运算比较容易且性能较好。\n\n\n\n完整的Softmax实现如下：\n\n\n\n以上Softmax实现直接访问DDR，由于J6没有配置向量缓存，故直接访问DDR内存非常耗时。常见的优化实现是，将数据分成tile块，通过idma搬移到TCM（\nTCM与缓存延迟相当）中，直接访问TCM进行计算。并且，为了将计算与idam搬运操作并行，DSP配备了两块TCM，以实现ping pong\nDMA的操作，如下图所示。\n\nping pong DMA的实现参考代码如下：\n\n\n\n完整的DSP softmax实现如下：\n\n\n\n\n注册运行#\n\n算子开发完成后，通过调用hb_dsp_register_fn接口注册hb_dsp_softmax算子，完成注册后，编译DSP镜像。\n\n\n\n注解\n\n如果是x86仿真运行，则无需启动镜像，如果是上板运行，需要执行脚本dsp_deploy.sh启动镜像。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_develop","lang":"zh","toc":[{"text":"整体框架","id":"整体框架","depth":2,"charIndex":3},{"text":"Softmax算子开发","id":"softmax算子开发","depth":2,"charIndex":136},{"text":"Softmax分析","id":"softmax分析","depth":3,"charIndex":365},{"text":"UCP API","id":"ucp-api","depth":3,"charIndex":523},{"text":"DSP算子实现","id":"dsp算子实现","depth":3,"charIndex":-1},{"text":"注册运行","id":"注册运行","depth":3,"charIndex":1300}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":487,"title":"DSP开发文档","content":"#\n\n\nDSP简介#\n\n地平线J6处理器中包含一个DSP核，时钟频率为1GHz，使用的是Cadence公司的Tensilica Vision Q8 DSP IP。\n\nVision Q8 DSP专用于支持计算机视觉或图像处理等算法，超长指令字（VLIW）与单指令多数据流（SIMD）可以很大程度提升计算的速度。Vision Q8\nDSP采用5路VLIW架构，每条指令最多可以包含两个64字节加载或一个64字节加载和一个64字节存储；SIMD支持1024bit的操作，如：128路8位整型，6\n4路16位整型等。更多关于Vision Q8 DSP的信息，可参见Cadence的Vision Q8 用户手册。\n\nDSP拥有强大的计算能力，在使用得当的情况下，将一些不能用BPU加速且ARM低效的计算部署到DSP上可以大大提高模型的推理性能。\n\n\nLinux开发环境安装#\n\n\n开发工具介绍#\n\nXtensa Xplorer是Cadence为客户提供的针对其DSP进行软件开发的一个集成开发环境，具有软件开发(software\ndevelopment)、编译(compile)、调试(debugging)、仿真(simulation)、性能分析(profiling)、硬件跟踪(hardw\nare trace)等功能。本开发文档中将只介绍Linux开发环境的安装，Windows开发环境的安装使用可参考Cadence提供的官方文档。\n\n\n安装DSP工具链以及配置核 #\n\n您可以从地平线获取DSP开发包，开发包中包含 Xplorer-10.1.11-linux-x64-installer.bin 和\nVision_Q8_linux.tgz 安装包。\n\n 1. 安装 Xtensa Develop Tools\n\n这里我们默认将 Xtensa Develop Tools 安装在 /opt/xtensa 目录下，您也可以自行指定其他目录。若安装在 /opt/xtensa\n目录则需要root权限。执行如下命令：\n\n\n\n 2. 安装 Vision Q8 Core Configuration\n\n将 Vision_Q8_linux.tgz 安装包解压，解压后将其放在 Xtensa Develop Tools\n安装目录下指定位置（如：/opt/xtensa/XtDevTools/install/builds/RI-2023.11-linux/\n），放到指定位置后进行安装。安装需要执行如下命令：\n\n\n\n 3. 配置环境变量\n\n为保证 Xtensa Develop Tools 的正常使用，您需要设置以下环境变量：\n\n\n\n注解\n\nXtensa Develop Tools 的使用需要正确配置License，申请和配置方式请联系地平线项目对接人。\n\n 4. Xtensa Develop Tools 测试\n\n执行如下两个命令，如果此两命令可以正常执行，则表明linux开发环境安装成功：\n\n\n\n\nDSP开发参考资料#\n\n为了更好地进行DSP开发，我们建议您参考以下文档，编译器安装成功后，以下文档可在路径 XtDevTools/downloads/RI-2023.11/docs\n及XtDevTools/install/builds/RI-2023.11-linux/Vision_Q8/html/ISA 下找到：\n\n编号   文档名称及描述                                          文档目录\n1    Vision Q8 DSP 介绍文档                               visionq8_ug.pdf\n2    Dev Toolkit 使用介绍文档                               sw_dev_toolkit_ug.pdf\n3    Compiler 使用介绍文档                                  xtensa_xt_clang_compiler_ug.pdf\n4    Xi Library 文档，位于XI_Library_7.14.2.xws 工程 Doc目录   XI_Library_UserGuide.pdf\n5    Profiler 使用说明文档                                  gun_profiler_ug.pdf\n6    Vision Q8 DSP 指令                                 NewISAhtml/index.html","routePath":"/guide/ucp/plugin/dsp_develop/dsp_intro","lang":"zh","toc":[{"text":"DSP简介","id":"dsp简介","depth":2,"charIndex":3},{"text":"Linux开发环境安装","id":"linux开发环境安装","depth":2,"charIndex":369},{"text":"开发工具介绍","id":"开发工具介绍","depth":3,"charIndex":384},{"text":"安装DSP工具链以及配置核","id":"安装dsp工具链以及配置核","depth":3,"charIndex":-1},{"text":"DSP开发参考资料","id":"dsp开发参考资料","depth":3,"charIndex":1239}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":488,"title":"DSP API概览","content":"#\n\n\n数据结构#\n\n\nUCP调用DSP数据结构#\n\nDSP后端数据结构#\n\n\n功能接口#\n\nUCP调用DSP接口#\n\nDSP后端接口#","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/api_introduction","lang":"zh","toc":[{"text":"数据结构","id":"数据结构","depth":2,"charIndex":3},{"text":"UCP调用DSP数据结构","id":"ucp调用dsp数据结构","depth":3,"charIndex":11},{"text":"DSP后端数据结构","id":"dsp后端数据结构","depth":4,"charIndex":26},{"text":"功能接口","id":"功能接口","depth":3,"charIndex":39},{"text":"UCP调用DSP接口","id":"ucp调用dsp接口","depth":4,"charIndex":46},{"text":"DSP后端接口","id":"dsp后端接口","depth":4,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":489,"title":"handle_fn","content":"#\n\n\n\nDSP算子函数指针。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_data_structure/handle_fn","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":490,"title":"hbDSPRpcCmd","content":"#\n\n\n\nDSP算子命令枚举。\n\n * 成员\n   \n   成员名称                    描述\n   HB_DSP_RPC_CMD_R_B      框架内部命令\n   HB_DSP_RPC_CMD_CONFIG   框架内部命令\n   HB_DSP_RPC_CMD_R_E      框架内部命令\n   HB_DSP_RPC_CMD_NN_B     NN算子起始命令\n   HB_DSP_RPC_CMD_NN_E     NN算子终止命令\n   HB_DSP_RPC_CMD_CV_B     CV算子起始命令\n   HB_DSP_RPC_CMD_CV_E     CV算子终止命令\n   HB_DSP_RPC_CMD_HPL_B    HPL算子起始命令\n   HB_DSP_RPC_CMD_HPL_E    HPL算子终止命令\n   HB_DSP_RPC_CMD_BUTT     算子终止命令","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_data_structure/hbDSPRpcCmd","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":491,"title":"环境变量","content":"#\n\n\n\n\n日志等级设置说明#\n\n * 日志等级：\n   \n   DSP 模块中ARM侧的日志主要分为7个等级：\n   \n   log等级可设置为0、1、2、3、4、5、6，分别对应Verbose、Debug、Info、Warning、Error、Critical、Never，默认\n   为Warning。\n   \n   DSP 模块中DSP侧的日志主要分为5个等级：\n   \n   log等级可设置为1、2、3、4、5，分别对应Debug、Info、Warning、Error、Always等级，默认为Warning。\n\n * 日志等级设置规则：\n   \n   * 若发生的日志等级 >= 设置的等级，则该日志可以被打印，反之被屏蔽。\n   * 设置的日志等级越小，打印信息越多。例如：设置日志等级为3，即为Warning级别，则3、4、5等级的日志均可以被打印。VP\n     模块默认日志等级为Warning级别，即以下日志级别的信息可以被打印：Warning、Error、Critical。\n\n注解\n\nDSP侧日志可以通过如下步骤获取：\n\n * 配置环境变量，使能DSP日志输出\n   \n   \n\n * 启动日志监听服务\n   \n   ","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_environment_variable","lang":"zh","toc":[{"text":"日志等级设置说明","id":"日志等级设置说明","depth":2,"charIndex":5}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":492,"title":"hbDSPAddrMap","content":"#\n\n\n\n将hbUCPSysMem映射为DSP可访问的内存地址。\n\n * 参数\n   * [out] out 仅DSP可访问的hbUCPSysMem。\n   * [in] in hbUCPSysMem输入地址。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPAddrMap","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":493,"title":"hbDSPAddrUnmap","content":"#\n\n\n\n将hbDSPAddrMap输出解映射。\n\n * 参数\n   * [out] out 仅DSP可访问的hbUCPSysMem。\n   * [in] in hbUCPSysMem输入地址。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPAddrUnmap","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":494,"title":"hbDSPRpcV2","content":"#\n\n\n\nDSP任务创建接口，J6接口，任务等待以及释放使用ucp通用接口。\n\n * 参数\n   * [out] task DSP任务句柄。\n   * [in] input DSP任务输入地址。\n   * [in] output DSP任务输出地址。\n   * [in] rpcCmd DSP任务命令。\n * 返回值\n   * 返回 0 则表示API成功执行，否则返回错误码。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPRpcV2","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":495,"title":"hb_dsp_env_init","content":"#\n\n\n\n初始化系统环境。\n\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_env_init","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":496,"title":"hb_dsp_env_init_ex","content":"#\n\n\n\n初始化系统环境，支持命令行参数。\n\n * 参数\n   * [in] argc 参数个数。\n   * [in] argv 具体参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_env_init_ex","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":497,"title":"hb_dsp_mem_map","content":"#\n\n\n\nDSP端地址映射，将ARM端传过来地址进行映射。\n\n * 参数\n   * [in] ddr_address arm端发送的内存地址。\n   * [in] size 内存大小\n * 返回值\n   * 返回映射后DSP可访问的地址。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_mem_map","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":498,"title":"hb_dsp_mem_unmap","content":"#\n\n\n\nDSP端地址解映射，将ARM端传过来地址进行解映射。\n\n * 参数\n   * [in] ddr_address hb_dsp_mem_map的返回值。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_mem_unmap","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":499,"title":"hb_dsp_register_fn","content":"#\n\n\n\nDSP端自定义算子注册\n\n * 参数\n   * [in] cmd 自定义算子函数编号。\n   * [in] handle 自定义算子函数指针。\n   * [in] handle 自定义算子延迟：单位 ms。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_register_fn","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":500,"title":"hb_dsp_start","content":"#\n\n\n\n启动框架多线程。\n\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_start","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":501,"title":"hb_dsp_unregister_fn","content":"#\n\n * 参数\n   * [in] cmd 自定义算子命令编号。\n   * [in] handle 自定义算子函数指针。\n   * [in] latency 函数计算延迟，单位ms。\n\n\n\nDSP端自定义算子注销\n\n * 参数\n   * [in] cmd 自定义算子函数编号。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_unregister_fn","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":502,"title":"点云前处理介绍","content":"#\n\n为了更好地帮助您理解DSP示例包提供的 CenterPoint\n五维点云前处理，本章节从算法原理出发，分析CPU和DSP参考实现，同时对CPU和DSP参考实现进行一致性校验和性能评测。示例包中同时也提供了\nPointPillars 四维点云前处理，与 CenterPoint 类似， 两者前处理的主要特点是通过体素化生成柱体（ pillars ）。本章节主要介绍\nCenterPoint 前处理实现，并使用单独的一节介绍 PointPillars 前处理在实现上的不同之处。\n\n\n原理介绍#\n\n本章节使用五维 CenterPoint 模型作为示例介绍，其相应的原理和前处理算法实现也可适用于四维的 PointPillars\n模型。模型的输入为五维点云数据，最大点数为30万。前处理主要由体素化，特征编码，量化和转置组成。前处理输出 1x5x20x40064(nchw) 的特征图和\n1x1x40000x4(nhwc) 的坐标信息。\n\n\n体素化#\n\n根据输入的五维点云数据的 x ， y 坐标计算出柱体的坐标，最大的柱体数目为 40000 ，每个柱体最大支持的点数为 20 。柱体坐标的计算原理如下：\n\n 1. 判断点云的 x ， y ， z 是否在有效的范围内，跳过无效的点云数据。\n\n 2. 点云的 x ， y 信息可以被视为伪图像，伪图像中的每一个像素对应一个柱体坐标。\n\n 3. 输入点云的坐标是无序的，因此需要按照坐标的输入顺序确定柱体坐标，当柱体数超过模型限制的最大值之后，不会生成新的柱体，而是将点写入到最后一个柱体中。\n\n 4. 相同坐标的点会放入同一个柱体中，模型限制每一个柱体中点的数目，当超过最大限制时，则直接跳过该点。\n\n\n\n\n特征编码#\n\n点云数据的特征编码使用如下的公式：\n\n$$P_0' = \\frac$$\n\n$$P_1' = \\frac$$\n\n$$P_2' = \\frac$$\n\n$$P_3' = \\frac$$\n\n$$P_4' = P_4$$\n\n\n量化#\n\n量化的scale可以通过模型获取，使用以下公式完成量化：\n\n$$P' = \\frac$$\n\n\n转置#\n\n模型输入的特征图 layout 模式为 NCHW ，转置将特征图的 shape 从 1x40000x20x5 转换为 1x5x20x40064。\n\n\nCPU参考实现#\n\nCPU参考实现的源码位于 AI Benchmark 中，其前处理基本流程与原理介绍部分保持一致，其中量化合并到特征编码步骤中。\n\n\n体素化#\n\n\n\n\n特征编码 & 量化#\n\n\n\n\n转置#\n\n\n\n\nDSP优化加速#\n\n主要从两个方面考虑算子在DSP上的加速实现，第一是算子的计算部分， J6上的DSP支持1024bit的SIMD\n，因此将计算部分尽可能向量化可以充分利用DSP的算力。第二是算子的访存部分，J6上的DSP有两个256kB的TCM\n，其访存性能接近CPU上的cache，因此将数据尽可能搬移到TCM上 ，能有效地节约访存的开销。同时DSP也提供向量化的访存指令，也可以提高访存的效率。\n\n\nDSP加速思路#\n\n向量化#\n\n对于五维模型，可以将计算过程分为5个cycle，每个cycle计算一个维度，使用IVP向量指令完成运算。\n\n具体到PointPillars点云前处理算法，在计算伪图像的坐标，特征编码以及量化等计算过程，都可以将每个维度独立计算，因此很容易转换成向量化的计算。\n\n访存优化#\n\n点云输入数据从x，y维度来看是无序的，导致计算出的柱体坐标和柱体中点的索引也是无序的，从而导致访存是随机而不是效率更高的顺序操作。由于DSP上没有data\ncache，随机访存非常不友好，因此需要充分利用DSP上的TCM。DSP上可用的TCM只有两个256kB，而特征数据输出的地址空间为40000x20=781.2\n5kB，远大于TCM的内存大小。同时计算体素坐标时，也需要一个伪图像坐标与柱体坐标的查找表，该表所需的内存为512x512x4=1MB（512为伪图像的宽和高，\n4为柱体坐标所需的字节数），也是远远超出TCM的内存大小。考虑到将随机访存搬移到DSP代价太大，因此将前处理算法中的计算和访存进行分离，计算部分放到DSP上，而\n把部分随机访存放到CPU上执行。同时为了减少访存的数据量，DSP加速实现调整了CPU实现的步骤，将特征编码和量化提前到计算伪图像坐标之后。通过这样的处理，后续的\n体素化和转置步骤访存所需的数据量将大大减少。 虽然这会增加更多的计算量，但是通过实际评测可见，减少访存数据量带来的收益会更大，特别是在有效点云数较大的情况下。\n\n分块策略#\n\n虽然有两个256kB的TCM，但是每一个计算周期只使用其中一个，原因主要有以下两个：\n\n 1. 需要使用Pingpong buffer，减少等待数据拷贝完成的时间；\n\n 2. Pingpong buffer需要位于不同的TCM中，减少data bank conflict。\n\nDSP上的计算被拆为两个部分，一个是计算伪图像坐标，另一个是特征编码，量化和转置：\n\n 1. 计算伪图像坐标，每个计算周期，输入为五维的点云数据，输出为单个维度的数据，输入与输出比为5：1，因此输入与输出的TCM内存大小比例也为5：1；\n\n 2. 特征编码，量化和转置，每个计算周期，输入为五维点云数据，输出为量化后的单维度数据，输入与输出比为20：1。\n\n\nDSP优化实现#\n\nDSP的IVP指令可以参考cadence提供的文档 ISA/NewISAhtml/index.html ，TCM内存管理接口的详细介绍可以参考 Xtensa\nVision Tile Manager Library User's Guide ，IDMA相关接口详细介绍可以参考 Xtensa System\nSoftware Reference Manual 。\n\n伪图像坐标计算，特征编码和转置（DSP计算部分）#\n\n与CPU参考实现中计算伪图像坐标方法基本一致，主要区别有以下几点：\n\n 1. 去掉一维坐标的计算，通过32位的高低16位存储伪图像x和y坐标（示例模型伪图像的宽高为512，16位已足够存储），同时以二维坐标作为索引，用于查找对应的\n    柱体坐标；\n\n 2. 在过滤无效的点云数据时，使用mask move取代条件判断语句。因为在内循环中使用条件判断会明显降低运算性能；\n\n\n\n体素化（CPU访存部分）#\n\n计算逻辑和CPU参考实现基本一致，主要有以下几点区别：\n\n 1. 使用二维数组查找表，并且直接使用伪图像的x，y作为索引；\n\n 2. 输出voxel_data的索引计算方式不同，因为DSP\n    gather/scatter不能超出64kB的地址空间限制，因此提前按照最终特征数据的shape进行计算输出索引；\n\n\n\n转置#\n\n将 AoS 格式的点云转置为 SoA 格式。\n\n\n\n\n一致性校验#\n\n分别导出CPU和DSP参考实现的点云前处理输出特征数据和坐标信息到文件，比对文件内容是否一致，如果一致则通过一致性校验。\n\n\n\n在AI Benchmark中，可以执行完整的全流程精度评测，当前DSP参考实现与CPU参考实现精度完全一致。\n\n小技巧\n\n如果您不要求DSP实现的精度与CPU实现具有完全一致的精度，那么可以通过优化浮点运算来进一步提升性能。例如，通过将耗时的除法运算替换为乘法运算。\n\n\n性能评测#\n\n在AI\nBenchmark中，执行CenterPoint模型的延时测试，比较CPU和DSP参考实现的单帧延时。通过实际测试发现，voxel_num对点云前处理的耗时影响\n较大，因此这里选择几组不同voxel_num的数据，以对比CPU参考实现和DSP参考实现的性能。\n\n单线程单帧延时测试结果如下：\n\nVOXEL_NUM   CPU参考实现     DSP参考实现\n6452        7.978 ms    4.809 ms\n11894       11.779 ms   5.037 ms\n24579       20.428 ms   5.320 ms\n40000       30.222 ms   5.677 ms\n\n\nPointPillars前处理实现简介#\n\nPointPillars四维点云前处理与CenterPoint五维点云前处理的基本处理逻辑相同，主要有以下两点区别：\n\n 1. 点云输入维度不同；\n\n 2. 点云前处理后的输出layout不同。\n\n针对以上两点差异，以下将分别阐述DSP加速实现上的不同之处：\n\n 1. 点云输入维度不同\n\n * PointPillars 四维点云输入经过量化后，其数据大小正好适配 int32_t 类型，无需像 CenterPoint 那样对第五维量化后的数据进行\n   padding 操作以适配 int32_t 类型。\n\n * 第一步计算伪图像坐标，特征编码和量化步骤的输入输出TCM大小比例不同。CenterPoint为5:3，PointPillar为2:1，输入为四维浮点点云数\n   据（float32_t），输出为伪图像坐标（int32_t）和量化后的xyzr数据（int32_t）。\n\n * 由于维度的差异，对输入点云的gather操作所需的offset计算不同。\n\n 2. 点云前处理后的输出layout不同\n\n * 在体素化（CPU访存）步骤中，voxel_data的offset计算方式存在差异。\n\n * 由于输出layout不同，转置部分的输入输出TCM大小比例存在差异。CenterPoint的比例为5:1，而PointPillars的比例为4:1。\n\n * 转置的处理逻辑不同，具体的差异参见下图。\n\n注解\n\n根据图示，CenterPoint和PointPillars在max_point_in_voxel和max_voxel_num的顺序上存在差异，这导致了无效数据的\n分别不同。\n具体来说，PointPillars的无效数据呈现出较为集中的分布特点，这有助于进行连续处理。相反，CenterPoint的无效数据则散布在各个voxel之间，呈\n现出交叉分布的特点。前处理的参数对算子的实现效率会有一定影响，可以进一步考虑根据不同的参数实现特定的逻辑。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_sample/centerpoint_pointpillar","lang":"zh","toc":[{"text":"原理介绍","id":"原理介绍","depth":2,"charIndex":242},{"text":"体素化","id":"体素化","depth":3,"charIndex":420},{"text":"特征编码","id":"特征编码","depth":3,"charIndex":727},{"text":"量化","id":"量化","depth":3,"charIndex":842},{"text":"转置","id":"转置","depth":3,"charIndex":894},{"text":"CPU参考实现","id":"cpu参考实现","depth":2,"charIndex":974},{"text":"体素化","id":"体素化-1","depth":3,"charIndex":1050},{"text":"特征编码 & 量化","id":"特征编码--量化","depth":3,"charIndex":1059},{"text":"转置","id":"转置-1","depth":3,"charIndex":1074},{"text":"DSP优化加速","id":"dsp优化加速","depth":2,"charIndex":1082},{"text":"DSP加速思路","id":"dsp加速思路","depth":3,"charIndex":1285},{"text":"向量化","id":"向量化","depth":4,"charIndex":1295},{"text":"访存优化","id":"访存优化","depth":4,"charIndex":1432},{"text":"分块策略","id":"分块策略","depth":4,"charIndex":1922},{"text":"DSP优化实现","id":"dsp优化实现","depth":3,"charIndex":2247},{"text":"伪图像坐标计算，特征编码和转置（DSP计算部分）","id":"伪图像坐标计算特征编码和转置dsp计算部分","depth":4,"charIndex":2438},{"text":"体素化（CPU访存部分）","id":"体素化cpu访存部分","depth":4,"charIndex":2655},{"text":"转置","id":"转置-2","depth":4,"charIndex":2830},{"text":"一致性校验","id":"一致性校验","depth":2,"charIndex":2862},{"text":"性能评测","id":"性能评测","depth":2,"charIndex":3070},{"text":"PointPillars前处理实现简介","id":"pointpillars前处理实现简介","depth":2,"charIndex":3394}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":503,"title":"简介","content":"#\n\nDSP示例包展示了如何在J6上使用DSP进行任务处理。DSP示例包中包含softamx、quantize、dequantize、centerpoint、poin\ntpillar、slam。\n\n各个示例主要分为ARM侧和DSP侧两部分，其中ARM侧负责准备数据然后发起调用，DSP侧负责接收ARM侧发来的任务，完成任务计算，将结果发送给ARM。\n\n您可以体验并基于该示例进行应用开发，降低开发门槛。\n\ndsp sample位于samples/ucp_tutorial/custom_operator/dsp_sample，代码结构如下：\n\n\n\n\n环境构建#\n\n\n开发板准备#\n\n拿到开发板后，确保使用推荐的系统镜像版本，以及本地开发机和开发板可以远程连接。\n\n\n编译#\n\ndsp sample编译需要分别编译ARM侧与DSP侧。\n\n 1. ARM侧需要当前环境安装好交叉编译工具（X86 GCC12.2，ARM GCC12.2）。\n    设置环境变量LINARO_GCC_ROOT为交叉编译工具的实际安装位置；\n\n 2. DSP侧需要当前环境安装好Cadence交叉编译工具，安装及配置具体方法可参考DSP开发文档章节中的介绍；\n\n 3. 安装完工具后，确认工具安装成功后，在目录下执行板端aarch64或者x86编译脚本，编译生成物分别在script，script_x86下。以编译板端生成\n    物为例，执行脚本如下：\n\n\n\n\n示例使用#\n\n 1. 上板运行，将script目录拷贝到板端，执行如下命令：\n\n\n\n 2. x86仿真运行，执行如下命令：\n\n","routePath":"/guide/ucp/plugin/dsp_develop/dsp_sample/dsp_sample_intro","lang":"zh","toc":[{"text":"环境构建","id":"环境构建","depth":2,"charIndex":274},{"text":"开发板准备","id":"开发板准备","depth":3,"charIndex":282},{"text":"编译","id":"编译","depth":3,"charIndex":332},{"text":"示例使用","id":"示例使用","depth":2,"charIndex":619}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":504,"title":"量化反量化介绍","content":"#\n\n为了更好地帮助您理解DSP示例包提供的quantize、dequantize前处理，本文从算法原理出发，分析CPU和DSP参考实现，同时对CPU和DSP参考实现\n进行一致性校验和性能评测。\n\n\n原理介绍#\n\n\n量化反量化#\n\n关于量化反量化的介绍、公式及参考实现可参考关键概念章节对模型量化的介绍。\n\n\nCPU参考实现#\n\n\n量化#\n\n\n\n\n反量化#\n\n\n\n\nDSP优化加速#\n\n\nDSP加速思路#\n\n向量计算#\n\nquantize/dequantize都是连续计算数据，非常适合SIMD向量优化。\n\n分块策略#\n\n 1. J6\n    DSP有两个256kB的TCM，实际可用大约在210KB，在不超过可用总量情况下，每个tile尽可能的大，quantize主要考虑输入及输出，并且比例为\n    4:1，dequantize同样主要考虑输入及输出，且比例为1:4；\n\n 2. 同时，因为quantize线性计算，且输出小于输出数据大小，可以考虑使用inplace（输入输出使用统一tile，进一步增大可用tile）。\n\n\nDSP优化实现#\n\n 1. quantize SIMD计算核心。\n\n\n\n 2. dequantize SIMD计算核心。\n\n\n\n 3. 框架部分可参考DSP算子实现小节中对ping pong IDMA实现的介绍。\n\n\n一致性校验#\n\n将计算结果与CPU参考实现进行比较，一致则通过校验","routePath":"/guide/ucp/plugin/dsp_develop/dsp_sample/quantize_dequantize","lang":"zh","toc":[{"text":"原理介绍","id":"原理介绍","depth":2,"charIndex":99},{"text":"量化反量化","id":"量化反量化","depth":3,"charIndex":107},{"text":"CPU参考实现","id":"cpu参考实现","depth":2,"charIndex":154},{"text":"量化","id":"量化","depth":3,"charIndex":165},{"text":"反量化","id":"反量化","depth":3,"charIndex":173},{"text":"DSP优化加速","id":"dsp优化加速","depth":2,"charIndex":182},{"text":"DSP加速思路","id":"dsp加速思路","depth":3,"charIndex":193},{"text":"向量计算","id":"向量计算","depth":4,"charIndex":203},{"text":"分块策略","id":"分块策略","depth":4,"charIndex":253},{"text":"DSP优化实现","id":"dsp优化实现","depth":3,"charIndex":465},{"text":"一致性校验","id":"一致性校验","depth":2,"charIndex":574}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":505,"title":"SLAM示例介绍","content":"#\n\nDSP示例包基于Cadence提供的SLAM示例包开发，可上板或仿真运行，本章将为您简单介绍SLAM DSP示例流程。\n\n\n原理介绍#\n\nSLAM(simultaneous localization and mapping)，也称为CML(Concurrent Mapping and\nLocalization)，\n即时定位与地图构建，或并发建图与定位。这里说的地图，是用来在环境中定位，以及描述当前环境以便于规划航线的一个概念；它通过记录以某种形式的感知获取的信息，用以和当\n前的感知结果相比较，以支撑对现实定位的评估。问题可以描述为：将一个机器人放入未知环境中的未知位置，是否有办法让机器人一边逐步描绘出此环境完全的地图，同时一边决定\n机器人应该往哪个方向行进。\n\n\nSLAM 实现流程#\n\n视觉SLAM应用示例基于开源的ORB_SLAM2实现。然而，该应用程序仅提取了最基本的功能，并在各种功能上进行了简化和修改。这些修改旨在适应Vision\nDSP上的SIMD实现，以提高循环性能并减少对系统内存的频繁随机访问。此外，还在各个模块中增加了用于剔除异常值的额外功能。\n\n同时，SLAM应用程序还支持“双目立体”与“RGBD”输入。然而，其方法与ORB_SLAM2的方法有显著不同。ORB_SLAM2使用额外信息（双目立体视觉中的右\n图像和RGBD中的深度信息）来减少重投影误差或代价函数。相反，应用程序使用了一种专有方法，利用这些额外的信息来执行异常值剔除。\n\n下图展示了SLAM应用程序的模块图。该SLAM应用程序可以接受单目、双目立体或RGBD输入。在RGBD输入的情况下，灰度单目图像和深度图像被作为输入提供。然后从\n输入图像中检测关键点或角点，并为这些关键点创建描述符，这些描述符用于匹配。在双目立体输入的情况下，深度信息从双目立体图像中生成。对于单目输入，在初始状态下，SL\nAM应用程序尝试找到一组两个关键帧以获得初始的3D点集和位姿。对于RGBD和立体输入，3D点在第一帧就会生成，并且应用程序会初始化。一旦应用程序初始化完成，它将\n进入跟踪状态，在每一帧中使用已知的3D点及其观察到的投影通过位姿优化来估计相机的位姿。根据条件，关键帧（KF）会被插入以生成新的3D点集，局部捆绑调整则会进一步\n优化估计的3D点和位姿。","routePath":"/guide/ucp/plugin/dsp_develop/dsp_sample/slam","lang":"zh","toc":[{"text":"原理介绍","id":"原理介绍","depth":2,"charIndex":64},{"text":"SLAM 实现流程","id":"slam-实现流程","depth":2,"charIndex":338}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":506,"title":"GPU开发文档","content":"#\n\nGPU具体硬件介绍，开发环境，以及开发软件接口可参考地平线征程6系统软件-J6X GPU模块开发和调试指南中的介绍。\n\n\n示例#\n\n本示例为您展示调用OpenCL API实现简单的threshold功能。\n示例位于samples/ucp_tutorial/gpu_sample目录中，代码结构如下：\n\n\n\n\n原理#\n\n$$\\begindst_x=\\beginmaxval, src_x > threshold\\0, src_x < = threshold\\end\\end$$\n\n输入图像   参数                         输出图像\n       threshold=100 maxval=200   \n\n\n线程配置#\n\n 1. 总的任务计算量WIDTHxHEIGHT，每个work item计算32x16的像素块大小，由此可以得出此划分下总work item数量如下：\n\n\n\n 2. 线程组配置为4x4。\n\n\n环境构建#\n\n开发板准备#\n\n拿到开发板后，确保使用推荐的系统镜像版本，以及本地开发机和开发板可以远程连接。\n\n编译#\n\ngpu sample仅支持编译生成板端执行的生成物。参考执行脚本如下：\n\n\n\n\n示例使用#\n\n上板运行，将script目录拷贝到板端，执行如下命令：\n\n","routePath":"/guide/ucp/plugin/gpu_develop","lang":"zh","toc":[{"text":"示例","id":"示例","depth":2,"charIndex":63},{"text":"原理","id":"原理","depth":3,"charIndex":156},{"text":"线程配置","id":"线程配置","depth":3,"charIndex":317},{"text":"环境构建","id":"环境构建","depth":3,"charIndex":419},{"text":"开发板准备","id":"开发板准备","depth":4,"charIndex":426},{"text":"编译","id":"编译","depth":4,"charIndex":475},{"text":"示例使用","id":"示例使用","depth":3,"charIndex":520}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":507,"title":"简介","content":"#\n\n自定义算子开发模块提供接口供您使用开发板提供的可编程硬件资源，从而进一步完成您多样化算子的需求，当前可编程后端包括Cadence Vision Q8\nDSP、Mali A78 GPU。\n\nDSP的开发主要分为两个步骤：\n\n 1. 使用Cadence提供的工具及资料完成算子开发；\n\n 2. 通过UCP提供的API注册算子，完成在开发板上的部署。\n\n其功能架构如下图所示：\n\n上方架构图中，DSP自定义算子应用通过自定义算子模块提供的接口，生成对应算子的任务句柄并将DSP算子注册到DSP镜像中进行编译部署。UCP提供了包含任务调度、会\n话管理、引擎管理等模块的Service，在对应算子的任务句柄生成后，通过UCP任务调度接口将算子任务提交到任务队列，分配到DSP底层硬件，实现算子的功能逻辑。\n\nGPU开发使用原生的OpenCL接口，其功能架构如下图所示：","routePath":"/guide/ucp/plugin/introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":508,"title":"AI Benchmark使用说明","content":"#\n\nAI Benchmark示例包提供了嵌入式应用开发常见分类、检测、分割、光流、追踪估计，雷达多任务，bev，深度估计模型和在线建图的性能和精度评测示例。\n其中性能评测示例包括单帧延迟评测和多线程评测示例，充分利用调用BPU多核的速度评测。\n示例包中预置了源码、可执行程序和评测脚本，您可以在地平线开发板上进行体验，并基于这些示例直接进行应用开发，降低开发门槛。\n\n\n发布物说明#\n\nAI Benchmark示例包位于 horizon_j6_open_explorer 发布物的\nsamples/ucp_tutorial/dnn/ai_benchmark/ 路径下，主要包括以下内容：\n\n编号   名称     内容\n1    code   包含示例源代码和编译脚本。\n2    j6     示例包上板运行环境。\n\n\n示例包结构#\n\n注解\n\n上板模型需要先在OE包的 samples/ai_toolchain/model_zoo/runtime/ai_benchmark/ 目录下的\nresolve_ai_benchmark_ptq.sh 和 resolve_ai_benchmark_qat.sh 脚本进行获取。\n\n示例包结构如下所示：\n\n\n\n * code：该目录内是评测程序的源码，用来进行模型性能和精度评测。\n * j6：该目录内提供了已经编译好的应用程序，以及各种评测脚本，用来测试多种模型在地平线BPU上运行的性能和精度等。\n * build_ptq_j6.sh：PTQ真机程序一键编译脚本。\n * build_qat_j6.sh：QAT真机程序一键编译脚本。\n\n\n示例模型#\n\nAI Benchmark示例包的模型发布物包括PTQ模型和QAT模型发布物：\n\n * PTQ模型model_zoo需要通过执行 samples/ai_toolchain/model_zoo/runtime/ai_benchmark/\n   路径下的 resolve_ai_benchmark_ptq.sh 脚本进行获取。\n * QAT模型model_zoo需要通过执行 samples/ai_toolchain/model_zoo/runtime/ai_benchmark/\n   路径下的 resolve_ai_benchmark_qat.sh 脚本进行获取。\n\n其中包含常用的分类、检测、分割和光流预测等模型，模型命名规则为 ___。\n\n注解\n\nmodel_zoo中的PTQ和QAT模型通过原始模型编译而得到的，PTQ原始模型的详细信息可以参考 PTQ转换示例\n章节进行了解，QAT原始模型的详细信息可以参考 示例说明 章节进行了解。\n\nAI Benchmark示例包内包含的PTQ和QAT模型发布物的性能数据可参考 模型性能Benchmark 章节。\n\n\n公共数据集#\n\n示例中会使用到数据集，对应数据集的下载您可参考 数据集下载 章节，数据准备过程中如遇问题请联系地平线。\n\n\n环境构建#\n\n使用AI Benchmark示例包之前，您需要保证开发板环境和编译环境可用：\n\n * 开发板准备\n   \n   1. 拿到开发板后，升级系统镜像到示例包推荐的系统镜像版本。\n   \n   2. 确保本地开发机和开发板可以远程连接。\n\n * 编译环境准备\n   \n   编译需要当前环境安装好交叉编译工具aarch64-none-linux-gnu-gcc和aarch64-none-linux-gnu-g++。 然后执行\n   code 目录下的build_ptq_j6.sh和build_qat_j6.sh脚本即可一键编译真机环境下的可执行程序，可执行程序和对应依赖会自动复制到\n   j6/ptq/script 和 j6/qat/script 目录下的 aarch64 目录下。\n\n注解\n\n需要注意 build_ptq_j6.sh 和 build_qat_j6.sh 脚本里指定的交叉编译工具链的位置是 opt\n目录下，如果安装在其他位置，需要手动修改下脚本中的如下内容。\n\n\n\n\n示例使用#\n\n\n评测示例#\n\n评测示例脚本主要在 script 和 tools 目录下。\n\nscript 目录下是板上运行的评测脚本，包括常见分类、检测、分割、光流、追踪等模型。每个模型下面有以下三个脚本：\n\n脚本            描述\nfps.sh        实现多线程fps统计（多线程调度，可以根据需求自由设置线程数）。\nlatency.sh    实现单帧延迟性能统计（一个线程，单帧）。\naccuracy.sh   用于精度评测。\n\n\n\nptq/tools 目录下主要包括 python_tools 下的精度计算脚本，用于PTQ模型的精度评测。\n\n\n\nqat/tools 目录下主要包括前处理脚本及精度计算脚本，用于QAT模型的精度评测。\n\n\n\n注意\n\n评测前需要执行以下命令，将 ptq （或者 qat ）目录拷贝到开发板上。\n\n\n\n\njson配置文件参数信息#\n\n本小节按照输入配置项（input_config）、输出配置项（output_config）及workflow配置项的维度，分别对workflow_fps.jso\nn、workflow_latency.json及workflow_accuracy.json中的配置项进行简单说明。\n\n注解\n\n下方给出的配置项参数信息为通用配置项信息，一些示例模型由于模型特殊性，还会有额外的配置项，具体请您参考示例模型json文件。\n\n输入配置项（input_config）#\n\n输出配置项（output_config）#\n\n参数名称                 参数说明                           涉及JSON文件\noutput_type          设置输出数据类型。                      fps.json、latency.json、accuracy.json\nin_order             设置是否按顺序进行输出。                   fps.json、latency.json、accuracy.json\nenable_view_output   设置是否将输出结果可视化。                  fps.json、latency.json\nimage_list_enable    可视化时，设置为true，则可将输出结果保存为图像类型。   fps.json、latency.json\nview_output_dir      设置可视化结果输出文件路径。                 fps.json、latency.json\neval_enable          设置是否对精度进行评估。                   accuracy.json\noutput_file          设置模型输出结果文件。                    accuracy.json\n\nworkflow配置项#\n\n模型推理配置项：\n\n后处理配置项：\n\n参数名称            参数说明                  涉及JSON文件\nthread_count    设置后处理线程数，取值范围为 1-8。   fps.json、latency.json、accuracy.json\nmethod_type     设置后处理方法。              fps.json、latency.json、accuracy.json\nmethod_config   设置后处理参数。              fps.json、latency.json、accuracy.json\n\n\n性能评测#\n\n性能评测分为latency和fps两方面。\n\n使用说明#\n\nlatency：\n\n进入到需要评测的模型目录下，执行 sh latency.sh 即可测试出单帧延迟。如下所示：\n\n\n\n注解\n * infer 表示模型推理耗时。\n * Post process 表示后处理耗时。\n\nfps：\n\n该功能采用多线程并发方式，旨在让模型可以在BPU上达到极致的性能。由于多线程并发及数据采样的原因，在程序启动阶段帧率值会较低，之后帧率会上升并逐渐趋于稳定，帧率\n的浮动范围控制在0.5%之内。 进入到需要评测的模型目录下执行 sh fps.sh 即可测试出帧率。如下所示：\n\n\n\n命令行参数说明#\n\nfps.sh脚本内容如下：\n\n\n\nlatency.sh脚本内容如下：\n\n\n\n结果可视化#\n\n如果您希望可以看到模型单次推理出来效果，可以修改workflow_latency.json，重新运行latency.sh脚本，即可在output_dir目录下生\n成展示效果。\n\n注意\n\n生成展示效果时，由于dump效果的原因，脚本运行会变慢。仅支持运行latency.sh脚本dump。\n\n可视化操作步骤 参考如下：\n\n 1. 修改workflow_latency.json配置文件\n    \n    \n\n 2. 执行latency.sh脚本\n    \n    \n\n注意\n\nbev模型可视化需要指定场景信息和homography矩阵路径，homography矩阵用于相机视角和鸟瞰图的转换，不同场景有各自的homography矩阵。在\n线建图模型可视化需要指定下对应的局部地图的感知范围。\n\nbev模型的workflow_latency.json配置文件我们建议修改成如下形式：\n\n\n\n在线建图模型的workflow_latency.json配置文件我们建议修改成如下形式：\n\n\n\n不同类别的模型可以实现的 可视化效果 也不相同，参考下表：\n\n注意\n\n轨迹预测可视化时如果需要可视化minidata以外的图片，需要额外配置道路信息、轨迹信息文件到\nminidata/argoverse1/visualization 中，生成配置文件可使用 densetnt_process.py 预处理脚本，将\n--is-gen-visual-config 参数设为 true。\n\n\n精度评测#\n\n模型评测分为五步：\n\n 1. 数据预处理。\n\n 2. 数据挂载。\n\n 3. 生成lst文件。\n\n 4. 模型推理。\n\n 5. 精度计算。\n\n数据预处理 #\n\n下文将分别对PTQ和QAT模型数据预处理方式进行介绍。\n\nPTQ模型数据预处理：\n\nPTQ模型数据预处理需要在x86仿真环境下运行 hb_eval_preprocess 工具，对数据集进行预处理。\n所谓预处理是指数据在送入模型之前的特定处理操作，比如：图片resize、crop和padding等操作。 该工具集成于 horizon_tc_ui\n工具内，安装对应的install脚本即可使用该工具。 原始数据集经过工具预处理之后，会生成模型对应的前处理二进制文件.bin文件集。\n\n小技巧\n\n关于 hb_eval_preprocess 工具命令行参数及使用方法，可键入 hb_eval_preprocess -h， 或查看PTQ工具文档中的\nhb_eval_preprocess工具 一节内容。\n\n下面将详细介绍示例包中每一个模型对应的数据集，以及对应数据集的预处理操作。\n\nPTQ模型使用到的数据集包括以下几种：\n\nQAT模型数据预处理：\n\nQAT模型数据预处理需要在x86仿真环境下执行 ai_benchmark_j6/j6/qat/tools/eval_preprocess\n中对应模型的前处理脚本。 下面将详细介绍示例包中模型对应的数据集，以及其预处理操作。\n\n小技巧\n\n使用前请修改脚本中的数据集路径及保存路径使脚本正常运行。\n\n数据挂载#\n\n由于数据集相对较大，不适合直接放在开发板上，可以采用挂载的方式供开发板读取，需要在服务器PC端和板端进行如下操作：\n\n服务器PC端：\n\n注意\n\n请注意，执行下方操作需要服务器PC端root权限。\n\n 1. 编辑 /etc/exports，增加一行：/nfs\n    *(insecure,rw,sync,all_squash,anonuid=1000,anongid=1000,no_subtree_check)。/n\n    fs 表示本机挂载路径，可替换为您所指定目录。\n\n 2. 执行命令 exportfs -a -r，使/etc/exports生效。\n\n板端：\n\n 1. 创建需要挂载的目录： mkdir -p /mnt。\n\n 2. 挂载： mount -t nfs {PC端IP}:/nfs /mnt -o nolock。\n\n完成将PC端的 /nfs 文件夹挂载至板端 /mnt 文件夹。按照此方式，将包含预处理数据的文件夹挂载至板端， 并将 /data 目录软链接至板端 /ptq 或\n/qat 目录下，与 /script 同级目录。\n\n生成lst文件#\n\n示例中精度计算脚本的运行流程是：\n\n 1. 根据 workflow_accurary.json 中的 image_list_file 参数值，去寻找对应数据集的 lst 文件。\n\n 2. 根据 lst 文件存储的前处理文件路径信息，去加载每一个前处理文件，然后进行推理。\n\n因此生成预处理文件之后，需要生成对应的lst文件，将每一张前处理文件的路径写入到lst文件中，而这个路径与数据集在板端的存放位置有关。 这里我们推荐其存放位置与\n./data/dataset_name/pre_model_name 预处理数据文件夹同级目录。\n\nPTQ预处理数据集结构如下：\n\n\n\nQAT预处理数据集结构如下：\n\n\n\n对应的lst文件，参考生成方式如下：\n\n除bev系列、motr_efficientnetb3_mot17、stereonetplus_mixvargenet_sceneflow,\nmaptroe_henet_tinym_bevformer_nuscenes和densetnt_argoverse1模型外，其余模型的lst文件参考生成方式：\n\n\n\n注解\n\n-name后的参数需要根据预处理后的数据集格式进行对应调整，如bin、png。\n\n这样生成的lst文件中存储的路径为一个相对路径：../../../data/coco/pre_centernet_resnet101/ ， 可以与\nworkflow_accuracy.json 默认的配置路径吻合。\n\n如果需要更改前处理数据集的存放位置，则需要确保对应的 lst 文件可以被 workflow_accuracy.json 读取到。 其次需要确保程序根据 lst\n中的路径信息，能读取到对应的前处理文件。\n\n对于bev系列、motr_efficientnetb3_mot17、stereonetplus_mixvargenet_sceneflow、maptroe_he\nnet_tinym_bevformer_nuscenes和densetnt_argoverse1模型，lst文件参考生成方式：\n\n模型推理#\n\naccuracy.sh脚本内容中各参数说明如下：\n\n\n\n挂载完数据后，登录开发板，执行 centernet_resnet101/ 目录下的accuracy.sh脚本，如下所示：\n\n\n\n板端程序会在当前目录生成eval.log文件，该文件就是预测结果文件。\n\n精度计算#\n\n注意\n\n请注意，精度计算部分需在docker环境或linux环境下进行操作。\n\n精度计算我们按照PTQ模型精度和QAT模型精度计算两种情况为您展开介绍。\n\nPTQ模型精度计算：\n\nPTQ模型精度计算的脚本在 ptq/tools/python_tools/accuracy_tools 目录下，其中：\n\n脚本                       描述\ncls_imagenet_eval.py     用于计算使用 ImageNet 数据集评测的分类模型的精度。\ndet_coco_eval.py         用于计算使用COCO数据集评测的检测模型的精度。\nseg_cityscapes_eval.py   用于计算使用Cityscapes数据集评测的分割模型的精度。\ndet_voc_eval.py          用于计算使用VOC数据集评测的检测模型的精度。\n\n以下为您说明不同类型的PTQ模型精度计算方式：\n\nQAT模型精度计算：\n\nQAT模型的精度计算脚本在 qat/tools/python_tools/accuracy_tools 目录下，其中：\n\n脚本                        描述\nbev_eval.py               用于计算bev模型的精度。\ncenterpoint_eval.py       用于计算centerpoint_pointpillar_nuscenes雷达3D模型的精度。\ncls_eval.py               用于计算分类模型的精度。\ndensetnt_eval.py          用于计算densetnt_argoverse1轨迹预测模型的精度。\ndetr_eval.py              用于计算detr和deform_detr_resnet50_mscoco检测模型的精度。\nfcos3d_eval.py            用于计算fcos3d_efficientnetb0_nuscenes检测模型的精度。\nfcos_eval.py              用于计算fcos检测模型的精度。\nganet_eval.py             用于计算ganet_mixvargenet_culane检测模型的精度。\nkeypoints_eval.py         用于计算keypoints检测模型的精度。\nlidar_multitask_eval.py   用于计算lidar多任务模型centerpoint_mixvargnet_multitask_nuscenes的精度。\nmotr_eval.py              用于计算motr_efficientnetb3_mot17检测模型的精度。\nparsing_eval.py           用于计算使用Cityscapes数据集评测的分割模型的精度。\npointpillars_eval.py      用于计算pointpillars检测模型的精度。\nstereonet_eval.py         用于计算stereonetplus_mixvargenet_sceneflow深度估计模型的精度。\nmaptroe_eval.py           用于计算maptroe_henet_tinym_bevformer_nuscenes在线建图模型的精度。\nflashocc_eval.py          用于计算flashocc模型的精度。\n\n以下为您说明不同类型的QAT模型精度计算方式：\n\n\n模型集成#\n\n\n前处理#\n\n您可根据需要自行添加模型前处理，将其部署到 CPU 或 DSP 上，以centerpoint_pointpillar_nuscenes为例：\n\n 1. 增加前处理文件qat_centerpoint_preprocess_method.cc，以及头文件qat_centerpoint_preprocess_\n    method.h。\n\n 2. 增加模型前处理配置文件。\n\n前处理文件及头文件添加#\n\n前处理文件 qat_centerpoint_preprocess_method.cc 放置于 ai_benchmark/code/src/method/\n路径下， 头文件 qat_centerpoint_preprocess_method.h 放置于\nai_benchmark/code/include/method/ 路径下：\n\n\n\n模型前处理配置文件添加#\n\n\n\ncenterpoint_pointpillar_nuscenes的前处理可以部署到 CPU 或 DSP 上，这取决于配置文件\ncenterpoint_pointpillar_5dim.json 中是否配置对应参数 run_on_dsp。 如果配置文件中的 run_on_dsp 设置为\ntrue ，那么前处理将在 DSP 上运行，否则它在 CPU 上运行。\n\n前处理单帧延时评测#\n\n执行 sh latency.sh 脚本可对前处理的单帧延迟情况进行测试。如下所示：\n\n\n\n其中：\n\n * Pre process 表示前处理耗时。\n * Infer 表示模型推理耗时。\n * Post process 表示后处理耗时。\n\n\n后处理#\n\n后处理集成主要有2个步骤，以CenterNet模型集成为例：\n\n 1. 增加后处理文件ptq_centernet_post_process_method.cc，以及头文件ptq_centernet_post_process_\n    method.h。\n\n 2. 增加模型运行脚本及配置文件。\n\n后处理文件及头文件添加#\n\n后处理代码文件可直接复用src/method目录下任意后处理文件，主要修改 InitFromJsonString 函数，以及 PostProcess 函数即可。\n\nInitFromJsonString 函数主要是读取workflow.json中的后处理相关的参数配置，您可自定义设置相应的输入参数。 PostProcess\n函数主要完成后处理的逻辑。\n\n后处理文件 ptq_centernet_post_process_method.cc 放置于 ai_benchmark/code/src/method/\n路径下，头文件 ptq_centernet_post_process_method.h 放置于\nai_benchmark/code/include/method/ 路径下：\n\n\n\n模型运行脚本及配置文件添加#\n\n模型运行脚本及配置文件完成添加后的目录结构参考如下：\n\n * centerpoint_pointpillar_nuscenes模型：\n   \n   \n\n如需在 DSP 上部署，需要执行 dsp_deploy.sh 脚本部署 DSP 环境，详细介绍请参考该示例文件夹下的 README.md 中的介绍。\n\n * motr_efficientnetb3_mot17模型：\n   \n   \n\n * 除centerpoint_pointpillar_nuscenes模型及motr_efficientnetb3_mot17外其他模型：\n   \n   \n\n\n辅助工具#\n\n\n日志#\n\n日志主要包括 示例日志 和 DNN日志 两部分。 其中示例日志是指交付包示例代码中所应用的日志，DNN日志是指嵌入式runtime库中的日志。\n根据不同的需求可以设置不同的日志。\n\n示例日志#\n\n 1. 日志级别\n\n示例日志主要采用glog中的vlog，主要分为四个自定义级别：\n\n * 0：SYSTEM级别，该级别日志主要用于输出报错信息。\n * 1：REPORT级别，该级别日志在示例代码中主要用于输出性能数据。\n * 2：DETAIL级别，该级别日志在示例代码中主要用于输出系统当前状态信息。\n * 3：DEBUG级别，该级别日志在示例代码中主要用于输出调试信息。\n\n 2. 日志级别设置\n\n日志处理机制说明：日志级别等级由高到低默认顺序为DEBUG>DETAIL>REPORT>SYSTEM，级别越高，输出日志越多。\n即设置高等级，则会输出自身及低于自身等级的日志。\n\n在运行示例时，日志级别需要通过 log_level 参数来进行设置。 例如，指定 log_level=0，则会输出SYSTEM级别日志。指定\nlog_level=3，则会输出DEBUG、DETAIL、REPORT及SYSTEM级别日志。\n\ndnn 日志#\n\n关于 dnn 日志的配置，请阅读模型推理API手册章节中的 配置信息 一节内容。\n\n\n算子耗时#\n\n概述#\n\n对OP性能的统计是通过设置 HB_DNN_PROFILER_LOG_PATH 环境变量实现的。对该变量的类型和取值说明如下：\n\nHB_DNN_PROFILER_LOG_PATH=${path}：表示OP节点dump的输出路径，程序正常运行完退出后，产生profiler.log文件。\n\n示例#\n\n以mobilenetv1模型为例，开启单线程同时RunModel，设置 export\nHB_DNN_PROFILER_LOG_PATH=./，则profiler.log文件中会输出OP的性能数据。 其中包含 model_latency 和\ntask_latency，model_latency中输出了模型每个OP运行所需要的耗时情况，task_latency中输出了模型运行中各个task模块的耗时情\n况。\n\n\ndump工具#\n\n通过开启 HB_DNN_DUMP_PATH 这个环境变量可以dump出模型推理过程中每个节点的输入和输出。\n通过dump工具，可以排查模拟器和真机是否存在一致性问题：即相同模型，相同输入，真机和模拟器的输出结果是否完全相同。","routePath":"/guide/ucp/runtime/ai_benchmark","lang":"zh","toc":[{"text":"发布物说明","id":"发布物说明","depth":2,"charIndex":185},{"text":"示例包结构","id":"示例包结构","depth":3,"charIndex":361},{"text":"示例模型","id":"示例模型","depth":3,"charIndex":691},{"text":"公共数据集","id":"公共数据集","depth":3,"charIndex":1179},{"text":"环境构建","id":"环境构建","depth":2,"charIndex":1241},{"text":"示例使用","id":"示例使用","depth":2,"charIndex":1691},{"text":"评测示例","id":"评测示例","depth":3,"charIndex":1699},{"text":"json配置文件参数信息","id":"json配置文件参数信息","depth":3,"charIndex":2070},{"text":"输入配置项（input_config）","id":"输入配置项input_config","depth":4,"charIndex":2294},{"text":"输出配置项（output_config）","id":"输出配置项output_config","depth":4,"charIndex":2316},{"text":"workflow配置项","id":"workflow配置项","depth":4,"charIndex":2931},{"text":"性能评测","id":"性能评测","depth":3,"charIndex":3235},{"text":"使用说明","id":"使用说明","depth":4,"charIndex":3265},{"text":"命令行参数说明","id":"命令行参数说明","depth":4,"charIndex":3526},{"text":"结果可视化","id":"结果可视化","depth":4,"charIndex":3574},{"text":"精度评测","id":"精度评测","depth":3,"charIndex":4216},{"text":"数据预处理","id":"数据预处理","depth":4,"charIndex":-1},{"text":"数据挂载","id":"数据挂载","depth":4,"charIndex":4872},{"text":"生成lst文件","id":"生成lst文件","depth":4,"charIndex":5354},{"text":"模型推理","id":"模型推理","depth":4,"charIndex":6243},{"text":"精度计算","id":"精度计算","depth":4,"charIndex":6379},{"text":"模型集成","id":"模型集成","depth":2,"charIndex":7895},{"text":"前处理","id":"前处理","depth":3,"charIndex":7903},{"text":"前处理文件及头文件添加","id":"前处理文件及头文件添加","depth":4,"charIndex":8095},{"text":"模型前处理配置文件添加","id":"模型前处理配置文件添加","depth":4,"charIndex":8277},{"text":"前处理单帧延时评测","id":"前处理单帧延时评测","depth":4,"charIndex":8473},{"text":"后处理","id":"后处理","depth":3,"charIndex":8605},{"text":"后处理文件及头文件添加","id":"后处理文件及头文件添加","depth":4,"charIndex":8759},{"text":"模型运行脚本及配置文件添加","id":"模型运行脚本及配置文件添加","depth":4,"charIndex":9117},{"text":"辅助工具","id":"辅助工具","depth":2,"charIndex":9407},{"text":"日志","id":"日志","depth":3,"charIndex":9415},{"text":"示例日志","id":"示例日志","depth":4,"charIndex":9511},{"text":"dnn 日志","id":"dnn-日志","depth":4,"charIndex":9929},{"text":"算子耗时","id":"算子耗时","depth":3,"charIndex":9981},{"text":"概述","id":"概述","depth":4,"charIndex":9988},{"text":"示例","id":"示例","depth":4,"charIndex":10136},{"text":"dump工具","id":"dump工具","depth":3,"charIndex":10347}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":509,"title":"基础示例包使用说明","content":"#\n\n基础示例包旨在帮助您熟悉和学习模型推理相关的接口以及各种进阶功能。\n基础示例包也可以作为新项目的起点，帮助您快速搭建起一个基本框架，并在此基础上进行进一步的开发和定制。\n\n基础示例包提供了三个方面的示例：\n\n * 提供了基于resnet50模型的快速入门示例。您可以体验并基于这个示例进行应用开发，降低开发门槛。\n\n * 提供了模型推理接口的使用示例。您可以通过该示例更好的熟悉各种基础功能的使用。\n\n * 提供了各种特色功能的进阶示例。您可以根据实际使用场景选择合适的功能进行模型推理。\n\n\n发布物说明#\n\n基础示例包位于 horizon_j6_open_explorer 发布物的 samples/ucp_tutorial/dnn/basic_samples/\n路径下，主要包括以下内容：\n\n名称              内容\nbasic_samples   包含示例源代码和运行脚本。\n\n注解\n\n上板模型需要先在OE包的 samples/ai_toolchain/model_zoo/runtime/basic_samples\n目录下执行resolve_runtime_sample.sh 脚本进行获取。\n\n示例包结构如下所示：\n\n\n\n * code：该目录内是示例的源码。\n\n * code/00_quick_start：快速入门示例，基于模型推理接口，用resnet50进行单张图片模型推理和结果解析。\n\n * code/01_api_tutorial：模型推理接口使用示例，包括增加/去除padding的示例 padding、量化反量化示例\n   quanti、内存使用示例 mem、模型加载与信息获取示例 model 以及带roi输入的模型推理示例 roi_infer。\n\n * code/02_advanced_samples：模型推理进阶示例，包括裁剪图像作为模型输入的示例 crop、存在roi输入时的模型推理示例\n   roi_infer 以及多模型批量推理示例 multi_model_batch。\n\n * code/build.sh：程序编译脚本，需要指定编译参数，被build_aarch64.sh和build_x86.sh调用。\n\n * code/build_aarch64.sh：程序一键编译脚本，编译产生的示例在aarch64环境下运行。\n\n * code/build_x86.sh：程序一键编译脚本，编译产生的示例在x86环境下运行。\n\n * code/resolve.sh：程序运行依赖获取脚本，在编译模型前需执行。\n\n * runtime：示例运行脚本，预置了数据和相关模型。\n\n\n环境构建#\n\n\n开发板准备#\n\n 1. 拿到开发板后，升级系统镜像到示例包推荐的系统镜像版本。\n\n 2. 确保本地开发机和开发板可以远程连接。\n\n\n编译#\n\n编译需要的步骤如下：\n\n 1. 当前环境安装好交叉编译工具aarch64-none-linux-gnu-gcc和aarch64-none-linux-gnu-g++。\n\n 2. 在 ucp_tutorial/tools/basic_samples 目录下有预先配置好的编译脚本 build.sh ，选项 -a x86 和 -a\n    aarch64 分别支持两种编译方式， 直接执行 build.sh 脚本即可完成一键编译，生成的文件会被保存到\n    ucp_tutorial/vp/vp_samples 目录下。 此外，目录中也包含了 build_aarch64.sh 和 build_x86.sh\n    两个编译脚本，分别对应了两个编译选项， 使用这两个脚本进行编译与使用 build.sh 脚本等效。\n\n 3. 执行编译脚本后，可执行程序和对应依赖会自动复制到 runtime/script 或者 runtime/script_x86目录下。\n\n注解\n\nbuild.sh脚本里指定的交叉编译工具链的位置是 /usr/bin 目录下，您如果安装在其他位置，可以手动修改下build.sh。\n\n\n示例使用（basic_samples示例）#\n\n注解\n\n下方log及命令行中的version仅为示例，实际打印的log中version以OE包中的version为准。\n\n若在x86环境下运行示例，请使用 runtime/script_x86 下的脚本。\n\n示例脚本主要在 runtime/script 目录下，编译程序后目录结构如下:\n\n\n\n运行前，把runtime目录部署到板端，执行对应脚本即可。\n\n注解\n\n * basic_samples 示例包的模型发布物需要在OE包的\n   samples/ai_toolchain/model_zoo/runtime/basic_samples 目录下执行\n   resolve_runtime_sample.sh 脚本进行获取。\n\n * basic_samples 示例包的其他依赖需要在OE包的 samples/ucp_tutorial/dnn/basic_samples/code\n   目录下执行 resolve.sh 脚本进行获取。\n\n * model 文件夹包含模型所在的路径， runtime 文件夹为软链接，链接路径指向\n   ../../../model_zoo/runtime/basic_samples 。\n\n\nquick_start#\n\n00_quick_start\n目录下，我们提供了一个快速上手的示例，旨在介绍模型推理的使用流程。这里以resnet50模型为例，分别展示了nv12输入和rgb输入两种模型推理的过程，包含了从\n准备数据到模型推理，再到执行后处理，生成分类结果的完整流程代码。\n\n代码主要分为六个部分：\n\n * 加载模型并获取模型指针：主要涉及到的接口有 hbDNNInitializeFromFiles、hbDNNGetModelNameList 以及\n   hbDNNGetModelHandle。\n\n * 准备输入以及输出张量，同时补全输入的动态信息：主要涉及到的接口有\n   hbDNNGetInputCount、hbDNNGetOutputCount、hbDNNGetInputTensorProperties、hbDNNGet\n   OutputTensorProperties、hbDNNGetInputName、hbDNNGetOutputName 以及\n   hbUCPMallocCached。\n\n * 准备输入数据：对于nv12输入，需要将图片进行颜色空间转换，而对于rgb输入，需要做U8转换到S8的前处理，这里涉及到的接口为\n   hbUCPMemFlush。\n\n * 模型推理：主要涉及到的接口有 hbDNNInferV2、hbUCPSubmitTask 以及 hbUCPWaitTaskDone。\n\n * 输出数据后处理：resnet50模型是分类模型，可以通过后处理获取置信度较高的分类结果，在处理输出前需要调用 hbUCPMemFlush\n   接口进行数据同步。\n\n * 释放任务、内存以及模型：主要涉及到的接口有 hbUCPReleaseTask、hbUCPFree 以及 hbDNNRelease。\n\n脚本的目录结构如下：\n\n\n\n使用的时候，进入 00_quick_start 目录，然后直接执行sh run_resnet_rgb.sh和sh\nrun_resnet_nv12.sh即可，脚本执行成功后会打印模型输入输出的名称以及置信度最高的5个分类结果，分类结果置信度最高的编号 340 表明是斑马。\n\n\napi_tutorial#\n\n该示例是指 01_api_tutorial 目录内的示例，旨在介绍模型推理API的使用。其目录包含以下脚本：\n\n\n\npadding#\n\n该示例主要帮助您熟悉模型输入如何增加padding，模型输出如何去除padding。主要涉及到的数据类型为 hbDNNTensorProperties 中的\nvalidShape 与 stride。\n\n为了加速计算，模型对于输入数据有对齐要求，详见 对齐规则。该示例分为两个部分：\n\n * 对于模型输入，当需要额外对数据进行对齐操作时，使用示例中提供的 add_padding 方法增加padding。\n\n * 对于模型输出，当输出中含有padding时，使用示例中提供的 remove_padding 方法去除padding以获取有效数据。\n\n使用的时候，直接进入 01_api_tutorial 目录，然后直接执行 sh padding.sh\n即可，脚本执行成功后会打印原始输入和增加padding后的输入数据、原始输出和去除padding后的输出数据。\n\nquanti#\n\n该示例主要帮助您熟悉模型输入输出的量化、反量化处理。涉及到的的数据类型为 hbDNNTensorProperties 中的 quantiType、scale\n以及 quantizeAxis。\n\n当模型的输入中记录有量化信息时，需要将输入的浮点数按指定规则进行量化处理；当模型的输出中记录有量化信息时，需要对输出进行反量化处理后再对数据进行后处理。\n处理方式可参考 hbDNNQuantiScale。\n\n该示例主要分为四个部分：\n\n * 输入per tensor的量化处理\n\n * 输入per axis的量化处理\n\n * 输出per tensor的反量化处理\n\n * 输出per axis的反量化处理\n\n使用的时候，直接进入 01_api_tutorial 目录，然后直接执行 sh quanti.sh\n即可，脚本执行成功后每个部分会打印原始的输入/输出和量化/反量化后的结果。\n\nmodel#\n\n该示例主要帮助您熟悉模型加载以及模型信息获取相关的接口使用。\n\n涉及到的接口主要包括\nhbDNNInitializeFromFiles、hbDNNGetModelNameList、hbDNNGetModelHandle、hbDNNGetInput\nCount、hbDNNGetOutputCount、hbDNNGetInputTensorProperties、hbDNNGetOutputTensorProp\nerties 以及 hbDNNRelease。\n\n使用的时候，直接进入 01_api_tutorial 目录，然后直接执行 sh model.sh 即可，脚本执行成功后会打印出模型的基本信息。\n\nsys_mem#\n\n该示例主要帮助您熟悉内存接口的使用。主要涉及的接口有 hbUCPMalloc、 hbUCPMallocCached、hbUCPMemFlush 和\nhbUCPFree。\n\n针对于可缓存的内存，使用步骤如下：\n\n * 申请可缓存的内存。\n\n * 将数据写入内存。\n\n * 调用 hbUCPMemFlush 接口使用 HB_SYS_MEM_CACHE_CLEAN 参数将数据同步到DDR。\n\n * 经过模型推理或算子计算后，调用 hbUCPMemFlush 接口使用 HB_SYS_MEM_CACHE_INVALIDATE 参数将数据同步至缓存。\n\n * 读取数据进行处理。\n\n * 释放内存。\n\n使用的时候，直接进入 01_api_tutorial 目录，执行 sh sys_mem.sh 即可。\n\n\nadvanced_samples#\n\n该示例是指 02_advanced_samples 目录内的示例，旨在介绍进阶示例的使用。其目录包含以下脚本：\n\n\n\ncrop#\n\n该示例主要帮助您熟悉如何对图像进行裁剪并作为模型的输入进行推理。示例代码的整体流程与 00_quick_start 相同，差异主要在于输入数据的准备。\n\n该示例裁剪的原理是通过已存在图像的内存地址进行偏移到ROI左上角点，通过控制stride的大小将图像多余的部分进行屏蔽从而准备好模型的输入。\n\n示例使用的限制：\n\n * 图像：要求图像分辨率较大，至少要大于模型的输入，并且提前将图像读入到BPU内存中。\n\n * 模型：要求模型的输入validShape为固定的，stride为动态的，这样能通过控制stride的大小对图像进行裁剪。\n\n * 裁剪位置：由于裁剪是对图像内存进行偏移，而对于输入内存的首地址要求 32 对齐，因此对偏移的大小有限制。\n\n输入数据的准备：\n\n * 图像：斑马图片的大小为376x376，我们将图片读入转化为Y和UV输入并进行 32 对齐后分别存在两块BPU内存中。 32 对齐后斑马图片的宽\n   image_width_stride = 384。\n\n * 模型：模型有两个输入\n   \n   * y: validShape = (1,224,224,1), stride = (-1,-1, 1,1)\n   \n   * uv: validShape = (1,112,112,2), stride = (-1,-1,2,1)\n\n * 裁剪：我们需要在图像中裁剪出一块224x224大小的区域作为模型的输入，选择(64,50)作为左上角点正好可以获取到大部分斑马图像，并且偏移后的内存地址正\n   好 32 对齐，满足要求。如下图所示。\n\n * 输入张量准备：\n   \n   * Y输入的stride为[224 *\n     image_width_stride,image_width_stride,1,1]，裁剪图像左上角点对应的坐标为[0,50,64,0]，则地址偏移为\n     50 * image_width_stride + 64 * 1；\n   \n   * UV输入的stride为[112 *\n     image_width_stride,image_width_stride,2,1]，裁剪图像左上角点对应的坐标为[0,25,32,0]，则地址偏移为\n     25 * image_width_stride + 32 * 2；\n\n使用的时候，直接进入 02_advanced_samples 目录，然后直接执行 sh run_crop.sh\n即可，脚本执行成功后会打印置信度最高的5个分类结果，分类结果置信度最高的编号 340 表明是斑马。\n\nmulti_model_batch#\n\n该示例主要帮助您熟悉小模型批量处理的功能。\n\n当有多个小模型运行时，如果一个个模型任务单独运行，整个框架的调度时间相对来说占比会较大，为了避免性能损失，您可以将多个小模型任务合成一个任务进行推理，减小框架调\n度时间的占比。\n\n该示例以两个小模型 googlenet 和 resnet50 为例，通过多次调用 hbDNNInferV2 接口来创建/添加任务。\n\n使用的时候，直接进入 02_advanced_samples 目录，执行 sh run_multi_model_batch.sh\n即可，脚本执行成功后会打印两个模型置信度最高的分类结果，分类编号 340 表明是斑马。\n\nroi_infer#\n\n该示例主要帮助您理解当存在roi输入时，如何准备数据进行模型推理。\n\n示例代码的整体流程与 00_quick_start 类似，差异主要在于输入数据的准备。该示例模型为 mobilenetv1，输入一共为3个，如下所示：\n\n * name: data_y；validShape: (1,-1,-1,1)；stride: (-1,-1,1,1)；\n\n * name: data_uv；validShape: (1,-1,-1,2)；stride: (-1,-1,2,1)；\n\n * name: data_roi；validShape: (1,4)；stride: (16,4)；\n\ndata_y 与 data_uv 为动态输入，通过读取图片并转化为Y和UV数据进行对齐后作为输入，并依照图片的大小对 validShape 与 stride\n进行补全。动态输入相关可参考 动态输入介绍。\n\ndata_roi 为图片中ROI区域的左上、右下角点的坐标，顺序为：左、上、右、下。\n\n使用的时候，直接进入 02_api_tutorial 目录，然后直接执行 sh roi_infer.sh\n即可，脚本执行成功后会分别打印模型两组输入置信度最高的5个分类结果，分类结果置信度最高的编号 340 表明是斑马。\n\n\n辅助工具（日志）#\n\n日志主要包括 示例日志 和 dnn日志\n两部分。其中示例日志是指交付包示例代码中所应用的日志，dnn日志是指嵌入式dnn库中的日志。您可以根据不同的需求设置不同的日志。\n\n\n示例日志#\n\n示例日志主要采用hlog，hlog的日志主要分为7个等级：\n\nlog等级可设置为0、1、2、3、4、5、6，分别对应trace、debug、info、warn、error、critical、never，默认为info。\n\n\ndnn 日志#\n\n关于 dnn 日志的配置，请阅读模型推理API手册章节中的 配置信息 一节内容。","routePath":"/guide/ucp/runtime/basic_sample","lang":"zh","toc":[{"text":"发布物说明","id":"发布物说明","depth":2,"charIndex":249},{"text":"环境构建","id":"环境构建","depth":2,"charIndex":1114},{"text":"开发板准备","id":"开发板准备","depth":3,"charIndex":1122},{"text":"编译","id":"编译","depth":3,"charIndex":1188},{"text":"示例使用（basic_samples示例）","id":"示例使用basic_samples示例","depth":2,"charIndex":1695},{"text":"quick_start","id":"quick_start","depth":3,"charIndex":2235},{"text":"api_tutorial","id":"api_tutorial","depth":3,"charIndex":3162},{"text":"padding","id":"padding","depth":4,"charIndex":3235},{"text":"quanti","id":"quanti","depth":4,"charIndex":3626},{"text":"model","id":"model","depth":4,"charIndex":4025},{"text":"sys_mem","id":"sys_mem","depth":4,"charIndex":4336},{"text":"advanced_samples","id":"advanced_samples","depth":3,"charIndex":4696},{"text":"crop","id":"crop","depth":4,"charIndex":4774},{"text":"multi_model_batch","id":"multi_model_batch","depth":4,"charIndex":5881},{"text":"roi_infer","id":"roi_infer","depth":4,"charIndex":6191},{"text":"辅助工具（日志）","id":"辅助工具日志","depth":2,"charIndex":6753},{"text":"示例日志","id":"示例日志","depth":3,"charIndex":6851},{"text":"dnn 日志","id":"dnn-日志","depth":3,"charIndex":6970}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":510,"title":"对齐规则","content":"#\n\n本节内容介绍使用BPU的对齐限制规则。\n\n\n模型输入要求#\n\nBPU不限制模型输入大小或者奇偶。既像YOLO这种416x416的输入可以支持，对于像SqueezeNet这种227x227的输入也可以支持。\n对于NV12输入比较特别，要求HW都是偶数，是为了满足UV是Y的一半的要求。\n\n\n对齐和有效数据#\n\nBPU对数据有对齐限制。有效数据排布和对齐数据排布用 hbDNNTensorProperties 中的 validShape 和 stride 表示。\n\n * validShape 是有效数据的shape。\n * stride 表示 validShape 各维度的步长，描述跨越张量各个维度所需要经过的字节数。注意的是 hbDNNDataType\n   为NV12或Y类型输入的模型比较特殊，为兼容模型，其获取到的 stride 均为0，因为这些类型输入只要求W方向32对齐。\n\n模型输入输出张量可以通过 validShape 和 stride 获取正确的数据排布， 比如获取到的模型某输入属性 hbDNNDataType =\nHB_DNN_TENSOR_TYPE_U16 ， validShape = [1, 3, 212, 212] ， stride = [301056,\n100352, 448, 2] ， 这里表示模型有效输入的大小为 1x3x212x212 。\n\n * stride[3] = 2 ，表示每个元素 16bit 大小。\n * stride[2] = 448 = 2 * 224 ，表示 index=3 的维度按照 224 对齐，因此 index=3 的维度步长为 448 。\n * stride[1] = 100352 = 448 * 224， ，表示 index=2 的维度也按照 224 对齐，因此 index=2 的维度步长为\n   100352 。\n * stride[0] = 301056 = 100352 * 3 ，表示 index=1 的维度按照 3 对齐，和有效尺寸一致，因此 index=1\n   的维度步长为 301056 。\n\n在后续场景使用时，考虑到对齐要求，当 alignedByteSize > 0 时，建议按照 alignedByteSize 大小来申请内存空间。\n\n您可以使用以下方式判断是否需要对模型的输入进行对齐，若公式不成立，您则需要额外对输入数据进行对齐操作。\n\n$\\prod_^validShape.dimensionSize[i] == stride[0]$，其中 n=validShape.numDimensions。\n\n\n模型输入输出内存要求#\n\nBPU对模型输入输出内存首地址有对齐限制，要求输入与输出内存的首地址 32 对齐。\n\n注解\n\n * 使用 hbUCPMalloc 与 hbUCPMallocCached 接口申请的内存首地址默认 32 对齐。\n\n * 当您申请一块内存，并使用偏移地址作为模型的输入或输出时，请检查偏移后的首地址是否 32 对齐。","routePath":"/guide/ucp/runtime/bpu_sdk_api/alignment_rule","lang":"zh","toc":[{"text":"模型输入要求","id":"模型输入要求","depth":3,"charIndex":24},{"text":"对齐和有效数据","id":"对齐和有效数据","depth":3,"charIndex":146},{"text":"模型输入输出内存要求","id":"模型输入输出内存要求","depth":3,"charIndex":1104}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":511,"title":"模型推理API概览","content":"#\n\n本章节主要介绍了模型推理相关的API、数据、结构体、排布及对齐规则等。\n通过阅读本章节，您可以在Horizon开发板上利用API完成模型的加载与释放，模型信息的获取，以及模型的推理等操作。\n\n\n数据结构#\n\n\n功能接口#","routePath":"/guide/ucp/runtime/bpu_sdk_api/bpu_sdk_api_overview","lang":"zh","toc":[{"text":"数据结构","id":"数据结构","depth":2,"charIndex":99},{"text":"功能接口","id":"功能接口","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":512,"title":"配置信息","content":"#\n\n\n环境变量#\n\n\n\n\n日志等级设置说明#\n\n * 日志等级：\n   \n   NN 模块中的日志主要分为7个等级：\n   \n   log等级可设置为0、1、2、3、4、5、6，分别对应Trace、Debug、Info、Warning、Error、Critical、Never，默认为W\n   arning。\n\n * 日志等级设置规则：\n   \n   * 若发生的log等级 >= 设置的等级，则该log可以被打印，反之被屏蔽。\n   * 设置的log等级越小，打印信息越多。例如：设置log等级为3，即为Warning级别，则3、4、5等级的log均可以被打印。NN\n     模块默认log等级为Warning级别，即以下log级别的信息可以被打印：Warning、Error、Critical。\n\n\n自定义算子库设置说明#\n\nDNN支持使用自定义算子进行模型推理，设置方式如下：\n\n\n\n注解\n\n * 多个算子库之间用冒号分开。\n\n * 若不同算子库中存在相同的算子，优先使用靠前算子库中的算子。\n\n * 若自定义算子库不存在，会打印警告信息并跳过该算子库的加载。\n\n\nHBIR模型推理说明#\n\nDNN支持X86环境下推理HBIR模型，由于您的环境中可能不存在GPU，因此默认不使用GPU加速。\n\n若您的机器上存在GPU，可以通过设置环境变量 HB_NN_HBIR_GPU_ENABLE 为 true 来使用GPU加速。\n\n注解\n\n * 使用GPU加速时，应确保您的机器上已安装了相应的GPU驱动和CUDA环境，环境要求请参考环境部署。\n\n * 使用GPU加速时，应确保 libhbdnn.so 在 LD_LIBRARY_PATH 所设置的目录下。\n\n * 使用GPU加速时，可以设置控制参数中的 deviceId 来指定GPU进行计算，应确保设置的值在 [0, gpu_dvice_count) 范围内。","routePath":"/guide/ucp/runtime/bpu_sdk_api/configuration_info","lang":"zh","toc":[{"text":"环境变量","id":"环境变量","depth":2,"charIndex":3},{"text":"日志等级设置说明","id":"日志等级设置说明","depth":2,"charIndex":13},{"text":"自定义算子库设置说明","id":"自定义算子库设置说明","depth":2,"charIndex":353},{"text":"HBIR模型推理说明","id":"hbir模型推理说明","depth":2,"charIndex":487}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":513,"title":"HB_DNN_TENSOR_MAX_DIMENSIONS","content":"#\n\n\n\n张量最大的维度设置为 8。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/HB_DNN_TENSOR_MAX_DIMENSIONS","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":514,"title":"hbDNNDataType","content":"#\n\n\n\n张量的类型。\n\nS 代表有符号，U 代表无符号，F 代表浮点型，后面的数字代表bit数。\n\n * 成员\n\n成员名称                       描述\nHB_DNN_TENSOR_TYPE_S4      张量类型为有符号4bit。\nHB_DNN_TENSOR_TYPE_U4      张量类型为无符号4bit。\nHB_DNN_TENSOR_TYPE_S8      张量类型为有符号8bit。\nHB_DNN_TENSOR_TYPE_U8      张量类型为无符号8bit。\nHB_DNN_TENSOR_TYPE_F16     张量类型为浮点型16bit。\nHB_DNN_TENSOR_TYPE_S16     张量类型为有符号16bit。\nHB_DNN_TENSOR_TYPE_U16     张量类型为无符号16bit。\nHB_DNN_TENSOR_TYPE_F32     张量类型为浮点型32bit。\nHB_DNN_TENSOR_TYPE_S32     张量类型为有符号32bit。\nHB_DNN_TENSOR_TYPE_U32     张量类型为无符号32bit。\nHB_DNN_TENSOR_TYPE_F64     张量类型为浮点型64bit。\nHB_DNN_TENSOR_TYPE_S64     张量类型为有符号64bit。\nHB_DNN_TENSOR_TYPE_U64     张量类型为无符号64bit。\nHB_DNN_TENSOR_TYPE_BOOL8   张量类型为布尔8bit。\nHB_DNN_TENSOR_TYPE_MAX     代表最大的张量类型编号。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNDataType","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":515,"title":"hbDNNDescType","content":"#\n\n\n\n描述信息类型，模型或者每个输入/输出均可以带有描述信息，方便使用。\n\n * 成员\n\n成员名称                       描述\nHB_DNN_DESC_TYPE_UNKNOWN   不带描述信息。\nHB_DNN_DESC_TYPE_STRING    string类型的描述信息。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNDescType","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":516,"title":"hbDNNHandle_t","content":"#\n\n\n\nDNN句柄，指向单一模型。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNHandle_t","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":517,"title":"hbDNNPackedHandle_t","content":"#\n\n\n\nDNN句柄，指向打包的多个模型。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNPackedHandle_t","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":518,"title":"hbDNNQuantiScale","content":"#\n\n\n\n量化/反量化的缩放数据。\n\n对于输入 ：若采集到浮点数据 data， 对应的缩放数据是 scale，零点偏移数据是 zeroPoint，则送入模型的推理数据为$g((data /\nscale) + zeroPoint)$，其中： $g(x) = clip(round(x))$，clip为截断函数，例如U8： $g(x)∈[0,\n255]$，S8： $g(x)∈[-128, 127]$。\n\n对于输出 ：若推理结果 data，对应的缩放数据是 scale，零点偏移数据是 zeroPoint，则最终的推理结果为 $(data - zeroPoint)\n* scale$。\n\n其中 scaleLen 由数据 data 按照 per-axis 或 per-tensor（反）量化方式决定。 当数据 data 按 per-tensor\n（反）量化时，scaleLen 等于 1，此时不需要关注 quantizeAxis 数值； 否则 quantizeAxis 表示数据量化轴所在维度索引，\nscaleLen 等于数据 data 第 quantizeAxis 维度大小。 zeroPointLen 与 scaleLen 保持一致。\n\n * 成员\n\n成员名称            描述\nscaleLen        缩放数据的长度。\nscaleData       缩放数据的首地址。\nzeropointLen    零点偏移数据的长度。\nzeropointData   零点偏移数据的首地址。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNQuantiScale","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":519,"title":"hbDNNQuantiType","content":"#\n\n\n\n定点浮点转换的量化/反量化类型。\n\nNONE 代表不需要对数据做任何处理。\n\nSCALE 对应的量化/反量化参数存储在 hbDNNQuantiScale 结构体中。\n\n * 成员\n\n成员名称    描述\nNONE    没有量化。\nSCALE   量化类型为 SCALE。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNQuantiType","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":520,"title":"hbDNNTensor","content":"#\n\n\n\n张量。\n\n用于存放输入输出的信息。\n\n * 成员","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensor","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":521,"title":"hbDNNTensorProperties","content":"#\n\n\n\n张量的信息。\n\n * 成员\n\n注解\n\nvalidShape 字段中若某些维度为 -1，则代表模型的 validShape 在该维度上为动态输入，需要您在创建任务前根据实际输入进行填写。\n\nquantizeAxis 表示量化轴索引所在维度，仅按 per-axis 量化时生效，当数据按照 per-tensor 量化时 quantizeAxis\n为负数。\n\n通过接口获取的张量信息为模型要求的，您可以根据实际输入修改对应的张量信息，目前只允许修改 stride 和 tensorType 的信息，而且必须符合要求。\n\nstride：\n\n 1. 若您根据 stride 准备输入，则无需更改 stride。\n\n 2. 若您根据 validShape 准备输入，则需更改 stride 的值为 validShape\n    各维度跳跃的步长，推理库内部会对数据进行padding操作。\n\n 3. 若从模型获取的 stride 中某些维度为 -1，则代表模型的 stride\n    在该维度上为动态输入，需要您在创建任务前根据实际输入进行填写。填写的约束请参考 动态输入介绍 章节。\n\nalignedByteSize 表示tensor中占据的内存空间大小，非动态场景下一般可用来申请输入输出内存。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensorProperties","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":522,"title":"hbDNNTensorShape","content":"#\n\n\n\n张量的形状。\n\n例如一个张量 numDimensions=4，其数据排布为 1x4x224x224 则 dimensionSize 数组中按顺序存储数据\ndimensionSize[0]=1、 dimensionSize[1]=4、dimensionSize[2]=224、\ndimensionSize[3]=224。\n\n * 成员\n\n成员名称            描述\ndimensionSize   张量每个维度的大小。\nnumDimensions   张量的维度。","routePath":"/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensorShape","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":523,"title":"动态输入介绍","content":"#\n\n当模型输入张量属性 validShape 或 stride 中含有 -1 时，代表该模型的输入是动态的，您需要根据实际输入对动态维度进行填写。\n\nstride 填写时应遵循以下规则：\n\n * 应确保填写数值满足 $stride[idx] >= stride[idx+1] * validShape.dimensionSize[idx+1]$，其中\n   idx 代表当前维度。\n\n * 对于 Y 和 UV 输入的 stride， 由于存在一些硬件约束，需要动态维度保证32对齐，这意味着您需要根据 stride 对输入进行对齐处理。\n\n如模型有三个输入：\n\n * input_y : validShape = [1,-1,-1,1]，stride = [-1,-1,1,1]\n * input_uv : validShape = [1,-1,-1,2]，stride = [-1,-1,2,1]\n * input_roi : validShape = [1,4]，stride = [16,4]\n\n其中 input_y 与 input_uv 为动态输入，且是 Y 和 UV 输入。假设实际输入: input_y 的 validShape =\n[1,220,220,1]， input_uv 的 validShape = [1,110,110,2]。 stride\n计算如下所示，保证动态维度32对齐，其中 ALIGN_32 代表32字节对齐：\n\n * input_y :\n   \n   stride[3] = 1;\n   \n   stride[2] = 1;\n   \n   stride[1] = ALIGN_32(stride[2] * validShape.dimensionSize[2]) = ALIGN_32(1 *\n   220) = 224;\n   \n   stride[0] = ALIGN_32(stride[1] * validShape.dimensionSize[1]) = ALIGN_32(224\n   * 220) = 49280;\n\n * input_uv :\n   \n   stride[3] = 1;\n   \n   stride[2] = 2;\n   \n   stride[1] = ALIGN_32(stride[2] * validShape.dimensionSize[2]) = ALIGN_32(2 *\n   110) = 224;\n   \n   stride[0] = ALIGN_32(stride[1] * validShape.dimensionSize[1]) = ALIGN_32(224\n   * 110) = 24640;\n\n您需要在准备输入时设置属性如下，并且将输入数据按照 stride 进行对齐处理：\n\n * input_y : validShape = [1,220,220,1]，stride = [49280,224,1,1]\n * input_uv : validShape = [1,110,110,2]，stride = [24640,224,2,1]\n * input_roi : validShape = [1,4]，stride = [16,4]","routePath":"/guide/ucp/runtime/bpu_sdk_api/dynamic_input_introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":524,"title":"模型推理","content":"#\n\n\nhbDNNInferV2#\n\n\n\n根据输入参数创建同步/异步推理任务。对于异步任务，调用方可以跨函数、跨线程使用返回的 taskHandle。\n\n * 参数\n   * [out] taskHandle 任务句柄指针。\n   * [in/out] output 推理任务的输出。\n   * [in] input 推理任务的输入。\n   * [in] dnnHandle DNN句柄指针。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n注解\n\n 1. 如果 taskHandle 置为 nullptr，则会自动创建同步任务，接口返回即推理完成。\n\n 2. 如果 *taskHandle 置为 nullptr，则会自动创建异步任务，接口返回的 taskHandle 可用于后续阻塞或回调。\n\n 3. 如果 *taskHandle 非空，并且指向之前已经创建但未提交的任务，则会自动创建新任务并添加进来。\n\n最多支持同时存在32个模型任务。","routePath":"/guide/ucp/runtime/bpu_sdk_api/function_interface/model_inference","lang":"zh","toc":[{"text":"hbDNNInferV2","id":"hbdnninferv2","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":525,"title":"模型信息","content":"#\n\n\nhbDNNGetModelNameList#\n\n\n\n获取 dnnPackedHandle 所指向模型的名称列表和个数。\n\n * 参数\n   * [out] modelNameList 模型名称列表。\n   * [out] modelNameCount 模型名称个数。\n   * [in] dnnPackedHandle Horizon DNN句柄，指向多个模型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetModelHandle#\n\n\n\n从 dnnPackedHandle 所指向模型列表中获取一个模型的句柄。调用方可以跨函数、跨线程使用返回的 dnnHandle。\n\n * 参数\n   * [out] dnnHandle DNN句柄，指向一个模型。\n   * [in] dnnPackedHandle DNN句柄，指向多个模型。\n   * [in] modelName 模型名称。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetInputCount#\n\n\n\n获取 dnnHandle 所指向模型输入张量的个数。\n\n * 参数\n   * [out] inputCount 模型输入张量的个数。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetInputName#\n\n\n\n获取 dnnHandle 所指向模型输入张量的名称。\n\n * 参数\n   * [out] name 模型输入张量的名称。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] inputIndex 模型输入张量的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetInputTensorProperties#\n\n\n\n获取 dnnHandle 所指向模型特定输入张量的属性。\n\n * 参数\n   * [out] properties 输入张量的信息。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] inputIndex 模型输入张量的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetOutputCount#\n\n\n\n获取 dnnHandle 所指向模型输出张量的个数。\n\n * 参数\n   * [out] outputCount 模型输出张量的个数。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetOutputName#\n\n\n\n获取 dnnHandle 所指向模型输出张量的名称。\n\n * 参数\n   * [out] name 模型输出张量的名称。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] outputIndex 模型输出张量的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetOutputTensorProperties#\n\n\n\n获取 dnnHandle 所指向模型特定输出张量的属性。\n\n * 参数\n   * [out] properties 输出张量的信息。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] outputIndex 模型输出张量的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetInputDesc#\n\n\n\n获取 dnnHandle 指向模型特定输入所关联的描述信息。\n\n * 参数\n   * [out] desc 描述信息的首地址。\n   * [out] size 描述信息的大小。\n   * [out] type 描述信息的类型，详细类型请查看 hbDNNDescType。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] inputIndex 模型输入的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetOutputDesc#\n\n\n\n获取 dnnHandle 指向模型特定输出所关联的描述信息。\n\n * 参数\n   * [out] desc 描述信息的首地址。\n   * [out] size 描述信息的大小。\n   * [out] type 描述信息的类型，详细类型请查看 hbDNNDescType。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n   * [in] outputIndex 模型输出的编号。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNGetModelDesc#\n\n\n\n获取 dnnHandle 指向模型所关联的描述信息。\n\n * 参数\n   * [out] desc 描述信息的首地址。\n   * [out] size 描述信息的大小。\n   * [out] type 描述信息的类型，详细类型请查看 hbDNNDescType。\n   * [in] dnnHandle DNN句柄，指向一个模型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/runtime/bpu_sdk_api/function_interface/model_info","lang":"zh","toc":[{"text":"hbDNNGetModelNameList","id":"hbdnngetmodelnamelist","depth":2,"charIndex":3},{"text":"hbDNNGetModelHandle","id":"hbdnngetmodelhandle","depth":2,"charIndex":223},{"text":"hbDNNGetInputCount","id":"hbdnngetinputcount","depth":2,"charIndex":458},{"text":"hbDNNGetInputName","id":"hbdnngetinputname","depth":2,"charIndex":620},{"text":"hbDNNGetInputTensorProperties","id":"hbdnngetinputtensorproperties","depth":2,"charIndex":807},{"text":"hbDNNGetOutputCount","id":"hbdnngetoutputcount","depth":2,"charIndex":1012},{"text":"hbDNNGetOutputName","id":"hbdnngetoutputname","depth":2,"charIndex":1176},{"text":"hbDNNGetOutputTensorProperties","id":"hbdnngetoutputtensorproperties","depth":2,"charIndex":1365},{"text":"hbDNNGetInputDesc","id":"hbdnngetinputdesc","depth":2,"charIndex":1572},{"text":"hbDNNGetOutputDesc","id":"hbdnngetoutputdesc","depth":2,"charIndex":1832},{"text":"hbDNNGetModelDesc","id":"hbdnngetmodeldesc","depth":2,"charIndex":2094}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":526,"title":"模型加载/释放","content":"#\n\n\nhbDNNInitializeFromFiles#\n\n\n\n从文件完成对 dnnPackedHandle 的创建和初始化。调用方可以跨函数、跨线程使用返回的 dnnPackedHandle。\n\n * 参数\n   * [out] dnnPackedHandle Horizon DNN句柄，指向多个模型。\n   * [in] modelFileNames 模型文件的路径。\n   * [in] modelFileCount 模型文件的个数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNInitializeFromDDR#\n\n\n\n从内存完成对 dnnPackedHandle 的创建和初始化。调用方可以跨函数、跨线程使用返回的 dnnPackedHandle。\n\n * 参数\n   * [out] dnnPackedHandle Horizon DNN句柄，指向多个模型。\n   * [in] modelData 模型文件的指针。\n   * [in] modelDataLengths 模型数据的长度。\n   * [in] modelDataCount 模型数据的个数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\nhbDNNRelease#\n\n\n\n将 dnnPackedHandle 所指向的模型释放。\n\n * 参数\n   * [in] dnnPackedHandle Horizon DNN句柄，指向多个模型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/runtime/bpu_sdk_api/function_interface/model_loading_releasing","lang":"zh","toc":[{"text":"hbDNNInitializeFromFiles","id":"hbdnninitializefromfiles","depth":2,"charIndex":3},{"text":"hbDNNInitializeFromDDR","id":"hbdnninitializefromddr","depth":2,"charIndex":260},{"text":"hbDNNRelease","id":"hbdnnrelease","depth":2,"charIndex":546}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":527,"title":"状态码","content":"#\n\n\nhbDNNGetErrorDesc#\n\n\n\n将错误码翻译成自然语言。\n\n * 参数\n   * [in] errorCode dnn错误码。\n * 返回值\n   * 返回 char * ，将内部错误码翻译成自然语言。","routePath":"/guide/ucp/runtime/bpu_sdk_api/function_interface/status_code","lang":"zh","toc":[{"text":"hbDNNGetErrorDesc","id":"hbdnngeterrordesc","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":528,"title":"版本信息","content":"#\n\n\nhbDNNGetVersion#\n\n\n\n获取 DNN 预测库版本信息。\n\n * 返回值\n   * 返回版本信息。","routePath":"/guide/ucp/runtime/bpu_sdk_api/function_interface/version_info","lang":"zh","toc":[{"text":"hbDNNGetVersion","id":"hbdnngetversion","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":529,"title":"模型推理应用开发指导","content":"#\n\n\n概述#\n\n本章节介绍如何在地平线平台进行模型推理应用开发，以及需要您注意的相关事项。\n\n注意\n\n在开始开发应用前，请确保您已经根据 环境部署 章节的内容完成了开发环境准备。\n\n最简易的开发过程包括工程创建、工程实现、工程编译与运行三个阶段。 考虑到实际业务场景开发的较复杂需求，对于常用的多模型控制概念和应用调优建议也都提供了一些说明。\n\n\n工程创建 #\n\n地平线推荐使用cmake进行应用工程管理，前文介绍的环境部署部分也已经完成了cmake安装。 在阅读本节内容前，我们希望您已经了解cmake的使用。\n\n地平线开发库提供了相关的工程依赖。具体依赖信息如下：\n\n * 地平线部署依赖库libdnn.so，libucp.so等，路径：${OE_DIR}/samples/ucp_tutorial/deps_aarch64/u\n   cp/。\n * C编译器 aarch64-none-linux-gnu-gcc。\n * C++编译器 aarch64-none-linux-gnu-g++。\n\n注解\n\n上方$ 指地平线提供的OE包路径。\n\n创建一个工程，您需要编写 CMakeLists.txt 文件，CMakeLists.txt 文件中定义了一些编译选项，以及依赖库、头文件的路径。参考如下：\n\n\n\n注意在以上示例中，我们没有指定编译器位置，会在工程编译阶段补充编译器指定，请参考 工程编译与运行 小节部分的介绍。\n\n\n工程实现#\n\n工程实现部分，我们主要为您介绍如何将hbm模型在地平线平台运行起来。\n最简单的步骤应该包括模型加载、准备输入数据、准备输出内存、推理和结果解析，以下是一份简单的模型部署参考代码：\n\n\n\n示例代码中，为了缩减篇幅，模型部分处理以注释形式描述，更详细的细节后续文档有说明，比如 动态输入说明请参考 动态输入说明， 内存对齐规则请参考 对齐规则 章节。\n更加全面的工程实现指导请阅读 模型推理API手册 和 模型推理基础示例包使用说明 。\n\n\n工程编译和运行 #\n\n结合 工程创建 一节中的cmake工程配置，参考如下编译脚本：\n\n\n\n根据 环境部署 部分的指引，您的开发机中应该已经安装有相应编译器，将上述脚本中的编译器配置指定为您的安装项目即可。\n\narm程序拷贝到地平线开发板上可运行，注意程序依赖的文件也需要一同拷贝到开发板，并在启动脚本中配置依赖。 例如我们的示例程序依赖库有：\nlibucp.so、libdnn.so 以及其他bsp库，这些依赖库都可在OE包的 ucp_tutorial/deps_aarch64/\n路径下找到，需要上传到板子的运行环境中。 我们建议您在板端的 /userdata 路径下新建 lib\n路径并将库传送至该目录下，则在板端运行程序前，需指定的依赖库路径信息如下：\n\n\n\n\n多模型控制策略#\n\n多模型场景中，每个模型都需要使用有限的计算资源完成推理，不可避免地会出现计算资源的争夺情况。\n为了便于您控制多模型的执行，地平线提供了模型优先级的控制策略供您使用。\n\n\n模型优先级控制 #\n\n注意\n\n请注意，此功能仅支持在开发板端实现，x86模拟器不支持此功能。\n\nJ6计算平台BPU计算单元硬件本身没有任务抢占功能，对于每一个推理任务，一旦它进到BPU模型计算之后，在该任务执行完成之前都会一直占用BPU，其他任务只能排队等\n待。 此时很容易出现BPU计算资源被一个大模型推理任务所独占，进而影响其他高优先级模型的推理任务执行。 针对这种问题，Runtime\nSDK基于模型的优先级通过软件的方式实现了BPU资源抢占的功能。\n\n其中有以下点需要被关注：\n\n * 编译后的数据指令模型在BPU上进行推理计算时，它将表现为1个或者多个function-call的调用，其中function-call是BPU的执行粒度，多\n   个function-call调用任务将在BPU的硬件队列上按序进行调度，当一个模型所有的function-call都执行完成，那么一个模型推理任务也就执行\n   完成了。\n * 基于上述描述，BPU模型任务抢占粒度设计为function-call更为简单，即BPU执行完一个function-call之后，暂时挂起当前模型，然后切入\n   执行另外一个模型，当新模型执行完成之后，再恢复原来模型的状态继续运行。\n   但是这里存在两个问题，第一是经过编译器编译出来的模型function-call都是merge在一起，此时模型只有一个大的function-call，它无法\n   被抢占；第二是每个function-call的执行时间比较长或者不固定，也会造成抢占时机不固定，影响抢占效果。\n\n为了解决上述的两个问题，地平线在模型编译和系统软件层面都给予了支持，下面分别介绍其实现原理和操作方法：\n\n * 首先，如果您选择使用QAT方案处理模型，则在 模型编译 阶段，您需要在编译接口中的额外参数配置中添加 max_time_per_fc\n   选项，用于设置每个function call的执行时间（以微秒为单位），其默认取值为 0 （即不做限制）。\n   您可以自行设置这个选项控制上板运行阶段个别大function-call的执行时间。假设某function-call执行时间为10ms，当模型编译时 将\n   max_time_per_fc 设置为 500，则这个function-call将会被拆分成20个。 而如果您使用PTQ方案处理模型，则在 模型转换\n   阶段，可以在模型的YAML配置文件中的编译器相关参数( compiler_parameters )中，添加 max_time_per_fc 参数。\n * 其次，需要在推理任务提交时设置 hbUCPSchedParam.priority 参数。按照优先级还可以支持高优抢占嵌套能力。 如：配置 infer\n   任务优先级小于 254，则为普通任务，不可抢占其他任务。 配置 infer 任务优先级等于 254，则为high抢占任务，可支持抢占普通任务。 配置\n   infer 任务优先级等于 HB_DNN_PRIORITY_PREEMP(255)，则为urgent抢占任务，可抢占普通任务和high抢占任务。\n\n\n应用调优建议#\n\n地平线建议的应用调优策略包括工程任务调度和算法任务整合两个方面。\n\n工程任务调度\n方面，我们推荐您使用一些workflow调度管理工具，充分发挥不同任务阶段的并行能力。一般应用可以简单拆分为输入前处理、模型推理、输出后处理三个阶段，在简易流程\n下，其处理流程如下图。\n\n充分利用workflow管理实现不同任务阶段并行后，理想的任务处理流程将达到下图效果。\n\n算法任务整合 方面，地平线推荐您使用多任务模型。\n这样一方面可以在一定程度上避免多模型调度管理的困难；另一方面多任务模型也能充分共享主干网络的计算量，较于使用各个独立的模型，可以在整个应用级别明显减少计算量，从\n而达到更高的整体性能。 在地平线内部和许多合作客户的业务实践中，多任务也是常见的应用级优化策略。\n\n\n其他应用开发工具#\n\nhrt_model_exec 是一个模型执行工具，可直接在开发板上评测模型的推理性能、获取模型信息。\n一方面可以让您在拿到模型时实际了解模型真实性能，另一方面也可以帮助您了解模型可以做到的速度极限，对于应用调优的目标极限具有指导意义。\nhrt_model_exec 工具分别提供了查看模型信息 model_info、模型推理 infer 和模型性能分析 perf 功能，工具使用方法请参考\nhrt_model_exec工具介绍 。\n\nUCP还提供了性能分析工具协助您定位应用程序的性能瓶颈，其中 UCP Trace 用于分析应用程序pipline调度的能力，hrt_ucp_monitor\n用于监控监控硬件后端的占用率。 工具使用方法请分别参考 UCP Trace 使用说明 和 hrt_ucp_monitor 工具介绍 章节。","routePath":"/guide/ucp/runtime/runtime_dev","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"工程创建","id":"工程创建","depth":2,"charIndex":-1},{"text":"工程实现","id":"工程实现","depth":2,"charIndex":614},{"text":"工程编译和运行","id":"工程编译和运行","depth":2,"charIndex":-1},{"text":"多模型控制策略","id":"多模型控制策略","depth":2,"charIndex":1175},{"text":"模型优先级控制","id":"模型优先级控制","depth":3,"charIndex":-1},{"text":"应用调优建议","id":"应用调优建议","depth":2,"charIndex":2593},{"text":"其他应用开发工具","id":"其他应用开发工具","depth":2,"charIndex":2939}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":530,"title":"常用操作","content":"#\n\n\n查看开发板镜像版本#\n\n使用 uname -a 命令可以查看到系统的版本，执行命令后如下所示，\n\n\n\n注解\n * SMP 代表该系统支持对称多处理（Symmetrical Multi-Processing）。\n * PREEMPT 代表系统支持抢占式内核。\n * Oct 23 10:47:39 CST 2020 代表系统镜像发布时间。\n * aarch64 代表系统支持平台为aarch64平台。\n\n\n查看系统日志#\n\n使用 dmesg 命令可以查看系统日志，如下所示：\n\n\n\n在上板运行程序的时候，假如发生系统错误（比如程序被killed或者mem分配失败等），执行 dmesg 后可以看到系统发生错误的具体原因。\n\n\n查看BPU使用率#\n\n使用 hrut_somstatus 命令可以查看当前开发板的BPU使用率，执行命令后如下所示：\n\n","routePath":"/guide/ucp/runtime/tool_introduction/auxiliary_tool","lang":"zh","toc":[{"text":"查看开发板镜像版本","id":"查看开发板镜像版本","depth":2,"charIndex":3},{"text":"查看系统日志","id":"查看系统日志","depth":2,"charIndex":205},{"text":"查看BPU使用率","id":"查看bpu使用率","depth":2,"charIndex":315}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":531,"title":"hrt_model_exec工具介绍","content":"#\n\n\n工具简介#\n\nhrt_model_exec 是一个模型执行工具，可直接在开发板上评测模型的推理性能、获取模型信息。 一方面可以让您拿到模型时实际了解模型真实性能；\n另一方面也可以帮助您了解模型可以做到的速度极限，对于应用调优的目标极限具有指导意义。\n\nhrt_model_exec 工具源码位于 horizon_j6_open_explorer 发布物的\nsamples/ucp_tutorial/tools/hrt_model_exec 路径下。结构如下所示：\n\n\n\nhrt_model_exec 工具分别提供了模型推理 infer、模型性能分析 perf 和查看模型信息 model_info 三类功能，如下表：\n\n编号   子命令          说明\n1    model_info   获取模型信息，如：模型的输入输出信息等。\n2    infer        执行模型推理，获取模型推理结果。\n3    perf         执行模型性能分析，获取性能分析结果。\n\n工具可以通过 -v 或者 --version 命令，查看工具的 dnn 预测库版本号。\n\n\n\n\n参数描述#\n\n参数                        类型       说明\nmodel_file                string   模型文件路径，多个路径可通过逗号分隔。\nmodel_name                string   指定模型中某个模型的名称。\ncore_id                   int      指定运行核。0：任意核，1：core0；默认为 0。\ninput_file                string   模型输入信息。 输入后缀必须为 PNG/JPG/JPEG/png/jpg/jpeg/bin/txt\n                                   中的一种。每个输入之间需要用英文字符的逗号隔开,，如：xxx.jpg,input.txt。\ninput_img_properties      string   模型图像输入的色彩空间信息。 input_file 中每一个图片类型输入都需要指定一个 Y/UV\n                                   类型，每个输入色彩空间之间需要用英文字符的逗号隔开,，如：Y,UV。\ninput_valid_shape         string   模型动态 validShape 输入信息。 若模型输入属性 validShape 中含有 -1，则需要将 -1\n                                   的部分进行补全，多个 validShape\n                                   间通过英文分号间隔。如：--input_valid_shape=\"1,376,376,1;1,188,188,2\"\ninput_stride              string   模型动态 stride 输入信息。 若模型输入属性 stride 中含有 -1，则需要将 -1 的部分进行补全，多个\n                                   stride\n                                   间通过英文分号间隔。如：--input_stride=\"144384,384,1,1;72192,384,2,1\"\nframe_count               int      执行模型运行帧数。\ndump_intermediate         string   dump模型每一层输入和输出。\nenable_dump               bool     使能dump模型输入和输出，默认为 false。\ndump_precision            int      控制txt格式输出float型数据的小数点位数，默认为 9。\ndequantize_process        bool     对模型输出进行反量化处理，在 enable_dump 为 true 时生效，默认为 false。\nremove_padding_process    bool     对模型输出进行去padding处理，在 enable_dump 为 true 时生效，默认为 false。\ndump_format               string   dump模型输入和输出的格式。\ndump_txt_axis             int      控制txt格式输入输出的换行规则。\nenable_cls_post_process   bool     使能分类后处理，默认为 false。 子命令为 infer 时配合使用，目前只支持ptq分类模型的后处理，打印分类结果。\nperf_time                 int      执行模型运行时间。\nthread_num                int      线程数(并行度)，数值可以表示最多有多少个任务在并行处理。测试延时，数值需要设置为1，没有资源抢占发生，延时测试更准确。\n                                   测试吞吐，建议设置>2 (BPU核心个数)，调整线程数使BPU利用率尽量高，吞吐测试更准确。\nprofile_path              string   统计工具日志产生路径，运行产生profiler.log和profiler.csv，分析op耗时和调度耗时。一般设置\n                                   --profile_path=\".\" 即可，代表在当前目录下生成日志文件。\ndump_path                 string   dump模型输入输出的路径，设置 enable_dump 或 dump_intermediate 时生效。\n\n设置profile_path参数且工具正常运行后会产生profiler.log和profiler.csv文件，文件中包括如下参数：\n\n * perf_result：记录perf结果。\n\n参数                说明\nFPS               每秒处理的帧数。\naverage_latency   指定模型中某个模型平均一帧运行所花费的时间。\n\n * running_condition：运行环境信息。\n\n参数            说明\ncore_id       程序运行设置的bpu核。\nframe_count   程序运行的总帧数。\nmodel_name    评测模型的名字。\nrun_time      程序运行时间。\nthread_num    程序运行的线程数。\n\n * model_latency：模型节点耗时统计。\n\n参数                               说明\nNode-pad                         模型输入padding耗时。\nNode-NodeIdx-NodeType-NodeName   模型节点耗时信息。注：NodeIdx为模型节点拓扑排序的序号，NodeType为具体的节点类型，如Dequantize，\n                                 NodeName为具体的节点名称。\n\n * processor_latency：模型处理器耗时统计。\n\n参数                        说明\nBPU_inference_time_cost   每帧推理BPU处理器耗时。\nCPU_inference_time_cost   每帧推理CPU处理器耗时。\n\n * task_latency：模型任务耗时统计。\n\n参数                说明\nTaskRunningTime   任务实际运行耗时，耗时时间包括UCP框架耗时。\n\n\n使用说明#\n\n工具提供三类功能：模型信息获取、单帧推理功能、多帧性能评测。\n\n运行 hrt_model_exec、hrt_model_exec -h 或 hrt_model_exec --help 获取工具使用详情。如下所示：\n\n\n\n\nmodel_info#\n\n概述#\n\n该参数用于获取模型信息，模型支持范围：qat模型，ptq模型。该参数与 model_file 一起使用，用于获取模型的详细信息，信息包括模型输入输出信息\nhbDNNTensorProperties。\n\n不指定 model_name 输出模型中所有模型信息，指定 model_name 则只输出对应模型的信息。\n\n示例#\n\n 1. 单模型\n\n\n\n 2. 多模型（输出所有模型信息）\n\n\n\n 3. 多模型--pack模型（输出指定模型信息）\n\n\n\n\ninfer#\n\n概述#\n\n该参数用于模型推理，用户自定义输入图片，推理一帧。该参数需要与 input_file\n一起使用，指定输入图片路径，工具根据模型信息resize图片，整理模型输入信息。\n\n程序单线程运行单帧数据，输出模型运行的时间。\n\n示例#\n\n 1. 单模型\n\n\n\n 2. 多模型\n\n\n\n可选参数#\n\n参数                        说明\ncore_id                   指定模型推理的核id。\ninput_img_properties      模型图像输入的色彩空间信息。\ninput_valid_shape         模型动态 validShape 输入信息。\ninput_stride              模型动态 stride 输入信息。\nframe_count               设置 infer 运行帧数，单帧重复推理，可与 enable_dump 并用，验证输出一致性，默认为 1。\ndump_intermediate         dump模型每一层输入数据和输出数据，默认值 0。\nenable_dump               dump模型输入和输出数据，默认为 false。\ndump_precision            控制txt格式输出float型数据的小数点位数，默认为 9。\ndequantize_process        对模型输出进行反量化处理，在 enable_dump 为 true 时生效，默认为 false。\nremove_padding_process    对模型输出进行去padding处理，在 enable_dump 为 true 时生效，默认为 false。\ndump_format               dump模型输出文件的类型，可选参数为 bin 或 txt，默认为 bin。\ndump_txt_axis             dump模型txt格式输出的换行规则；若输出维度为n，则参数范围为[0, n]， 默认为 -1，一行一个数据。\nenable_cls_post_process   使能分类后处理，目前只支持ptq分类模型，默认 false。\ndump_path                 dump模型输入输出的路径，设置 enable_dump 或 dump_intermediate 时生效。\n\n\nperf#\n\n概述#\n\n该参数用于测试模型性能。 该模式下，您无需输入数据，程序根据模型信息自动构造输入tensor，tensor数据为随机数。\n程序默认单线程运行200帧数据，当指定perf_time参数时，frame_count参数失效，程序会执行指定时间后退出。\n输出模型运行的latency、以及帧率信息。程序每200帧打印一次性能信息： latency的最大、最小、平均值，不足200帧程序运行结束打印一次。\n\n程序最后输出running相关数据， 包括：程序线程数、帧数、模型推理总时间，模型推理平均latency，帧率信息。\n\n示例#\n\n 1. 单模型\n\n\n\n 2. 多模型\n\n\n\n可选参数#\n\n参数                     说明\ncore_id                指定模型推理的核id。\ninput_file             模型输入信息，多个可通过逗号分隔。\ninput_img_properties   模型图像输入的色彩空间信息。\ninput_valid_shape      模型动态 validShape 输入信息。\ninput_stride           模型动态 stride 输入信息。\nframe_count            设置 perf 运行帧数，当perf_time为0时生效，默认为 200。\ndump_intermediate      dump模型每一层输入数据和输出数据，默认值 0。\nperf_time              设置 perf 运行时间，单位：分钟，默认为 0。\nthread_num             设置程序运行线程数，范围[1, 8], 默认为 1, 设置大于8时按照8个线程处理。\nprofile_path           统计工具日志产生路径，运行产生profiler.log和profiler.csv，分析op耗时和调度耗时。\n\n\n多输入模型说明#\n\n工具 infer 推理功能支持多输入模型的推理，支持图片输入、二进制文件输入以及文本文件输入，输入数据用逗号隔开。 模型的输入信息可以通过 model_info\n进行查看。\n\n示例:\n\n\n\n\n动态输入说明#\n\n若模型的输入是动态的，您需要根据输入实际情况使用 input_valid_shape 和 input_stride\n参数来补全动态信息。您可以选择以下两种方式指定参数：\n\n * 只给定动态输入的 validShape 或 stride 信息。\n\n * 给定所有输入的 validShape 或 stride 信息，非动态输入的信息必须与模型信息保持一致。\n\n注解\n\n工具内部会对于动态输入的信息会进行最大限度的自动补全，方便您更简单的进行性能评测，您可以根据实际情况选择是否交由工具内部进行自动补全。\n\n * 若输入 stride 为动态的，validShape 为固定的，您可以不指定 input_stride\n   参数，工具会自动按照最小对齐规则进行补齐并打印对齐信息。\n\n * 若输入 validShape 和 stride 都是动态的：\n   \n   * 若指定输入为图片类型，您需要指定图片的色彩空间信息 input_img_properties，其余参数可不指定，工具内部按照图片的大小补全\n     input_valid_shape 和 input_stride 信息。\n   \n   * 若指定输入为图片之外的类型或不指定输入，您需要设置 input_valid_shape 信息，内部会自动补全 input_stride 信息并打印。\n\n以 动态输入介绍 章节中的模型为例，您可以通过以下命令运行模型：\n\n\n\n\n图像类型输入说明#\n\n当 input_file 给定图像输入时，需要使用 input_img_properties 参数来指定您想使用该图片的哪个色彩空间当做模型的输入，目前只支持\nY 和 UV 两种色彩空间。\n\n\n\n\n工具运行说明#\n\n构建#\n\n在 ucp_tutorial/tools/hrt_model_exec 目录下有预先配置好的编译脚本 build.sh，选项 -a x86 和 -a\naarch64 分别支持两种编译方式，您可以使用该脚本并指定编译选项进行编译。 此外，目录中也包含了 build_aarch64.sh 和\nbuild_x86.sh 两个编译脚本，分别对应了两个编译选项，使用这两个脚本进行编译与使用 build.sh 脚本并指定编译选项等效。\n\n\n\n运行#\n\n构建板端运行工具后，将生成 output_shared_J6_aarch64 文件夹。 您可以将文件夹复制到板端环境中并执行\noutput_shared_J6_aarch64/script/run_hrt_model_exec.sh 来使用该工具。\n\n构建x86端运行工具后，将生成 output_shared_J6_x86 文件夹。 您可以在x86端直接执行\noutput_shared_J6_x86/script_x86/run_hrt_model_exec.sh 来使用该工具。\n\nrun_hrt_model_exec.sh 脚本分为设置环境变量和获取模型信息并推理模型两部分。\n\n\n\n注解\n\n运行前需要修改 run_hrt_model_exec.sh 相应参数，以确保模型和输入文件正确。您还可以灵活使用其他参数来使用更多功能。\n\n\n常见问题#\n\n\nLatency、FPS数据是如何统计的？#\n\nLatency是指单流程推理模型所耗费的平均时间，重在表示在资源充足的情况下推理一帧的平均耗时，体现在上板运行是单核单线程统计；统计方法伪代码如下：\n\n\n\nFPS是指多流程同时进行模型推理平均一秒推理的帧数，重在表示充分使用资源情况下模型的吞吐，体现在上板运行为双核多线程；统计方法是同时起多个线程进行模型推理，计算\n平均1s推理的总帧数。\n\n\n通过Latency推算FPS与工具测出的FPS为什么不一致？#\n\nLatency与FPS的统计情景不同，Latency为单流程（单核单线程）推理，FPS为多流程（多核多线程）推理，因此推算不一致；若统计FPS时将流程（线程）数\n量设置为 1 ，则通过Latency推算FPS和测出的一致。","routePath":"/guide/ucp/runtime/tool_introduction/hrt_model_exec","lang":"zh","toc":[{"text":"工具简介","id":"工具简介","depth":2,"charIndex":3},{"text":"参数描述","id":"参数描述","depth":2,"charIndex":494},{"text":"使用说明","id":"使用说明","depth":2,"charIndex":3413},{"text":"model_info","id":"model_info","depth":3,"charIndex":3531},{"text":"概述","id":"概述","depth":4,"charIndex":3544},{"text":"示例","id":"示例","depth":4,"charIndex":3705},{"text":"infer","id":"infer","depth":3,"charIndex":3772},{"text":"概述","id":"概述-1","depth":4,"charIndex":3780},{"text":"示例","id":"示例-1","depth":4,"charIndex":3894},{"text":"可选参数","id":"可选参数","depth":4,"charIndex":3921},{"text":"perf","id":"perf","depth":3,"charIndex":4809},{"text":"概述","id":"概述-2","depth":4,"charIndex":4816},{"text":"示例","id":"示例-2","depth":4,"charIndex":5080},{"text":"可选参数","id":"可选参数-1","depth":4,"charIndex":5107},{"text":"多输入模型说明","id":"多输入模型说明","depth":3,"charIndex":5644},{"text":"动态输入说明","id":"动态输入说明","depth":3,"charIndex":5750},{"text":"图像类型输入说明","id":"图像类型输入说明","depth":3,"charIndex":6376},{"text":"工具运行说明","id":"工具运行说明","depth":3,"charIndex":6486},{"text":"构建","id":"构建","depth":4,"charIndex":6495},{"text":"运行","id":"运行","depth":4,"charIndex":6717},{"text":"常见问题","id":"常见问题","depth":2,"charIndex":7093},{"text":"Latency、FPS数据是如何统计的？","id":"latencyfps数据是如何统计的","depth":3,"charIndex":7101},{"text":"通过Latency推算FPS与工具测出的FPS为什么不一致？","id":"通过latency推算fps与工具测出的fps为什么不一致","depth":3,"charIndex":7297}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":532,"title":"HB_UCP_ALL_BACKENDS","content":"#\n\n\n\n定义宏，指定所有backend。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucp_all_backends","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":533,"title":"HB_UCP_INITIALIZE_SCHED_PARAM","content":"#\n\n\n\n初始化控制参数。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucp_initialize_sched_param","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":534,"title":"hbUCPBackend","content":"#\n\n\n\n任务执行硬件，可组合使用，例如：HB_UCP_DSP_CORE_0 | HB_UCP_GDC_CORE_0 表示当前任务既可以使用 DSP\n0核，也可以使用GDC 0核，调度交由UCP根据负载自行决策。\n\n注意\n\nHB_UCP_CORE_ANY需单独使用，不能与其它backend进行或运算。\n\n * 成员\n   \n   成员名称                      描述\n   HB_UCP_CORE_ANY           Soc上任意可执行的硬件。\n   HB_UCP_BPU_CORE_0         BPU核0。\n   HB_UCP_BPU_CORE_1         BPU核1。\n   HB_UCP_BPU_CORE_2         BPU核2。\n   HB_UCP_BPU_CORE_3         BPU核3。\n   HB_UCP_BPU_CORE_ANY       任意的BPU核。\n   HB_UCP_DSP_CORE_0         DSP核0。\n   HB_UCP_DSP_CORE_1         DSP核1。\n   HB_UCP_DSP_CORE_ANY       任意的DSP核。\n   HB_UCP_GDC_CORE_0         GDC核0。\n   HB_UCP_GDC_CORE_ANY       任意的GDC核。\n   HB_UCP_STITCH_CORE_0      STITCH核0。\n   HB_UCP_LKOF_CORE_0        LKOF核0。\n   HB_UCP_JPU_CORE_0         JPU核0。\n   HB_UCP_JPU_CORE_1         JPU核1。\n   HB_UCP_JPU_CORE_2         JPU核2。\n   HB_UCP_JPU_CORE_ANY       任意的JPU核。\n   HB_UCP_VPU_CORE_0         VPU（Video Processing Unit）核0。\n   HB_UCP_VPU_CORE_ANY       任意的VPU（Video Processing Unit）核。\n   HB_UCP_PYRAMID_CORE_0     PYRAMID核0。\n   HB_UCP_PYRAMID_CORE_1     PYRAMID核1。\n   HB_UCP_PYRAMID_CORE_2     PYRAMID核2。\n   HB_UCP_PYRAMID_CORE_ANY   任意的PYRAMID核。\n   HB_UCP_ISP_CORE_0         ISP核0。\n   HB_UCP_ISP_CORE_1         ISP核1。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpbackend","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":535,"title":"hbUCPSchedParam","content":"#\n\n\n\n控制参数结构体。\n\n * 成员\n   \n   成员名称       描述\n   priority   任务优先级。\n   customId   自定义优先级，例如：时间戳、frame id等，数值越小优先级越高。优先级：priority > customId。\n   backend    任务执行硬件，例如：HB_UCP_BPU_CORE_0。\n   deviceId   设备ID。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpschedparam","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":536,"title":"hbUCPSysMem","content":"#\n\n\n\n系统内存结构体。\n\n * 成员\n   \n   成员名称      描述\n   phyAddr   物理地址。\n   virAddr   虚拟地址。\n   memSize   内存大小。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpsysmem","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":537,"title":"hbUCPSysMemFlushFlag","content":"#\n\n\n\n系统内存与缓存同步参数。\n\n * 成员\n   \n   成员名称                          描述\n   HB_SYS_MEM_CACHE_INVALIDATE   将内存同步到缓存中，CPU读前使用。\n   HB_SYS_MEM_CACHE_CLEAN        将缓存数据同步到内存中，CPU写后使用。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpsysmemflushflag","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":538,"title":"hbUCPTaskDoneCb","content":"#\n\n\n\n用户自定义任务完成后需要执行的回调函数。\n\n * 参数\n   \n   * [in] taskHandle 任务句柄指针。\n   \n   * [in] status 任务返回的状态码。\n   \n   * [in] userdata 用户自定义的数据。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskdonecb","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":539,"title":"hbUCPTaskHandle_t","content":"#\n\n\n\nUCP任务句柄，指向已创建的任务。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskhandle_t","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":540,"title":"hbUCPTaskPriority","content":"#\n\n\n\nTask优先级配置，提供默认参数。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskpriority","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":541,"title":"版本信息类型","content":"#\n\n注解\n\n注意，本小节中的版本信息类型的版本号随版本变化有所不同，此处的版本号仅供参考，实际版本请以您获取到的发布物为准。\n\n\nHB_UCP_VERSION_MAJOR #\n\n\n\nUCP主版本号信息。\n\n\nHB_UCP_VERSION_MINOR #\n\n\n\nUCP次版本号信息。\n\n\nHB_UCP_VERSION_PATCH #\n\n\n\nUCP补丁版本号信息。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_data_structure/version_information","lang":"zh","toc":[{"text":"HB_UCP_VERSION_MAJOR","id":"hb_ucp_version_major","depth":2,"charIndex":-1},{"text":"HB_UCP_VERSION_MINOR","id":"hb_ucp_version_minor","depth":2,"charIndex":-1},{"text":"HB_UCP_VERSION_PATCH","id":"hb_ucp_version_patch","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":542,"title":"hbUCPFree","content":"#\n\n\n\n释放系统内存。\n\n * 参数\n   \n   * [in] mem 内存指针。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpfree","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":543,"title":"hbUCPGetErrorDesc","content":"#\n\n\n\n获取错误码的自然语言描述。\n\n * 参数\n   \n   * [in] errorCode UCP中的错误码。\n\n * 返回值\n   \n   * 返回错误码的描述信息。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgeterrordesc","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":544,"title":"hbUCPGetSocName","content":"#\n\n\n\n获取soc名称。\n\n * 返回值\n   \n   * 返回soc名称。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgetsocname","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":545,"title":"hbUCPGetVersion","content":"#\n\n\n\n获取ucp版本号。\n\n * 返回值\n   \n   * 返回版本号。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgetversion","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":546,"title":"hbUCPMalloc","content":"#\n\n\n\n申请系统内存。\n\n * 参数\n   \n   * [out] mem 内存指针。\n   * [in] size 申请内存的大小。\n   * [in] deviceId 保留参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmalloc","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":547,"title":"hbUCPMallocCached","content":"#\n\n\n\n申请缓存的系统内存。\n\n * 参数\n   \n   * [out] mem 内存指针。\n   * [in] size 申请内存的大小。\n   * [in] deviceId 保留参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmalloccached","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":548,"title":"hbUCPMemFlush","content":"#\n\n\n\n对缓存的系统内存进行刷新。\n\n * 参数\n   \n   * [in] mem 内存指针。\n   * [in] flag 刷新标志符。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmemflush","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":549,"title":"hbUCPReleaseTask","content":"#\n\n\n\n释放UCP任务资源。\n\n * 参数\n   \n   * [in] taskHandle 任务句柄。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpreleasetask","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":550,"title":"hbUCPSetTaskDoneCb","content":"#\n\n\n\n注册一个回调函数，这个回调函数在任务完成后自动执行。\n\n * 参数\n   \n   * [in] taskHandle 任务句柄指针。\n   * [in] taskDoneCb 回调函数指针。\n   * [in] userdata 用户自定义的数据。\n\n * 返回值\n   \n   * 返回 0 则表示回调函数注册成功，否则注册失败。\n\n注解\n\n该接口可以注册一个回调函数，任务执行完成后会调用这个回调函数去执行用户自定义的功能。如果不需要自定义输入，可以将userdata设为nullptr。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpsettaskdonecb","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":551,"title":"hbUCPSubmitTask","content":"#\n\n\n\n提交UCP任务至调度器。\n\n * 参数\n   \n   * [in] taskHandle 任务句柄指针。\n   * [in] schedParam 任务调度参数。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpsubmittask","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":552,"title":"hbUCPWaitTaskDone","content":"#\n\n\n\n等待任务完成或超时。\n\n * 参数\n   \n   * [in] taskHandle 任务句柄指针。\n   * [in] timeout 超时配置（单位：毫秒）。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。\n\n注解\n 1. timeout > 0 表示等待时间。\n 2. timeout <= 0 表示一直等待，直到任务完成。","routePath":"/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpwaittaskdone","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":553,"title":"UCP通用API概览","content":"#\n\n\n数据结构#\n\n\n功能接口#","routePath":"/guide/ucp/ucp_api_reference/ucp_api_overview","lang":"zh","toc":[{"text":"数据结构","id":"数据结构","depth":2,"charIndex":3},{"text":"功能接口","id":"功能接口","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":554,"title":"环境变量","content":"#\n\n","routePath":"/guide/ucp/ucp_api_reference/ucp_environment_variable","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":555,"title":"错误码","content":"#\n\n注解\n\n错误码共有7位，前两位表示错误类型，后5位表示具体错误描述。\n\n","routePath":"/guide/ucp/ucp_faq_errorcode/ucp_error_code","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":556,"title":"常见问题","content":"#\n\n\n通用问题#\n\n\n任务创建或提交后返回HB_UCP_INVALID_ARGUMENT错误码是什么原因？#\n\n可根据UCP的错误日志来判断可能的问题，可能存在如下情况：\n\n * 算子约束问题：大部分加速算子在创建时应满足使用约束，否则会返回错误码。\n * 如果遇到日志打印 op $1 of task has no proper backend, user expect\n   $2，表示没有合适的后端可执行；其中$1表示任务的类型，$2是任务提交时的backend参数，以二进制形式打印，需要按照J6系列每种backend可支持的\n   core数量进行配置。\n\n\n如何理解hbUCPSysMem的物理地址和虚拟地址？#\n\n在J6计算平台架构中，所有硬件的DDR内存共享，通过 hbUCPMallocCached 和 hbUCPMalloc\n接口可以申请到一段物理空间连续的内存，其函数返回值被包装在 hbUCPSysMem 数据结构体中，phyAddr 和 virAddr\n两个字段分别对应其内存空间的物理地址和虚拟地址，虚拟地址可直接被CPU访问，物理地址不可访问。\n\n\n如何理解cacheable和非cacheable的hbmem？#\n\nucp的内存管理接口提供了hbUCPMallocCached 和 hbUCPMalloc\n来分配DDR读写内存，这种内存都是物理地址连续，可被bpu/dsp等ip访问使用的，其中\nhbUCPMallocCached表示分配cacheable属性的内存，并配套了 hbUCPMemFlush 函数来对Cache进行刷新。\nCache机制是由计算平台的内存架构来决定的，可参考如下图所示。CPU与主存之间存在的Cache会缓存数据，而BPU/DSP/JPU/VPU(Video\nProcessing\nUnit)/PYRAMID/STITCH/GDC等其他后端硬件与主存之间则没有cache。此时若错误使用Cache将会直接影响最终数据读写的准确性和效率。\n\n * 当CPU写完数据后，需要主动将Cache中的数据flush到memory中，否则其他硬件访问同一块内存空间时可能会读取到之前的旧数据。\n * 而当其他后端硬件写完数据后，CPU在访问之前也需要主动将Cache中的数据invalidate掉，否则CPU可能会优先读取到之前缓存在cache中的旧数据\n   。\n * 在模型连续推理过程中，需要cpu读的，比如模型输出，建议申请带cacheable的内存，以加速CPU反复读写的效率，而不需要读的，只写的，比如模型输入，可\n   以申请非cacheable的内存。\n\n\n模型推理#\n\n\n导致模型推理hbUCPWaitTaskDone接口timeout超时的原因可能有哪些？#\n\n * 模型本身执行的时间较长，而异步等待接口设置的超时时间不足，或者当前计算资源负载较高导致任务排队时间较长，可能引发接口超时。\n * 存在内存泄露情况。在系统内存不足的情况下，分配内存慢，可能会导致推理超时。\n * CPU负载过高。调度线程获取不到CPU，此时即使任务完成也无法及时同步到用户接口，导致推理超时情况。\n\n\n模型推理卡住的原因#\n\n模型问题：模型指令原因导致的底层运行错误，错误没有上报，导致hang住。此时，可通过cat\n/sys/devices/system/bpu/bpu0/task_running对bpu任务情况进行查看，如下图所示：\n\ns_time不为空表示任务已经正常开始，而p_time如果为空则表示没有正常返回，即可认为BPU任务hang住了，可联系sr或者编译器团队解决。\n\n\nROI输入模型约束有哪些？#\n\n * NV12输入的原图尺寸要求是 1 < = Win < = 4096，W方向的对齐要求 32 < = stride < = 131072，\n   且必须为32的倍数。\n * ROI 的尺寸要求当前为不能超出输入图像边界，此限制后续版本会放宽。\n * ROI 由左上和右下两个角点表示，右下角点包含在roi范围内，ROI tensor输入坐标为左，上，右，下四个int32类型的值。\n * Resize后的输出图像尺寸要求是 2 < = Wout, 2 < = Hout。\n * ROI的大小和输出尺寸整体要求当前为 ROI_w * ROI_h + Wout * Hout < 1.5MB，此限制后续版本会放宽。\n * ROI缩放倍数限制 0 < = step < = 229375，这里step计算公式为 step = (src_len * 65536 + dst_len\n   / 2) / dst_len，其中src_len为ROI的W或H，resize后的输出图像尺寸的W或H。\n\n\n自定义算子开发#\n\n\nDSP 相关#\n\nJ6选用的DSP型号？#\n\nJ6 选用了 Cadence 公司的 Tensilica Vision Q8 DSP IP（以下简称 Q8）， 是专用于视觉/图像处理的数字信号处理器，DSP\nIP数量根据开发板型号略有不同。更多信息可见：DSP开发文档 或 Cadence 官方文档。\n\n如何获取 Cadence 官方文档？#\n\n安装步骤可参考：DSP开发文档 章节；安装完成Cadence开发套件后，可以在开发包内查看部分文档，获取完整文档包请联系地平线技术支持人员。\n\nDSP 支持哪些计算精度？#\n\n支持 int8/int16/int32 的整型计算，以及 float32、double 的浮点计算。\n\n如何查看 DSP 侧的日志输出？#\n\n在X86 仿真环境中，可以通过在执行示例脚本时修改日志打印等级环境变量 HB_DSP_VDSP_LOG_LEVEL\n来查看日志输出，日志等级设置方法与UCP一致，修改方式请参考脚本内容；\n\n在开发板中，可以通过监听日志文件的方式查看DSP侧的日志。具体方法如下：\n\n 1. 增加环境变量 export HB_DSP_WRITE_VDSP_LOG_TO_ARM=true，使能DSP日志输出。\n\n\n\n 2. 修改DSP日志打印等级环境变量 HB_DSP_VDSP_LOG_LEVEL。\n 3. 使用如下命令启用文件监听。\n\n\n\nDSP 示例是否可以直接运行?#\n\n可以的。在X86 仿真环境中，J6 的 OE 开发包在 samples/ucp_tutorial/deps_x86/ucp/bin 路径中提供了预编译好的\nDSP 镜像，在 VP 示例中可以直接执行 samples/ucp_tutorial/vp/code/ 目录下的 build.sh\n脚本来生成可执行文件，然后执行\nsamples/ucp_tutorial/vp/vp_samples/script_x86/01_basic_processing/run_basic_proc\nessing.sh 来运行示例。\n\n在开发板上，示例提供了对应环境的编译脚本， 在 VP 示例中可以直接执行 samples/ucp_tutorial/vp/code/ 目录下的 build.sh\n脚本来生成可执行文件，然后将samples/ucp_tutorial/vp/vp_samples/script 目录下的生成物拷贝到开发板上，随后执行\nsamples/ucp_tutorial/vp/vp_samples/script/01_basic_processing/run_basic_processi\nng.sh 来运行示例。\n\n但若您想进行相关代码开发，或者编译新的镜像，则需要 DSP 开发软件 Xplorer 和对应的 License 授权文件，获取方式请联系地平线技术支持人员。","routePath":"/guide/ucp/ucp_faq_errorcode/ucp_faq","lang":"zh","toc":[{"text":"通用问题","id":"通用问题","depth":2,"charIndex":3},{"text":"任务创建或提交后返回HB_UCP_INVALID_ARGUMENT错误码是什么原因？","id":"任务创建或提交后返回hb_ucp_invalid_argument错误码是什么原因","depth":3,"charIndex":11},{"text":"如何理解hbUCPSysMem的物理地址和虚拟地址？","id":"如何理解hbucpsysmem的物理地址和虚拟地址","depth":3,"charIndex":285},{"text":"如何理解cacheable和非cacheable的hbmem？","id":"如何理解cacheable和非cacheable的hbmem","depth":3,"charIndex":490},{"text":"模型推理","id":"模型推理","depth":2,"charIndex":1112},{"text":"导致模型推理hbUCPWaitTaskDone接口timeout超时的原因可能有哪些？","id":"导致模型推理hbucpwaittaskdone接口timeout超时的原因可能有哪些","depth":3,"charIndex":1120},{"text":"模型推理卡住的原因","id":"模型推理卡住的原因","depth":3,"charIndex":1328},{"text":"ROI输入模型约束有哪些？","id":"roi输入模型约束有哪些","depth":3,"charIndex":1523},{"text":"自定义算子开发","id":"自定义算子开发","depth":2,"charIndex":1986},{"text":"DSP 相关","id":"dsp-相关","depth":3,"charIndex":1997},{"text":"J6选用的DSP型号？","id":"j6选用的dsp型号","depth":4,"charIndex":2006},{"text":"如何获取 Cadence 官方文档？","id":"如何获取-cadence-官方文档","depth":4,"charIndex":2148},{"text":"DSP 支持哪些计算精度？","id":"dsp-支持哪些计算精度","depth":4,"charIndex":2241},{"text":"如何查看 DSP 侧的日志输出？","id":"如何查看-dsp-侧的日志输出","depth":4,"charIndex":2309},{"text":"DSP 示例是否可以直接运行?","id":"dsp-示例是否可以直接运行","depth":4,"charIndex":2591}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":557,"title":"总览","content":"#\n\n\n总体介绍#\n\n * UCP是什么？\n   \n   统一计算平台（Unify Compute Platform，以下简称 UCP）定义了一套统一的异构编程接口， 提供应用程序接口（Application\n   Programming Interface，以下简称API）实现对计算平台上所有资源的调用。\n   UCP将SOC上的功能硬件抽象出来并进行封装，对外提供基于功能的API，用于创建相应的UCP任务（如VP算子任务），并支持设置硬件Backend提交至UC\n   P调度器， UCP可基于硬件资源，完成SOC上任务的统一调度。 具体提供了以下几个功能： 视觉处理(Vision\n   Process)、神经网络模型推理（Neural Network）、高性能计算库（High Performance Library）、自定义算子插件开发。\n   \n   Backend相关介绍请查看 Backend说明 。\n\n * UCP应用场景:\n   \n   单算子调用：可直接使用UCP中的视觉算子、高性能处理算子。\n   \n   算子插件开发使用：可进行自定义算子的开发。\n   \n   深度学习模型推理：可完成深度学习模型的推理任务，UCP内部完成模型的解析及硬件部署。\n\n * UCP的优势:\n   \n   高度抽象：对于单算子的功能，不会受到硬件差异带来的困扰，可通过指定backend选择需要执行的硬件，降低硬件部署难度。\n   \n   集成度高：作为地平线统一的异构编程接口，一套接口完成所有需求开发。\n\n注解\n\n本章节用于指导如何使用UCP进行模型硬件部署等开发任务，具备基本的嵌入式开发的知识、经验和技能可以更好地理解本章节内容。\n\n\n工作模式#\n\nUCP框架支持两种主要工作模式：直连模式和中继模式。系统默认运行在直连模式下。 中继模式下，支持多进程任务的统一调度，使用中继模式前，首先启动\nucp_service，service文件位于 deps_aarch64/ucp/bin/service/ 路径下， 并通过设置环境变量\nHB_UCP_ENABLE_RELAY_MODE=true 来启用Relay模式，使得用户进程可以通过中继服务进行通信。\n无论是直连模式还是中继模式，UCP接口的调用方式保持一致，不会对编程逻辑产生影响。您可以根据实际需求灵活选择这两种模式，以满足系统在性能和灵活性方面的要求。\n\n\n接口使用流程#\n\nUCP任务执行有三种方式：同步执行、异步执行、注册回调函数执行。 以调用VP接口hbVPRoate(hbVPRotate) 为例，说明三种任务执行方式。\n\n\n同步执行#\n\n创建任务时，将taskHandle对应的参数传入nullptr，可立即同步执行，参考代码如下：\n\n\n\n\n异步执行#\n\n创建任务时，需要提前将taskHandle参数初始化为nullptr；\n提交UCP任务（hbUCPSubmitTask）后，在线程指定位置执行接口（hbUCPWaitTaskDone），等待任务完成，参考代码如下：\n\n\n\n\n注册回调函数执行#\n\n创建任务时，需要提前将taskHandle参数初始化为nullptr； 回调函数需要在任务提交（hbUCPSubmitTask）前注册，否则注册接口会报错。\n\n如下给出设置回调函数的参考代码：\n\n\n\n注解\n 1. UCP内置了Neural Network Kernel，Vision Process Kernel，以及High Performance\n    Kernel，这些kernel算子可能由不同的Backend支持。\n 2. UCP任务调度统一按照优先级调度策略，提交任务时可指定任务执行的优先级。\n 3. 同步执行不支持配置任务的控制参数和后端选择，UCP会根据当前任务的可执行backend，以及硬件负载信息，选择合适的硬件执行。\n 4. 异步执行支持配置任务的控制参数和后端选择，不指定则基于各后端负载均衡选择。\n 5. 除了视频编解码和图像信号处理任务，所有任务创建时的输入输出内存均需要您按照UCP提供的内存接口自行申请管理。\n\n\nDEB部署包#\n\nUCP提供DEB部署包位于 deps_aarch64/ucp/bin/deb/hbucp_aarch64_3.1.4.deb\n路径下，该包旨在简化板端的部署过程，确保系统配置更加便捷和高效。 通过自动安装所需的二进制文件和相关依赖库，可以快速设置并运行UCP相关的应用程序。\n\n成功安装DEB包后，二进制文件文件 ucp_service 和 hrt_ucp_monitor 将自动安装到 /usr/hobot/bin\n目录，可以直接通过命令行访问这些工具。相关的动态库将安装到 /usr/hobot/lib 目录， 确保所有运行时依赖都已正确配置。\n\n在安装过程中，系统会自动配置所需的库路径，无需手动干预。这确保了在运行应用程序时，所有依赖库都能够被正确加载，进而保障系统的稳定性和可靠性。\n\n\n安装命令示例#\n\n首次安装DEB包时，系统会提示安装过程中的版本信息。\n\n\n\n\nupgrade安装命令示例#\n\n使用upgrade安装DEB包时，若安装过程失败，系统会自动回滚到安装前的版本。\n\n\n\n\n卸载DEB包#\n\n如需卸载已安装的DEB包可参考如下命令：\n\n\n\n\nBackend 说明 #\n\nbackend是指UCP任务执行时的后端计算硬件，当前UCP支持的backend包括BPU、DSP、GDC、STITCH、JPU、VPU、PYRAMID、ISP\n。\n\nBACKEND   描述\nBPU       Brain Process Unit，地平线神经网络计算单元。\nDSP       Digital Signal Processor，数字信号处理器，是一个可编程的硬件单元。\nGDC       Geometric Distortion\n          Correction，是ARM上的一个硬件IP，可将输入图像进行视角变换、畸变矫正、图像仿射变换等操作。\nSTITCH    stitch是J6的一个IP单元，可对输入的图像进行裁剪，拼接，拼接模式分别有：alpha融合、alpha\n          beta融合、直接拷贝。\nJPU       JPEG Processing Unit，主要用以完成JPEG的编解码功能。\nVPU       Video Processing Unit，是一种专用的视觉处理单元。\nPYRAMID   全称Image Pyramid，是一个硬件处理模块，可对整幅原始图像进行缩小。\nISP       全称Image Signal\n          Processor，是一个硬件处理模块，可以将富含图像原始信息的RAW格式转换为易于传输处理的YUV格式。\n\n\n硬件特性描述#\n\n\nDSP#\n\n硬件特性            特性标识\nHW number       1\nmaximum input   4096x2160\nminimum input   32x16\nformat          Y/NV12\n\n\nGDC#\n\n硬件特性            特性标识\nHW number       1\nmaximum input   3840x2160\nminimum input   100x100\nformat          NV12\n\n\nSTITCH#\n\n硬件特性             特性标识\nHW number        1\nmaximum input    2000x2000\nminimum input    16x2\nmaximum output   3840x3840\nminimum output   16x2\nformat           Y/NV12\n\n\nJPEG Processing Unit#\n\n硬件特性            特性标识\nHW number       1\nmaximum input   8192x8192\nminimum input   32x32\nmax instance    64\nformat          NV12/YUV420/YUV444/YUV444_P\n\n\nVideo Processing Unit#\n\n硬件特性            特性标识\nHW number       1\nmaximum input   8192x4096\nminimum input   256x128\nmax instance    32\nformat          NV12/YUV420\n\n\nPYRAMID#\n\n硬件特性            特性标识\nHW number       3\nmaximum input   4096x4096\nminimum input   32x32\nformat          Y/NV12\n\n\nISP#\n\n硬件特性            特性标识\nHW number       2\nmaximum input   4096x2160\nminimum input   480x240\nformat          RAW12\n\n\n环境及工具#\n\nUCP适用于地平线J6及更高架构的计算平台，您需要具备基本的嵌入式开发的知识技能，以完成交叉编译、部署，使用环境及工具要求参考下表：\n\n环境/工具    支持版本\n操作系统     Linux\n开发板      J6开发板\n开发语言     C++11\n交叉编译器    Linaro 12.2.0\n工具链DSP   Cadence Vsion Q8 2023.11\n\n除开发板外，UCP还在x86仿真环境下提供了与板端同样的开发能力支撑。\n\n\nx86仿真说明#\n\n\n功能说明#\n\n与开发板端相同，UCP在 x86 架构上通过仿真形式提供了同样的视觉处理、模型推理和高性能计算能力，所有的示例及接口代码均可以在仿真环境中等效使用。\n您可以在x86环境中进行代码开发和调试，开发过程中获得可即时反馈，并在早期发现和解决问题，从而提高开发效率和代码质量，以确保代码能够无缝迁移到SoC硬件上运行。\n\nUCP支持的各Backend仿真方式如下：\n\n * BPU和DSP硬件采用指令级仿真。\n * GDC、JPU 和 VPU（Video Processing Unit）硬件采用CModel可执行文件仿真。\n   * GDC硬件使用的CModel可执行文件是 gdc_cmodel 。\n   * JPU硬件使用的CModel可执行文件是 Nieuport_JpgEnc 和 Nieuport_JpgDec ，分别用于JPEG编码和解码。\n   * VPU硬件使用的CModel可执行文件是 hevc_enc、hevc_dec、avc_enc 和 avc_dec。其中 hevc_enc 和\n     hevc_dec 分别用于H.265编码和解码，avc_enc 和 avc_dec 分别用于H.264编码和解码。\n * STITCH和PYRAMID硬件采用仿真库。\n\n\n环境说明#\n\n编译环境#\n\nUCP仿真使用Docker镜像自带的编译器环境即可。\n\n运行环境#\n\n 1. 使用DSP硬件进行仿真时，需要配置Xtensa开发环境并指定DSP仿真镜像路径。 环境配置方法可参考 DSP工具链安装\n    。指定DSP仿真镜像路径可以通过设置环境变量 HB_DSP_CMODEL_IMAGE，以确保应用程序能够找到正确的仿真镜像文件。 参考命令如下：\n\n\n\n 2. 运行CModel可执行文件时，需要将可执行文件路径添加到PATH环境变量中，以便在终端可直接运行，参考命令如下：\n\n\n\n 3. 其余硬件仿真运行时不需要进行额外的配置。\n\n\n性能说明#\n\n但x86仿真环境下的性能通常会低于实际硬件的性能，主要原因如下：\n前文我们提到，目前UCP支持的各Backend使用的仿真方式包括指令级仿真、CModel可执行文件仿真以及使用仿真库。\n\n * 指令级仿真：指令级仿真方式需要逐条指令进行模拟和执行，从而导致较高的计算开销和较低的仿真速度。\n * CModel 可执行文件仿真：CModel 可执行文件通过读写文件进行输入输出操作，而文件 I/O 操作较为耗时，在一定程度上会对仿真速度有所影响。\n * 仿真库：仿真库运行在 CPU 上，CPU 模拟硬件加速器的行为，但 CPU 的架构和设计并不专门针对这些任务，因此运行效率较低。\n   尽管仿真环境下的性能通常低于实际硬件的性能，但仿真环境提供了完整的API支持和精确的功能验证，能够极大地提高您的开发效率和代码质量，有助于您在早期阶段发现\n   并解决潜在的问题。","routePath":"/guide/ucp/ucp_overview","lang":"zh","toc":[{"text":"总体介绍","id":"总体介绍","depth":2,"charIndex":3},{"text":"工作模式","id":"工作模式","depth":2,"charIndex":727},{"text":"接口使用流程","id":"接口使用流程","depth":2,"charIndex":1017},{"text":"同步执行","id":"同步执行","depth":3,"charIndex":1105},{"text":"异步执行","id":"异步执行","depth":3,"charIndex":1164},{"text":"注册回调函数执行","id":"注册回调函数执行","depth":3,"charIndex":1283},{"text":"DEB部署包","id":"deb部署包","depth":2,"charIndex":1720},{"text":"安装命令示例","id":"安装命令示例","depth":3,"charIndex":2078},{"text":"upgrade安装命令示例","id":"upgrade安装命令示例","depth":3,"charIndex":2118},{"text":"卸载DEB包","id":"卸载deb包","depth":3,"charIndex":2179},{"text":"Backend 说明","id":"backend-说明","depth":2,"charIndex":-1},{"text":"硬件特性描述","id":"硬件特性描述","depth":2,"charIndex":2839},{"text":"DSP","id":"dsp","depth":3,"charIndex":2849},{"text":"GDC","id":"gdc","depth":3,"charIndex":2967},{"text":"STITCH","id":"stitch","depth":3,"charIndex":3085},{"text":"JPEG Processing Unit","id":"jpeg-processing-unit","depth":3,"charIndex":3259},{"text":"Video Processing Unit","id":"video-processing-unit","depth":3,"charIndex":3434},{"text":"PYRAMID","id":"pyramid","depth":3,"charIndex":3596},{"text":"ISP","id":"isp","depth":3,"charIndex":3718},{"text":"环境及工具","id":"环境及工具","depth":2,"charIndex":3837},{"text":"x86仿真说明","id":"x86仿真说明","depth":2,"charIndex":4068},{"text":"功能说明","id":"功能说明","depth":3,"charIndex":4079},{"text":"环境说明","id":"环境说明","depth":3,"charIndex":4627},{"text":"编译环境","id":"编译环境","depth":4,"charIndex":4634},{"text":"运行环境","id":"运行环境","depth":4,"charIndex":4669},{"text":"性能说明","id":"性能说明","depth":3,"charIndex":4911}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":558,"title":"hrt_ucp_monitor 工具介绍","content":"#\n\nhrt_ucp_monitor 是一个监控硬件 IP 占用率的工具，支持的 IP\n包括BPU，DSP，GDC，STITCH，PYM，ISP，Codec（VPU(Video Processing Unit) 和 JPU）。\n在提交任务时，UCP可以指定任务部署的后端（Backend)，这些后端与本文档中的硬件IP相对应。\n\n\n参数描述#\n\n参数             类型       说明\n-b, --batch    null     指定以非交互的批处理模式运行, 默认为交互模式。\n-d, --delay    int      指定占用率更新间隔，单位毫秒，默认为 1000 毫秒，有效范围为[100, 10000]。\n-f, --freq     int      指定采样频率，默认为每秒采样 1000 次，仅对 BPU 和 DSP 有效，有效范围为[10, 1000]。\n-n, --number   int      指定占用率的刷新次数，有效范围为[1, INT32_MAX]。\n-t, --time     int      指定工具运行时间，单位秒，有效范围为[1, INT32_MAX]。\n-e, --enable   string   指定需要监控的 IP，有效值为bpu, dsp, gdc, stitch, pym, isp, jpu,\n                        vpu，不区分大小写，多个参数通过分号隔开。\n\n\n使用说明#\n\n运行 hrt_ucp_monitor -h 或 hrt_ucp_monitor --help 获取工具的使用详情如下所示：\n\n\n\n在 hrt_ucp_monitor 的运行过程中，您可以按键盘的Q或执行CTRL+C提前退出。\n\n\n默认参数#\n\n概述#\n\n运行hrt_ucp_monitor时，如果不指定参数，则使用默认参数运行。默认开启所有硬件IP监控，以交互模式运行， BPU 和 DSP 每秒采样 500\n次，硬件 IP 占用率每 1000ms 刷新一次。\n\n示例#\n\n\n\n\nbatch#\n\n概述#\n\n批处理模式下，不支持接收按键操作，hrt_ucp_monitor 每次刷新数据都将换行重新输出，适用于将数据重定向到文件。\n\n示例 1：在终端输出结果#\n\n\n\n示例 2：重定向输出到文件#\n\n\n\n\ndelay#\n\n概述#\n\n指定硬件 IP 占用率的刷新时间，单位毫秒，当前支持的设置范围为[100, 10000]。如果设置的时间不在有效范围内，将设置失败，并打印错误提示信息。\n\n示例#\n\n\n\n刷新时间设置为 3 秒，表示每 3 秒刷新一次占用率数据。\n\n\nfreq#\n\n概述#\n\n仅适用于 BPU 和 DSP，用于控制硬件 IP 状态采样频率。当前支持的范围是[10,1000]，即最少每秒采样 10 次，最多每秒采样 1000 次。\n由于占用率的统计方式不同，其他硬件 IP，比如 GDC，STITCH 不需要设置采样频率，当设置刷新时间时，会获取指定周期内的占用率数据。\n\n示例#\n\n\n\n设置 BPU 和 DSP 的 Busy 状态的采样周期为每秒采样 100 次。\n\n\nnumber#\n\n概述#\n\n用于指定硬件 IP 占用率最大刷新次数，当刷新次数到达最大刷新次数后，hrt_ucp_monitor 工具会自动退出，同时您可以按 CTRL+C 提前退出。\n\n示例#\n\n\n\n\ntime#\n\n概述#\n\n用于指定工具运行时间，单位秒，在 hrt_ucp_monitor 中会根据刷新时间计算最大刷新次数，当刷新次数到达最大刷新次数后，hrt_ucp_monitor\n会自动退出， 与 number 参数一样，您也可以按 CTRL+C 提前退出。time 和 number 参数不支持同时设置，如果同时设置，将提示错误信息。\n\n示例#\n\n\n\n工具默认刷新时间为 1s，设置运行 10s 后退出。\n\n\nenable#\n\n概述#\n\n您可以设置需要开启的硬件 IP，这样可以只监控关心的数据，降低工具的 CPU 占用率。\n\n示例#\n\n\n\n工具将只输出 BPU 的占用率数据。\n\n\nverbose#\n\n概述#\n\n用于显示 hrt_ucp_monitor 运行过程中更详细的日志信息，比如工具使用的参数。\n\n示例#\n\n","routePath":"/guide/ucp/ucp_tools/ucp_monitor","lang":"zh","toc":[{"text":"参数描述","id":"参数描述","depth":2,"charIndex":165},{"text":"使用说明","id":"使用说明","depth":2,"charIndex":637},{"text":"默认参数","id":"默认参数","depth":3,"charIndex":759},{"text":"概述","id":"概述","depth":4,"charIndex":766},{"text":"示例","id":"示例","depth":4,"charIndex":876},{"text":"batch","id":"batch","depth":3,"charIndex":884},{"text":"概述","id":"概述-1","depth":4,"charIndex":892},{"text":"示例 1：在终端输出结果","id":"示例-1在终端输出结果","depth":4,"charIndex":960},{"text":"示例 2：重定向输出到文件","id":"示例-2重定向输出到文件","depth":4,"charIndex":977},{"text":"delay","id":"delay","depth":3,"charIndex":996},{"text":"概述","id":"概述-2","depth":4,"charIndex":1004},{"text":"示例","id":"示例-1","depth":4,"charIndex":1087},{"text":"freq","id":"freq","depth":3,"charIndex":1126},{"text":"概述","id":"概述-3","depth":4,"charIndex":1133},{"text":"示例","id":"示例-2","depth":4,"charIndex":1287},{"text":"number","id":"number","depth":3,"charIndex":1336},{"text":"概述","id":"概述-4","depth":4,"charIndex":1345},{"text":"示例","id":"示例-3","depth":4,"charIndex":1430},{"text":"time","id":"time","depth":3,"charIndex":1438},{"text":"概述","id":"概述-5","depth":4,"charIndex":1445},{"text":"示例","id":"示例-4","depth":4,"charIndex":1611},{"text":"enable","id":"enable","depth":3,"charIndex":1647},{"text":"概述","id":"概述-6","depth":4,"charIndex":1656},{"text":"示例","id":"示例-5","depth":4,"charIndex":1706},{"text":"verbose","id":"verbose","depth":3,"charIndex":1734},{"text":"概述","id":"概述-7","depth":4,"charIndex":1744},{"text":"示例","id":"示例-6","depth":4,"charIndex":1796}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":559,"title":"UCP Trace 使用说明","content":"#\n\nUCP trace 通过在 UCP 执行的关键路径上嵌入 trace 记录，提供深入分析 UCP 应用程序调度逻辑的能力。在出现性能异常时，可以通过分析UCP\ntrace，快速找到异常发生的时间点。\n\nUCP trace 提供了两种 trace 后端选项：Perfetto Trace 和 Chrome\nTrace。您可以通过设置环境变量，在这两者之间进行选择，以满足您特定的性能跟踪需求。\n\n * Perfetto Trace可以获取到 UCP 记录的 trace，以及系统状态， ftrace 等信息。\n * Chrome Trace只能获取 UCP 记录的 trace ，主要用于分析 UCP 本身的调度逻辑。\n\nUCP trace 工具和配置文件位于 samples/ucp_tutorial/tools 路径下，目录结构为：\n\n\n\n\n环境变量#\n\n环境变量                          取值范围                      说明\nHB_UCP_ENABLE_PERFETTO        true，false        false   是否启用 Perfetto trace，true 启动，false 不启动，默认不启动。\nHB_UCP_PERFETTO_CONFIG_PATH   Perfetto 配置文件路径   \"\"      指定 pefetto 配置文件的路径，默认为空，如果不需要 UCP 初始化 perfetto，则可不配置该环境变量。\nHB_UCP_TRACE_LOG_LEVEL        [0, 6]            6       指定 UCP Trace 日志等级，默认为 6，不输出。\nHB_UCP_USE_ALOG               true，false        false   是否启用 alog sink，true 启动，false 不启动，默认不启动。启动 alog sink 之后会输出日志到\n                                                        alog buffer，使用 logcat 抓取日志，同时关闭日志输出到终端。\n\n注解\n\nPerfetto trace的优先级更高，如果export HB_UCP_ENABLE_PERFETTO=true 的同时 export\nUCP_TRACE_LOG_LEVEL=0，那么只启动 Perfetto trace，忽略UCP trace日志级别。\n\n\nUCP Trace 记录点#\n\nUCP 在应用程序 API 及内部关键调度路径嵌入了 trace 记录，其中包括任务 trace 记录和算子 trace 记录。\n\n\n任务 Trace 记录点#\n\n名称                  说明\nhbDNNInfer          创建模型推理任务\nhbVPxxx             创建视觉处理任务\nhbHPLxxx            创建高性能计算任务\nhbUCPSubmitTask     任务提交\n${TaskType}::Wait   等待任务执行结束\nTaskSetDone         通知任务执行完成\nhbUCPReleaseTask    任务释放\n\n\n算子 Trace 记录点#\n\n名称         说明\nSubmitOp   算子提交\nOpInfer    算子开始执行\nOpFnish    算子执行结束\n\n\nPerfetto Trace#\n\n\n概述#\n\nPerfetto 是 Google 开发和开源的系统性能分析工具，能够收集来自不同数据源的性能数据， 同时提供了 Perfetto UI\n用于数据的可视化和分析。更多 perfetto 的详细信息，请参考 。\n\n\n配置文件#\n\nUCP Trace 配置文件#\n\n用于配置 UCP 如何使用 Perfetto，您可以通过环境变量HB_UCP_PERFETTO_CONFIG_PATH来指定。\n\n参数描述#\n\n参数             类型       取值范围                    说明\nbackend        string   \"in_process\"，\"system\"   in_process 表示进程内模式，进程内直接保存 perfetto trace 到文件。system\n                                                表示系统模式，由后台服务 traced，traced_probe 统一进行 trace 捕获。\ntrace_config   string   perfetto 的配置文件路径        backend 设置为\"in_process\"时有效，为 protobuf 文本格式的配置文件。\n\n注解\n\nUCP trace 配置文件非必须指定，如果您的应用程序已经初始化了Perfetto，那么只需要 export\nHB_UCP_ENABLE_PERFETTO=true 即可开启Perfetto。\n\n示例 1：in_process 模式#\n\n\n\n示例 2：system 模式#\n\n\n\n注解\n\nbackend 选择system时，无需单独为 UCP 指定trace_config。\n\nPerfetto 配置文件#\n\n关于 Perfetto 配置文件的详细信息可以参考 。 UCP 提供了参考配置文件 ucp_in_process.cfg 和ucp_system.cfg\n，您可根据应用场景进行修改。\n\n示例：ucp_in_process.cfg#\n\n\n\n\n使用示例#\n\nin_process 模式#\n\nin_process 模式下只能抓取 UCP 进程内的 trace 信息，可以不用启动 perfetto 的后台进程。\n\n操作步骤#\n\n 1. 配置环境变量。\n\n\n\n注解\n\nucp_in_process.json中指定了 Perfetto 的配置文件ucp_in_process.cfg，其中output_path指定了 trace\n文件的输出路径。由于 Perfetto 不支持直接覆盖原有的 trace 文件，若该文件已经存在，则需要先将其删除。\n\n 2. 运行 UCP 应用程序，这里以hrt_model_exec为例。\n\n由于指定的文件路径是相对路径，因此trace配置文件和脚本需要放置在运行程序的同级目录下。同时，需要确保在同一个shell环境中配置环境变量和运行程序。\n\n\n\n 3. 生成的 trace 保存在 ucp.pftrace，您可以使用 打开。\n\n 4. 在时间线上点击创建的任务，可以看到任务从创建到释放的完整调度流程。\n\n 5. Perfetto UI 常用操作如下，更详细的操作说明，请参考help界面。\n\n操作                说明\nw 或 ctrl+鼠标滚轮上滑   放大\ns 或 ctrl+鼠标滚轮下滑   缩小\na 或 向左拖动上方的时间条    左移\nd 或 向右拖动上方的时间条    右移\n?                 打开 help 界面\n\nsystem 模式#\n\nsystem 模式下，UCP trace 只是其中的一个数据源，因此需要运行 tracebox 相对应的指令来完成 trace 的捕获。\n\n操作步骤#\n\n 1. 运行 Perfetto 后台进程。\n\n\n\n 2. 触发数据抓取。\n\n\n\n 3. 另起一个终端，配置 UCP 环境变量。\n\n\n\n 4. 运行 UCP 应用程序，这里以hrt_model_exec为例。\n\n为了能够抓取完整的数据，需要确保hrt_model_exec执行结束前，perfetto进程未退出。\n\n\n\n 5. 生成的 trace 保存在 perfetto 命令指定的输出文件 ucp.pftrace，您可以使用 打开。\n\n\nBPU Trace#\n\n在 system 模式下支持抓取 BPU trace，只需在 perfetto 配置文件中加入 BPU trace\n的数据源即可，ucp_bpu_trace.cfg 中已默认添加了BPU trace数据源，具体配置项如下所示：\n\n\n\nbputrace_period_ms 用于设置读取BPU\ntrace的周期，您可根据实际使用场景调整该参数，当BPU负载较大时，可以适当缩短读取周期，避免发生因读写速度不匹配导致的trace数据被覆盖的问题。\n\n为了展示多个模型推理过程中的 BPU trace，这里提供了一个多进程应用的示例。除了启动的运行程序有所不同外，其余的步骤与前一章节相同。\n\n\n\nBPU Trace 的可视化需要使用地平线定制开发的 hbperfetto 工具进行打开，您可以通过联系地平线技术支持人员来获取该工具。使用\nhbperfetto 打开trace文件的效果如下图所示：\n\nBPU Trace中所呈现的不同模型推理任务的调度情况如下图所示：\n\nhbperfetto 支持 UCP trace 和 BPU trace 的关联。以下图示展示了从 UCP\n模型推理任务的创建，提交，调度执行，直至任务完成执行并最终释放的完整流程。\n\n\nChrome Trace#\n\nChrome trace 只支持捕获 UCP trace，不支持多数据源的捕获，多数据源的捕获请使用 Perfetto trace。Chrome trace\n的特点是简单， 易用，使用文本日志来记录 trace，不依赖于额外的三方库和工具。如果只关心 UCP 的调度逻辑，可以使用 Chrome trace\n来进行捕获。\n\n\n使用示例#\n\n 1. 配置环境变量。\n\n\n\n注解\n\n在开始新的捕获前，建议先删除旧的日志文件，避免旧的数据带来的干扰。\n\n 2. 运行 UCP 应用程序，这里以hrt_model_exec为例。\n\n\n\n 3. 执行trace抓取脚本。\n\n在完成trace日志的捕获后，运行UCP发布包中提供了catch_trace.sh，将原始的trace日志转换为json格式的trace文件。\n\n\n\n 4. 使用 或 Chrome UI(chrome://tracing/) 打开 ucp_trace_task.json 和\n    ucp_trace_thread.json。\n\nChrome UI 打开 ucp_trace_task.json：\n\nPerfetto UI 打开 ucp_trace_thread.json：","routePath":"/guide/ucp/ucp_tools/ucp_trace","lang":"zh","toc":[{"text":"环境变量","id":"环境变量","depth":2,"charIndex":374},{"text":"UCP Trace 记录点","id":"ucp-trace-记录点","depth":2,"charIndex":1090},{"text":"任务 Trace 记录点","id":"任务-trace-记录点","depth":3,"charIndex":1173},{"text":"算子 Trace 记录点","id":"算子-trace-记录点","depth":3,"charIndex":1409},{"text":"Perfetto Trace","id":"perfetto-trace","depth":2,"charIndex":1492},{"text":"概述","id":"概述","depth":3,"charIndex":1510},{"text":"配置文件","id":"配置文件","depth":3,"charIndex":1622},{"text":"UCP Trace 配置文件","id":"ucp-trace-配置文件","depth":4,"charIndex":1629},{"text":"参数描述","id":"参数描述","depth":4,"charIndex":1711},{"text":"示例 1：in_process 模式","id":"示例-1in_process-模式","depth":4,"charIndex":2167},{"text":"示例 2：system 模式","id":"示例-2system-模式","depth":4,"charIndex":2190},{"text":"Perfetto 配置文件","id":"perfetto-配置文件","depth":4,"charIndex":2258},{"text":"示例：ucp_in_process.cfg","id":"示例ucp_in_processcfg","depth":4,"charIndex":2367},{"text":"使用示例","id":"使用示例","depth":3,"charIndex":2394},{"text":"in_process 模式","id":"in_process-模式","depth":4,"charIndex":2401},{"text":"system 模式","id":"system-模式","depth":4,"charIndex":3021},{"text":"BPU Trace","id":"bpu-trace","depth":3,"charIndex":3330},{"text":"Chrome Trace","id":"chrome-trace","depth":2,"charIndex":3868},{"text":"使用示例","id":"使用示例-1","depth":3,"charIndex":4046}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":560,"title":"简介","content":"#\n\nVP模块在以NN为中心的计算方案中，主要作用于模型的前后处理环节。在地平线统一架构中，多种硬件均已搭载了图像处理的算子，而VP模块将图像处理相关的硬件调用进行了\n封装， 通过设置backend来选择不同的硬件方案（若不指定backend，UCP会自动适配负载更低的处理单元），从而平衡开发板负载，充分对开发板算力进行挖掘，\n规避了不同硬件调用区别带来的不便，您可更多地关注软件功能。其功能架构如下图所示：\n\n上述架构图中，应用通过VP模块提供的算子的任务构造函数，如hbVPResize、hbVPRotate等，生成对应算子的任务句柄。\nUCP提供了包含任务调度、会话管理、引擎管理等模块的Service，在对应算子的任务句柄生成后，通过UCP任务调度接口将算子任务提交到任务队列，分配到不同的底层\n硬件，实现算子的功能逻辑。\n最底层为不同处理单元中封装实现的算子功能，例如remap算子在DSP与GDC中都有实现，此时VP中的remap算子就可以在这两种硬件上灵活部署。\n\n在阅读本章节前，请先明确如下基础概念，相关内容在下文可能会多次被提及：\n\n * 视觉算法，也称Vision Algorithm，是用于实现图像处理功能的函数。\n * VDSP，全称Vision Digital Signal Processing，指视觉数字信号处理。\n * VP，全称Vision Process，指UCP中的视觉处理功能模块。\n * Backends，指UCP框架中的可分配处理单元。\n * Kernel，指VP算子中的滤波核参数。\n * 图片尺寸，算子对输入输出图片尺寸有各自的约束。","routePath":"/guide/ucp/vp/vp1_Introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":561,"title":"快速上手","content":"#\n\n本章节介绍了UCP示例包 ucp_tutorial 中VP模块的示例使用方法，以及如何配置开发环境、编译并运行示例应用代码，帮助快速上手UCP中的VP功能模块。\n其主要架构如下：\n\n\n示例包使用#\n\n示例包结构如下所示：\n\n\n\n其中 ucp_tutorial/vp\n文件夹为VP模块的示例，包括了基础图像处理示例、图像变换示例、图片特征提取示例、光流示例、环视图像拼接示例、视频编解码示例，可同时支持板端运行编译和x86仿真编\n译两种方式。 具体的示例原理及实现流程请查阅 示例 章节。\n\n\n编译示例算子#\n\n在编译运行示例应用代码前，您需要确保环境满足要求，根据 环境部署 部分的指引，您的开发机中应该已经安装有相关环境，要求如下：\n\n * cmake >= 3.0。\n * 对于板端编译需要指定交叉编译工具链，对于x86仿真docker自带编译器即可。\n\n在示例的 ucp_tutorial/vp/code 目录下有预先配置好的编译脚本 build.sh，选项 -a x86 和 -a aarch64\n分别支持两种编译方式， 直接执行 build.sh 脚本即可完成一键编译，生成的文件会被保存到 ucp_tutorial/vp/vp_samples 目录下。\n此外，目录中也包含了 build_aarch64.sh 和 build_x86.sh 两个编译脚本，分别对应了两个编译选项， 使用这两个脚本进行编译与使用\nbuild.sh 脚本等效。 编译板端运行的VP模块需要执行的命令如下：\n\n\n\n执行编译脚本后，会生成运行示例所需要的执行程序和依赖文件，保存在 ucp_tutorial/vp/vp_samples 目录下。\n\n以VP为例，其生成物如下所示，包含图片数据、示例运行脚本、运行依赖库、可执行文件以及运行示例的脚本目录，共同组成了完整的运行环境和运行依赖。\n\n\n\n\n运行示例#\n\n正确地完成编译的全部步骤后，可执行示例会被配置完毕并保存在 vp_samples 文件夹内。下面根据执行环境不同，介绍上板与仿真两种运行示例的方式。\n\n上板运行#\n\n将 vp_samples 文件夹整个拷贝到开发板上，进入 vp_samples/script 文件夹，直接执行示例文件夹中提供的运行脚本即可看到示例的运行结果。\n开发板上执行脚本的参考指令如下：\n\n\n\nx86仿真运行#\n\n进入 vp_samples/script_x86 文件夹，直接执行示例文件夹中提供的运行脚本即可看到示例的运行结果。 执行脚本的参考指令如下：\n\n\n\n注解\n\n地平线J6 SOC使用的是Cadence公司的Tensilica Vision Q8\nDSP，因此x86仿真实例中dsp算子运行依赖Cadence提供的一套工具链，环境配置可参考 DSP工具链安装 的指引， 需要正确配置 License\n以及环境变量 XTENSA_ROOT 即可。\n\n\n输出物说明#\n\n以板端运行为例，示例运行时会在控制台内打印流程日志并生成对应输出文件。日志中会包含全部算子的调用流程，输出结果会被保存在\nscript/01_basic_processing 文件夹中。示例部分输出如下：\n\n\n\n生成物会被保存到 vp_samples/script/01_basic_processing 目录下，内容如下：\n\n\n\n\nVP算子使用示例#\n\n本节通过一个简单的算子调用展示了如何使用VP封装的算子实现图片处理的功能。主要步骤包含了图片载入、任务创建、任务提交、任务完成、销毁任务、保存输出等。\n您可以阅读相应源码和注释进行学习。\n\n该示例的作用为使用hbVPRotate算子将图片旋转90度，具体实现如下：\n\n","routePath":"/guide/ucp/vp/vp2_Quick_Start","lang":"zh","toc":[{"text":"示例包使用","id":"示例包使用","depth":2,"charIndex":94},{"text":"编译示例算子","id":"编译示例算子","depth":3,"charIndex":248},{"text":"运行示例","id":"运行示例","depth":3,"charIndex":794},{"text":"上板运行","id":"上板运行","depth":4,"charIndex":877},{"text":"x86仿真运行","id":"x86仿真运行","depth":4,"charIndex":985},{"text":"输出物说明","id":"输出物说明","depth":3,"charIndex":1218},{"text":"VP算子使用示例","id":"vp算子使用示例","depth":2,"charIndex":1393}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":562,"title":"模块架构","content":"#\n\n\nVP模块架构#\n\n\n\n通过VP模块中的API创建算子的任务句柄，每个算子接口都有至少一种硬件实现（如remap算子可在DSP或GDC硬件上执行）。\n创建好任务句柄后，可以通过设置调度参数指定后端、任务优先级，设备ID和自定义ID，从而将任务提交到对应的处理单元。\n提交任务后，需要调用API等待任务完成。任务完成后，使用API释放任务句柄和相关资源，以确保系统资源得到有效管理和释放。\n在从任务创建到任务提交、释放的过程中，UCP Service层对各个环节均提供接口及功能支持。\n\n\n算子执行流程#\n\n此处以Rotate算子异步执行为例，展示算子实际的调用流程，其他算子的使用方法基本与其流程一致。\n\n\n\n注解\n\n虚线框表示该步骤为非必须步骤，可通过复用参数、使用默认参数等方式略过。\n\n1.准备输入输出数据：即申请图片的内存空间并构建相关的描述信息。\n\n2.创建算子任务：此步骤为直接调用算子任务接口，同时传入算子执行所需的参数，执行完成后输出UCP任务句柄。\n\n3.提交任务：通过传入调度参数将算子任务提交到不同处理核心，任务提交支持指定backend，如不指定则系统会自动适配backend。\n\n4.指定接口等待任务结束：任务结束时，系统会根据不同的执行状态返回不同的返回值，此时，您可根据返回值来查看任务执行结果。\n\n5.销毁任务：任务成功执行后需要销毁任务，并释放申请的内存。","routePath":"/guide/ucp/vp/vp3_Architecture","lang":"zh","toc":[{"text":"VP模块架构","id":"vp模块架构","depth":2,"charIndex":3},{"text":"算子执行流程","id":"算子执行流程","depth":2,"charIndex":246}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":563,"title":"性能指标","content":"#\n\n本章节为您提供VP性能测试说明，并为您展示性能数据结果统计供参考。\n\n\n测试平台#\n\n测试条件（硬件及软件平台要求）：\n\n * 测试开发板：J6开发板。\n * 系统软件：LNX-PL4.0-DB-20240822 release 20240822-222755。\n * OpenCV版本：3.4.5。\n * OpenCV运行环境：A78单线程。\n\n\n测试方法#\n\n * 性能测试方法：因算子运行耗时会有波动，所以这里我们取用算子运行100次的平均值性能数据。\n * 使用steady_clock测试时间\n\n\n\n * 数据说明：\n   * Task时间：从包含任务创建（例如：hbVPBilateralFilter）到释放任务结束（hbUCPReleaseTask）\n   * Baseline时间：OpenCV执行时间\n   * Ratio：Baseline Avg / Task Avg\n * 误差评估：误差主要是由算法中的插值实现、边缘处理以及计算数据类型与部分过程不同而导致，如下几个指标可用于对误差进行评估。\n   * MaxAbsDev：最大像素值绝对值偏差\n   * SumAbsDev：像素值绝对值偏差总和\n   * MSE： 像素值均方误差\n\n\n性能对比#\n\n注解\n\nDSP GDC 测试用的图像尺寸为1080P(1920x1080)，Codec(3840x2160)、Lkof(1080P)、Stitch(输入 2 张\n320x320，输出 640x640 )、Pyramid(输入1080p, 输出 960x540)。其中pyramid core 2 的性能约为core 0/1\n的 2/3。\n\nALGORITHM         PARAMETERS                                                   VISION PROCESSES(US)                         OPENCV 3.4.5 A78(US)   RATIO(DEFAULT DSP)             \nbilateralFilter   1920x1080 Y, kernelSize=5, sigmaColor=15, sigmaSpace=2       4224                                         54712                  13.0\nboxFilter         1920x1080 Y, kernelHW=3x3                                    510                                          1956                   3.833\ncanny             1920x1080 Y, kernelSize=3, threshold1=100, threshold2=400    3956                                         6069                   1.5\ncornerHarris      1920x1080 Y, blockSize=3, sensitivity=0.04, kernelSize=3     3748                                         27754                  6.442\ncvtColor          1920x1080 RGB, dst: y                                        939                                          2066                   2.2\ndilate            1920x1080 Y, kernelHW=3x3                                    537                                          608                    1.1\nequalizeHist      1920x1080 Y                                                  715                                          2697                   3.77\nerode             1920x1080 Y, kernelHW=3x3                                    534                                          610                    1.1\nfilter2D          1920x1080 Y, kernelHW=5x5                                    1144                                         22887                  20.0\nflip              1920x1080 Y, y-axis                                          495                                          1127                   2.28\ngaussianBlur      1920x1080 Y, kernelSize=3, sigmaX=0, sigmaY=0                508                                          2460                   4.8\nintegral          1920x1080 Y, y-axis                                          1204                                         1288                   1.07\nlaplacianFilter   1920x1080 Y, kernelSize=3, normalize=0                       836                                          5334                   6.37\nlkof              1920x1080 nv12, pyrLevel=5, winSize=7, criteriaEpsilon=0,    778                                          7429                   9.5\n                  maxIteration=5, minEigThreshold=1e-4\nmedianBlur        1920x1080 Y, maskWidth=7                                     24302                                        100173                 4.1\npyrDown           1920x1080 nv12, DSP:interpolation=gaussian                   DSP: 1211 PYM(normal): 1996 PYM(low): 2905   5243                   DSP: 4.3 PYM(normal): 2.6 PYM(low): 1.8\n                  PYM:interpolation=HB_VP_INTER_LINEAR\npyrUp             1920x1080 Y                                                  1171                                         6835                   5.8\nremap             1920x1080 nv12, interpolation=HB_VP_INTER_LINEAR             DSP: 10646 GDC: 3569                         20287                  DSP: 1.9 GDC: 5.3\nresize            1920x1080 Y, xScale=1.5 yScale=1.5,                          960                                          2692                   2.8\n                  interpolation=HB_VP_INTER_LINEAR\nrotate            1920x1080 Y, rotateCode=HB_VP_ROTATE_90_CLOCKWISE            558                                          3382                   6.06\nsepFilter2D       1920x1080 Y, kernelHW=1x5                                    551                                          12831                  23.2\nsobel             1920x1080 Y, kernelSize=3, scale=1, delta=0, dx=1, dy=0      1185                                         1840                   1.55\ntranspose         1920x1080 Y                                                  557                                          2316                   4.15\nwarpAffine        1920x1080 nv12, scale=1.0, rotate=45, translate=0            1064                                         5433                   5.1\nwarpPerspective   1920x1080 Y, interpolation=HB_VP_INTER_LINEAR,               1923                                         20435                  10.6\n                  transformMatrix=[0.9, 0.05, 15.0, 0.05, 0.9, 15.0, 0.0001,\n                  0.0001, 1.1]\nstitch            640x480 nv12, stitch num=4                                   369                                          969                    Stitch:2.6\njpegDecode        3840x2160 src: jpg, dst: nv12                                7398                                         127983                 Codec: 17.3\njpegEncode        3840x2160 src: nv12, dst: jpg                                9271                                         212206                 Codec: 22.9\nH265Decode        3840x2160 src: h.265, dst: nv12                              9390                                         -                      -\nH265Encode        3840x2160 src: nv12, dst: h.265                              10800                                        -                      -\nH264Decode        3840x2160 src: h.264, dst: nv12                              11104                                        -                      -\nH264Encode        3840x2160 src: nv12, dst: h.264                              10251                                        -                      -","routePath":"/guide/ucp/vp/vp4_Performance","lang":"zh","toc":[{"text":"测试平台","id":"测试平台","depth":2,"charIndex":38},{"text":"测试方法","id":"测试方法","depth":2,"charIndex":177},{"text":"性能对比","id":"性能对比","depth":2,"charIndex":533}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":564,"title":"Bilateral Filter","content":"#\n\n双线性滤波是一种非线性、边缘保留的平滑滤波器，通常来对图片进行平滑处理。它将每个输入图像中附近像素的强度值加权平均值，\n并通过像素间的欧式距离和颜色强度差异计算权重，最终得到保留边缘并平滑相似区域的图片处理效果。\n\n\n算子效果#\n\n输入图像   参数                                                    输出图像\n       kernelSize = 5 sigmaColor = 15.0f sigmaSpace = 2.0f   \n       borderType = HB_VP_BORDER_REPLICATE\n\n\n原理#\n\n双线性滤波的主要计算过程为使用滤波核对输入图片进行卷积，主体公式如下：\n\n$$dst(x)=\\frac\\displaystyle\\sum_$$\n\n其中， $dst$ 是输出图片，$src$ 是输入图片，$x$ 是当前计算的像素，$\\Omega$ 是当前滤波窗口。\n\n$$f_r(x,xi)=e^$$\n\n$$g_s(x,xi)=e^$$\n\n其中，$\\vert$ 为绝对值，$\\Vert$ 为欧氏距离，$\\sigma_r$ 为颜色sigma参数，用于平滑滤波器窗口的强度差异，$\\sigma_d$\n为空间sigma参数，用于平滑跨窗口的空间坐标差异。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPBilateralFilter 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/bilateral_filter","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":111},{"text":"原理","id":"原理","depth":2,"charIndex":291},{"text":"API接口","id":"api接口","depth":2,"charIndex":572},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":616}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":565,"title":"Box Filter","content":"#\n\n盒滤波是一种低通滤波器，它通过将像素点周围的像素加权平均后得到生成像素值，常用来消除图片细节、去噪声、模糊边缘等。\n\n\n算子效果#\n\n输入图像   参数                                                            输出图像\n       kernelHeight = 3 kernelWidth = 3 pointLocX = -1 pointLocY =   \n       -1 normalize = 1 borderType = 0\n\n\n原理#\n\n盒滤波使用以下滤波核，将滤波窗口内的像素值平均加权，计算得到输出图片的像素值：\n\n$$\\begin box_=\\frac\\begin1 & 1 & \\cdots & 1\\1 & 1 & \\cdots & 1\\\\vdots & \\vdots &\n\\ddots & \\vdots\\1 & 1 & \\cdots & 1\\\\end \\end$$\n\n其中 $m$ 和 $n$ 为滤波核尺寸。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPBoxFilter 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/box_filter","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":62},{"text":"原理","id":"原理","depth":2,"charIndex":254},{"text":"API接口","id":"api接口","depth":2,"charIndex":452},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":490}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":566,"title":"Canny","content":"#\n\nCanny算子是一种边缘检测器，它使用多阶段算法来检测图像中的大范围边缘。\n\n\n算子效果#\n\n输入图像   参数                                                        输出图像\n       threshold1 = 100 threshold2 = 400 kernelSize = 3 norm =   \n       HB_VP_NORM_L1 overlap = 64 borderType =\n       HB_VP_BORDER_REPLICATE\n\n\n原理#\n\n算子的计算过程分为多个阶段：\n\n 1. 梯度计算\n\n输入图像在垂直和水平方向上使用Sobel内核进行卷积，核大小由Sobel尺寸参数确定。然后在水平和垂直方向上使用边缘检测过滤器进行过滤，从而产生 Gx 和\nGy。 每个像素的强度和角度计算如下：\n\n$$Intensity=\\sqrt$$\n\n$$Angle=arctan(\\frac)$$\n\n 2. 非极大值抑制 （non-maximum suppression）\n\n对于图像中的每个像素，检查强度是否是梯度方向上的局部最大值。如果是，则将其保留为边缘像素，否则将其作为非边缘像素移除。\n\n 3. 双重阈值化\n\n将每个像素的强度大小与强阈值（ℎ𝑖𝑔ℎ_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑）和弱阈值（𝑙𝑜𝑤_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑）进行比较，如果强度大于强\n阈值，则将其标记为强边缘。如果强度小于弱阈值，则标记为无边缘。强阈值和弱阈值之间的强度值都被标记为弱边缘。\n\n 4. 边缘追踪\n\n如果弱边连接到强边，那么这条弱边就变成了强边。重复这个过程，直到找到所有连接到强边的弱边。然后将其余的弱边标记为非边，就得到了整个图像的边缘检测结果。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPCanny 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/canny","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":42},{"text":"原理","id":"原理","depth":2,"charIndex":264},{"text":"API接口","id":"api接口","depth":2,"charIndex":774},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":808}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":567,"title":"Corner Harris","content":"#\n\n该算法实现了用于检测关键点和推断图像特征的Harris关键点检测算子。\n\n\n算子效果#\n\n输入图像   参数                                                            输出图像\n       blockSize = 3 sensitivity = 0.04 kernelSize = 3borderType =   \n       HB_VP_BORDER_REPLICATE\n\n\n原理#\n\n计算过程如下：\n\n$$dst(x,y)=det(A)-k*trace(A)^2$$\n\n其中，$dst$ 为输出图片。\n\n$$trace(A)=\\displaystyle\\sum_A ^2+\\displaystyle\\sum_A ^2 $$\n\n$$det(A)=\\displaystyle\\sum_A\\displaystyle\\sum_A-(\\displaystyle\\sum_A)^2$$\n\n其中，$G_x$ 为x方向上的sobel卷积，$G_y$ 为y方向上的sobel卷积，$A$ 为卷积窗口。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPCornerHarris 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/corner_harris","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":40},{"text":"原理","id":"原理","depth":2,"charIndex":223},{"text":"API接口","id":"api接口","depth":2,"charIndex":480},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":521}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":568,"title":"Cvtcolor","content":"#\n\n算子将输入图片从一种颜色空间转换到另一种颜色空间，支持色彩通道、格式和深度的转换，常来转换图片格式。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n颜色空间转换的过程中有多种公式可以选择，下面列举一种方法：\n\nRGB to Gray\n\n$$Y=0.299R+0.587G+0.114*B$$\n\nRGB/BGR $\\Leftrightarrow$ YUV420\n\n$$Y=16+0.257R+0.504G+0.098*B$$\n\n$$Cb=128-0.148R-0.291G+0.439*B$$\n\n$$Cr=128+0.439R-0.368G-0.071*B$$\n\n$$R=1.164*(Y-16)+1.596*(Cr-128)$$\n\n$$G=1.164*(Y-16)-0.392*(Cb-128)-0.812*(Cr-128)$$\n\n$$B=1.164*(Y-16)+2.016*(Cb-128)$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPCvtColor 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/cvtcolor","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":55},{"text":"原理","id":"原理","depth":2,"charIndex":94},{"text":"API接口","id":"api接口","depth":2,"charIndex":427},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":464}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":569,"title":"Dilate","content":"#\n\n扩张（膨胀）算法使用提供的 2D 布尔核对输入图像执行二维滤波操作，布尔的核大小定义了滤波操作的像素邻域。\n作为一种形态学操作算子，相当于最大化操作，会增加或扩大图片中的前景区域（在本例中为下图中的白色），识别并填充图片中的孔洞。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n$$\\begindst(x,y)=\\max_\nsrc(x-\\lfloor\\frac\\rfloor+i,y-\\lfloor\\frac\\rfloor+j)\\end$$\n\n形态学操作是一组图像处理操作，它根据图像的形状来处理数字图像。扩张过滤器相当于最大操作，会扩展图像中的明亮结构。\n另一个形态滤波器是Erode，相当于最小化操作。可以在扩张滤波器之后使用腐蚀滤波器来构成图像的形态闭合。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPDilate 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/dilate","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":120},{"text":"原理","id":"原理","depth":2,"charIndex":159},{"text":"API接口","id":"api接口","depth":2,"charIndex":359},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":394}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":570,"title":"Equalizehist","content":"#\n\n直方图均衡化是一种利用图像强度分布的直方图进行对比度调整的图像处理方法。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n要达到均衡效果，需要对图像进行重映射，其累积分布函数计算如下：\n\n$$H'(i)=\\displaystyle\\sum_H(j)$$\n\n其中，$H$ 为输入直方图。\n\n得到累积分布后需要将其规范化，得到函数形式如下：\n\n\n\n最后使用重映射函数得到均衡后的图像，函数如下：\n\n$$dst(x,y)=H'(src(x,y))$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPEqualizeHist 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/equalizehist","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":41},{"text":"原理","id":"原理","depth":2,"charIndex":80},{"text":"API接口","id":"api接口","depth":2,"charIndex":249},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":290}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":571,"title":"Erode","content":"#\n\n侵蚀（腐蚀）算法使用提供的 2D 布尔核对输入图像执行二维滤波操作。 布尔核大小定义了滤波操作的像素邻域。\n作为一种形态学操作算子，相当于最小化操作，会减少或缩小图片中的前景区域（在本例中为下图中的白色），导致不需要的小或薄物体消失，识别并填充图片中的孔洞。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n$$\\begindst(x,y)=\\min_\nsrc(x-\\lfloor\\frac\\rfloor+i,y-\\lfloor\\frac\\rfloor+j)\\end$$\n\n侵蚀过滤器相当于最小操作，会缩小图像中的明亮结构，另一个形态滤波器是Dilate，相当于最大操作。 扩张过滤器可以在腐蚀过滤器之后使用，以构成图像的形态闭合。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPErode 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/erode","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":134},{"text":"原理","id":"原理","depth":2,"charIndex":173},{"text":"API接口","id":"api接口","depth":2,"charIndex":343},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":377}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":572,"title":"Filter2d","content":"#\n\n使用指定的滤波核对二维图像进行滤波（卷积），其中锚点默认为卷积核中心。\n\n\n算子效果#\n\n输入图像   参数                                                           输出图像\n       $$\\beginkernel = \\begin0.00390625& 0.015625& 0.02734375&     \n       0.015625& 0.00390625\\ 0.015625& 0.0625& 0.1015625& 0.0625&\n       0.015625&\\ 0.02734375& 0.1015625& 0.09375& 0.1015625&\n       0.02734375\\ 0.015625& 0.0625& 0.1015625& 0.0625& 0.015625\\\n       0.00390625& 0.015625& 0.02734375& 0.015625&\n       0.00390625\\end\\end$$\n\n\n原理#\n\n$$\\begindst(x,y)=\\sum_kernel(x',y')*src(x+x'-anchor.x,y+y'-anchor.y)\\end$$\n\n其中，$dst$ 为输出图像，$kernel$ 为滤波核，$anchor$ 为锚点坐标，默认锚点为（-1， -1）\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPFilter2D 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/filter2d","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":40},{"text":"原理","id":"原理","depth":2,"charIndex":463},{"text":"API接口","id":"api接口","depth":2,"charIndex":604},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":641}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":573,"title":"Flip","content":"#\n\nFlip算子通过像素的重映射实现了图像翻转的功能，形式包括水平方向翻转和垂直方向翻转。\n\n\n算子效果#\n\n输入图像   参数       输出图像\n       mode=0   \n       mode=1   \n\n\n原理#\n\n水平翻转公式如下：\n\n$$dst(x,y)=src(width-x,y)$$\n\n垂直翻转公式如下：\n\n$$dst(x,y)=src(x,height-y)$$\n\n其中，$dst$ 为输出图像，$src$ 为输入图像，$height$ 为图像高度，$width$ 为图像宽度。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPFlip。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/flip","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":48},{"text":"原理","id":"原理","depth":2,"charIndex":112},{"text":"API接口","id":"api接口","depth":2,"charIndex":257},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":289}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":574,"title":"Gaussian Blur","content":"#\n\n高斯滤波使用特定的滤波核对图片进行卷积，是一种线性平滑滤波，广泛应用于图像处理的噪声消除过程。\n\n\n算子效果#\n\n输入图像   参数               输出图像\n       kernelSize = 3   \n\n\n原理#\n\n3X3高斯滤波核如下：\n\n$$\\beginkernel = \\frac\\begin1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \\end\\end$$\n\n5X5高斯滤波核如下：\n\n$$\\beginkernel = \\frac\\begin1 & 4 & 6 & 4 & 1 \\ 4 & 16 & 24 & 16 & 4 \\ 6 & 24 &\n36 & 24 & 6 \\ 4 & 16 & 24 & 16 & 4 \\ 1 & 4 & 6 & 4 & 1 \\end\\end$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPGaussianBlur。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/gaussian_blur","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":52},{"text":"原理","id":"原理","depth":2,"charIndex":115},{"text":"API接口","id":"api接口","depth":2,"charIndex":368},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":408}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":575,"title":"Integral","content":"#\n\n计算图像的积分图。\n\n\n算子效果#\n\n示意图像\n\n\n\n原理#\n\n算子的原理为累积从原点到点（X, Y）矩形中的像素值，将其作为点（X,Y）处的像素值，计算公式如下：\n\n$$sum(X,Y)=\\displaystyle\\sum_src(x,y)$$\n\n在神经网络中，积分图的常见应用场景为获取一块区域的像素值积分，计算公式如下：\n\n$$\\displaystyle\\sum_image(x,y)=sum(x_2,y_2)-sum(x_1,y_2)-sum(x_2,y_1)+sum(x_1,y_\n1)$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPIntegral 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/integral","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":14},{"text":"原理","id":"原理","depth":2,"charIndex":29},{"text":"API接口","id":"api接口","depth":2,"charIndex":255},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":292}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":576,"title":"ISP","content":"#\n\nISP算子用于将富含原始图像数据的RAW图像转换为视频处理中应用更为广泛的YUV图像。\n\n\n算子效果#\n\n输入图像   参数                                                           输出图像\n       hbVPISPCtxParam.bufNum = 3 hbVPISPCtxParam.bufCached = 0     \n       hbVPISPCtxParam.backend = HB_UCP_ISP_CORE_0\n       hbVPISPCtxParam.width = 1920 hbVPISPCtxParam.height = 1080\n\n\n原理#\n\n在ISP对RAW图像的处理过程中，包括但不限于以下子模块：\n\n * BLC (Black Level Correction)\n   黑电平矫正：sensor的电路本身存在暗电流，导致在没有光线照射的时候，像素单位也有一定的输出电压。BLC通过调整图像传感器输出中的黑电平偏移，使得图像暗\n   部区域更加真实地反应场景的亮度。\n\n * LSC (Lens Shading Correction)\n   镜头阴影校正：在实际图像捕捉过程中，由于镜头对于光的折射不均匀会导致镜头周围出现阴影。LSC对图像亮度和颜色进行校正，在实际图像处理过程中，测量镜头的光线\n   衰减特性，生成一个校正模型并将其应用于图像的每个像素，通过提高边缘像素的亮度以及调整颜色通道的强度，使图像亮度和颜色分布更加均匀。\n\n * DPC (Defect Pixel Correction)\n   坏点校正：由于sensor中的传感器感光阵列工艺存在缺陷，或光信号进行转化的过程中出现错误，导致图像中像素值不准确。DPC通常通过临近插值法、模式匹配法使\n   坏点与周围正常像素一致。\n\n * 2DNR/3DNR (2D/3D Noise Reduce)\n   2D/3D降噪：图像噪声通常是信号在采集、传输及记录过程中，受到成像设备自身因素和外界环境的影响而产生的。ISP中包含了2D和3D降噪模块，分别对单帧图像\n   进行空间降噪，同时使用时间域滤波技术减少时域噪声。\n\n * Demosaic:\n   CFA插值：将单色图像传感器（如拜耳滤波器阵列）捕获的原始像素数据转换为全彩色图像。大多数数字相机和图像传感器都使用拜耳滤波器，它将传感器的像素分成红、绿\n   、蓝三个颜色通道。Demosaicing 的过程就是从这些单色采样数据中恢复出完整的 RGB 彩色图像。\n\n * CSC (Color space conversion)\n   颜色空间转换：CSC模块将图像数据格式由RGB转换成YUV。颜色空间转换通常通过矩阵变换或非线性变换来实现。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPISP。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/isp","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":48},{"text":"原理","id":"原理","depth":2,"charIndex":316},{"text":"API接口","id":"api接口","depth":2,"charIndex":1189},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":1220}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":577,"title":"JPEG Codec","content":"#\n\n\n算子效果#\n\n示意图像\n\n\n\n原理#\n\nJPEG（Joint Photographic Experts\nGroup）是一个广泛使用的有损压缩标准，由联合图像专家小组开发，并已成为国际通用的图像压缩标准。\nJPEG不仅适用于静态图像编码，还被推广用于电视图像序列的帧内压缩。JPU硬件支持的JPEG标准是ISO/IEC 10918-1的Baseline\nSequential模式。\n\n\nJPEG编码原理#\n\nJPEG编码，将YUV格式图片编码成JPEG压缩格式的图片文件，例如*.jpg。\n\n示意图像\n\n\n\nJPEG解码原理#\n\nJPEG解码，实现.jpg、.jpeg、.JPG、.JPEG图片文件的解码。\n\n示意图像\n\n\n以图像的某个8x8子区域为例，介绍JPEG编码的主要过程。8x8图像子区域数值如下所示：\n\n$$\\begin\\begin 52 & 55 & 61 & 66 & 70 & 61 & 64 & 73\\ 63 & 59 & 55 & 90 & 109 &\n85 & 69 & 72\\ 62 & 59 & 68 & 113 & 144 & 104 & 66 & 73\\ 63 & 58 & 71 & 122 & 154\n& 106 & 70 & 69\\ 67 & 61 & 68 & 104 & 126 & 88 & 68 & 70\\ 79 & 65 & 60 & 70 & 77\n& 68 & 58 & 75\\ 85 & 71 & 64 & 59 & 55 & 61 & 65 & 83\\ 87 & 79 & 69 & 68 & 65 &\n76 & 78 & 94\\ \\end\\end$$\n\n首先对8x8子区域进行二维的离散余弦变化（Discrete Cosine Transform，DCT），目的是将YUV颜色空间转换至频域空间。\n由于离散余弦变化所接受的数值范围是[-128, 127]，故将8x8图像子区域数值减去-128，得到如下矩阵：\n\n$$\\begin\\begin -76&-73&-67&-62&-58&-67&-64&-55\\ -65&-69&-73&-38&-19&-43&-59&-56\\\n-66&-69&-60&-15&16&-24&-62&-55\\ -65&-70&-57&-6&26&-22&-58&-59\\\n-61&-67&-60&-24&-2&-40&-60&-58\\ -49&-63&-68&-58&-51&-60&-70&-53\\\n-43&-57&-64&-69&-73&-67&-63&-45\\ -41&-49&-59&-60&-63&-52&-50&-34 \\end\\end$$\n\n标准化后矩阵进行DCT，变换公式：\n\n$$F(u,v)=\\cfracC(u)C(v)[\\displaystyle\\sum_^\n\\displaystyle\\sum_^f(m,n)cos\\cfraccos\\cfrac]$$\n\n变换后得到DCT系数矩阵：\n\n$$\\begin\\begin -415&-30&-61&27&56&-20&-2&0\\ 4&-22&-61&10&13&-7&-9&5\\\n-47&7&77&-25&-29&10&5&-6\\ -49&12&34&-15&-10&6&2&2\\ 12&-7&-13&-4&-2&2&-3&3\\\n-8&3&2&-6&-2&1&4&2\\ -1&0&0&-2&-1&-3&4&-1\\ 0&0&-1&-4&-1&0&1&2\\end\\end$$\n\n其中（0，0）点处的系数称作直流分量（DC系数），其余63个点的系数称为交流分量（AC系数）。\n\n其次对亮度和色度分量的DCT系数进行量化，即DCT系数除以量化表后按四舍五入取最接近的整数。\n由于人眼对亮度信号比对色差信号更敏感，因此使用了亮度分量和色度分量两种量化表，默认的量化表是从广泛的实验中得出，也可自定义量化表。\n\n亮度分量默认的量化表：\n\n$$\\begin\\begin 16& 11& 10& 16& 24& 40& 51& 61 \\ 12& 12& 14& 19& 26& 58& 60& 55 \\\n14& 13& 16& 24& 40& 57& 69& 56\\ 14& 17& 22& 29& 51& 87& 80& 62\\ 18& 22& 37& 56&\n68& 109& 103& 77\\ 24& 35& 55& 64& 81& 104& 113& 92\\ 49& 64& 78& 87& 103& 121&\n120& 101\\ 72& 92& 95& 98& 112& 100& 103& 99\\ \\end\\end$$\n\n色度分量默认的量化表：\n\n$$\\begin\\begin 17 & 18 & 24 & 47 & 99 & 99 & 99 & 99\\ 18 & 21 & 26 & 66 & 99 &\n99 & 99 & 99\\ 24 & 26 & 56 & 99 & 99 & 99 & 99 & 99\\ 47 & 66 & 99 & 99 & 99 & 99\n& 99 & 99\\ 99 & 99 & 99 & 99 & 99 & 99 & 99 & 99\\ 99 & 99 & 99 & 99 & 99 & 99 &\n99 & 99\\ 99 & 99 & 99 & 99 & 99 & 99 & 99 & 99\\ 99 & 99 & 99 & 99 & 99 & 99 & 99\n& 99\\ \\end\\end$$\n\n将前面所得到的DCT系数矩阵与默认值的亮度表进行相除，并四舍五入后可得到量化后的DCT系数：\n\n$$\\begin\\begin -26 & -3 & -6 & 2 & 2 & -1 & 0 & 0\\ 0 & -2 & -4 & 1 & 1 & 0 & 0 &\n0\\ -3 & 1 & 5 & -1 & -1 & 0 & 0 & 0\\ -4 & 1 & 2 & -1 & 0 & 0 & 0 & 0\\ 1 & 0 & 0\n& 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 &\n0\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ \\end\\end$$\n\n观察到量化后的DCT系数量化后的数据，直流系数相对于交流系数更大一些，并且交流系数种含有大量的0。\n因此使用Z字形编码可将大量的0连接到一起，以减小编码后的大小。 主要思路就是从量化后的DCT系数左上角第一个像素开始以Z字形进行编排：\n\n示意图像\n\n\n由于Z字形编码后的DCT系数的直流系数数值较大，并且相邻8x8图像区域的直流系数值变化不大，因此使用差分脉冲编码技术，对相邻图像区域之间DC系数的差值进行编码；\n对交流系数重复且连续出现多次的字符，使用行程长度编码。这两种编码方式都有中间格式，目的是进一步减小存储量。\n\n得到 DC 系数的中间格式和 AC 系数的中间格式之后，为进一步压缩图像数据，需要对两者进行熵编码，通过对出现概率较高的字符采用较小的 bit\n数编码达到压缩的目的。 JPEG 基本系统规定采用 Huffman 编码方法， Huffman 编码时 DC 系数与 AC 系数分别采用不同的 Huffman\n编码表，对于亮度和色度也采用不同的 Huffman 编码表。 因此，需要 4 张 Huffman 编码表才能完成熵编码的工作，等到具体 Huffman\n编码时再采用查表的方式来高效地完成。 然而，在 JPEG 标准中没有定义缺省的 Huffman 表，您可以根据实际应用自由选择，也可以使用 JPEG 标准推荐的\nHuffman 表，或者预先定义一个通用的 Huffman 表，也可以针对一副特定的图像，在压缩编码前通过搜集其统计特征来计算 Huffman 表的值。\n\n\nAPI接口#\n\n\nJPEG编码接口#\n\n\n\n\nJPEG解码接口#\n\n\n\n详细接口信息请查看 hbVPJPEGEncode 及 hbVPJPEGDecode 。\n\n\n使用方法#\n\n\nJPEG编码使用方法#\n\n\n\n\nJPEG解码使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/jpeg_codec","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":3},{"text":"原理","id":"原理","depth":2,"charIndex":18},{"text":"JPEG编码原理","id":"jpeg编码原理","depth":3,"charIndex":195},{"text":"JPEG解码原理","id":"jpeg解码原理","depth":3,"charIndex":256},{"text":"API接口","id":"api接口","depth":2,"charIndex":3274},{"text":"JPEG编码接口","id":"jpeg编码接口","depth":3,"charIndex":3283},{"text":"JPEG解码接口","id":"jpeg解码接口","depth":3,"charIndex":3297},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":3356},{"text":"JPEG编码使用方法","id":"jpeg编码使用方法","depth":3,"charIndex":3364},{"text":"JPEG解码使用方法","id":"jpeg解码使用方法","depth":3,"charIndex":3380}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":578,"title":"Laplacian Filter","content":"#\n\n拉普拉斯滤波是是一种常用于图像处理中的二阶导数算子，主要用于边缘检测。它通过计算图像灰度值的二阶导数来突出图像中的区域变化。由于它对噪声非常敏感，通常会先对图像\n进行高斯平滑处理，以减少噪声的影响。Laplacian\n算子的计算涉及到一个卷积核在图像上进行卷积操作，从而得到图像中每个像素的二阶导数值，这些值表示图像中像素值变化的速率。这个过程可以有效地检测出图像中的边缘。\n\n\n算子效果#\n\n输入图像   参数                                                            输出图像\n       kernelSize = 1 borderType = HB_VP_BORDER_CONSTANT normalize   \n       = 0\n\n\n原理#\n\n拉普拉斯滤波的主要计算过程为使用特定滤波核对输入图片进行卷积，主体公式如下：\n\n$$ = \\Delta \\text = \\frac + \\frac$$\n\n其中， $dst$ 是输出图片，$src$ 是输入图片。当前VP只支持OpenCV ksize = 1，即[0, 1, 0, 1, -4, 1, 0, 1,\n0]。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPLaplacianFilter 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/laplacian_filter","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":192},{"text":"原理","id":"原理","depth":2,"charIndex":356},{"text":"API接口","id":"api接口","depth":2,"charIndex":523},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":567}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":579,"title":"Median Blur","content":"#\n\n中值滤波使用提供的2D滤波核对输入图像执行二维滤波操作，滤波核的尺寸即为滤波操作使用的像素邻域范围。\n过滤器执行非线性操作，计算方式为按照滤波核权重把输入像素值加权计算，并取中值作为输出的结果。中值滤波操作常被用于脉冲降噪、图像平滑、分析等。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n中值滤波计算公式如下：\n\n\n\n其中，$dst$ 为输出图片，$src$ 为输入图片，$K$ 为滤波核，$k_w, k_h$ 为滤波核的宽和高。 具体计算过程如图所示：\n\n\n\n其中5X5的一种滤波核如下：\n\n\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPMedianBlur。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/median_blur","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":126},{"text":"原理","id":"原理","depth":2,"charIndex":165},{"text":"API接口","id":"api接口","depth":2,"charIndex":276},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":314}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":580,"title":"OpticalFlowPyrLK","content":"#\n\nOpticalFlowPyrLK算子用于计算连续多帧图像特定像素点的变化，常用于目标特征点跟踪、提升BOX稳定性等场景。\n\n\n算子效果#\n\n输入连续帧图像   参数   输出图像\n          -    \n\n\n原理#\n\n稀疏光流算法即Lucas-Kanade算法，是一种计算机视觉中常用的运动估计方法，用于估计图像序列中像素的运动方向和速度。Lucas-Kanade算法基于两个假\n设：\n\n 1. 亮度恒定假设：在短时间内，同一个物体的像素亮度保持不变。\n\n假设 $$t$$ 时刻，位于 $$(x,y)$$ 像素位置的物体，在 $$t+Δt$$ 时刻位于 $$(x+u,y+v)$$ 位置，基于亮度不变假设有:\n\n$$I(x,y,t)=I(x+u,y+v,t+\\Delta t)$$\n\n将等式右边进行一阶泰勒展开得：\n\n$$I(x+u,y+v,t+\\Delta t)=I(x,y,t) + I'_xu + I'_yv+I'_t\\Delta t$$\n\n由上面两个公式可以得到：\n\n$$I'_xu + I'_yv+I'_t\\Delta t=0$$\n\n整理后可表示成：\n\n$$\\begin\\begin I'_x , I'_y \\end\\begin u \\v \\end=-\\Delta I_t \\end$$\n\n其中，$$I'_x$$ ， $$I'_y$$ 分别为 $$(x,y)$$ 像素点处图像亮度在 $$x$$ 方向和 $$y$$\n方向的偏导数，即图像$$x$$和$$y$$方向的梯度。$$ΔI_t$$ 即为两图之间的 $$(x,y)$$ 坐标位置的亮度差。\n\n 2. 邻域光流相似假设：一个小的图像区域里像素移动方向和大小是基本一致的。\n\n借助该假设，像素点 $$(x,y)$$ 领域内的所有像素都有下面的公式:\n\n$$\\begin\\begin I' , I' \\I' , I' \\end\\begin u \\v \\end= \\begin -\\Delta I_ \\-\\Delta\nI_ \\end\\end$$\n\n上式即为 $$Ax=b$$ 的形式，可求得光流向量的最小二乘解：\n\n$$x=(A^TA)^A^Tb$$\n\n其中要求 $$A^TA$$\n可逆，为了满足这个要求，Lucas-Kanade方法选取角点作为特征点。除了基于亮度不变假设和邻域光流相似假设，Lucas-Kanade算法还借助了图像金字塔的\n方式解决图像偏移较大的情况，在高层低分辨率图像上，大的偏移将变为小的偏移，从而求解出光流。\n\n因此，OpticalFlowPyrLK算子需要前后两帧的金字塔图层，和前一帧的特征点作为输入，其中特征点通常选用角点。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPOpticalFlowPyrLK。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/opticalflowpyrlk","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":65},{"text":"原理","id":"原理","depth":2,"charIndex":110},{"text":"API接口","id":"api接口","depth":2,"charIndex":1115},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":1159}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":581,"title":"PyrDown","content":"#\n\nPyrdowm算子用于获得图像的不同分辨率表达。当前该算子支持的插值方式有两种：高斯插值和双线性插值。\n两种插值方式所对应的金字塔分别称为高斯金字塔和双线性金字塔。\n\n * 高斯金字塔通过高斯平滑、采样等流程将图片进行降采样，生成长宽为原图一半大小的图像。\n * 双线金字塔直接通过双线性插值、降采样等流程生成长宽为当前层的上一层图像一半大小的新图像。\n * 当前金字塔支持5层计算。\n\n\n算子效果#\n\n输入图像   参数                                                     输出图像\n       hbVPPymParam.levels = 5 hbVPPymParam.Interpolation =   \n       HB_VP_INTER_LINEAR\n\n\n原理#\n\n根据下采样原理，可以分为高斯下采样和双线性下采样两种方法。\n\n高斯金字塔的下采样的实现流程为先对原图进行高斯平滑，再将偶数行列删除，最终得到目标图像。\n\n在高斯平滑阶段，使用的高斯滤波核如下：\n\n$$\\beginkernel = \\frac\\begin 1 & 4 & 6 & 4 & 1 \\ 4 & 16 & 24 & 16 & 4 \\ 6 & 24 &\n36 & 24 & 6 \\ 4 & 16 & 24 & 16 & 4 \\ 1 & 4 & 6 & 4 & 1 \\end\\end$$\n\n双线性金字塔使用双线性插值的方式完成图像的下采样计算，双线性插值的原理如下图所示：\n\n\n\n在插值过程中，假设要输出的插值像素点为P，P临近的四个待插值源点P1，P2，P3，P4之间的距离为1。由于插值方式为线性插值，所以水平方向和竖直方向的插值顺序不\n影响最终结果。 具体的插值点计算公式如下：\n\n$$P_ = (1 - a) * P_1 + a * P_2$$\n\n$$P_ = (1 - a) * P_3 + a * P_4$$\n\n$$P = (1 - a) * (1 - b) * P_1 + a * (1 - b) * P_2 + (1 - a) * b * P_3 + a * b *\nP_4$$\n\n公式中各个点坐标的系数可以认为是插值源点对于输出像素点的权重值。在该算子中所介绍的pyramid接口使用的双线性插值默认输出点到每个插值源点的距离相等，\n对输入的图像数据的长宽尺寸进行二分之一的下采样。在获得一个2*2的图像区域数据后，进行相应的均值化处理，得到一个结果像素。\n\n该算子下采样的层数最大可达到5层，每一层图像尺寸的宽度和高度均为上一层的1/2。具体的计算方式如下：\n\n$$BL_(i,j) = \\left( \\text(2i, 2j) + \\text(2i + 1, 2j) + \\text(2i, 2j + 1) +\n\\text(2i + 1, 2j + 1) + 2 \\right) >> 2$$\n\n其中，SRC为输入图像像素，i、j为输出bl层的像素坐标，BL为当前层输出的双线性下采样结果。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPPyrDown。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/pyrdown","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":197},{"text":"原理","id":"原理","depth":2,"charIndex":362},{"text":"API接口","id":"api接口","depth":2,"charIndex":1277},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":1312}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":582,"title":"PyrUp","content":"#\n\nPyrup算子通过平滑、采样等流程将图片进行上采样，生成长宽为原图两倍大小的图片，常用于构建图像金字塔、获取高分辨率原图等场景。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n上采样的实现流程为先对原图进行扩充，在原图的偶数行列上插入0，再对图像进行高斯平滑，最终得到目标图像。长宽符合以下约束：\n\n$$dst.width=src.width*2$$\n\n$$dst.height=src.height*2$$\n\n在高斯平滑阶段，使用的高斯滤波核如下：\n\n$$\\beginkernel = \\frac\\begin 1 & 4 & 6 & 4 & 1 \\ 4 & 16 & 24 & 16 & 4 \\ 6 & 24 &\n36 & 24 & 6 \\ 4 & 16 & 24 & 16 & 4 \\ 1 & 4 & 6 & 4 & 1 \\end\\end$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPPyrUp。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/pyrup","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":69},{"text":"原理","id":"原理","depth":2,"charIndex":108},{"text":"API接口","id":"api接口","depth":2,"charIndex":401},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":434}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":583,"title":"Remap","content":"#\n\nRemap算子根据传入的特定映射关系，将图像按照输入信息进行重映射，可以应用于图像在2D空间、3D空间各种变换的场景。\n该算子相较于特定的图像变换算子，例如transpose、resize，由于加速实现不同，通常在执行效率上更低一些。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n重映射算子的原理为根据传入参数map中的映射关系将输入图片重新排布在输出图片上，实现图片的重构，主体公式如下：\n\n$$dst(x,y)=src(map_x(x,y),map_y(x,y))$$\n\n其中， $dst$ 是输出图片，$src$ 是输入图片。\n\n\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPRemap。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/remap","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":122},{"text":"原理","id":"原理","depth":2,"charIndex":161},{"text":"API接口","id":"api接口","depth":2,"charIndex":297},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":330}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":584,"title":"Resize","content":"#\n\nResize算子通过插值算法，将图片按照需求缩放到目标尺寸，用以调整图片的大小。\n\n\n算子效果#\n\n输入图像   参数                      输出图像\n       xscale=1.5 yscale=1.5   \n\n\n原理#\n\n算子公式如下，根据不同的插值算法有不同的实现细节：\n\n$$\\begin\\begindst_=xscalesrc_\\dst_=yscalesrc_\\end\\end$$\n\n对双线性插值方法，其像素映射原理如下：\n\n\n\n$$dst(x,y)=Q*(1-b)+R*b$$\n\n$$Q=src(m,n)*(1-a)+src(m,n+1)*a$$\n\n$$R=src(m+1,n)*(1-a)+src(m+1,n+1)*a$$\n\n其中，$src$ 为输入图片，$dst$ 为输出图片，$m,n,x,y$ 为像素点坐标。\n\n对最邻近插值方法，其像素映射原理如下：\n\n$$\\begin\\beginsrc_x=xscaledst_x\\src_y=yscaledst_y\\end\\end$$\n\n其中，$x,y$ 为图片的坐标。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPResize 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/resize","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":45},{"text":"原理","id":"原理","depth":2,"charIndex":122},{"text":"API接口","id":"api接口","depth":2,"charIndex":481},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":516}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":585,"title":"Roi Resize","content":"#\n\nRoiResize算子功能为将输入图片按照ROI区域进行裁剪，然后将裁剪后的ROI区域等比例缩放到预定目标尺寸，最后对短边部分按照预置参数进行padding，\n常用于对图片中的ROI区域提取并调整输出尺寸。算子支持双线性插值和最临近插值两种方式，不同的插值方式生成的图片结果也会有所差异。\n\n\n算子效果#\n\n输入图像   参数     输出图像\n       ROI=   \n\n\n原理#\n\n算子实现原理如下：\n\n其中，resize过程请参考 hbVPResize 算子。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPRoiResize。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/roi_resize","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":149},{"text":"原理","id":"原理","depth":2,"charIndex":192},{"text":"API接口","id":"api接口","depth":2,"charIndex":240},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":277}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":586,"title":"Rotate","content":"#\n\nRotate算子通过建立图像间像素的映射关系，计算得到旋转一定角度的输入图像，常用来进行图片调整。\n\n\n算子效果#\n\n输入图像   参数                    输出图像\n       90clockwise           \n       180clockwise          \n       90counter_clockwise   \n\n\n原理#\n\n算子实现原理如下：\n\n根据不同旋转角度，将输入图片映射到不同的旋转位置，算子的长宽符合以下约束：\n\n$\\begindst.width=src.width\\dst.height=src.height\\end180clockwise$\n\n$\\begindst.height=src.width\\dst.width=src.height\\end90clockwise、90counter_clockw\nise$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPRotate。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/rotate","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":54},{"text":"原理","id":"原理","depth":2,"charIndex":187},{"text":"API接口","id":"api接口","depth":2,"charIndex":397},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":431}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":587,"title":"SepFilter2d","content":"#\n\n算子对图像应用可分离的线性滤波器，把对二维图像的滤波分解为横向一维滤波和纵向一维滤波。每一行与一维的滤波核kernelX做卷积，\n然后运算结果的每一列与一维的滤波核kernelY做卷积。可分解的滤波核包括但不限于Sobel导数核、高斯滤波核、盒式滤波核、中值滤波核等。\n\n\n算子效果#\n\n输入图像   参数                                                         输出图像\n       kernel=[0.0833333,0.0833333,0.66666,0.0833333,0.0833333]   \n\n\n原理#\n\n将一个二维滤波核拆解为两个可分离的一维滤波核，可以提高计算效率。具体原理为一个可以分解的滤波核可以理解成两个一维核，\n在卷积时先调用x滤波核，然后调用y滤波核。两个矩阵进行卷积所产生的消耗可以用两个矩阵的面积的积来估算，如此一来，\n用n×n的卷积核对面积为A的图像进行卷积所需的时间是A*n^2，但如果分解成 n×1 和 1×n 的两个核，那么代价就是 A*n + A*n =\n2A*n，因此分解卷积核可以提高卷积计算的效率。\n\n注解\n\n只要n不小于3，这种计算方式就能提高效率，并且随着n的增大，这种效益愈发明显。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPSepFilter2D 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/sepfilter2d","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":139},{"text":"原理","id":"原理","depth":2,"charIndex":286},{"text":"API接口","id":"api接口","depth":2,"charIndex":552},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":592}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":588,"title":"Sobel","content":"#\n\nsobel算子用于获得数字图像的一阶梯度图像，常见的应用是边缘检测。算子根据图像中每个像素的上下左右四领域的灰度值加权差，在边缘处达到极值从而检测边缘的目的。\n\n\n算子效果#\n\n输入图像   参数                                                            输出图像\n       $$\\beginkernel = \\begin-1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1   \n       \\end\\end$$\n\n\n原理#\n\n3X3卷积核如下所示，其只适用于一个方向上的梯度计算。第一个卷积核适用于X方向上的卷积，当像素点左侧和右侧像素值差距较大时，\n卷积后才会得到较大的目标像素值，即凸显的轮廓。第二个卷积核适用于Y方向上的卷积，原理相同。\n\n$$\\begin&kernel\\quad X\\quad direction = \\begin-1 & 0 & 1\\ -2 & 0 & 2\\ -1 & 0 &\n1\\end\\ &kernel\\quad Y\\quad direction = \\begin-1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 &\n1 \\end\\end$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPSobel。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/sobel","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":84},{"text":"原理","id":"原理","depth":2,"charIndex":255},{"text":"API接口","id":"api接口","depth":2,"charIndex":544},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":577}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":589,"title":"Stitch","content":"#\n\n通过拼接，实现将一张图片与另一张图片拼接融合的效果，常用于APA和全景图拼接等场景。\n\n\n算子效果#\n\n\n原理#\n\n如果是拼接模式，将两张图做拼接粘贴至dst图层，那么公式如下：\n\n$$dst(x, y) = src0(x, y) * alpha + src1(x, y) * (255-alpha)$$\n\n其中， dst 是输出图片，src0 是输入图片0，src1 是输入图片1。\nsrc0像素点乘以alpha系数，另一张图的对应像素点乘以beta系数，将两个结果相加，更新dst图层的像素点。\n\n如果是粘贴原图模式，将一张图粘贴至dst图层，那么公式如下：\n\n$$dst(x, y) = src(x, y)$$\n\n其中， dst 是输出图片，src 是输入图片。 使用src像素点，更新dst图层的像素点。\n\nlut表，就是由上述n个alpha组成的，能够表示w*h区域每个像素点融合情况的表。通过配置每个像素点不同的alpha值，可以实现不同的拼接值。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPStitch 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/stitch","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":47},{"text":"原理","id":"原理","depth":2,"charIndex":55},{"text":"API接口","id":"api接口","depth":2,"charIndex":435},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":470}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":590,"title":"Threshold","content":"#\n\n阈值函数，通过不同的阈值方法对图像进行处理。该函数将固定级别的阈值应用于多通道数组，通常用于从灰度图像中获取二值图像或用于去除噪声，即过滤掉值过小或过大的像素。\n该函数当前支持HB_VP_THRESH_TOZERO类型的阈值。\n\n\n算子效果#\n\n输入图像   参数                                                 输出图像\n       type = HB_VP_THRESH_TOZERO thresh = 0 maxVal = 0   \n\n\n原理#\n\n不同的阈值类型有不同的处理效果，其原理如下：\n\n原理表达公式如下：\n\n$$\\begin\\beginTHRESH\\quad BINARY : \\quad dst(x,y)&=\\begin maxval & if\nsrc(x,y)>thresh \\ 0 & otherwise \\end \\THRESH\\quad BINARY\\quad INV : \\quad\ndst(x,y)&=\\begin 0 & if src(x,y)>thresh \\ maxval & otherwise \\end \\THRESH\\quad\nTRUNC : \\quad dst(x,y)&=\\begin threshold & if src(x,y)>thresh \\ src(x,y) &\notherwise \\end \\THRESH\\quad TOZERO: \\quad dst(x,y)&=\\begin src(x,y) & if\nsrc(x,y)>thresh \\ 0 & otherwise \\end \\THRESH\\quad TOZERO\\quad INV: \\quad\ndst(x,y)&=\\begin 0 & if src(x,y)>thresh \\ src(x,y) & otherwise \\end\\end\\end$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPThreshold。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/threshold","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":118},{"text":"原理","id":"原理","depth":2,"charIndex":249},{"text":"API接口","id":"api接口","depth":2,"charIndex":813},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":850}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":591,"title":"Transpose","content":"#\n\n通过坐标关系间的映射，实现输入图片的转置。常用于图片调整。\n\n\n算子效果#\n\n输入图像   参数   输出图像\n       -    \n\n\n原理#\n\n算子原理为图像像素间的映射，映射原理如下：\n\n输入输出图片的尺寸符合以下约束：\n\n$$\\begin&dst.width&=src.height \\&dst.height&=src.width\\end$$\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPTranspose 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/transpose","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":34},{"text":"原理","id":"原理","depth":2,"charIndex":73},{"text":"API接口","id":"api接口","depth":2,"charIndex":182},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":220}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":592,"title":"Video Codec","content":"#\n\n\n原理#\n\n视频编码的原理是对视频信号进行冗余信息的消除和压缩，以减少数据量并方便存储和传输。\n视频编码的实现通常依赖于具体的视频编码标准，如H.264、H.265等。这些标准规定了具体的编码算法和参数，以实现高效的视频压缩和传输。\n\n\nVideo编码原理#\n\n以H.265编码协议为例，视频编码涉及以下关键步骤：\n\n 1. 分块\n\nH.265首先将视频划分为若干个序列，一个序列划分为若干个图像组（GOP，Group of Picture），每个GOP代表一组连续的视频帧。\n\n 2. 空间预测\n\nH.265使用空间预测来去除图像块之间的冗余信息。 如下图所示，从空间的角度看，单个视频帧内部的像素点之间的像素值相差很小。\n从时间的角度看，两个连续的视频帧之间也有很多相同的像素点。\n\n空间采样示意图像   时间采样示意图像\n           \n\n预测编码是基于图像统计特性进行数据压缩的一种方法，利用图像在时间和空间上的相关性，通过已经重建的像素数据预测当前正在编码的像素。\n\n帧内预测是指用于预测的像素和当前正在编码的像素均处于同一个视频帧内，且一般都在邻近的区域内。\n由于邻近的像素之间有很强的相关性，像素值一般都非常接近，发生突变的概率非常小，差值均为0或者非常小的数。\n所以，帧内预测编码后传输的是预测值和真实值之间的差值，即0附近的值，称为预测误差或残差。 这样就可以用较少的比特传输，达到压缩的效果。\n\nH.265帧内预测编码以块为单位，使用相邻已经重建的块的重建值对正在编码的块进行预测。 预测分量分为亮度和色度两个，对应的预测块分别是亮度预测块和色度预测块。\n为了适应高清视频的内容特征，提高预测精度，H.265采用了更加丰富的预测块尺寸和预测模式。\n\n帧间预测是指用于预测的像素和当前正在编码的像素不在同一个视频帧内，但是一般在相邻或附近的位置。一般情况下，帧间预测编码的压缩效果要比帧内预测好，主要原因是视频帧\n之间的相关性非常强。 如果视频帧中的运动物体变化速度很慢，那么视频帧之间的像素差值也就很小，时间冗余度就非常大。\n\n帧间预测评估运动物体运动状况的方法是运动估计，它的主要思想就是对预测块从参考帧的给定范围中搜索匹配块，计算匹配块和预测块之间的相对位移，该相对位移就是运动矢量。\n得到运动矢量后，需要对预测修正，也就是运动补偿。将运动矢量输入到运动补偿模块，\"补偿\"参考帧，即可得到当前编码帧的预测帧。预测帧和当前帧的差，就是帧间预测误差。\n\n如果帧间预测只用到了前一帧图像，就称为前向帧间预测或单向预测。该预测帧也就是P帧，P帧可以参考前面的I帧或者P帧。\n\n如果帧间预测不仅用到了前一帧图像预测当前块，还用到了后一帧图像，那么就是双向预测。该预测帧也就是B帧，B帧可以参考前面的I帧或P帧和后面的P帧。\n由于P帧需要参考前面的I帧或P帧，而B帧需要参考前面I帧或P帧和后面的P帧，如果在一个视频流中，先到了B帧，而依赖的I帧、P帧还没有到，\n那么该B帧还不能立即解码，那么应该怎么保证播放顺序呢？\n\n其实，在视频编码时，会生成PTS和DTS。通常情况下，编码器在生成一个I帧后，会向后跳过几个帧，用前面的I帧作为参考帧对P帧编码，I帧和P帧之间的帧被编码为B帧\n。 推流的视频帧顺序在编码的时候就已经按照I帧、P帧、B帧的依赖顺序编好了，收到数据后直接解码即可。所以，不可能先收到B帧，再收到依赖的I帧和P帧。\n\n示意图像\n\n\n 3. 变换和量化\n\n在H.265中，变换和量化是用来进一步压缩数据的。通过将预测误差进行变换，可以将数据从时域转换到频域，从而更好地去除数据冗余。\n然后，对变换后的数据进行量化，将数据映射到较低的精度，从而进一步压缩数据。其过程可参考JPEG编码过程。\n\n4.熵编码\n\n在编码过程的最后一步，H.265使用熵编码来对数据进行无损压缩。熵编码的主要目的是最小化编码后数据的冗余，以提高数据压缩效率。其过程可参考JPEG编码过程。\n\n\nVideo解码原理#\n\n视频解码指将压缩的视频数据转换回原始视频格式的过程，视频解码的过程主要分为熵解码、逆量化、逆变换、运动补偿和解卷积以及后处理，每个步骤都是为了从高度压缩的数据中\n恢复尽可能接近原始视频的画面。 由于视频编码的目的是尽可能减少文件大小，解码过程必须精确地逆向执行编码过程中的每一个步骤，以恢复视频内容：\n\n 1. 熵解码（Entropy Decoding）\n\n熵解码是将压缩数据转换回更易于处理的视频格式的过程。在视频编码的过程中，通常会使用霍夫曼编码或算术编码等熵编码技术来减少数据量，\n而熵解码的目的就是为了恢复视频编码过程中使用的符号，为下一步的逆量化做准备。\n\n 2. 逆量化（Inverse Quantization）\n\n逆量化是将量化（编码过程中减少数据精度以实现节省空间的步骤）翻转以便恢复原始数据精度的过程，该步骤对于恢复图像质量至关重要。\n\n 3. 逆变换（Inverse Transform）\n\n逆变换是逆转编码时使用的变化（例如离散余弦变换，DCT）以将数据从变换域（例如频率域）恢复到空间域（即原始图像）的过程，该步骤是图像重建的关键步骤。\n\n 4. 运动补偿和解卷积（Motion Compensation & Deblocking）\n\n运行补偿是指，对于预测帧（即基于前/后一帧生成的帧），需要使用运行矢量数据对完整帧进行重建。解卷积则是去除编码过程中产生的块效应（block\nartifacts）的过程。 这些步骤对于恢复流畅且连贯的视频播放至关重要。\n\n 5. 后处理（Post-Processing）\n\n后处理作为最后一步，会涉及一些去噪、锐化等提升视频质量的技术，该步骤可选，是否进行取决于视频播放的要求和硬件能力能否支撑。\n\n\nAPI接口#\n\n\nVideo编码接口#\n\n\n\n\nVideo解码接口#\n\n\n\n详细接口信息请查看 hbVPVideoEncode 及 hbVPVideoDecode。\n\n\n使用方法#\n\n\nVideo编码使用方法#\n\n\n\n\nVideo解码使用方法#\n\n\n\n\n补充说明#\n\n码率控制模式#\n\n编码器支持H.264和H.265协议的码率控制，并提供CBR、VBR、AVBR、FixQp和QpMap五种码率控制方式，分别适用于H.264和H.265编码通道\n。\nCBR（恒定码率）能够保证整体编码码率的稳定性；VBR（可变码率）确保编码图像质量的稳定性；AVBR（自适应码率）在码率和图像质量之间找到平衡，使两者相对稳定；\nFixQp模式下，固定每个I帧和P帧的QP值；QpMap为每帧图像中的每个块指定QP值，其中H.265的块大小为32x32，H.264的块大小为16x16。以下\n将以H.265协议为例介绍码率参数。\n\n 1. CBR说明\n\nCBR（恒定码率）表示编码过程中保持码率的稳定，确保视频流的整体编码码率恒定。以下是CBR模式下各个参数的含义：\n\n数据项           描述                                         取值范围          推荐值\nintraPeriod   I帧间隔                                       [0, 2047]     28\nbitRate       目标平均比特率，单位是kbps                            [1, 700000]   1000\nframeRate     目标帧率，单位是fps                                [1, 240]      30\ninitialRcQp   指定码率控制时的初始QP值。当该值不在[0,51]范围内，编码器内部会决定初始值   [0, 51]       63\n\n 2. VBR说明\n\nVBR（可变码率）表示编码过程中根据场景的复杂性动态调整码率。在简单场景下，分配较大的QP（量化参数）以实现更高的压缩率；\n在复杂场景下，分配较小的QP以保证图像质量的稳定性。以下是VBR模式下各参数的含义：\n\n数据项           描述            取值范围        推荐值\nintraPeriod   I帧间隔          [0, 2047]   28\nintraQp       I帧的QP值        [0, 51]     30\nframeRate     目标帧率，单位是fps   [1, 240]    30\n\n 3. AVBR说明\n\nAVBR（恒定平均目标码率）是一种在编码过程中保持平均码率稳定的策略。它结合了CBR和VBR的优点：在简单场景下分配较低的码率，在复杂场景下分配足够的码率，从而\n使码率在不同场景下得到合理分配。 同时，AVBR确保在一定时间内，平均码率接近预设的目标码率，从而控制输出文件的大小。\nAVBR可以被视为CBR和VBR之间的折中方案，生成码率和图像质量相对稳定的码流。以下是AVBR模式下各参数的含义：\n\n数据项           描述                                         取值范围          推荐值\nintraPeriod   I帧间隔                                       [0, 2047]     28\nbitRate       目标平均比特率，单位是kbps                            [1, 700000]   1000\nframeRate     目标帧率，单位是fps                                [1, 240]      30\ninitialRcQp   指定码率控制时的初始QP值。当该值不在[0,51]范围内，编码器内部会决定初始值   [0, 51]       63\n\n 4. FixQp说明\n\nFixQp（固定QP）模式下，编码器为每个I帧、P帧和B帧分配固定的量化参数（QP）值，不进行动态调整。该模式通常用于需要严格控制图像质量和压缩比的场景。\n以下是FixQp模式下各参数的含义：\n\n数据项           描述            取值范围        推荐值\nintraPeriod   I帧间隔          [0, 2047]   28\nframeRate     目标帧率，单位是fps   [1, 240]    30\nqpI           强制I帧的QP值      [0, 51]     0\nqpP           强制P帧的QP值      [0, 51]     0\nqpB           强制B帧的QP值      [0, 51]     0\n\n 5. QPMAP说明\n\nQpMap 模式允许为一帧图像中的每一个宏块指定不同的量化参数（QP）值，从而实现更精细的码率控制。\n对于H.265编码，宏块的大小为32x32。以下是QpMap模式下各参数的含义：\n\n数据项               描述                                                             取值范围                     推荐值\nintraPeriod       I帧间隔                                                           [0, 2047]                28\nframeRate         目标帧率，单位是fps                                                    [1, 240]                 30\nqpMapArray        QPMap，每个宏块需要指定一个QP值，每个QP值占一个字节，并且按照光栅扫描方向排序。                   指针地址                     -\nqpMapArrayCount   QPMap表的大小，其数值与图像的宽度和高度有关，计算公式：(ALIGN64(width)>>5)*(ALIGN64(h   [1, 8192x4096/(32x32)]   -\n                  eight)>>5)\n\nGOP结构#\n\nH.264和H.265编码支持GOP结构的设置，可从预置的GOP结构中选择。以下为GOP预置结构介绍：\n\nGOPPRESETIDX   GOP STRUCTURE   LOW DELAY   GOP SIZE   ENCODING ORDER    GOP结构说明\n1              I               Yes         1          I0-I1-I2-I3,…     只有I帧，没有互相参考\n2              P               Yes         1          I-P0-P1-P2,…      只有I帧和P帧，并且P帧参考2个前向参考帧\n3              B               Yes         1          I-B0-B1-B2,…      只有I帧和B帧，并且B帧参考2个前向参考帧\n6              PPPP            Yes         4          I-P0-P1-P2-P3,…   只有I帧和P帧，并且P帧参考2个前向参考帧\n7              BBBB            Yes         4          I-B0-B1-B2-B3,…   只有I帧和B帧，并且B帧参考2个前向参考帧\n9              P               Yes         1          I-P0,…            只有I帧和P帧，并且P帧参考1个前向参考帧","routePath":"/guide/ucp/vp/vp5_op/video_codec","lang":"zh","toc":[{"text":"原理","id":"原理","depth":2,"charIndex":3},{"text":"Video编码原理","id":"video编码原理","depth":3,"charIndex":121},{"text":"Video解码原理","id":"video解码原理","depth":3,"charIndex":1654},{"text":"API接口","id":"api接口","depth":2,"charIndex":2398},{"text":"Video编码接口","id":"video编码接口","depth":3,"charIndex":2407},{"text":"Video解码接口","id":"video解码接口","depth":3,"charIndex":2422},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":2483},{"text":"Video编码使用方法","id":"video编码使用方法","depth":3,"charIndex":2491},{"text":"Video解码使用方法","id":"video解码使用方法","depth":3,"charIndex":2508},{"text":"补充说明","id":"补充说明","depth":3,"charIndex":2525},{"text":"码率控制模式","id":"码率控制模式","depth":4,"charIndex":2532},{"text":"GOP结构","id":"gop结构","depth":4,"charIndex":5175}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":593,"title":"总览","content":"#\n\n以下表格列出了各个VP算子支持的后端。\n\n算子名                说明           DSP   GDC   STITCH   JPU   VIDEO PROCESSING UNIT   PYRAMID   ISP\nBilateral Filter   双线性滤波        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nBox Filter         盒式滤波         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCanny              Canny边缘检测    Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCvtColor           颜色空间转换       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCorner Harris      角点检测         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nDilate             膨胀           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nEqualizehist       直方图均衡        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nErode              腐蚀           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nFilter2d           二维滤波         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nFlip               翻转           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nGaussian Blur      高斯滤波         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nIntegral           积分           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nISP                图像信号处理       N/A   N/A   N/A      N/A   N/A                     N/A       Y\nJPEG Codec         JPEG图像编解码    N/A   N/A   N/A      Y     N/A                     N/A       N/A\nLaplacian Filter   拉普拉斯滤波       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nMedian Blur        中值滤波         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nOpticalFlowPyrLK   稀疏光流         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nPyrDown            下采样          Y     N/A   N/A      N/A   N/A                     Y         N/A\nPyrUp              上采样          Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRemap              图像重映射        Y     Y     N/A      N/A   N/A                     N/A       N/A\nResize             缩放           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRoi Resize         感兴趣区域缩放      Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRotate             旋转           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nSepFilter2d        分离式二维滤波      Y     N/A   N/A      N/A   N/A                     N/A       N/A\nStitch             拼接           N/A   N/A   Y        N/A   N/A                     N/A       N/A\nSobel              索贝尔函数        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nThreshold          阈值函数         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nTranspose          转置           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nVideo Codec        Video视频编解码   N/A   N/A   N/A      N/A   Y                       N/A       N/A\nWarp Affine        仿射变换         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nWarp Perspective   透视变换         Y     N/A   N/A      N/A   N/A                     N/A       N/A","routePath":"/guide/ucp/vp/vp5_op/vp5_Overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":594,"title":"Warp Affine","content":"#\n\n仿射变换是指在向量空间中进行一次线性变换(乘以一个矩阵)并加上一个平移(加上一个向量)，变换为另一个向量空间的过程。\nWarpAffine算子功能为对输入图片进行仿射变换，包括图片旋转、缩放、平移等平面操作，将图片投射为任意平行四边形。\n\n\n算子效果#\n\n输入图像   参数                                                          输出图像\n       rotation_angle = 45.0f interpolation = HB_VP_INTER_LINEAR   \n       borderType = HB_VP_BORDER_CONSTANT\n\n\n原理#\n\n算子通过大小为2X3的转换矩阵参数，将输入图片按照仿射的形式进行映射，映射关系如下：\n\n$$\\begin\\beginX \\Y \\end=\\begina & b \\ c & d \\end\\beginx \\y \\end+\\begine \\f\n\\end\\end$$\n\n其中 x,y 为输入图片像素坐标，X,Y 为输出图片像素坐标，a,b,c,d,e,f 为转换矩阵。\n\n对输入图片，在加速计算过程中会被分解为多个子图（下图红色区域），每个子图都有独立的映射关系，并且转换系数与原图一致。 展示如下图：\n\n\n\n其中，右侧为输入图片，左侧为输出图片。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPWarpAffine 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/warp_affine","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":123},{"text":"原理","id":"原理","depth":2,"charIndex":314},{"text":"API接口","id":"api接口","depth":2,"charIndex":592},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":631}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":595,"title":"Warp Perspective","content":"#\n\n透视变换是按照物体成像投影规律进行变换，即将物体重新投影到新的成像平面，常用于视觉畸变校正。\n\n\n算子效果#\n\n输入图像   参数                                                             输出图像\n       $$\\beginkernel = \\begin0.9 & 0.05 & 15.0 \\ 0.05 & 0.9 & 15.0   \n       \\ 0.0001 & 0.0001 & 1.1 \\end\\end$$\n\n\n原理#\n\n算子通过大小为3X3的转换矩阵参数，将输入图片按照透视的形式进行映射，映射关系如下：\n\n$$dst(x,y)=src(\\frac,\\frac)$$\n\n其中，$dst$ 为输出图片，$src$ 为输入图片，$M$ 为3X3的转换矩阵。\n\n\nAPI接口#\n\n\n\n详细接口信息请查看 hbVPWarpPerspective 。\n\n\n使用方法#\n\n","routePath":"/guide/ucp/vp/vp5_op/warp_perspective","lang":"zh","toc":[{"text":"算子效果","id":"算子效果","depth":2,"charIndex":51},{"text":"原理","id":"原理","depth":2,"charIndex":248},{"text":"API接口","id":"api接口","depth":2,"charIndex":372},{"text":"使用方法","id":"使用方法","depth":2,"charIndex":416}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":596,"title":"环视图像拼接示例","content":"#\n\n该示例介绍了使用了 hbVPStitch、 hbVPRemap 、 hbVPJPEGDecode 、hbVPJPEGEncode\n等API，涉及到stitch、gdc和codec硬件ip，完成一次360环视场景拼接的应用。\n\n该示例实现的功能为：对使用车上鱼眼摄像头拍摄的四张前视、右视、后视、左视畸变图，进行去畸变与环视拼接，完成一次360环视场景拼接的应用。\n首先会为四张畸变图创建map，使用codec硬件ip读取鱼眼畸变图转换为nv12数据，然后运行gdc硬件ip，将畸变图变为俯视图，接着会运行stitch硬件ip\n，将俯视图进行拼接成环视拼接图。\n\n代码主要分为以下步骤：\n\n 1. 初始化log和环境变量。\n 2. 设置后视、前视、左视、右视的相机参数，调用 ProcessIpmRois() 接口，得到俯视图在IPM（图像透视映射的坐标系）上的roi。\n 3. 使用gdc硬件生成四个视角的俯视图map，使用codec硬件ip读取鱼眼畸变图转换为nv12数据，将鱼眼畸变数据转换为俯视图数据，并释放四个map。\n 4. 创建lut表和准备拼接区的信息。\n 5. 使用stitch硬件生成环视拼接图。\n 6. 释放lut表和申请的内存。\n\n概念声明\n\n 1. VCS（Vehicle Coordinate System） 车辆的坐标系：\n\n\n\nVCS坐标系描述了车辆的坐标系。坐标轴x对应车辆的前方，y对应车辆的左侧，z对应车辆的顶部。这个坐标系的原点不是车辆的中心，而是车辆后轴的中心。\n\n 2. IPM（Image Perspective Mapping）图像透视映射的坐标系：\n\n\n\nIPM坐标系描述了图像透视映射的坐标系。\n\n鱼眼畸变图转换map生成详述\n\n生成map主要目的为使用remap功能，可以生成像素映射关系：\n\n\n\n创建将鱼眼畸变图转换为俯视图的核心函数为 TransformMapGenerate()，主要逻辑如流程图所示：\n\n其中，TransformMapGenerate() 中使用的\nCalculateExtrinsicTransform()，对应流程图中的倒数第二步，作用为将车辆坐标系数据转换到相机坐标系，主要逻辑如流程图所示：\n\n其中：\n\n 1. IntrinsicXYZ2R()\n    作用为：将欧拉角表示的旋转（Roll-Pitch-Yaw）转换为一个旋转矩阵，用于描述三维空间中的旋转变换。旋转矩阵能够将旋转操作以矩阵乘法的形式应用到\n    向量上。\n 2. adas相机到相机坐标系的变换矩阵中：\n\n * 第一行表示新的 x 轴在相机坐标系中的方向，指向 ADAS 相机坐标系的 z 轴方向。\n * 第二行表示新的 y 轴在相机坐标系中的方向，指向 ADAS 相机坐标系的负 x 轴方向。\n * 第三行表示新的 z 轴在相机坐标系中的方向，指向 ADAS 相机坐标系的负 y 轴方向。\n * 最后一行表示平移部分，其中最后一个元素为1，表示没有平移。\n\n这个矩阵的作用是将 ADAS 相机坐标系下的点转换到相机坐标系下，实现坐标系的变换和对齐。这种变换在计算机图形学和计算机视觉中常常被使用。\n\nstitch lut表生成详述\n\nlut表函数的作用为：表示每个像素点融合状况，主要目的为生成过渡带，形成模糊效果。\n\n解释：lut表的dump出情况如下图。0和255是对立关系，255是取src0，0是取src1。0\n\n\n\n运行方法\n\n该示例位于 vp_samples/script/05_avm 目录下，包含以下脚本：\n\n\n\n运行命令行说明\n\nrun_avm.sh：该脚本实现四张鱼眼畸变图环视拼接的示例功能。run_avm.sh脚本中命令行内容如下：\n\n\n\n${bin}：编译好的可执行文件\n\n使用的时候，进入 vp_samples/script/05_avm 目录，然后直接执行run_avm.sh即可，如下代码块所示：\n\n\n\n结果说明\n\n示例运行结束后会在当前目录下生成可视化图像。部分结果如下所示：\n\n原图：\n\n去畸变+俯视图：\n\n拼接环视图：\n\n注意\n\n 1. 该示例的remap默认运行在gdc上。\n\n 2. 如需替换图片，请将图片对应的相机参数配置数据进行修改，否则运行会有问题。\n\n 3. 环境变量：\n    \n    HB_AVM_GDC_MAP_DUMP 选择是否dump gdc的map。\n    \n    \n    \n    HB_AVM_GDC_RES_DUMP 选择是否dump俯视图的结果。\n    \n    \n    \n    HB_AVM_LOG_LEVEL log等级，1为debug。\n    \n    \n\n 4. 当YUV图像dump为JPG文件时，如果YUV图像的宽度或者高度不满足Codec对齐规则，则默认使用opencv的方式进行dump。","routePath":"/guide/ucp/vp/vp6_sample/op_sample_avm","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":597,"title":"基础图像处理示例","content":"#\n\n基础图像处理示例执行脚本位于 ucp_tutorial/vp/vp_samples/script/01_basic_processing 目录下，\n该示例展示了如何对图片进行相关的处理，包括图片滤波、图片形态学处理、图片均衡，详细的实现方法请结合示例源码对比实践。\n\n此示例中，直接执行示例脚本会执行默认的图片处理流程：使用sepFilter2D滤波、检测形态学边缘、图片均衡。\n若希望更改图片的处理流程，则可以通过在执行示例脚本时追加参数来控制执行的流程，参数的追加规则如下：\n\n\n\n其中：\n\n * filter type：可选参数，滤波算子。\n * morphology type：可选参数，形态学操作。\n\n此外还可以通过追加 -help 命令获取所有可用的追加参数列表。\n\n算子执行后会在 vp_samples/script/01_basic_processing 目录下保存图片处理结果，本示例的生成物内容如下：\n\n\n\n示例的执行流程如下图：\n\n如示例的执行流程图所示，该示例对图片的处理过程可以大致分为以下四个阶段：\n\n 1. 对加噪后的图片进行滤波。按照滤波算法不同，可分为双线性滤波、盒式滤波、高斯滤波、中值滤波、二维滤波、分离式二维滤波。\n\n 2. 对滤波后的图片进行形态学处理。按照处理方法不同可分为腐蚀、膨胀、开运算、闭运算、检测边缘。\n\n 3. 对形态学处理后的图片进行直方图均衡。\n\n图像滤波\n\n在图像滤波环节，示例内将6种滤波算法封装在了对应的函数中，并使用std::map建立字符串与滤波函数的联系，如下图，参数 filter_op\n保存了关键字与函数间的映射关系。\n\n\n\n在示例代码执行过程中，根据argv参数中的追加关键字不同，自动适配到相应的滤波算法。核心逻辑如下：\n\n\n\n以sepFilter2D滤波算法为例，对加噪图片的处理效果如图所示：\n\n滤波前：\n\n\n\n滤波后：\n\n\n\n形态学处理\n\n形态学处理算子包含了腐蚀、膨胀、开闭运算、顶帽、黑帽等，本示例中通过对 hbVPDilate 和 hbVPErode\n接口的调用，封装了其中一部分的形态学功能，具体实现方法请查看函数源码。 与滤波环节类似，此处创建了 morphology_op\n参数来存储字符串与形态学函数之间的联系， 代码如下：\n\n\n\n调度形态学函数时同样采用了字符串适配的方法，核心逻辑如下：\n\n\n\n以形态学梯度边缘算法（示例中 edge 函数）为例，对滤波图片的处理效果如图所示：\n\n滤波原图：\n\n\n\n形态学梯度边缘检测图：\n\n\n\n直方图均衡\n\n为了增强图片的对比度，凸显图片细节，示例通过 equalizeHist 函数，对形态学处理后的图片进行直方图均衡处理。 均衡效果如下图所示：\n\n原图：\n\n\n\n均衡后效果图：\n\n","routePath":"/guide/ucp/vp/vp6_sample/op_sample_basic_process","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":598,"title":"视频编解码示例","content":"#\n\n视频编解码示例主要展示了视频编解码的典型使用场景，具体包括以下内容：\n\n * 基于UCP编码结果的解码示例：演示如何使用UCP编码器生成的码流进行解码，展示完整的编码-解码流程。\n * 基于UCP解码结果的编码示例：演示如何从提供的码流文件中进行解码，并在解码完成后使用UCP编码器进行重新编码，适用于处理和转换已有视频数据的场景。\n\n示例脚本允许配置编码参数，以优化视频质量、压缩率和性能，适应不同的应用场景。通过这个示例，您可以了解并实践如何使用UCP框架进行视频的编解码操作，\n并根据具体需求调整编码参数，详细的实现方法请结合示例源码对比实践。\n\n\n功能概述#\n\n\n基于UCP编码结果的解码示例#\n\n\n\n本示例实现了一个完整的视频编码-解码流程，输入视频序列为YUV420格式，经过H.265编码后，生成相应的码流。\n在编码过程中，实时将每一帧码流输入解码器，经过H.265解码后，生成相应的码流。\n注意由于视频编码的过程是有损的，所以解码得到的YUV视频序列与输入的YUV视频序列不是完全等价的。\n\n该示例的代码过程可以大致分为以下六个步骤，详细的实现请结合示例源码。\n\n 1. 准备编码器和解码器参数。\n\n 2. 创建编码器和解码器上下文。\n\n 3. 准备编码器输入内存。\n\n 4. 将输入的YUV420视频序列逐帧送入编码器进行编码，等待编码任务完成后，再将生成的H.265码流逐帧送入解码器。\n\n 5. 视频编解码全部完成后，释放输入内存。\n\n 6. 释放解码器和编码器上下文。\n\n\n基于UCP解码结果的编码示例#\n\n\n\n本示例实现了一个如何从提供的码流文件中进行解码，并在解码完成后使用UCP编码器进行重新编码，适用于处理和转换已有视频数据的场景。\n输入视频序列为H.265格式的码流，首先通过H.265解码器将码流解码为YUV420格式的视频序列。\n随后，将解码得到的YUV420视频序列逐帧送入编码器，重新编码为H.265码流。\n\n该示例的代码过程可以大致分为以下六个步骤，详细的实现请结合示例源码。\n\n 1. 准备解码器和编码器参数。\n\n 2. 创建解码器和编码器上下文。\n\n 3. 准备解码器输入内存。\n\n 4. 调用avformat函数将H.265码流逐帧解析送入至解码器进行解码，等待编码任务完成后，再将生成的YUV数据逐帧送入编码器。\n\n 5. 视频编解码全部完成后，释放输入内存。\n\n 6. 释放编码器和解码器上下文。\n\n\n运行指南#\n\n视频编解码示例执行脚本位于 ucp_tutorial/vp/vp_samples/script/06_codec 目录下，包含以下脚本：\n\n\n\nrun_codec.sh脚本中命令行内容如下：\n\n\n\n执行的时候，进入 vp_samples/script/06_codec 目录，然后直接执行run_codec.sh即可，执行方法及日志输出如下：\n\n\n\n此示例中，直接执行示例脚本会使用H.265编码的默认参数。如果希望更改H.265编码的参数，可以在执行示例脚本时追加参数来控制执行流程，追加参数的规则如下：\n\n\n\n其中，rc_mode 为可选参数，用于指定H.265编码器的码率控制方式；gop_preset_idx\n也是可选参数，用于指定H.265编码器的GOP结构预设方式。\n\n此外还可以通过追加 -help 命令获取所有可用的追加参数列表。编码参数 rc_mode 和 gop_preset_idx 定义如下：\n\n\n\n\n结果说明#\n\n示例执行后会在 vp_samples/script/06_codec 目录下保存图片处理结果，本示例的默认参数下生成物内容如下：\n\n\n\n以解码为例，第一帧和第n帧的解码效果如下所示：\n\n","routePath":"/guide/ucp/vp/vp6_sample/op_sample_codec","lang":"zh","toc":[{"text":"功能概述","id":"功能概述","depth":2,"charIndex":280},{"text":"基于UCP编码结果的解码示例","id":"基于ucp编码结果的解码示例","depth":3,"charIndex":288},{"text":"基于UCP解码结果的编码示例","id":"基于ucp解码结果的编码示例","depth":3,"charIndex":651},{"text":"运行指南","id":"运行指南","depth":2,"charIndex":1030},{"text":"结果说明","id":"结果说明","depth":2,"charIndex":1450}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":599,"title":"图像特征提取示例","content":"#\n\n图像变换示例执行脚本位于 ucp_tutorial/vp/vp_samples/script/03_feature_extraction 目录下，\n图片特征提取通常分为点特征和边缘特征，该示例主要展示了不同的边缘特征提取方法，包括形态学处理获取边缘、组合XY方向梯度获取边缘和canny算法，详细的实现方法请\n结合示例源码对比实践。\n\n此示例中，直接执行示例脚本会默认通过canny算法计算图片边缘，若希望更改检测图片边缘的方法，则可以通过在执行示例脚本时追加参数来控制执行的流程，参数的追加规则\n如下：\n\n\n\n其中，edge operate为可选参数，边缘检测算法。\n\n此外还可以通过追加 -help 命令获取所有可用的追加参数列表。\n\n算子执行后会在 vp_samples/script/03_feature_extraction 目录下保存图片处理结果，本示例的生成物内容如下：\n\n\n\n在示例实现过程中，为了建立字符串与边缘检测函数之间的联系，参数 edge_op 定义如下：\n\n\n\n追加参数的适配方法与其他示例一致，代码如下：\n\n\n\nmorphology_edge\n\n使用形态学方法提取图片的边缘特征，主要使用了 hbVPDilate 和 hbVPErode 算子，处理步骤如下：\n\n * 对原图使用dilate算子处理，获取膨胀后的图像，膨胀操作会让原图中的浅色区域更为凸出。\n * 对原图使用erode算子处理，获取腐蚀后的图像，腐蚀操作会让原图中的深色区域更为凸出。\n * 将膨胀后的图像减去腐蚀后的图像就可以得到深色与浅色区域间的分界线，即图像的边缘。\n\n核心源码如下：\n\n\n\n边缘提取效果如下图：\n\n\n\nsobel_edge\n\n使用 hbVPSobel 算子提取图片的边缘特征，处理步骤如下：\n\n * 对原图使用 hbVPSobel 算子获取X方向梯度图像。\n * 对原图使用 hbVPSobel 算子获取Y方向梯度图像。\n * 将XY方向上的梯度图像组合起来，即可获得XY方向上的边缘特征。\n\n其中XY方向梯度图像组合的核心源码如下：\n\n\n\n边缘提取效果如下图：\n\n\n\ncanny_edge\n\n使用 hbVPCanny 算子提取图片的边缘特征，算子原理及接口请参考 hbVPCanny 。","routePath":"/guide/ucp/vp/vp6_sample/op_sample_feature_extraction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":600,"title":"光流处理示例","content":"#\n\n光流处理示例使用了 hbVPPyrDown， hbVPCornerHarris 和 hbVPOpticalFlowPyrLK\n接口，展示了如何对连续帧图像进行光流追踪，详细的实现方法请结合示例源码对比实践。\n\n功能概述\n\n\n\n该示例实现的功能是对连续帧的图像输入进行特征点跟踪，输入图像是NV12类型，转成gray类型后使用 hbVPPyrDown 生成5层金字塔图层。 第一帧使用\nhbVPCornerHarris 生成harris角点，后续帧会使用 hbVPOpticalFlowPyrLK 对第一帧生成的角点进行跟踪，每隔5帧会再次调用\nhbVPCornerHarris 生成先验特征点用于校准跟踪结果。\n\n该示例的代码过程可以大致分为以下四个步骤，详细的实现请结合示例源码。\n\n 1. 分配输入图像和输入输出特征点的内存。\n\n 2. 准备光流计算参数。\n\n 3. 依次执行每一帧的计算，包括生成金字塔图层，角点检测和光流追踪。\n\n 4. 释放内存资源。\n\n运行指南\n\n光流处理示例执行脚本位于 ucp_tutorial/vp/vp_samples/script/04_optical_flow 目录下，包含以下脚本：\n\n\n\nrun_optical_flow.sh脚本中命令行内容如下：\n\n\n\n命令行参数含义如下：\n\n * $：编译好的可执行文件。\n * image_file_list：输入图像列表。\n * frame_num：运行的帧数，可根据实际需求调整计算的帧数。\n * lkof_threshold：光流置信度阈值，用于筛选光流计算结果，可自行调整。\n * harris_top_k：用于筛选前top k角点计算结果，可自行调整。\n\n使用的时候，进入 vp_samples/script/04_optical_flow\n目录，然后直接执行run_optical_flow.sh即可，执行方法及日志输出如下：\n\n\n\n结果说明\n\n示例运行结束后会在 vp_samples/script/04_optical_flow 目录下生成可视化图像，本示例的生成物内容如下：\n\n\n\n其中第一帧会显示harris特征点检测结果，后续帧会显示光流追踪轨迹，其中蓝色点为前一帧图像的特征点，红色点为当前帧图像的特征点，绿色是光流轨迹。第一帧和第n帧\n的追踪效果如下所示：\n\n第一帧   第N帧\n      \n\n注意\n 1. 该示例的PyrDown和CornerHarris计算都是在DSP上进行处理的，请提前配置好DSP环境。\n 2. 如需替换图片，请保证替换的输入图片为连续帧图像，否则光流计算的结果可能会不准确。","routePath":"/guide/ucp/vp/vp6_sample/op_sample_optical_flow","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":601,"title":"图像变换示例","content":"#\n\n图像变换示例执行脚本位于 ucp_tutorial/vp/vp_samples/script/02_transformation\n目录下，该示例主要展示了不同转换类算子的应用效果，包括图片缩放、上下采样、旋转、翻转、转置、仿射、透视，详细的实现方法请结合示例源码对比实践。\n\n此示例中，直接执行示例脚本会执行默认的图片变换示例：生成一张resize和PyrUp的效果对比图片，若希望更改图片的处理方法，则可以通过在执行示例脚本时追加参数\n来控制执行的流程，参数的追加规则如下：\n\n\n\n其中，transform operate为可选参数，示例算法。\n\n此外还可以通过追加 -help 命令获取所有可用的追加参数列表。\n\n算子执行后会在 vp_samples/script/02_transformation 目录下保存图片处理结果，本示例的生成物内容如下：\n\n\n\n不同的追加参数会导致示例调用不同的接口，生成的处理结果也不相同，具体的实现如下：\n\ncompare_resize_pyrUp\n\n将输入图片分为了左右两份，并将输入图片的一半分别通过 hbVPPyrUp 和 hbVPResize 接口进行处理，并将处理结果拼接成输出图片。\n输出图片的左侧为pyrUp处理效果，右侧为resize处理效果，具体输出如下图：\n\ncompare_pyrDown_rotate_flip_transpose\n\n此示例流程中将输入图片分为了四份，其中，左上部分使用 hbVPPyrDown 处理，右上使用 hbVPTranspose 处理，左下使用 hbVPRotate\n处理，右下使用 hbVPFlip 处理，最后将处理结果按照部位组合为输出图片，具体输出如下图：\n\naffine\n\n此示例流程中对输入图片进行了仿射变换，将图片逆时针旋转了45°，并将旋转后的图片置于长宽均为输入图片两倍的内存中，具体输出如下图：\n\nperspective\n\n此示例流程中对输入图片进行了透视变换，将图片按照特定的转换矩阵进行变换，若需要对该变化的形式进行变更，请手动更改相关源码中的变换矩阵，具体输出如下图：","routePath":"/guide/ucp/vp/vp6_sample/op_sample_transformation","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":602,"title":"总览","content":"#\n\n本章节为您介绍VP模块的各个示例功能、实现流程及实现细节，帮助您快速了解VP模块的使用方法，示例介绍均以板端运行为例。 示例的编译及运行请参考 快速上手 章节。\n\n示例列表，其中dsp、gdc等列表示对应示例是否使用了相应backend，（Y表示使用，N表示未使用）。\n\n示例名                  说明         DSP   GDC   STITCH   JPU   VIDEO PROCESSING UNIT\nbasic process        基础图像处理示例   Y     N     N        N     N\ntransformation       图像转换示例     Y     N     N        N     N\nfeature extraction   特征提取示例     Y     N     N        N     N\noptical flow         光流示例       Y     N     N        Y     N\navm                  环视图像拼接示例   Y     Y     Y        Y     N\ncodec                视频编解码示例    N     N     N        N     Y","routePath":"/guide/ucp/vp/vp6_sample/vp6_Overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":603,"title":"VP API 概览","content":"#\n\n\n数据结构#\n\n\n功能接口#","routePath":"/guide/ucp/vp/vp7_reference/vp7_Overview","lang":"zh","toc":[{"text":"数据结构","id":"数据结构","depth":2,"charIndex":3},{"text":"功能接口","id":"功能接口","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":604,"title":"hbVPArray","content":"#\n\n\n\n动态数组结构体。\n\n * 成员\n   \n   成员名称       描述\n   phyAddr    数组的物理地址\n   virAddr    数组的虚拟地址\n   memSize    数组的内存大小\n   capacity   数组可以存储的最大元素数量\n   size       数组实际存储的元素数量","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvparray","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":605,"title":"hbVPBorderType","content":"#\n\n\n\n填充类型。\n\n * 成员\n   \n   成员名称                     描述\n   HB_VP_BORDER_CONSTANT    值填充， iiiiii|abcdefgh|iiiiiii。\n   HB_VP_BORDER_REPLICATE   边界复制填充， aaaaaa|abcdefgh|hhhhhhh。\n\n注解\n\n当使用HB_VP_BORDER_CONSTANT时，仅支持使用0作为填充值。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpbordertype","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":606,"title":"hbVPFilterKernel","content":"#\n\n\n\n算子的滤波核结构体。\n\n * 成员\n   \n   成员名称          描述\n   dataType      数据元素类型。\n   width         滤波核宽度。\n   height        滤波核高度。\n   dataPhyAddr   滤波核物理地址。\n   dataVirAddr   滤波核虚拟地址。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpfilterkernel","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":607,"title":"hbVPImage","content":"#\n\n\n\n图片结构体。只支持使用BPU内存。\n\n图像通道说明：以 HB_VP_IMAGE_FORMAT_RGB 为例，其通道数为3时，imageType为 HB_VP_IMAGE_TYPE_U8C3\n，当通道数为4时，由于对齐的要求，数据中存在额外的内存， 此时需要指定imageType为 HB_VP_IMAGE_TYPE_U8C4 （RRR* GGG*\nBBB*）。\n\n * 成员\n   \n   成员名称          描述\n   imageFormat   图片格式。\n   imageType     图片类型。\n   width         图片的像素宽度。\n   height        图片的像素高度。\n   stride        图片的像素偏移量，以字节数量表示。\n   dataVirAddr   图片在arm侧的逻辑地址。\n   dataPhyAddr   图片的物理地址。\n   uvVirAddr     图片格式为NV12时，uv数据逻辑地址。\n   uvPhyAddr     图片格式为NV12时，uv数据物理地址。\n   uvStride      图片格式为NV12时，uv数据的偏移量，以字节数量表示。\n\n偏移量说明：stride 范围在[bytes_per_pixel * width，bytes_per_pixel * MAX_IMAGE_WIDTH\n]，并且满足 bytes_per_pixel对齐， 其中 bytes_per_pixel 表示每个像素的字节数，其大小取决于图像格式\nimageFormat，width 表示图像的像素宽度，MAX_IMAGE_WIDTH 表示图像像素的最大宽度。 当图像格式为NV12时， uvStride\n范围在[2*uv_width，\nMAX_IMAGE_WIDTH]，并且必须为偶数。在使用pyramid和gdc硬件时，stride和uvStride的值需要在大于width的同时满足16字节对\n齐的要求。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimage","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":608,"title":"hbVPImageFormat","content":"#\n\n\n\n图片格式的枚举。\n\n * 成员\n   \n   成员名称                          描述\n   HB_VP_IMAGE_FORMAT_Y          灰度格式。\n   HB_VP_IMAGE_FORMAT_NV12       NV12格式，通道排布为YYYYYYYY...UVUV...，图片Y和UV需要指定内存地址。\n   HB_VP_IMAGE_FORMAT_RGB_P      RGB_P格式，通道排布为RRRR...GGGG...BBBB...。\n   HB_VP_IMAGE_FORMAT_RGB        RGB格式，通道排布为RGBRGBRGB...(C3) or RGB*RGB*RGB*...(C4)。\n   HB_VP_IMAGE_FORMAT_BGR_P      BGR_P格式，通道排布为BBBB...GGGG...RRRR。\n   HB_VP_IMAGE_FORMAT_BGR        BGR格式，通道排布为BGRBGRBGR...(C3) or BGR*BGR*BGR*...(C4)。\n   HB_VP_IMAGE_FORMAT_YUV444     YUV格式，通道排布为YUVYUVYUV... or YUV*YUV*YUV*...。\n   HB_VP_IMAGE_FORMAT_YUV444_P   YUV_P格式，通道排布为YYYY...UUUU...VVVV...。\n   HB_VP_IMAGE_FORMAT_YUV420     YUV格式，通道排布为YYYY...U...V...。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimageformat","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":609,"title":"hbVPImageType","content":"#\n\n\n\n图片的类型枚举，图片的类型包括图片的深度和通道数。 以uint8数据类型的图像为例，当图像的imageFormat为\nHB_VP_IMAGE_FORMAT_RGB ，其通道数为3时，其imageType为 HB_VP_IMAGE_TYPE_U8C3 ，\n其通道数为4时，其imageType为 HB_VP_IMAGE_TYPE_U8C4 ；当图像的imageFormat为\nHB_VP_IMAGE_FORMAT_Y，其通道数为1时，其imageType为 HB_VP_IMAGE_TYPE_U8C1 。\n\n * 成员\n   \n   成员名称                     描述\n   HB_VP_IMAGE_TYPE_U8C1    单通道uint_8类型。\n   HB_VP_IMAGE_TYPE_U8C3    三通道uint_8类型。\n   HB_VP_IMAGE_TYPE_U8C4    四通道uint_8类型。\n   HB_VP_IMAGE_TYPE_S16C1   单通道int_16类型。\n   HB_VP_IMAGE_TYPE_S16C2   双通道int_16类型。\n   HB_VP_IMAGE_TYPE_S16C3   三通道int_16类型。\n   HB_VP_IMAGE_TYPE_S32C1   单通道int_32类型。\n   HB_VP_IMAGE_TYPE_F32C1   单通道float_32类型。\n   HB_VP_IMAGE_TYPE_F64C1   单通道float_64类型。\n   HB_VP_IMAGE_TYPE_F64C2   双通道float_64类型。\n   HB_VP_IMAGE_TYPE_U10C1   单通道uint_10类型。\n   HB_VP_IMAGE_TYPE_U12C1   单通道uint_12类型。\n   HB_VP_IMAGE_TYPE_U16C1   单通道uint_16类型。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimagetype","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":610,"title":"hbVPInterpolationType","content":"#\n\n\n\n插值方式。\n\n * 成员\n   \n   成员名称                   描述\n   HB_VP_INTER_NEAREST    最邻近插值。\n   HB_VP_INTER_LINEAR     双线性插值。\n   HB_VP_INTER_GAUSSIAN   高斯插值。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpinterpolationtype","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":611,"title":"hbVPRoi","content":"#\n\n\n\n图片ROI区域结构体，参数为ROI区域的四条边下标，编号从 0 开始。ROI区域的 width 等于 right - left + 1 ，height =\nbottom - top + 1 。\n\n * 成员\n   \n   成员名称     描述\n   left     ROI区域的左边下标。\n   top      ROI区域的上边下标。\n   right    ROI区域的右边下标。\n   bottom   ROI区域的下边下标。","routePath":"/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvproi","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":612,"title":"环境变量","content":"#\n\n\n\n\n日志等级设置说明#\n\n * 日志等级：\n   \n   vp 模块中的日志主要分为7个等级：\n   \n   log等级可设置为0、1、2、3、4、5、6，分别对应Verbose、Debug、Info、Warning、Error、Critical、Never，默认\n   为Warning。\n   \n   DSP 模块中ARM侧的日志主要分为7个等级：\n   \n   log等级可设置为0、1、2、3、4、5、6，分别对应Verbose、Debug、Info、Warning、Error、Critical、Never，默认\n   为Warning。\n   \n   DSP 模块中DSP侧的日志主要分为5个等级：\n   \n   log等级可设置为1、2、3、4、5，分别对应Debug、Info、Warning、Error、Always等级，默认为Warning。\n\n * 日志等级设置规则：\n   \n   * 若发生的log等级 >= 设置的等级，则该log可以被打印，反之被屏蔽。\n   * 设置的log等级越小，打印信息越多。例如：设置log等级为3，即为Warning级别，则3、4、5等级的log均可以被打印。VP\n     模块默认log等级为Warning级别，即以下log级别的信息可以被打印：Warning、Error、Critical。\n\n注解\n\nDSP侧日志可以通过如下步骤获取：\n\n * 配置环境变量，使能DSP日志输出\n   \n   \n\n * 启动日志监听服务\n   \n   ","routePath":"/guide/ucp/vp/vp7_reference/vp7_environment_variable","lang":"zh","toc":[{"text":"日志等级设置说明","id":"日志等级设置说明","depth":2,"charIndex":5}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":613,"title":"hbVPBilateralFilter","content":"#\n\n\n\nBilateralFilter算子参数。\n\n * 成员\n   \n   成员名称         描述\n   sigmaColor   颜色空间中的sigma滤波参数。\n   sigmaSpace   坐标空间中的sigma滤波参数。\n   kernelSize   滤波核大小，支持大小为 5 或者 9 。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用BilateralFilter API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] bilateralParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpbilateralfilter","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":614,"title":"hbVPBoxFilter","content":"#\n\n\n\nBoxFilter算子参数。\n\n * 成员\n   \n   成员名称           描述\n   kernelHeight   滤波核的高度，值为[3,31)间的奇数，核高和宽相等。\n   kernelWidth    滤波核的宽度，值为[3,31)间的奇数，核宽和高相等。\n   pointLocX      保留参数。\n   pointLocY      保留参数。\n   normalize      保留参数。\n   borderType     填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用boxFilter API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] boxFilterParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpboxfilter","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":615,"title":"hbVPCanny","content":"#\n\n\n\nCanny算子参数。\n\n * 成员\n   \n   枚举名称            描述\n   HB_VP_NORM_L1   norm L1滤波。\n\n\n\nCanny算子参数。\n\n * 成员\n   \n   成员名称         描述\n   threshold1   低位阈值。\n   threshold2   高位阈值。\n   kernelSize   滤波核大小，支持大小为3、5或7。\n   norm         支持hbVPCannyNorm中的类型。\n   overlap      保留参数。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n注解\n\n因算法实现不同，当前仅在borderType为HB_VP_BORDER_REPLICATE时，得到的处理结果与OpenCV基本一致。\n\n\n\n调用Canny API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] cannyParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcanny","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":616,"title":"hbVPCornerHarris","content":"#\n\n\n\nCornerHarris算子参数。\n\n * 成员\n   \n   成员名称          描述\n   borderType    填充类型，支持 hbVPBorderType 中的类型。\n   kernelSize    滤波核大小，支持大小为3、5或7。\n   blockSize     邻域尺寸，取值为[3, 27]内的奇数。\n   sensitivity   检测器自由参数，推荐取值范围为[0.04, 0.06]。\n\n\n\n调用CornerHarris API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，format与输入图像一致，type支持S32C1，大小与输入一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] cornerHarrisParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcornerharris","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":617,"title":"hbVPCvtColor","content":"#\n\n\n\n调用CvtColor API，支持RGB转换为GRAY，RGB与BGR转换为NV12。\n\n * 参数\n   \n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，大小与输入图像一致，type支持U8C1，format支持Y和nv12。\n   * [in] srcImg 输入图像，type支持U8C3，format支持RGB和BGR。\n\n * 返回值\n   \n   * 返回 0 则表示API成功执行，否则执行失败。\n\n * 转换支持表\n   \n   SRCFMT/DSTFMT   GRAY   NV12   RGB_P   RGB   BGR_P   BGR   YUV_P   YUV\n   gray            N      N      N       N     N       N     N       N\n   nv12            N      N      N       N     N       N     N       N\n   rgb_p           N      N      N       N     N       N     N       N\n   rgb             Y      Y      N       N     N       N     N       N\n   bgr_p           N      N      N       N     N       N     N       N\n   bgr             N      Y      N       N     N       N     N       N\n   yuv_p           N      N      N       N     N       N     N       N\n   yuv             N      N      N       N     N       N     N       N","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcvtcolor","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":618,"title":"hbVPDilate","content":"#\n\n\n\nDilate算子参数。\n\n * 成员\n   \n   成员名称          描述\n   pointLocX     保留参数。\n   pointLocY     保留参数。\n   iterations    保留参数。\n   borderType    填充类型，支持 hbVPBorderType 中的类型。\n   borderValue   保留参数。\n\n\n\n调用Dilate API。\n\n * 参数\n   * [out] task 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] dilateKernel 算子处理核，type支持U8C1，宽和高为小于等于9的正奇数。\n   * [in] dilateParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpdilate","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":619,"title":"hbVPEqualizehist","content":"#\n\n\n\n调用EqualizeHist API。\n\n * 参数\n   * [out] task 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpequalizehist","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":620,"title":"hbVPErode","content":"#\n\n\n\nErode算子参数。\n\n * 成员\n   \n   成员名称          描述\n   pointLocX     保留参数。\n   pointLocY     保留参数。\n   iterations    保留参数。\n   borderType    填充类型，支持 hbVPBorderType 中的类型。\n   borderValue   保留参数。\n\n\n\n调用Erode API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] erodeKernel 算子处理核，type支持U8C1，宽和高为小于等于9的正奇数。\n   * [in] erodeParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvperode","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":621,"title":"hbVPFilter2d","content":"#\n\n\n\nFilter2D算子参数。\n\n * 成员\n   \n   成员名称         描述\n   delta        保留参数。\n   pointLocX    保留参数。\n   pointLocY    保留参数。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用Filter2D API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] filterKernel 算子处理核，type支持F32C1，宽和高相等且为小于等于9的正奇数。\n   * [in] filter2DParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpfilter2d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":622,"title":"hbVPFlip","content":"#\n\n\n\n调用Flip API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] flipMode 算子参数，参数中值为0表示沿x轴翻转，值为正整数表示沿y轴翻转。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpflip","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":623,"title":"hbVPGaussianBlur","content":"#\n\n\n\nGaussianBlur算子参数。\n\n * 成员\n   \n   成员名称         描述\n   sigmaX       保留参数。\n   sigmaY       保留参数。\n   kernelSize   滤波核大小，支持大小为3或5。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用 GaussianBlur API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] gaussianParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpgaussianblur","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":624,"title":"hbVPGetVersion","content":"#\n\n\n\n获取vp版本号。\n\n * 返回值\n   * 返回版本号。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpgetversion","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":625,"title":"hbVPIntegral","content":"#\n\n\n\n调用 Integral API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg\n     输出图像，format与输入图像一致，type支持S32C1，其width为输入图像的width加一，其height为输入图像的height加一。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpintegral","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":626,"title":"hbVPISP","content":"#\n\n\n\nISP配置参数结构体。\n\n * 成员\n   \n   成员名称        描述\n   bufNum      ISP 底层分配输出buffer个数，最少支持3个，最多支持10个。\n   bufCached   是否使用cached buffer，0：不激活cache，1：激活cache\n   backend     选择执行ISP任务的后端\n   width       输入图像的宽度\n   height      输入图像的高度\n\n注解\n\n在使用ISP时，输入图像的width要满足[480, 4096]的限制，height要满足[240， 2160]的限制,\n提交ISP任务时指定的执行backend需要与配置参数中的backend参数保持一致。\n\n\n\n创建ISP上下文API，创建前需要提前将context初始化为nullptr。\n\n * 参数\n   * [out] context ISP上下文。\n   * [in] ispCtxParam 输入ISP配置结构体。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用ISP。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [in] srcImg ISP的输入图像。\n   * [in] context ISP上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 执行ISP时，要求输入图像的Image Format为HB_VP_IMAGE_FORMAT_Y，Image Type为\n     HB_VP_IMAGE_TYPE_U12C1。\n   * 仅支持异步方式创建任务。\n\n\n\n获取ISP输出buffer。\n\n * 参数\n   * [out] outImg ISP的输出图像。\n   * [in] taskHandle 任务句柄，负责算子与UCP架构的交互。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 图像缓冲区 dstImg 由ISP内部申请。\n   * 当任务成功完成后，输出缓冲区中的数据有效；在任务释放阶段，输出缓冲区将被释放。\n\n\n\n释放ISP上下文API。\n\n * 参数\n   * [in] context ISP上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpisp","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":627,"title":"hbVPJPEGDecode","content":"#\n\n\n\n创建解码器上下文API，创建前需要提前将context初始化为nullptr。\n\n * 参数\n   * [out] context 解码器上下文。\n   * [in] outBufCount JPU内部输出缓冲区个数。outBufCount 取值范围[1, 31]，通常情况下设置为5。\n   * [in] imageFormat 输出图像的格式。imageFormat 支持\n     HB_VP_IMAGE_FORMAT_NV12，HB_VP_IMAGE_FORMAT_YUV420，HB_VP_IMAGE_FORMAT_YUV444\n     和 HB_VP_IMAGE_FORMAT_YUV444_P 格式。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 由于硬件限制，解码器内部以 16x16 为单元做解码处理，因此解码后的分辨率变化为16x16对齐后的分辨率。\n   * JPU最高支持创建64路编码或解码的上下文。\n   * 仅支持8bit解码。\n\n\n\n释放解码器上下文API。\n\n * 参数\n   * [in] context 解码器上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用JPEG解码API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [in] srcBuf 存放JPEG数据的内存地址。\n   * [in] context 解码器上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * JPEG解码时，若解码格式为 HB_VP_IMAGE_FORMAT_YUV444，要求输入的JPEG数据的格式为\n     HB_VP_IMAGE_FORMAT_YUV444 或者 HB_VP_IMAGE_FORMAT_YUV444_P。\n   * JPEG解码时，支持的最小分辨率为32x32，最大分辨率为8192x8192，并且输入图像的大小必须大于1024字节。\n   * 仅支持异步方式创建任务。\n   * 为了避免系统资源浪费并提高JPU解码性能，建议复用输入地址。\n\n\n\n调用JPEG编码API。\n\n * 参数\n   * [out] outImg 存放解码图像的内存地址。\n   * [in] taskHandle 任务句柄，负责算子与UCP架构的交互。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 图像缓冲区 outImg 由JPU内部申请。\n   * 当任务成功完成后，输出缓冲区中的数据有效；在任务释放阶段，输出缓冲区将被释放。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpjpegdecode","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":628,"title":"hbVPJPEGEncode","content":"#\n\n\n\nhbVPJPEGEncode 算子和 hbVPJPEGDecode 算子的上下文描述句柄，句柄包含了JPU上运行所必要的描述信息，可重复使用。\n\n\n\n用于JPEG编码的参数。\n\n * 成员\n   \n   成员名称                 描述\n   extendedSequential   仅支持8bit编码，参数值为0。\n   imageFormat          输入图像的格式。 当前支持 HB_VP_IMAGE_FORMAT_NV12，\n                        HB_VP_IMAGE_FORMAT_YUV420， HB_VP_IMAGE_FORMAT_YUV444 和\n                        HB_VP_IMAGE_FORMAT_YUV444_P 格式。\n   width                输入图像的宽度。取值范围[32, 8192]。\n   height               输入图像的高度。取值范围[32, 8192]。\n   qualityFactor        编码质量，数值越小图片质量越差。取值范围[1, 100]，推荐值50。\n   outBufCount          JPU内部输出缓冲区个数。取值范围[1, 1000]，推荐值5。\n\n\n\n创建编码器上下文API，创建前需要提前将context初始化为nullptr。\n\n * 参数\n   * [out] context 编码器上下文。\n   * [in] param 编码参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 当 imageFormat 为 HB_VP_IMAGE_FORMAT_NV12 或者 HB_VP_IMAGE_FORMAT_YUV420 格式时，要求\n     width 满足 16 对齐， height 满足 8 对齐。\n   * 当 imageFormat 为 HB_VP_IMAGE_FORMAT_YUV444 或者 HB_VP_IMAGE_FORMAT_YUV444_P\n     格式时，要求 width 满足 8 对齐， height 满足 8 对齐。\n   * 由于硬件限制，编码器内部以 16x16 为单元做编码处理。当待编码数据为非 16x16\n     对齐时，编码后的数据最后一部分随机填充的部分会存在差异，但是不会影响有效数据，编码后的数据分辨率也不会发生变化。因此做md5比较时需要注意这点。\n   * JPU最高支持创建 64 路编码或解码的上下文。\n   * 仅支持8bit数据编码。\n\n\n\n释放编码器上下文API。\n\n * 参数\n   * [in] context 编码器上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用JPEG编码API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [in] srcImg 存放编码图像的内存地址。\n   * [in] context 编码器的上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 仅支持异步方式创建任务。\n   * 为了避免系统资源浪费并提高JPU编码性能，建议复用输入地址。\n\n\n\n调用JPEG编码输出缓冲区API。\n\n * 参数\n   * [out] outBuf 存放编码后JPEG数据的内存地址。\n   * [in] taskHandle 任务句柄，负责算子与UCP架构的交互。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * JPEG缓冲区 outBuf 由JPU内部申请。\n   * 当任务成功完成后，输出缓冲区中的数据有效；在任务释放阶段，输出缓冲区将被释放。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpjpegencode","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":629,"title":"hbVPLaplacianFilter","content":"#\n\n\n\nLaplacianFilter算子参数。\n\n * 成员\n   \n   成员名称         描述\n   kernelSize   滤波核大小，支持大小为 1 。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n   normalize    归一化，非零表示为真。\n\n\n\n调用LaplacianFilter API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，format和大小与输入图像一致，tyep支持S16C1。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] laplacianParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvplaplacianfilter","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":630,"title":"hbVPMedianBlur","content":"#\n\n\n\n调用medianBlur API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] maskWidth 算子参数，滤波核大小可配置为3、5或者7。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpmedianblur","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":631,"title":"hbVPOpticalFlowPyrLK","content":"#\n\n\n\n * 成员\n   \n   成员名称   描述\n   x      特征点x坐标。\n   y      特征点y坐标。\n\n\n\n稀疏光流任务参数结构体。\n\n * 成员\n   \n   成员名称              描述\n   pyrLevels         用于计算的金字塔层数。\n   winSize           光流窗口大小，取值范围3~23，奇数，需大于角点检测窗口大小。\n   criteriaEpsilon   优化终止阈值，取值范围0~255。\n   maxIteration      最大迭代次数，取值范围1~10。\n   minEigThreshold   最小特征值阈值\n   confEnable        光流置信度使能开关 非0: 打开, 0:关闭, J6不支持, 使用默认值\n\n注解\n\nJ6没有LKOF硬件，采用DSP后端实现。\n\n\n\n调用OpticalFlowPyrLK API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] currPoints 输出特征点，存储元素类型为hbVPKeyPoint，内存大小为数据size *\n     sizeof(hbVPKeyPoint)，size与prevPoints的size相同。\n   * [out] currPointsStatus 追踪点状态，0表示不收敛, 非0表示收敛，存储元素类型为uint8_t，内存大小为size *\n     sizeof(uint8_t)，size与currPoints点数相同。\n   * [out] currPointsConf 光流置信度，J6 DSP后端不支持，可为nullptr。\n   * [in] prevPoints 前一帧的特征点，数据类型与currPoints相同。\n   * [in] currPym 输入当前帧图像的金字塔图层，需要和前一帧格式一致。\n   * [in] prevPym 输入前一帧图像的金字塔图层，其image_format为 HB_VP_IMAGE_FORMAT_Y 或\n     HB_VP_IMAGE_FORMAT_NV12，image_type为 HB_VP_IMAGE_TYPE_U8C1 。\n   * [in] lkofParam 稀疏光流任务参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpopticalflowpyrlk","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":632,"title":"hbVPPyrDown","content":"#\n\n\n\nPym配置参数结构体。\n\n * 成员\n   \n   成员名称            描述\n   levels          选择金字塔激活层数。\n   interpolation   选择金字塔的插值方式，支持 hbVPInterpolationType 中的GAUSSIAN和LINEAR类型。\n\n\n\n注解\n\n在使用双线性金字塔时，输入图像和每一层的输出图像的尺寸需要满足[32, 4096]的范围限制。\n\n调用PyrDown API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImgs\n     输出图像数组，每一层的type和format与输入图像一致，每一层的width和height分别为上一层图像的一半。取整策略为：高斯金字塔为向上取整，\n     双线性金字塔为向下取整。\n   * [in] srcImg 输入图像，type支持U8C1，高斯金字塔format支持Y和nv12，双线性金字塔format支持nv12。\n   * [in] pymCfg 输入金字塔配置结构体，金字塔支持1到5层计算。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvppyrdown","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":633,"title":"hbVPPyrUp","content":"#\n\n\n\n调用PyrUp API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type和format与输入图像一致，其长和宽均为输入图像的两倍。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvppyrup","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":634,"title":"hbVPRemap","content":"#\n\n\n\n视觉算子hbVPRemap在创建任务前，需要准备hbVPRemapParam得到hbVPMapWrap，用于执行remap任务。\n\n注解\n * gdc底层仅支持双线性插值，不需要配置borderType、padValue参数；\n * 由于gdc实现原理与dsp不同，同一map传入gdc和dsp输出的图像精度会不同；\n * gdc对于map有如下要求：\n   * map坐标必须是正数；\n   * 尽量保持同一行里面的采样点是平滑过渡的，如果某两点之间过渡急剧变化，样条线的精度将不足以处理它；\n   * 不能4个采样点组成三角形；\n   * 如果采样精度不足，可能导致锯齿，扭曲等情况，此时推荐使用亚像素坐标；\n\n * 成员\n   \n   成员名称          描述\n   mapPhyAddr    map数据物理地址，数据类型为double。\n   mapVirAddr    map数据虚拟地址。\n   srcWidth      输入宽度。\n   srcHeight     输入高度。\n   mapWidth      map宽度（即目标宽度）。\n   mapHeight     map高度（即目标高度）。\n   interpoType   Remap中的插值类型，支持 hbVPInterpolationType 中的NEAREST和LINEAR类型。\n   borderType    保留参数。\n   padValue      保留参数。\n   dataType      map中的数据类型，支持 hbVPImageType 中的 HB_VP_IMAGE_TYPE_F64C2 类型。\n\n注解\n\n注意，map中数据排布（即phyAddr物理地址下的内存数据）为 x_0, y_0, x_1, y_1, ... ，内存长度应为 2 * mapWidth *\nmapHeight * sizeof(dataType)，详细使用情况请参考示例。\n\n 1. 当任务部署在GDC上时，srcWidth和mapWidth范围[100,3840]，srcHeight和mapHeight范围[100,2160]，且满\n    足：mapWidth<=srcWidth，mapHeight<=srcHeight。\n    由于GDC对数据有对齐限制，要求输入的stride按照width方向16对齐，即输入输出的stride范围[112,3840],\n    申请内存请以heigh和stride大小为准。\n 2. 当任务部署在DSP上时，srcWidth和mapWidth范围[32,4096]，srcHeight和mapHeight范围[16, 2160]。\n\n\n\nhbVPRemap 算子的参数描述句柄，句柄包含了不同Backend上运行所必要的描述信息，可重复使用。\n\n\n\n创建Remap的mapWrap参数。\n\n * 参数\n   * [out] mapWrap 创建出的参数，在接口中被使用，该参数必须指向nullptr。\n   * [in] param remap参数，用来创建统一的mapWrap参数。\n   * [in] backend 选择后端硬件，不同的硬件后端需要为map准备不同的硬件资源\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n注解\n 1. hbVPCreateMapWrap用于创建remap的map资源，由于不同backend所需要准备的硬件资源不同，因此此处需要指定backend用以确定\n    该map适用于哪个资源。\n 2. 接口支持硬件资源为GDC，DSP。指定DSP部署资源时，backend需要根据开发板硬件配置情况设置。例如，在单核DSP开发板上，backend可以为：\n    HB_UCP_DSP_CORE_ANY、HB_UCP_DSP_CORE_0。\n 3. 支持同时指定dsp和gdc两种backend，分别创建两种硬件map资源，如：backend=HB_UCP_GDC_CORE_0|HB_UCP_DSP_\n    CORE_ANY（指定创建GDC和DSP两种backend的硬件资源，且参数需要同时满足GDC与DSP尺寸约束,\n    HB_UCP_CORE_ANY不能与其它backend进行或运算）。\n\n\n\n释放Remap的mapWrap参数。\n\n * 参数\n   * [in] mapWrap 需要释放的mapWrap参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用Remap API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type和format与输入图像一致，大小和map的大小一致。\n   * [in] srcImg 输入图像，type支持U8C1，format dsp硬件支持Y和nv12，gdc支持nv12。\n   * [in] mapWrap map参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpremap","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":635,"title":"hbVPResize","content":"#\n\n\n\n调用Resize API，当图片格式为Y时，长宽的有效缩放范围为[0.25, 4]，当图片格式为NV12时，长宽的有效缩放范围为(0.25, 4]。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type和format与输入图像一致，图像尺寸可由输入图像和缩放比例计算得出。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] interpolation 算子插值类型。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpresize","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":636,"title":"hbVPRoiResize","content":"#\n\n\n\nRoiResize算子参数。\n\n * 成员\n   \n   成员名称              描述\n   interpolation     插值类型，支持 hbVPInterpolationType 中的NEAREST和LINEAR类型。\n   paddingValue[4]   Padding值，每个通道对应一个值，NV12使用三个通道。\n\n\n\n调用RoiResize API，其长宽的有效缩放范围为[0.25, 4]。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，format和type与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] roi ROI区域，有效范围取ROI区域与srcImg的交集。\n   * [in] roiResizeParam 算子参数,。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvproiresize","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":637,"title":"hbVPRotate","content":"#\n\n\n\nRotate算子参数。\n\n\n\n调用Rotate API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type和format与输入图像一致，其大小根据旋转角度与输入图像的大小决定。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] rotateDegree 算子参数，图片旋转角度。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvprotate","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":638,"title":"hbVPSepFilter2d","content":"#\n\n\n\nSepFilter2D算子参数。\n\n * 成员\n   \n   成员名称         描述\n   delta        保留参数。\n   pointLocX    保留参数。\n   pointLocY    保留参数。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用SepFilter2D API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和大小与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] filterKernelX 算子每行滤波系数，type支持F32C1，大小为1xN，N为小于等于9的正奇数。\n   * [in] filterKernelY 算子每列滤波系数，type支持F32C1，大小为Nx1，N为小于等于9的正奇数。\n   * [in] sepFilter2DParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpsepfilter2d","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":639,"title":"hbVPSobel","content":"#\n\n\n\nSobel算子参数。\n\n * 成员\n   \n   成员名称         描述\n   scale        保留参数。\n   delta        保留参数。\n   dx           导数 X 的阶。\n   dy           导数 Y 的阶。\n   kernelSize   sobel扩展核大小，支持大小为3或5。\n   borderType   填充类型，支持 hbVPBorderType 中的类型。\n\n\n\n调用Sobel API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，format和大小与输入图像一致，type支持S16C1。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] sobelParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpsobel","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":640,"title":"hbVPStitch","content":"#\n\n\n\n描述点位置的数据结构。hbVPStitch在对任务进行创建之前，需要准备hbVPPoint描述src源图在目标图上的位置，用于执行stitch任务。\n\n * 成员\n   \n   成员名称   描述\n   x      左上角的坐标x。\n   y      左上角的坐标y。\n\n\n\n调用创建Stitch的alpha-blend lut表API。\n\n * 参数\n   * [out] alphaBlendLut 指向lut表信息的句柄，具有stitch的alpha-blend lut表信息，该参数必须指向nullptr。\n   * [in] alphaDatas lut表的内存，由用户传入。\n   * [in] alphaBlendRegions\n     在dst图层需要alpha-blend的区域坐标。单个roi的宽度最大支持2000，高度最大支持2000。\n   * [in] alphaBlendRegionNum 在dst图层需要alpha-blend的区域个数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n注解\n 1. alpha-blend lut表是对每一个像素进行操作的系数，通过配置lut表，可以实现两张图的任意融合效果；\n 2. alpha-blend\n    lut表中的alpha值，会根据src给入顺序，赋值给指定的src图。如下图：src顺序为：src0、src1；那么blend-reg区域的alpha值\n    属于src0，（255-alpha）属于src1；\n\n\n\n调用释放Stitch的alpha-blend lut表API。\n\n * 参数\n   * [in] alphaBlendLut 指向lut表信息的句柄。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n注解\n 1. 算子中，源图在目标图上的位置仅需提供x、y坐标即可，w和h的值会从src中获取。\n 2. 调用一次hbVPStitch接口，实现不了重叠区域三张图融合的结果（比如想重叠区域实现src0 20%，src1 30%，src2\n    50%拼接效果），需多次执行hbVPStitch接口实现。\n 3. Stitch接口，调用一次最多支持输入四张src图，内部会首先将src图拼接到指定区域。\n 4. 如果拼接图像为nv12格式，dstPoses参数所指定的x和y坐标，以及拼接图像的宽高均需为偶数。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像。\n   * [in] srcImgs 输入图像数组。\n   * [in] dstPoses 输入图像在输出图像上的起点坐标数组，点的个数与顺序需要和输入图片一致。\n   * [in] srcImgCount 输入图像的个数。\n   * [in] alphaBlendLut 用于拼接的lut表信息。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * dstImg：type支持U8C1，format支持Y和nv12。dst图片的宽度最大支持3840，最小支持16；高度最大支持3840，最小支持2。宽\n     度需要16字节对齐。\n   * srcImgs：type支持U8C1，format支持Y和nv12，且type和format需要与输出图像一致，最多支持四张输入图。src图片的宽度最\n     大支持2000，最小支持16；高度最大支持2000，最小支持2。宽度需要16字节对齐。\n * 使用说明\n\n使用小于等于两张src图做拼接时：\n\n1.pipline流程(src0—copy；src1-copy)\n\n1). 首先会将src0贴到dst图层上。\n\n2). 然后将src1贴到dst图层上。\n\n3). 由于重叠区域不需要blend，重叠区域按照pipline顺序，由src1的区域进行覆盖。\n\n\n\n2.pipline流程(src0—copy；src1-copy ；src0-src1-blend)\n\n1). 首先会将src0贴到dst图层上。\n\n2). 然后将src1贴到dst图层上。\n\n3). 由于有重叠区域，且重叠区域需要进行blend操作，将重叠区域覆盖为src0和src1指定区域blend的结果。\n\n\n\n使用大于两种src图做拼接时：\n\n1.pipline流程(src0—copy；src1-copy；src2-copy；src0-src1-blend)\n\n1). 首先会将src0贴到dst图层上。\n\n2). 接着将src1贴到dst图层上。\n\n3). 然后将src2贴到dst图层上。\n\n4). 最后由于有重叠区域，且重叠区域需要进行blend操作，将重叠区域覆盖为src0和src1指定区域blend的结果。\n\n\n\n2.pipline流程(src0—copy；src1-copy；src2-copy；src0-src1-blend；src0-src2-blend)\n\n注解\n\n调用一次接口不支持，由于stitch是按pipline流程顺序刷新机制，做不到一次pipline完成三图融合结果展示（比如想重叠区域实现src0\n20%，src1 30%，src2 50%拼接效果）内部有判断机制，会对上述场景进行拦截报错\n\n正确做法（分为两个pipline执行）：\n\npipline0流程(src0—copy；src1-copy ；src0-src1-blend)\n\n1). 首先会将src0贴到dst图层上。\n\n2). 接着将src1贴到dst图层上。\n\n3). 最后由于有重叠区域，且重叠区域需要进行blend操作，将重叠区域覆盖为src0和src1指定区域blend的结果。\n\n\n\npipline1流程(src3-copy；src2-copy；src3-src2-blend)\n\n1). 首先会将src3贴到dst图层上。\n\n2). 接着会将src2贴到dst图层上。\n\n3). src3与src2有重叠区域，且重叠区域需要进行blend操作，将重叠区域覆盖为src3和src2指定区域blend的结果。\n\n","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpstitch","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":641,"title":"hbVPThreshold","content":"#\n\n\n\nThreshold算子参数。\n\n\n\nThreshold算子参数。\n\n * 成员\n   \n   成员名称     描述\n   thresh   阈值，值小于等于255。\n   maxVal   保留参数。\n   type     阈值类型，支持hbVPThresholdType中的类型。\n\n\n\n调用Threshold API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，支持U8C1，format Y，大小和输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y。\n   * [in] thresholdParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpthreshold","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":642,"title":"hbVPTranspose","content":"#\n\n\n\n调用 Transpose API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg\n     输出图像，type与format与输入图像一致，其width等于输入图像的height，height等于输入图像的width。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvptranspose","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":643,"title":"hbVPVideoDecode","content":"#\n\n\n\nVideo解码的参数。\n\n * 成员\n   \n   成员名称          描述\n   pixelFormat   输入图像的像素格式。当前支持 HB_VP_IMAGE_FORMAT_NV12 和\n                 HB_VP_IMAGE_FORMAT_YUV420 格式。\n   inBufSize     Video Processing Unit内部输入缓冲区大小，满足1024对齐。取值范围[1024,\n                 2^31-1)，通常可设置为height * width * Size(pixelFormat)。推荐值为10 *\n                 1024 * 1024。\n   outBufCount   Video Processing Unit内部输出缓冲区个数。取值范围[1，31]，推荐值为5。\n   videoType     编码协议的类型。当前支持 HB_VP_VIDEO_TYPE_H264 和 HB_VP_VIDEO_TYPE_H265\n                 格式。\n\n\n\n获取解码器默认的解码参数，其中 pixelFormat 参数为 HB_VP_IMAGE_FORMAT_YUV420 ，inBufSize 参数为 10 *\n1024 * 1024 ，outBufCount 参数为5。\n\n * 参数\n   * [in] param 解码参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 在调用接口前，需要将 videoType 指定为 HB_VP_VIDEO_TYPE_H264 或 HB_VP_VIDEO_TYPE_H265。\n\n\n\n创建解码器上下文API，创建前需要提前将context初始化为nullptr。\n\n * 参数\n   * [out] context 解码器上下文。\n   * [in] param 解码参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * Video Processing Unit最高支持创建32路编码或解码的上下文。\n   * 解码器首帧必须是包含VPS+SPS+PPS帧头信息的IDR帧，否则不能正常解码。\n   * 解码器不支持包含后向参考帧的B帧码流解码。\n\n\n\n释放解码器上下文API。\n\n * 参数\n   * [in] context 解码器上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用Video Processing Unit解码API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [in] srcBuf 存放H.264或者H.265数据的内存地址。\n   * [in] context 解码器的上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * Video解码时，支持的最小分辨率为256x128，最大分辨率为8192x4096。\n   * 由于Video Processing Unit硬件限制，Video Processing Unit编码或解码时最大支持同时提交31个任务。\n   * 仅支持使用异步方式创建任务。\n   * 为了避免系统资源浪费并提高Video Processing Unit解码性能，建议复用输入地址。\n\n\n\n获取解码数据buffer的API。\n\n * 参数\n   * [out] outImg 存放解码后数据的内存地址。\n   * [in] taskHandle 任务句柄，负责算子与UCP架构的交互。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 输出缓冲区 outImg 由Video Processing Unit内部申请。\n   * 当任务成功完成后，输出缓冲区中的数据有效；在任务释放阶段，输出缓冲区将被释放。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpvideodecode","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":644,"title":"hbVPVideoEncode","content":"#\n\n\n\nhbVPVideoEncode 算子和 hbVPVideoDecode 算子的context上下文描述句柄，句柄包含了Video Processing\nUnit上运行所必要的描述信息，可重复使用。\n\n\n\n视频编码类型枚举。\n\n * 成员\n   \n   成员名称                    描述\n   HB_VP_VIDEO_TYPE_H264   H.264视频编码协议。\n   HB_VP_VIDEO_TYPE_H265   H.265视频编码协议。\n\n\n\n码率控制模式枚举。\n\n * 成员\n   \n   成员名称                             描述\n   HB_VP_VIDEO_RC_MODE_H264_CBR     H.264编码协议CBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H264_VBR     H.264编码协议VBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H264_AVBR    H.264编码协议AVBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H264_FIXQP   H.264编码协议FixQp码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H264_QPMAP   H.264编码协议QPMap码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H265_CBR     H.265编码协议CBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H265_VBR     H.265编码协议VBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H265_AVBR    H.265编码协议AVBR码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H265_FIXQP   H.265编码协议FIXQP码率控制模式。\n   HB_VP_VIDEO_RC_MODE_H265_QPMAP   H.265编码协议QPMAP码率控制模式。\n\n\n\nH.264编码协议恒定码率参数。\n\n * 成员\n   \n   成员名称          描述\n   intraPeriod   I帧间隔。取值范围[0, 2047]，推荐值28。\n   bitRate       目标平均比特率，单位是kbps。取值范围[1, 700000]，推荐值1000kps。\n   frameRate     目标帧率，单位是fps。取值范围[1, 240]，推荐值30。\n   initialRcQp   初始码率量化值，取值范围[0, 51]，其中参数值越大，编码质量越差。若设置的参数值超出51时，则由Video\n                 Processing Unit硬件决定该参数的具体值。\n\n\n\nH.264编码协议可变码率参数。\n\n * 成员\n   \n   成员名称          描述\n   intraPeriod   I帧间隔。取值范围[0, 2047]，推荐值28。\n   intraQp       帧QP值。取值范围[0, 51]，推荐值30。\n   frameRate     目标帧率，单位是fps。取值范围[1, 240]，推荐值30。\n\n\n\nH.264编码协议固定量化参数。\n\n * 成员\n   \n   成员名称          描述\n   intraPeriod   I帧间隔。取值范围[0, 2047]，推荐值28。\n   frameRate     目标帧率，单位是fps。取值范围[1, 240]，推荐值30。\n   qpI           I帧的QP值。取值范围[0, 51]，推荐值0。\n   qpP           P帧的QP值。取值范围[0, 51]，推荐值0。\n   qpB           B帧的QP值。取值范围[0, 51]，推荐值0。\n\n\n\nH.264编码协议QP映射参数。\n\n * 成员\n   \n   成员名称              描述\n   intraPeriod       I帧间隔。取值范围[0, 2047]，推荐值28。\n   frameRate         目标帧率，单位是fps。取值范围[1, 240]，推荐值30。\n   qpMapArrayCount   指定QP\n                     Map的大小。计算公式为(ALIGN16(width)>>4)*(ALIGN16(height)>>4)，取值范围[1,\n                     8192x4096/(16x16)]。\n   qpMapArray        指定QP Map的数值。宏块大小为16x16，每个宏块需要指定一个QP值。每个QP值占用一个字节，并按光栅扫描顺序排列。\n\n\n\nH.264编码协议平均可变码率参数。\n\n * 成员\n   \n   成员名称          描述\n   intraPeriod   I帧间隔。取值范围[0, 2047]，推荐值28。\n   bitRate       目标平均比特率，单位是kbps。取值范围[1, 700000]，推荐值1000kps。\n   frameRate     目标帧率，单位是fps。取值范围[1, 240]，推荐值30。\n   initialRcQp   初始码率量化值，取值范围[0, 51]，其中参数值越大，编码质量越差。若设置的参数值超出51时，则由Video\n                 Processing Unit硬件决定该参数的具体值。\n\n\n\nH.265编码协议恒定码率参数，参数范围限制同hbVPVideoH264Cbr。\n\n\n\nH.265编码协议可变码率参数，参数范围限制同hbVPVideoH264Vbr。\n\n\n\nH.265编码协议平均可变码率参数，参数范围限制同hbVPVideoH264Avbr。\n\n\n\nH.265编码协议固定量化参数，参数范围限制同hbVPVideoH264FixQp。\n\n\n\nH.265编码协议中的QP映射参数。其宏块大小为 32x32，每个宏块需要指定一个QP值。 qpMapArrayCount 参数的计算公式为\n(ALIGN64(width)>>5) * (ALIGN64(height)>>5)，其取值范围为 [1,\n8192x4096/(32x32)]。其余参数范围限制同hbVPVideoH264QpMap。\n\n\n\nVideo Processing Unit编码协议码率控制参数。\n\n * 成员\n   \n   成员名称        描述\n   mode        码率控制模式。\n   h264Cbr     H.264编码协议CBR码率控制模式的参数。\n   h264Vbr     H.264编码协议VBR码率控制模式的参数。\n   h264Avbr    H.264编码协议AVBR码率控制模式的参数。\n   h264QpMap   H.264编码协议QpMap码率控制模式的参数。\n   h264FixQp   H.264编码协议FixQp码率控制模式的参数。\n   h265Cbr     H.265编码协议CBR码率控制模式的参数。\n   h265Vbr     H.265编码协议VBR码率控制模式的参数。\n   h265Avbr    H.265编码协议AVBR码率控制模式的参数。\n   h265QpMap   H.265编码协议QpMap码率控制模式的参数。\n   h265FixQp   H.265编码协议FixQp码率控制模式的参数。\n\n\n\nGOP结构参数。\n\n * 成员\n   \n   成员名称                  描述\n   decodingRefreshType   指定在I帧周期应用的解码刷新类型。取值范围[0, 2]，推荐值2。 0：NON_IRAP，1：CRA，2: IDR。\n   gopPresetIdx          GOP预设结构。推荐值2。支持1、2、3、6、7和9。\n\n\n\nVideo编码的参数。\n\n * 成员\n   \n   成员名称          描述\n   pixelFormat   输入图像的像素格式。当前支持 HB_VP_IMAGE_FORMAT_NV12 和\n                 HB_VP_IMAGE_FORMAT_YUV420 格式。\n   width         输入图像的宽度，满足32对齐。取值范围[256, 8192]。\n   height        输入图像的高度，满足8对齐。取值范围[128, 4096]。\n   outBufCount   Video Processing Unit内部输出缓冲区个数。取值范围[1, 1000]，推荐值5。\n   videoType     编码协议的类型。\n   rcParam       码率控制参数。\n   gopParam      GOP参数。\n\n\n\n获取编码器默认的编码参数，其中pixelFormat 参数为 HB_VP_IMAGE_FORMAT_YUV420， width 和 height\n参数均为0，需要根据实际尺寸进行指定， outBufCount 参数为5，decodingRefreshType 参数为2，gopPresetIdx\n参数为2，rcParam 的码率控制方式为 hbVPVideoH265Cbr，其参数默认值参考 hbVPVideoEncParam 结构体定义。\n\n * 参数\n   * [in] param 编码参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 在调用接口前，需要将 videoType 指定为 HB_VP_VIDEO_TYPE_H264 或 HB_VP_VIDEO_TYPE_H265。\n\n\n\n创建编码器上下文API，创建前需要提前将context初始化为nullptr。\n\n * 参数\n   * [out] context 编码器上下文。\n   * [in] param 编码参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * Video Processing Unit最高支持创建32路编码或解码的上下文。\n   * 编码器不支持包含B帧的后向参考帧的码流编码。\n\n\n\n释放编码器上下文API。\n\n * 参数\n   * [in] context 编码器上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n\n\n\n调用Video Processing Unit编码API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [in] srcImg 存放编码图像的内存地址。\n   * [in] context 编码器的上下文。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 由于Video Processing Unit硬件限制，Video Processing Unit编码或解码时最大支持同时提交31个任务。\n   * 仅支持使用异步方式创建任务。\n   * 为了避免系统资源浪费并提高Video Processing Unit编码性能，建议复用输入地址。\n\n\n\n获取编码数据buffer的API。\n\n * 参数\n   * [out] outBuf 存放编码后数据的内存地址。\n   * [in] taskHandle 任务句柄，负责算子与UCP架构的交互。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。\n * 接口限制说明\n   * 输出缓冲区 outBuf 由Video Processing Unit内部申请。\n   * 当任务成功完成后，输出缓冲区中的数据有效；在任务释放阶段，输出缓冲区将被释放。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpvideoencode","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":645,"title":"hbVPWarpAffine","content":"#\n\n\n\nWarpAffine算子参数。\n\n * 成员\n   \n   成员名称                 描述\n   transformMatrix[6]   WarpAffine转换矩阵（dst->src），2X3矩阵。\n   interpolation        WarpAffine中的插值类型，只支持 hbVPInterpolationType\n                        中的NEAREST和LINEAR类型。\n   borderType           保留参数。\n   borderValue          保留参数。\n   isInverse            标志transformMatrix是否为逆矩阵，非零表示逆矩阵（src = M * dst），0表示正矩阵（dst =\n                        M * src）\n\n\n\n调用WarpAffine API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type和format与输入图像一致，尺寸大小由转换矩阵计算得出。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] affineParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpwarpaffine","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":646,"title":"hbVPWarpPerspective","content":"#\n\n\n\nWarpPerspective算子参数。\n\n * 成员\n   \n   成员名称                 描述\n   transformMatrix[9]   WarpPerspective转换矩阵，3X3矩阵。\n   interpolation        WarpPerspective中的插值类型，支持 hbVPInterpolationType\n                        中的NEAREST和LINEAR类型。\n   borderType           保留参数。\n   borderValue          保留参数。\n   isInverse            标志transformMatrix是否为逆矩阵，非零表示逆矩阵（src = M * dst），0表示正矩阵（dst =\n                        M * src）\n\n\n\n调用WarpPerspective API。\n\n * 参数\n   * [out] taskHandle 任务句柄，负责算子与UCP架构的交互。\n   * [out] dstImg 输出图像，type、format和size与输入图像一致。\n   * [in] srcImg 输入图像，type支持U8C1，format支持Y和nv12。\n   * [in] perspectiveParam 算子参数。\n * 返回值\n   * 返回 0 则表示API成功执行，否则执行失败。","routePath":"/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpwarpperspective","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":648,"title":"","content":"由于当前地平线征程6算法工具链版本仍属于试用版本，仅面向试用客户提供，如有需要，现阶段您可联系相应的地平线项目接口人进行获取。","routePath":"/oe_obtain","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"}]