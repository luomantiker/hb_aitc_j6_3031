[{"id":0,"title":"Bev Multi-task Model Training","content":"#\n\nThe BEV reference algorithm is developed based on Horizon Torch Samples\n(Horizon's own deep learning framework), and you can refer to the Horizon Torch\nSamples usage documentation for an introduction to the use of Horizon Torch\nSamples. The training config for the BEV reference algorithm is located under\nthe HAT/configs/bev/ path. The following part takes\nHAT/configs/bev/bev_ipm_efficientnetb0_multitask_nuscenes.py as an example to\ndescribe how to configure and train the BEV reference algorithm.\n\n\nTraining Process#\n\nIf you just want to simply train the BEV model, then you can read this section\nfirst.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nHere is an example of the nuscense dataset, which can be downloaded from\nhttps://www.nuscenes.org/nuscenes . Also, in order to improve the speed of\ntraining, we have done a packing of the original jpg format dataset to convert\nit to lmdb format. Just run the following script and it will be successful to\nachieve the conversion.\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. After the packing is completed, the file\nstructure in the data directory should look as follows.\n\n\n\nThe train_lmdb and val_lmdb are the training and validation datasets after\npackaging, and are the datasets that the network will eventually read. metas is\nthe map information needed for the segmentation model.\n\n\nModel Training#\n\nBefore the network starts the training, you can first calculate the number of\nnetwork operations and parameters using the following command:\n\n\n\nThe next step is to start the training. Training can also be done with the\nfollowing script. Before training, you need to make sure that the dataset path\nspecified in the configuration has already been changed to the path of the\npackaged dataset.\n\n\n\nSince the HAT algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\nwhich, respectively, indicates the training of the floating-point model and the\nquantitative model, and the conversion of the quantitative model to the\nfixed-point model, where the training of the quantitative model depends on the\nfloating-point model produced by the previous floating-point training.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model, and quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\n\n\nWhere type under model indicates the name of the defined model, and the\nremaining variables indicate the other components of the model. The advantage of\ndefining the model this way is that we can easily replace the structure we want.\nFor example, if we want to train a model with a backbone of resnet50, we just\nneed to replace backbone under model .\n\n\nData Augmentation#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts data_loader and val_data_loader in the config configuration\nfile, corresponding to training set and the processing flow of the validation\nset. Take data_loader as an example.\n\n\n\nWhere type directly uses the interface torch.utils.data.DataLoader that comes\nwith pytorch, which represents the combination of batch_size size images\ntogether. The only thing to be concerned about here is probably the dataset\nvariable, CocoFromLMDB means reads the image from the lmdb dataset, and the path\nis the same path we mentioned in the first part of the dataset preparation.\ntransforms contains a series of data enhancements underneath. Except for the\nimage flip (RandomFlip), the other data transformations of the val_data_loader\nare the same as data_loader . You can also achieve the data augmentation you\nwant by inserting a new dict in transforms.\n\n\nTraining Strategies#\n\nIn order to train a model with high accuracy, a good training strategy is\nessential. For each training task, the corresponding training strategy is\ndefined in the config file as well, as can be seen from the variable\nfloat_trainer.\n\n\n\nThe float_trainer defines our training approach in the big picture, including\nthe use of distributed_data_parallel_trainer, the number of epochs for model\ntraining, and the choice of optimizer. Also, the callbacks reflect the small\nstrategies used by the model during training and the operations that you want to\nimplement, including the way to transform the learning rate\n(WarmupStepLrUpdater), the metrics to validate the model during training\n(Validation), and the operations to save (Checkpoint) the model. Of course, if\nyou have operations that you want the model to implement during training, you\ncan also add them in this dict way.\n\nNote\n\nIf reproducibility accuracy is needed, the training strategy in config is best\nnot modified. Otherwise, unexpected training situations may occur.\n\nWith the above introduction, you should have a clearer understanding of the\nfunctions of the config file. Then you can train a high-precision pure\nfloating-point detection model by the training script mentioned earlier. Of\ncourse, training a good detection model is not our ultimate goal, it is only\nused as a pretrain for us to train a fixed-point model later.\n\n\nQuantized Model Training#\n\nOnce we have a pure floating-point model, we can start training the\ncorresponding fixed-point model. As with floating-point training, we only need\nto run the following script to get a pseudo-quantization model, which can\nachieve the goal using only calibration.\n\n\n\nAs you can see, our config file has not changed, only the type of stage has been\nchanged. The training strategy we use at this point comes from\ncalibration_trainer in the config file.\n\n\n\n\nThe Value of Quantize Parameter is Different#\n\nWhen we train the quantized model, we need to set quantize=True, at this time\nthe corresponding floating point model will be converted into a quantized model,\nthe relevant code is as follows.\n\n\n\nFor key steps in quantization training, such as preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantization\nnodes, setting quantization parameters, and fusing operators, please read the\nQuantized Awareness Training (QAT) section.\n\n\nDifferent Training Strategies#\n\nAs we said before, quantization training is in fact finetune based on pure\nfloating-point training, so when quantization training, our initial learning\nrate is set to one-tenth of the floating-point training, the number of epochs\nfor training is largely reduced, most importantly, when defining the model , our\npretrained needs to be set to the address of a pure floating-point model that\nhas already been trained.\n\nAfter making these simple adjustments, we can start training our quantitative\nmodel.","routePath":"/en/guide/advanced_content/hat/examples/bev","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":505},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":827},{"text":"Model Training","id":"model-training","depth":2,"charIndex":1606},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2777},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2951},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3644},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3766},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3964},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4656},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4806},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":5182},{"text":"Training Strategies","id":"training-strategies","depth":2,"charIndex":6139},{"text":"Quantized Model Training","id":"quantized-model-training","depth":2,"charIndex":7553},{"text":"The Value of Quantize Parameter is Different","id":"the-value-of-quantize-parameter-is-different","depth":3,"charIndex":8033},{"text":"Different Training Strategies","id":"different-training-strategies","depth":3,"charIndex":8554}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":1,"title":"Car Keypoint Detection Model Training","content":"#\n\nThis tutorial primarily focuses on how to train a key point detection model from\nscratch using HAT on the CarFusion car key point dataset. The tutorial covers\nfloating-point, quantization, and fixed-point models.\n\nCarFusion is a car keypoint dataset that includes annotations for 12 keypoints\non cars: the centers of the front, rear, left, and right wheels, as well as the\npositions of the car lights and the corners of the car roof. Before starting the\nmodel training, the first step is to prepare the dataset. To do this, we need to\napply for the dataset on the official website. The official website link for\nCarFusion is: CarFusion. After scrolling down, you will find the dataset\napplication form. Once you submit the form, you will receive an email with the\ndataset download link. After downloading and extracting the dataset, the\ndirectory structure will be as follows:\n\n\n\nHere, we only consider a simpler scenario, which is to detect keypoints from\npre-detected car images. Therefore, we need to first crop the cars from the\nimages based on the annotated bounding boxes.\n\nTo convert the data into the cropped format, you can simply run the following\ncommand:\n\n\n\nThe directory structure of the processed cropped data will be as follows:\n\n\n\nThe file \"keypoints_test.json\" contains annotations for the test dataset, while\n\"keypoints_train.json\" contains annotations for the training dataset. Each\nsample is stored in JSON format as a dictionary with the following structure:\n\"img_path\": keypoints_list. The \"keypoints_list\" has a shape of (12, 3), where\neach row represents a keypoint. The first two elements of each row are the x and\ny coordinates of the keypoint, and the third element indicates the validity of\nthe keypoint. If the keypoint is outside the image or is invalid, the third\nelement is set to 0.\n\nNext, you can package the data into a LMDB format dataset. You can successfully\naccomplish this conversion by running the following script:\n\n\n\nAfter the packaging is completed, the file structure in the directory should\nlook as follows:\n\n\n\ntrain_lmdb and val_lmdb are the training and validation datasets, respectively,\nafter packaging. Now you can proceed to train the model using these datasets.\n\n\nModel Training#\n\nconfigs/keypoint/keypoint_efficientnetb0_carfusion.py includes all the settings\nrelated to model training in this tutorial.\n\nBefore starting the network training, you can use the following command to\ncalculate the computational complexity and the number of parameters in the\nnetwork:\n\n\n\nThe next step is to start the training. Training can also be accomplished using\nthe following script. Before training, make sure to confirm that the dataset\npaths in the configuration have been switched to the packaged dataset path.\n\n\n\nSince the HAT algorithm package utilizes a registration mechanism, it allows\neach training task to follow this pattern, it allows each training task to\nfollow this pattern, using train.py and config file to launch. Here train.py is\na unified training script, that is irrelevant to the task. The type of task we\nneed to train, the dataset to be used, and the hyperparameter settings for\ntraining are all specified in the designated config configuration file. In the\ncommand above, the parameter following --stage can be set to float or\ncalibration . These options correspond to training a floating-point model and\ntraining a quantized model, respectively. Training a quantized model relies on\nthe floating-point model generated from the previous floating-point training\nstep.\n\n\nExporting Fixed-point Model#\n\nAfter completing quantization training, you can proceed to export the\nfixed-point model. You can use the following command to perform the export:\n\n\n\n\nModel Validation#\n\nAfter completing the training, you will obtain the trained floating-point and\nquantized models. Similar to the training process, you can use the same method\nto perform metric evaluation on the trained models. This evaluation will provide\nmetrics labeled as Float and QAT corresponding to the floating-point and\nquantized models, respectively.\n\n\n\nSimilar to the training process, when the parameter following --stage is set to\nfloat or calibration , tools/predict.py can be used to perform validation on the\ntrained floating-point model or quantized model respectively.\n\nFor the fix-point model validation, we could use following command, but it\nshould be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script, which allows visualization of the\ninference results of the integer model.\n\n\n\n\nSimulation On-board Accuracy Validation#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Check and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide a script for model checking in HAT, allowing you to define the\nquantitative model and then check whether it can work properly on the BPU .\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into a hbm file that supports on-board running. This tool\ncan also predict the model performance on the BPU .\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this explanation, we will outline some considerations to be aware of during\nmodel training, primarily focusing on relevant settings in the config file.\n\n\nNetwork Structure#\n\nFor the lightweight car keypoint detection task, the network model\nHeatmapKeypointModel utilizes efficientnet-b0 as the backbone. It adds three\ntranspose convolutional layers to generate heatmaps, from which the keypoint\ncoordinates are decoded. By defining a dictionary variable, such asmodelin\ntheconfigconfiguration file, we can easily define and modify the model.\n\n\n\nThe model consists of the backbone , decode_head composed of transpose\nconvolutions, losses , and post_process modules. In the HeatmapKeypointModel ,\nthe input is cropped car images. The backbone is responsible for extracting\nimage features, while the decoder upsamples and generates heatmaps. The losses\nmodule utilizes the weighted MSELoss based on the heatmap positions as the\ntraining loss. The post_process module uses the HeatmapDecoder to convert the\nheatmap output into predicted keypoint locations.\n\n\nData Augmentation#\n\nSame as the definition of model , the data augmentation pipeline is achieved by\ndefining data_loader and val_data_loader two dicts in the config file, which\ncorresponds to the process pipelines in train and test set.\n\nTaking the example of the data_loader, data augmentation techniques used include\nRandomFlip , Resize , RandomPadLdmkData , and GaussianNoise . For the keypoint\ndetection task, it is also necessary to use GenerateHeatmapTarget to generate\nheatmap targets from the keypoint annotations.\n\n\n\nBecause the model that runs on the BPU eventually uses YUV444 images as input,\nwhile regular training images are typically in RGB format, HAT provides the\nto_yuv=True option in the ToTensor data transform to convert RGB format images\nto YUV444 format.\n\nHAT also offers the batch_processor interface for batch processing the data, but\nno additional augmentation is added in this case. The loss_collector is a\nfunction that retrieves the loss for the current batch data. As the model\nreturns a tuple (pred, loss) , the loss value is obtained by indexing the\ntuple's element at index 1.\n\n\n\nThe data transformation for the validation set is relatively simpler, as shown\nbelow:\n\n\n\n\nTraining Strategy#\n\nIn the configs/keypoint/keypoint_efficientnetb0_carfusion.py file, the\nfloat_trainer and calibration_trainer correspond to the training strategies for\nfloating-point, and quantized models, respectively. Below is an example of the\nfloat_trainer training strategy:\n\n\n\nThe float_trainer defines our overall training approach, including the use of\nmulti-GPU distributed training (distributed_data_parallel_trainer), the number\nof epochs for model training, and the choice of optimizer.\n\nThe model_convert_pipeline defines the transformation operations before the\nmodel starts training. In this case, the model first loads the pre-trained model\nof Efficientnet-b0 on ImageNet .\n\nThe callbacks define the strategies and operations used during the training\nprocess, including the learning rate variation CosLrUpdater , validation of\nmodel metrics (Validation), and saving the model (Checkpoint). If you have any\nspecific operations you want the model to perform during training, you can add\nthem in this dictionary format.\n\nThe train_metrics and val_metrics define the metrics to be monitored during\nmodel training and validation, respectively.\n\nIn summary, the float_trainer is responsible for connecting the entire logic of\nfloating-point model training.\n\n\nQuantization Model Calibration#\n\nOnce we have the pure floating-point model, we can proceed to the quantization\nprocess. We first use some data samples to calculate the scale parameters of\neach layer of the model quantization through the Calibration operation, so as to\nperform int8 quantization on the model. The relevant config is:\n\n\n\nThe converter defines the conversion steps before model calibration. The model\nis first loaded as a floating-point model and then, through the\nFloat2Calibration operation, pseudo-quantization nodes are inserted into each\nlayer of the model, transforming it into a calibration model.\n\nAfter the calibration process, the \"calibration\" model achieves performance of\nover 99% compared to the floating-point model. Therefore, there is no need for\nquantization training, and we can proceed with inference using the model\nconverted to a fixed-point model.\n\n\nModel Checking & Compilation & Simulation of On-board Accuracy Validation#\n\nFor HAT, the significance of quantized models lies in their ability to run\ndirectly on the BPU (Binary Processing Unit). Therefore, model checking and\ncompilation for quantized models are essential steps.\n\nThe compile_perf_hbir script mentioned above also allows you to define the\nquantitative model and then check if it runs properly on the BPU.\n\nYou can obtain the on-board accuracy of the model by using the validation_hbir\nscript. The usage is the same as described in the previous section.","routePath":"/en/guide/advanced_content/hat/examples/car_keypoint","lang":"en","toc":[{"text":"Model Training","id":"model-training","depth":2,"charIndex":2219},{"text":"Exporting Fixed-point Model","id":"exporting-fixed-point-model","depth":2,"charIndex":3536},{"text":"Model Validation","id":"model-validation","depth":2,"charIndex":3716},{"text":"Model Inference","id":"model-inference","depth":2,"charIndex":4434},{"text":"Simulation On-board Accuracy Validation","id":"simulation-on-board-accuracy-validation","depth":2,"charIndex":4569},{"text":"Fixed-point Model Check and Compilation","id":"fixed-point-model-check-and-compilation","depth":2,"charIndex":4768},{"text":"Training Details","id":"training-details","depth":2,"charIndex":5453},{"text":"Network Structure","id":"network-structure","depth":3,"charIndex":5629},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":6530},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":7733},{"text":"Quantization Model Calibration","id":"quantization-model-calibration","depth":3,"charIndex":9005},{"text":"Model Checking & Compilation & Simulation of On-board Accuracy Validation","id":"model-checking--compilation--simulation-of-on-board-accuracy-validation","depth":3,"charIndex":9893}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":2,"title":"CenterPoint Detection Model Training","content":"#\n\nThis tutorial focuses on how to use HAT to train a CenterPoint model on the\nlidar point cloud dataset nuscenes from scratch, including floating-point, QAT\nand quantized models.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\ndownload the full dataset (v1.0) from nuscenes dataset.\n\nAfter downloading, unzip and organize the folder structure as follows:\n\n\n\nto improve the speed of training, we did a package of data information files to\nconvert them into lmdb format datasets. The conversion can be successfully\nachieved by simply running the following script:\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. After the packing is completed, the file\nstructure in the data directory should look as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training and validation datasets, which\nare also the final datasets read by the network. meta contains information for\nmetrics initialization, copying from nuscenes.\n\nAlso, for nuscenes point cloud data training, it is necessary to generate\ndatabase files for each individual training target in the dataset and store it\nusing a .bin format file in tmp_nuscenes/lidar/nuscenes_gt_database. The path\ncan be modified as needed. At the same time, a file containing these database\ninformation in .pkl format needs to be generated. In addition, we need to\ncollect category information of all samples in train dataset and resample the\nwhole dataset, thus we can generate these information and save them in .pkl\nformat to accelerate training. Then, create these files by running the following\ncommand:\n\n\n\nAfter executing the above command, the following file directory is generated:\n\n\n\nnuscenes_gt_database and nuscenes_dbinfos_train.pkl are the samples that are\nused for sampling during training, and nuscenes_infos_train.pkl can be used to\naccelerate train dataset initialization.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the floating-point CenterPoint\ndetection network. Before the network training starts, you can test the\ncomputation and the number of parameters of the network by using the following\ncommand.\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nSince the HAT algorithm package uses an ingenious registration mechanism, each\ntraining task can be started in the form of this train.py plus the config\nconfiguration file. The train.py is the unified training script has nothing to\ndo with the task. What kind of task we need to train, what kind of dataset to\nuse, and training-related hyperparameter settings are all in the specified\nconfig configuration file. The config file provides key dicts such as model\nbuilding and data reading.\n\n\nModel Building#\n\nThe network structure of CenterPoint can refer to Paper , this is not described\nin detail here. We can easily define and modify the model by defining a\ndict-type variable such as model in the config configuration file.\n\n\n\nAmong them, the type under model means the name of the defined model, and the\nremaining variables mean the other components of the model. The advantage of\ndefining the model this way is that we can easily replace the structure we want.\nAfter starting, the training script calls the build_model interface to convert\nsuch a model of type dict into a model of type torch.nn.Module.\n\n\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts data_loader and val_data_loader in the config file, which\ncorrespond to the processing of the training and validation sets, respectively.\nHere we take data_loader as an example:\n\n\n\nHere, the type directly uses the interface torch.utils.data.DataLoader that\ncomes with pytorch, which represents the combination of batch_size size samples\ntogether. Here you may only need to pay attention the dataset variable, and the\ndata_path path is the path we mentioned in the first part of the dataset\npreparation. transforms contains a series of data augmentations.In\nval_data_loader, only point cloud reading, Format and Collect3D are available.\nYou can also implement your desired data augmentation operations by inserting a\nnew dict in transforms.\n\n\nTraining Strategy#\n\nTo train a model with high accuracy, a good training strategy is essential. For\neach training task, the corresponding training strategy is also defined in the\nconfig file, which can be seen from the variable float_trainer.\n\n\n\nThe float_trainer defines our training approach from the big picture, including\nthe use of distributed_data_parallel_trainer, the number of epochs for model\ntraining, and the choice of optimizer. At the same time, callbacks reflects the\nsmall strategies used by the model in the training process and the operations\nthat you want to implement, including the transformation method of the learning\nrate (CyclicLrUpdater), the indicator (Validation), and save (Checkpoint) the\noperation of the model. Of course, if you have operations that you want the\nmodel to implement during training, you can also add it in this dict way.\nfloat_trainer is responsible for concatenating the entire training logic, which\nis also responsible for model pretraining.\n\nNote\n\nIf you need to reproduce the accuracy, it is best not to modify the training\nstrategy in the config. Otherwise, unexpected training situations may arise.\n\nThrough the above introduction, you should have a clear understanding of the\nrole of the config file. Then, through the training script mentioned above, a\nhigh-precision pure floating-point detection model can be trained. Of course,\ntraining a good detection model is not our ultimate goal, it is just a\npre-training for our future training of quantitative models.\n\n\nQuantitative Model Training#\n\nWhen we have a floating-point model, we can start training the corresponding QAT\nmodel. In the same way as floating-point training, we can train a QAT model just\nby running the following script: BTW, it is recommended to add a calibration\nstage before quantization aware training. Calibration can provide better\ninitialization parameters for QAT.\n\n\n\nAs you can see, our configuration file has not changed, only the type of stage\nhas been changed. At this point, the training strategy we use comes from the\nqat_trainer in the config file.\n\n\n\n\nWith Different model_convert_pipeline Parameters#\n\nBy setting model_convert_pipeline when training quantitative models, the\ncorresponding floating-point model can be converted into a quantitative model,\nas below:\n\n\n\nFor the key steps in quantitative training, such as preparing floating-point\nmodels, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please read the Quantized\nAwareness Training (QAT) section.\n\n\nWith Different Training Strategies#\n\nAs we said before, the quantitative training is actually finetue on the basis of\npure floating-point training. Therefore, when quantized training, our initial\nlearning rate is set to one-tenth of the floating-point training, the number of\nepochs for training is also greatly reduced, and most importantly, when model is\ndefined, our pretrained needs to be set to the address of the pure\nfloating-point model that has been trained.\n\nAfter making these simple adjustments, we can start training our quantitative\nmodel.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Validation#\n\nAfter the model is trained, we can also validate the performance of the trained\nmodel. Since we provide two stages of training process, float and qat, we can\nvalidate the performance of the model trained in these two stages. Run the\nfollowing two commands:\n\n\n\nAt the same time, we also provide a performance test of the quantization model,\njust run the following command, but it should be noted that hbir must be\nexported first:\n\n\n\nThe displayed accuracy is the real accuracy of the final int8 model. Of course,\nthis accuracy should be very close to the accuracy of the qat verification\nstage.\n\n\nSimulation of On-board Accuracy Validation#\n\nIn addition to the model validation described above, we offer an accuracy\nvalidation method that is identical to the board side, you can refer to the\nfollowing:\n\n\n\n\nModel Inference and Results Visualization#\n\nIf you want to see the detection effect of the trained model for a lidar point\ncloud file, we also provide point cloud prediction and visualization scripts in\nour tools folder, you just need to run the following script.\n\n\n\n\nModel Checking and Compilation#\n\nAfter training, the quantized model can be compiled into an hbm file that can be\nrun on the board by using the compile_perf_hbir tool. At the same time, the tool\ncan also estimate the running performance on the BPU. The following scripts can\nbe used:\n\n","routePath":"/en/guide/advanced_content/hat/examples/centerpoint","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":181},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":1950},{"text":"Model Building","id":"model-building","depth":3,"charIndex":2801},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":3423},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":4280},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":5801},{"text":"With Different model_convert_pipeline Parameters","id":"with-different-model_convert_pipeline-parameters","depth":3,"charIndex":6373},{"text":"With Different Training Strategies","id":"with-different-training-strategies","depth":3,"charIndex":6861},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":7417},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":7591},{"text":"Simulation of On-board Accuracy Validation","id":"simulation-of-on-board-accuracy-validation","depth":3,"charIndex":8206},{"text":"Model Inference and Results Visualization","id":"model-inference-and-results-visualization","depth":3,"charIndex":8416},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":8684}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":3,"title":"MobileNetV1 Classification Model Training","content":"#\n\nThis tutorial focuses on how to train a state-of-art floating-point and\nfixed-point model on ImageNet using HAT.\n\nImageNet is the most often used dataset for image classification, and many\nstate-of-the-art image classification studies are primarily based on this\ndataset for validation.\n\nAlthough there are many ways to obtain a state-of-art classification model in\nthe community or through other means, training a state-of-art classification\nmodel from scratch is still not a simple task.\n\nStarting from dataset preparation, this tutorial will focus on how to train a\nstate-of-art model on ImageNet, including floating-point, quantitative, and\nfixed-point models.\n\nThe ImageNet dataset can be downloaded from ImageNet official website. The\ndownloaded dataset will be in the following formats:\n\n\n\nHere we use MobileNetV1 as an example to describe the whole classification\nprocess in detail.\n\n\nTraining Process#\n\nIf you just want to use the HAT interface for simple experiments, it is a good\nidea to read this section first.\n\nHAT uses the tools + config format for all the training and evaluation tasks.\nAfter preparing the raw dataset, we can easily complete the training process by\ntaking the procedure below.\n\n\nDataset Preparation#\n\nThe first thing is dataset packaging. Comparing with raw datasets, packed\ndatasets have obvious advantages in terms of processing speed. Here we choose\nthe LMDB packaging method, which has the same style as PyTorch. Thanks to the\nflexibility of HAT in handling datasets, other forms of dataset packing and\nreading, such as MXRecord, are also supported independently.\n\nPackaging scripts for common datasets such as cityscapes, imagenet, voc, and\nmscoco are provided in the tools/datasets directory. For example, the\nimagenet_packer script can directly convert the original public ImageNet dataset\nto Numpy or Tensor format by using the default public dataset processing method\nprovided by torchvision, and finally compress the result data into the LMDB file\nusing the msgpack method.\n\nThe dataset packing process can be easily accomplished with the following\nscript:\n\n\n\nAfter the dataset is packed, you can get the LMDB dataset containing ImageNet\nand move on to the next stage - training.\n\n\nModel Training#\n\nOnce you have prepared the packed dataset, you can start training the model\nsimply by running the following command:\n\n\n\nThe HAT algorithm toolkit uses a registration mechanism that allows each\ntraining task to be started in the form of train.py plus a config file.\n\ntrain.py is a uniform training script and independent of the task. The task we\nneed to train, the dataset we need to use, and the hyperparameters we need to\nset for the training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\" and\n\"calibration\", which, respectively, indicates the training of the floating-point\nmodel and the quantitative model, where the training of the quantitative model\ndepends on the floating-point model produced by the previous floating-point\ntraining. The details are described in the preceding section about quantization.\n\n\nExporting Fixed-point Model#\n\nAfter completing quantization training, you can proceed to export the\nfixed-point model. You can use the following command to perform the export:\n\n\n\n\nModel Validation#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively, as described in Quantization details.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model and quantitative\nmodel respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results of the\nmodels trained at each phase:\n\n\n\n\nSimulation On-board Accuracy Validation#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Compilation#\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into a hbm file that supports on-board running. This tool\ncan also predict the model performance on the BPU.\n\n\n\nFor Horizon BPUs with different architectures, you can set march = March.NASH_E\nor march = March.NASH_M in configs/classification/mobilenetv1_imagenet.py.\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nWe still use MobileNetV1 as an example to illustrate some things that need to be\ntaken into account for model training, which mainly includes settings related to\nconfig.\n\n\nTraining Conditions#\n\nTraining a deep learning model on ImageNet with more than 1 million images is\nvery resource intensive, and the main bottlenecks are matrix computation and\ndata reading.\n\nFor matrix computation, it is highly recommended to use a high-performance GPU\ninstead of CPU for the training, and using multiple GPUs at the same time can\neffectively reduce the training time.\n\nFor data reading, it is recommended to use a better CPU and SSD storage.\nMulti-threaded CPU acceleration and better SSD storage can help a lot in data\nreading. Note that the whole ImageNet will roughly take up 300G of storage, so\nthe SSD storage should have at least 300G of storage space.\n\n\nNetwork Structure#\n\nAs you can find many implementations of MobileNetV1 in both HAT and other\ncommunities, here we skip the specific implementation methods of MobileNetV1.\n\nIn the config of HAT, we can build a floating-point MobileNetV1 classification\nmodel directly with the dict below. You can modify the model by directly\nmodifying the configuration parameters in backbone.\n\n\n\nIn addition to backbone, the model also has a losses module.\n\nIn common classification models, we often use Cross-Entropy as the training\nloss, but more and more experiments prove that adding Label-Smooth to the\nclassification loss can help to improve the training results, especially when\ncombined with Cosine's lr update method.\n\nAfter defining a model, especially for some public models, we usually have the\nneed to check the FLOPs. HAT calculates the operations of the model by using the\ncalops tool, which is implemented as below:\n\n\n\nSuch ops-counting tool can support both floating-point and fixed-point models.\n\n\nData Enhancement#\n\nThere is an emerging consensus on the data enhancement for ImageNet training,\nand we use the data enhancement provided by torchvision as the basis to build\nthe data enhancement for classification training, including RandomResizedCrop,\nRandomHorizontalFlip, and ColorJitter.\n\nSince the final model running on the BPU uses YUV444 as image input, while\ntraining image input is generally in the RGB format, HAT provides BgrToYuv444\ndata enhancement to convert RGB to YUV444.\n\nTo optimize the training process, some enhancement can be processed in\nbatch_processor to optimize the training.\n\n\n\nThe corresponding batch_processor part:\n\n\n\nThe data conversion of the validation set is relatively simpler, and the main\ndifference is the short edge Resize to 256 and CenterCrop.\n\nThe other color space transformations are the same as the training set.\n\n\n\n\nTraining Strategies#\n\nThe training strategies for training different classification models on ImageNet\nare roughly the same with minor differences.\n\nHere we focus on the details that have improved effects.\n\nThe learning strategy of Cosine with Warmup has some boosting effect compared\nwith the regular StepLr. Appropriately extending the epoch training length also\nhas a boost for small models.\n\nIn addition, applying L2 norm only to the parameters of weight is also a\nrecommended training strategy. The float_trainer, calibration_trainer, and\nint_trainer in the configs/classification/mobilenetv1_imagenet.py file\ncorrespond to the training strategies for floating-point, quantitative, and\nfixed-point models, respectively.\n\nNext we use float_trainer as an example of training strategy:\n\n\n\n\nQuantitative Training#\n\nFor key stages in quantitative training, e.g., preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantitative\nnodes, setting quantitative parameters, and operator fusion, etc., please read\nthe Quantized Awareness Training (QAT) section.\n\nHere we focus on how to define and use quantitative models in the HAT\nclassification.\n\nWhen the model is ready and some existing modules are quantized, HAT uses the\nfollowing scripts in the training script to map the floating-point model to the\nfixed-point model.\n\n\n\nAs the strategies for quantitative training are not necessarily the same, here\nwe briefly introduce some common strategies used in training classification\nmodels.\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately.\n\nBecause there is a floating-point pre-training model, the learning rate Lr of\nquantitative training can be rather small, usually starting from 0.001 or\n0.0001, and can perform Lr adjustments of scale=0.1 for 1 or 2 times with\nStepLrUpdater; meanwhile, the training length does not need to be long.\n\nIn addition, weight decay will also have some influence on the training results.\n\n\nPre-trained Models#\n\nHAT already provides a rich set of pre-trained models on ImageNet, you can refer\nto modelzoo. All models are included in the release package.","routePath":"/en/guide/advanced_content/hat/examples/classification","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":895},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":1215},{"text":"Model Training","id":"model-training","depth":3,"charIndex":2228},{"text":"Exporting Fixed-point Model","id":"exporting-fixed-point-model","depth":3,"charIndex":3116},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":3296},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":4023},{"text":"Simulation On-board Accuracy Validation","id":"simulation-on-board-accuracy-validation","depth":3,"charIndex":4155},{"text":"Fixed-point Model Compilation","id":"fixed-point-model-compilation","depth":3,"charIndex":4354},{"text":"Training Details","id":"training-details","depth":3,"charIndex":4869},{"text":"Training Conditions","id":"training-conditions","depth":3,"charIndex":5060},{"text":"Network Structure","id":"network-structure","depth":3,"charIndex":5740},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":6740},{"text":"Training Strategies","id":"training-strategies","depth":3,"charIndex":7604},{"text":"Quantitative Training","id":"quantitative-training","depth":3,"charIndex":8396},{"text":"Pre-trained Models","id":"pre-trained-models","depth":2,"charIndex":9700}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":4,"title":"Deformable DETR Detection Model Training","content":"#\n\nThis tutorial uses Deform-detr as an example of how to train a detection model\nfor a fixed point using the HAT algorithm package. Before starting Quantized\nAwareness Training (QAT), which is also known as fixed-point model training, you\nfirst need to train a pure floating-point model with high accuracy, and then you\ncan quickly train a fixed-point model by doing finetune based on this pure\nfloating-point model. So we start by training a pure floating-point Deform-detr\nmodel.\n\n\nTraining Process#\n\nAs with other tasks, for all training and evaluation tasks, HAT is unified in\nthe form of tools + config. After preparing the raw dataset, the following\nprocess can be used to conveniently completes the entire training process.\n\n\nDataset Preparation#\n\nBefore we start training the model, the first step is to prepare the dataset,\nhere we download MSCOCO's train2017.zip and val2017.zip as the training and\nvalidation sets for the network, and we also need to download the corresponding\nlabeled data annotations_trainval2017.zip , and the structure of the data\ndirectory after unzipping is shown below:\n\n\n\nMeanwhile, in order to improve the speed of training, we have done a packing of\nthe original dataset in jpg format and converted it to lmdb format dataset.\nSimply run the following script for successful conversion:\n\n\n\nThe above two commands correspond to converting the training dataset and the\nvalidation dataset, respectively. After the packaging is completed, the file\nstructure in the data directory should be as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training dataset and validation\ndataset, and the final dataset read by the network.\n\n\nModel Training#\n\nOnce the dataset is ready, it is time to start training the floating-point\nDeformable Detr detection network. You can use the following commands to test\nthe amount of computation and the number of parameters of the network first\nbefore the network training starts:\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nThe above commands complete the training of floating-point model and fixed-point\nmodel respectively, where the training of fixed-point model needs to be based on\nthe trained floating-point model. For details, please refer to Quantized\nAwareness Training (QAT) section.\n\n\nExporting Fixed-point Model#\n\nAfter completing the quantization training, you can start exporting the\nfixed-point model. It can be exported with the following command:\n\n\n\n\nModel Validation#\n\nAfter completing the training, the trained floating-point, quantized or\nfixed-point model can be obtained. Similar to the training method, we can use\nthe same method to do metrics validation on the trained model, obtaining metrics\nas Float, Calibration, and Quantized, which are floating-point, calibrated,\nquantized, and fully fixed-point metrics, respectively.\n\n\n\nSimilar to training models, --stage followed by \"float\", \"calibration\" and \"qat\"\ncan be used to validate trained floating-point models and quantized models,\nrespectively. The following command can be used to verify the accuracy of the\nfixed-point model, but it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to provide a visual presentation of the\ninference results from the fixed-point model:\n\n\n\n\nSimulation on Board Accuracy Verification#\n\nIn addition to the model validation described above, we also provide an accuracy\nvalidation method that is identical to that of the upper board, which can be\naccomplished in the following manner:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nThe quantization training toolchain integrated in HAT is primarily intended for\nHorizon's computing platforms, and is therefore necessary for the checking and\ncompilation of quantization models. We provide an interface for model checking\nin HAT, which allows you to define a quantitative model and then check whether\nit can work properly on BPU:\n\n\n\nAfter the model has been trained, the compile_perf_hbir script can be used to\ncompile the quantization model into a hbm file that can be run on the board, and\nthe tool can also predict the performance of the run on the BPU:\n\n\n\nThe above is the whole process from data preparation to generation of quantized\ndeployable models.\n\n\nTraining Details#\n\n\nModel Structure#\n\nThe network structure of Deformable DETR can be found in Paper, which is not\ndescribed in detail here. We can easily define and modify the model by defining\na dict variable like model in the config file.\n\n\n\nWhere type under model indicates the name of the defined model, and the\nremaining variables indicate the other components of the model. The advantage of\ndefining the model this way is that we can easily replace the structure we want.\nFor example, if we want to train a model where backbone is efficientnet, just\nreplace backbone under model and set neck accordingly.\n\n\nData Enhancement#\n\nAs with the definition of model, the process of data enhancement is realized by\ndefining the dicts data_loader and val_data_loader in the config configuration\nfile, which correspond to the processing flow of the training set and validation\nset, respectively. Take the data_loader as an example:\n\n\n\nWhere type directly uses pytorch's own interface torch.utils.data.DataLoader,\nwhich represents the combination of pictures of batch_size size. The only thing\nto focus on here is probably the dataset variable, CocoFromLMDB, which indicates\nthe image to be read from the lmdb dataset. The path is the same path we\nmentioned in the first part of the dataset preparation. A number of data\nenhancements are included under transforms. The val_data_loader imposes no data\nenhancements. You can also achieve your desired data enhancement by inserting\nnew dicts into transforms.\n\n\nTraining Strategy#\n\nIn order to train a model with high accuracy, a good training strategy is\nessential. For each training task, the corresponding training strategy is also\ndefined in one of the config files. This can be seen in the variable\nfloat_trainer.\n\n\n\nThe float_trainer defines our training approach in the big picture, including\nthe use of multi-card distributed training (distributed_data_parallel_trainer),\nthe number of epochs for which the model will be trained, and the choice of\noptimizer. One of the optimizers is the AdamW optimizer wrapped by\ncustom_param_optimizer, which allows for finer control of optimization\nparameters for backbone and norm layers in the model. The callbacks also\nrepresent the small strategies used by the model during training and the actions\nthat you want to implement, including the way the learning rate is transformed\n(StepDecayLrUpdater), the way the model is validated during training\n(Validation), and the action of saving (Checkpoint) the model. Of course, if you\nhave your own operations that you want the model to implement during training,\nyou can also add them in this dict way.\n\nNote\n\nIf you need to reproduce the accuracy, the training strategy in config is best\nleft unchanged. Otherwise unexpected training situations may occur.\n\nWith the above introduction, you should have a clearer understanding of the\nfunction of the config file. Then with the training script mentioned earlier, a\nhighly accurate pure floating-point detection model can be trained. Of course,\ntraining a good detection model is not our ultimate goal, it just serves as a\npretrain for us to train the fixed-point model later.\n\n\nQuantitative Model Training#\n\nOnce we have a pure floating-point model, we can start training the\ncorresponding fixed-point model. In the same way as the floating point training,\nwe can train the fixed point model simply by running the following script.\n\n\n\nAs you can see, our configuration file has not changed, only the type of stage.\nDeformable DETR is recommended to be added to the calibration process during\nquantization training. Calibration can provide a better initialization parameter\nfor quantization training of QAT. At this point we use the training strategy\nfrom the calibration_trainer and qat_trainer in the config file.\n\n\n\nWe first obtain a good initialization parameter for QAT through calibration,\ncalibration loads the trained floating-point model, and then searches for\nquantization parameters using mse's calibration, where Float2Calibration\nconverts the model from a floating-point model to a calibrated model.\n\n\n\nNext, we perform QAT quantization training based on the calibration model. The\ninitial learning rate is set to one-tenth of the floating-point training, and\nthe number of training epochs is greatly reduced. Notice that Float2QAT in\nconverter converts the model from a floating point model to a QAT model and then\nthe loaded calibration model weights. After quantization training, the accuracy\nof the Deformable DETR quantization model can reach more than 99% of the\naccuracy of the floating-point model.","routePath":"/en/guide/advanced_content/hat/examples/deform_detr","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":484},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":733},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1664},{"text":"Exporting Fixed-point Model","id":"exporting-fixed-point-model","depth":3,"charIndex":2305},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":2477},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3177},{"text":"Simulation on Board Accuracy Verification","id":"simulation-on-board-accuracy-verification","depth":3,"charIndex":3323},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3567},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4289},{"text":"Model Structure","id":"model-structure","depth":3,"charIndex":4309},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":4903},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5792},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":7450}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":5,"title":"DenseTNT Trajectory Prediction Model Training","content":"#\n\nThis tutorial mainly shows you how to train a DenseTNT model from scratch on the\ndataset Argoverse 1.1 , including floating-point, quantized, and fixed-point\nmodels.\n\n\nTraining Process#\n\nIf you just want to simply train the GaNet model, then you can read this section\nfirst.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\nwhich can be downloaded in the Argoverse 1 . At the same time, you need to\nprepare the list of files corresponding to the training data and validation\ndataset, and you can download Training and Validation and HD Map data.\n\nAfter downloading, unzip and organize the folder structure as follows:\n\n\n\nIn order to improve the speed of training, we made a package of data information\nfiles and converted them into a dataset in LMDB format. Just run the script\nbelow to successfully achieve the conversion:\n\n\n\nThe above two commands correspond to the transformation training dataset and the\nvalidation dataset respectively, after packaging, the file structure in the\n${target-data-dir} directory should be as follows:\n\n\n\ntrain_lmdb and val_lmdb are packaged training datasets and validation datasets,\nand then you can start training the model.\n\n\nModel Training#\n\nThe next step is to start training. Training can also be done through the\nfollowing script, and you need to confirm whether the dataset path in the\nconfiguration has been switched to the packaged dataset path before training.\n\n\n\nSince the algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\n\"qat\", or \"int_infer\", which, respectively, indicates the training of the\nfloating-point model and the quantitative model, and the conversion of the\nquantitative model to the fixed-point model, where the training of the\nquantitative model depends on the floating-point model produced by the previous\nfloating-point training, and the conversion of the fixed-point model depends on\nthe quantitative model generated in the quantitative training.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model, and quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\nThe network structure of DenseTNT can be found in the Paper, which is not\ndescribed in detail here.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nIn addition to the backbone, the model also has head, post_process, losses\nmodules. Among them, backbone is mainly to extract the features of the image,\nand head is mainly used by the features to obtain the predicted parallax value.\nThe post_process is mainly the post-processing part, and the losses module uses\nthe SmoothL1Loss in the paper as the training loss, and the loss_weights is the\nweight of the corresponding loss.\n\n\nData Loader#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining data_loader and val_data_loader in the config file, which correspond to\nthe processing of the training sets and verification sets, respectively.\n\n\n\nA loss_collector function is passed in batch_processor to get the loss for the\ncurrent batch data, as follows:\n\n\n\nThe data transformation of the validation set is relatively simpler, as follows:\n\n\n\n\n\n\nTraining Strategy#\n\nTraining a floating-point model on the SceneFlow dataset uses the learning\nstrategy of Cosine with Warmup, and impose L2 norm on the weight parameter.\n\nThe float_trainer, qat_trainer, and int_trainer in the\nconfigs/traj_pred/densetnt_vectornet_argoverse1.py file correspond to the\ntraining strategies for floating-point, quantitative, and fixed-point models,\nrespectively.\n\nThe following is an example of float_trainer training strategy:\n\n\n\n\nQuantization Training#\n\nFor key steps in quantitative training, such as preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantization\nnodes, setting quantitative parameters, and operator fusion, please read the\nQuantized Awareness Training (QAT) section. Here we focus on how to define and\nuse the quantization models in trajectory prediction model.\n\nIf the model is ready, and some existing modules are quantized, you can use the\nfollowing script in the training script to map the floating-point model to the\nfixed-point model uniformly.\n\n\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately. Due to the existence of the floating-point pre-training\nmodel, the learning rate Lr for quantitative training can be very small.\nGenerally, you can start from 0.001 or 0.0001, and you can do one or two times\nLr adjustments of scale=0.1 with StepLrUpdater without prolonging the training\ntime. In addition, weight decay will also have some effect on the training\nresults.\n\nThe quantitative training strategy for the DenseTNT example model can be found\nin the configs/traj_pred/densetnt_vectornet_argoverse1.py file.","routePath":"/en/guide/advanced_content/hat/examples/densetnt","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":170},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":494},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1433},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2576},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2750},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3443},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3565},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3763},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4455},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4605},{"text":"Data Loader","id":"data-loader","depth":3,"charIndex":5265},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5714},{"text":"Quantization Training","id":"quantization-training","depth":3,"charIndex":6176}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":6,"title":"Docker Image","content":"#\n\n\nHow to Start#\n\nTo facilitate the use of the HAT algorithm toolkit, we provide a command to\nstart the docker environment with one key to simplify the procedure of\nenvironment setup.\n\nAfter getting the release package, you can go to the path where the package is\nlocated, and then complete the basic docker environment installation, add docker\ngroup users, and pull docker images according to the Docker Container Deployment\n, and then use the following command to prepare the environment for running the\nHAT example.\n\n\n\nNote\n\nThe version number here is only an example, please replace it with the real\nversion number of the image you have obtained.\n\nwhere openexplorer/ai_toolchain_ubuntu_22_j6_gpu:{version} is the image name and\n-v is used to mount the local path to the docker path.\n\n\nGet Execution Script#\n\nAfter successfully starting the docker environment, get the compressed file from\nthe release package and extract the configs and tools folders.\n\nOnce you have all the environments, you can follow the training tutorial to\ntrain a fixed-point model using HAT step by step.","routePath":"/en/guide/advanced_content/hat/examples/docker","lang":"en","toc":[{"text":"How to Start","id":"how-to-start","depth":2,"charIndex":3},{"text":"Get Execution Script","id":"get-execution-script","depth":2,"charIndex":790}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":7,"title":"FCOS Detection Model Training","content":"#\n\nThis tutorial shows how to train a fixed-point detection model using the HAT\nalgorithm toolkit by using FCOS-efficientnet as an example. Fcos suggests adding\nthe calibration process in the quantization aware training. Calibration can\nprovide a better initialization parameter for quantization aware training.\n\nBefore starting the quantization aware training, namely, the fixed-point model\ntraining, you need to first train a pure floating-point model with high\naccuracy. Then, by finetuning this pure floating-point model, you can train the\nfixed-point model quickly.\n\nLet's start from training a pure floating-point FCOS-efficientnet model.\n\n\nTraining Process#\n\nIf you just want to simply train the GaNet model, then you can read this section\nfirst.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first stage is to prepare the dataset,\nhere we download MSCOCO's train2017.zip and val2017.zip as the training and\nvalidation sets for the network, and we need to download the corresponding label\ndata annotations_trainval2017.zip.\n\nAfter unpacking, the data directory structure is shown as below:\n\n\n\nAlso, to improve the training speed, we packaged the original JPG format dataset\nand converted it to the LMDB format. You can perform the conversion by simply\nrunning the following script:\n\n\n\nThese two commands are for training dataset conversion and validation dataset\nconversion respectively. When the packing is done, the file structure of data\nshould look as follows:\n\n\n\nThe above train_lmdb and val_lmdb are the packed training dataset and validation\ndataset, which are also the final datasets read by the network.\n\n\nFloating-point Model Training#\n\nOnce datasets are ready, you can start training the floating-point\nFCOS-efficientnet detection network. Before the network training starts, you can\nfirst test the number of operations and parameters of the network using the\nfollowing commands:\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nSince the HAT algorithm toolkit uses an ingenious registration mechanism, each\ntraining task can be started in the form of train.py plus a config file.\n\ntrain.py is a uniform training script and independent of the task. The task we\nneed to train, the dataset we need to use, and the hyperparameters we need to\nset for the training are all in the specified config file.\n\nThe config file provides the key dict for model building, data reading, etc.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model, and quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Building#\n\nThe network structure of fcos can be found in the Paper and here we will skip\nthe details.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nThe type under model is the name of the defined model, and the remaining\nvariables stand for the other components of the model.\n\nBy defining the model in this way, we can easily replace the structure we want.\nFor example, if we want to train a model with a backbone of resnet50, we just\nneed to replace backbone under model.\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts, data_loader and val_data_loader, in the config file,\ncorresponding to the processing process of the training set and validation set,\nrespectively.\n\nTake data_loader as an example:\n\n\n\nWhere the type directly uses Pytorch's own interface\ntorch.utils.data.DataLoader, which means to combine images of the size\nbatch_size together. The only thing to note here is probably the dataset\nvariable. CocoFromLMDB means to read images from the LMDB dataset, and the path\nuses the one mentioned in the first section Dataset Preparation. The transforms\ncontains a series of data enhancements. The data transformation in\nval_data_loader is the same as data_loader except for image flipping\n(RandomFlip). You can also impelement your own data enhancements by inserting\nnew dict in transforms.\n\n\nTraining Strategy#\n\nA good training strategy is essential for training a model with high accuracy.\n\nFor each training task, the corresponding training strategy is also defined in\nthe config file, as can be seen from the float_trainer variable.\n\n\n\nfloat_trainer defines the training approach in general, including the use of\nmulti-card distributed training (distributed_data_parallel_trainer), the number\nof epochs for model training, and the choice of optimizer.\n\nThe callbacks reflects the small strategies used by the model during the\ntraining and the operations that you want to implement, including the way to\ntransform the learning rate (WarmupStepLrUpdater), the metrics to validate the\nmodel during training (Validation), and the operations to save (Checkpoint) the\nmodel.\n\nOf course, if you have your own operations that you want the model to implement\nduring the training, you can also add them in this way using dict.\n\nThe float_trainer is responsible for linking the entire training logic together,\nwhich will also be responsible for the pretrain of the model.\n\nNote\n\nIf you need reproduce the accuracy, then better not change the training strategy\nin config to avoid unexpected training situations.\n\nWith the above information, you should have a clearer understanding of the\nfunctions of the config file.\n\nThe training script mentioned earlier can help you train a pure floating point\ndetection model with high accuracy. However, a well-trained detection model is\nnot our goal, it is just a pretrain for us to train a fixed-point model later.\n\n\nQuantitative Model Training#\n\nWith a pure floating-point model in place, we can start training the\ncorresponding fixed-point model. Similar to the floating-point training, we can\ntrain the fixed-point model simply by running the following script.\n\nBy the way, it is recommended to add a calibration stage before quantization\naware training. Calibration can provide better initialization parameters for\nQAT.\n\n\n\nAs you can see, the configuration file is not changed except the stage type. At\nthis point, the training strategy we use comes from the qat_trainer in the\nconfig file.\n\n\n\n\nWith Different Quantitative Parameters#\n\nBy setting quantize=True when training quantitative models, the corresponding\nfloating-point model can be converted into a quantitative model, as below:\n\n\n\nFor key stages in quantitative training, e.g., preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantitative\nnodes, setting quantitative parameters, and operator fusion, etc., please read\nthe Quantized Awareness Training (QAT) section.\n\n\nWith Different Training Strategies#\n\nAs previously mentioned, the quantitative training is actually the finetuning\nbased on the pure floating-point training. Therefore, in the quantitative\ntraining, we set the initial learning rate to one-tenth of the floating-point\ntraining, and the number of epochs of the training will greatly decrease as\nwell. More importantly, when defining model, we need to set pretrained to the\naddress of the trained pure floating-point model.\n\nAfter these simple adjustments, we can start training our quantitative model.\n\n\nModel Validation#\n\nAfter the model is trained, we can also validate the performance of the trained\nmodel. Since we provide two stages of training, float and QAT, we can validate\nthe performance of the trained model in these two stages.\n\nRun the following two commands:\n\n\n\nAlso, we provide performance tests for the quantitative model by running the\nfollowing command:\n\n\n\nThis displayed accuracy is the real accuracy of the final int8 model, which of\ncourse should be very close to the accuracy of the QAT verification phase.\n\n\nSimulation of On-board Accuracy Validation#\n\nIn addition to the above model validation, we also provide the exact same\naccuracy validation method simulating the on-board conditions, as below:\n\n\n\n\nResult Visualization#\n\nIf you want to see the results of the trained model detecting a single image, we\nalso provide scripts for single image prediction and visualization under our\ntools folder. Run the following script:\n\n\n\n\nModel Checking and Compilation#\n\nAfter the training, you can use the compile tool to compile the quantitative\nmodel into a board-ready HBM file. The compile tool can also predict the\non-computing-platform running performance. Run the following script:\n\n","routePath":"/en/guide/advanced_content/hat/examples/fcos","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":646},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":970},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":3,"charIndex":1854},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2667},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2841},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3534},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3656},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3854},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4546},{"text":"Model Building","id":"model-building","depth":3,"charIndex":4696},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":5241},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":6137},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":7694},{"text":"With Different Quantitative Parameters","id":"with-different-quantitative-parameters","depth":3,"charIndex":8276},{"text":"With Different Training Strategies","id":"with-different-training-strategies","depth":3,"charIndex":8756},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":9308},{"text":"Simulation of On-board Accuracy Validation","id":"simulation-of-on-board-accuracy-validation","depth":3,"charIndex":9835},{"text":"Result Visualization","id":"result-visualization","depth":3,"charIndex":10031},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":10256}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":8,"title":"FCOS3D Detection Model Training","content":"#\n\nThis tutorial shows how to train a fixed-point 3D detection model using the HAT\nalgorithm toolkit by using FCOS3D-efficientnetb0 as an example.\n\nBefore starting the quantization aware training, namely, the fixed-point model\ntraining, you need to first train a pure floating-point model with high\naccuracy, by finetuning this pure floating-point model, you can train the\nfixed-point model quickly.\n\nLet's start from training a pure floating-point FCOS3D-efficientnetb0 model.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset.\nHere we use nuscenes to train FCOS3D. After unpacking The data directory\nstructure is shown as below:\n\n\n\nAlso, to improve the training speed, we packaged the original jpg format dataset\nand converted it to the lmdb format. The conversion can be successfully achieved\nby simply running the following script:\n\n\n\nThese two commands are for training dataset conversion and validation dataset\nconversion respectively. When the packing is done, the file structure of data\nshould be as below:\n\n\n\nThe above train_lmdb and val_lmdb are the packed training and validation\ndatasets, which are also the final datasets read by the network.\n\n\nFloating-point Model Training#\n\nOnce datasets are ready, you can start training the floating-point\nFCOS3D-efficientnetb0 detection network. If you simply want to start such a\ntraining task, just run the following command:\n\n\n\nSince the HAT algorithm toolkit uses an ingenious registration mechanism, each\ntraining task can be started in the form of train.py plus a config file.\n\ntrain.py is a uniform training script and independent of the task. The task we\nneed to train, the dataset we need to use, and the hyperparameters we need to\nset for the training are all in the specified config file.\n\nThe config file provides the key dict for model building, data reading, etc.\n\n\nModel Building#\n\nThe network structure of retinanet can be found in the Paper and here we will\nskip the details.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nThe type under model is the name of the defined model, and the remaining\nvariables stand for the other components of the model. By defining the model in\nthis way, we can easily replace the structure we want. For example, if we want\nto train a model with a backbone of resnet50, we just need to replace backbone\nunder model.\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts, data_loader and val_data_loader, in the config file,\ncorresponding to the processing process of the training and validation sets,\nrespectively. Take data_loader as an example:\n\n\n\n\nTraining Strategy#\n\nA good training strategy is is essential for training a model with high\naccuracy. For each training task, the corresponding training strategy is also\ndefined in the config file, as can be seen from the float_trainer variable.\n\n\n\nfloat_trainer defines the training approach in general, including the use of\nmulti-card distributed training (distributed_data_parallel_trainer), the number\nof epochs for model training, and the choice of optimizer. The callbacks\nreflects the small strategies used by the model during the training and the\noperations that you want to implement, including the way to transform the\nlearning rate (WarmupStepLrUpdater), the metrics to validate the model during\ntraining (Validation), and the operations to save (Checkpoint) the model. Of\ncourse, if you have your own operations that you want the model to implement\nduring the training, you can also add them in this way using dict.\n\nThe above information may have give you a clearer understanding of the functions\nof the config file. The training script mentioned earlier can help you train a\npure floating point detection model wih high accuracy. However, a well-trained\ndetection model is not our goal, it is just a pretrain for us to train a\nfixed-point model later.\n\n\nQuantitative Model Training#\n\nWith a pure floating-point model in place, we can start training the\ncorresponding fixed-point model. Similar to the floating-point training, we can\ntrain the fixed-point model simply by running the following script:\n\n\n\nAs you can see, the configuration file is not changed except the stage type. At\nthis point, the training strategy used comes from the qat_trainer and\ncalibration_trainer in the config file.\n\n\n\n\nWith Different model_convert_pipeline Parameters#\n\nBy setting model_convert_pipeline when training quantitative models, the\ncorresponding floating-point model can be converted into a quantitative model,\nas below:\n\n\n\nFor key steps in quantitative training, e.g., preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantitative\nnodes, setting quantitative parameters, and operator fusion, etc., please read\nthe Quantized Awareness Training (QAT) section.\n\n\nWith Different Training Strategies#\n\nAs previously mentioned, the quantitative training is actually the finetuning\nbased on the pure floating-point training. Therefore, in the quantitative\ntraining, set the initial learning rate is to one-tenth of the floating-point\ntraining, and the number of epochs of the training will greatly decrease as\nwell. The most important thing is that when defining model, we need to set\npretrained to the address of the trained pure floating-point model.\n\nAfter these simple adjustments, we can start training our quantitative model.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Validation#\n\nAfter the model is trained, we can also validate the performance of the trained\nmodel. Since we provide two stages of training, float, calibration and qat, we\ncan validate the performance of the trained model in these stages.\n\n\n\nAlso, we provide performance tests for the quantitative model by running the\nfollowing command, but it should be noted that hbir must be exported first:\n\n\n\nThis displayed accuracy is the real accuracy of the final int8 model, which of\ncourse should be very close to the accuracy of the qat verification phase.\n\n\nSimulation of On-board Accuracy Validation#\n\nIn addition to the above model validation, we also provide the exact same\naccuracy validation method simulating the on-board conditions, as below:\n\n\n\n\nResult Visualization#\n\nIf you want to see the results of the trained model detecting a single image, we\nalso provide scripts for single image prediction and visualization under our\ntools folder. Run the following script:\n\n\n\n\nModel Checking and Compilation#\n\nAfter the training, you can use the compile_perf_hbir tool to compile the\nquantitative model into a board-ready hbm file. The compile tool can also\npredict the on-computing platform running performance. Run the following script:\n\n","routePath":"/en/guide/advanced_content/hat/examples/fcos3d","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":479},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":1208},{"text":"Model Building","id":"model-building","depth":3,"charIndex":1882},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":2431},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":2727},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":3995},{"text":"With Different model_convert_pipeline Parameters","id":"with-different-model_convert_pipeline-parameters","depth":3,"charIndex":4439},{"text":"With Different Training Strategies","id":"with-different-training-strategies","depth":3,"charIndex":4937},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":5504},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":5678},{"text":"Simulation of On-board Accuracy Validation","id":"simulation-of-on-board-accuracy-validation","depth":3,"charIndex":6238},{"text":"Result Visualization","id":"result-visualization","depth":3,"charIndex":6434},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":2,"charIndex":6659}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":9,"title":"Occupancy Prediction Model","content":"#\n\nThe Occupancy prediction model is developed based on Horizon Torch Samples, a\ndeep learning framework developed by Horizon. For information on using Horizon\nTorch Samples, please refer to the Horizon Torch Samples documentation. The\ntraining configuration for the Occupancy prediction model is located in the\n\"configs/occ/\" directory. The following example uses\n\"configs/occ/flashocc_henet_lss_occ3d_nuscenes.py\" to illustrate how to\nconfigure and train the Occupancy prediction model.\n\n\nTraining Process#\n\nIf you want to quickly train the flashocc_henet_lss_occ3d_nuscenes model, you\ncan start by reading this section. Like other tasks, for all training and\nevaluation tasks, HAT uniformly uses the tools + config approach to complete\nthem. After preparing the original dataset, you can easily follow the steps\nbelow to complete the entire training process.\n\n\nDataset Preparation#\n\nFor this example, using the nuScenes dataset, you can download the dataset from\nhttps://www.nuscenes.org/nuscenes. For the Occupancy prediction task, you will\nalso need to download the OCC ground truth data, which can be obtained from\nhttps://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Predic\ntion. It is recommended to unzip the downloaded dataset into the occ3d/gts\ndirectory within the nuScenes dataset folder. Additionally, to improve training\nspeed, we have packaged the original JPEG format dataset into an LMDB format\ndataset.\n\n\n\nThe previous command will generate two files, nuscenes_infos_train.pkl and\nnuscenes_infos_val.pkl, in the \"./tmp_data/nuscenes/occ3d directory\". Next,\nexecute the following command to perform the packaging:\n\n\n\nThe above two commands correspond to converting the training dataset and the\nvalidation dataset, respectively. After the packaging is complete, the file\nstructure in the data directory should be as follows:\n\n\n\n\"train_lmdb\" and \"val_lmdb\" are the packaged training dataset and validation\ndataset, respectively. They are also the datasets that the network will\nultimately read.\n\n\nModel Training#\n\nOnce the dataset is prepared, you can start training the floating-point\nOccupancy prediction model.\n\nIf you simply want to start such a training task, you only need to run the\nfollowing command:\n\n\n\nThe commands above separately train the floating-point model and the fixed-point\n(quantized) model. The training of the fixed-point model needs to be based on\nthe previously trained floating-point model. For more details, please refer to\nthe Quantized Awareness Training section.\n\n\nExporting the Quantized Model#\n\nAfter completing quantization training, you can start exporting the quantized\nmodel. You can use the following command to export it:\n\n\n\n\nModel Verification#\n\nAfter training is complete, you will obtain floating-point, quantized, or\nfixed-point models. Similar to the training process, you can validate the\ntrained models using the same method to obtain metrics for Float, Calibration,\nand Quantized, corresponding to the floating-point, quantized, and fully\nfixed-point metrics, respectively.\n\n\n\nSimilar to when training the model, when the parameter after --stage is \"float\"\nor \"calibration\", you can validate the trained floating-point or quantized\nmodels, respectively.\n\nYou can also validate the accuracy of the fixed-point model using the command\nbelow, but it’s important to note that you must first export the HBIR:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results of the\nquantized model:\n\n\n\n\nOn-Board Accuracy Validation in Simulation#\n\nIn addition to the model verification mentioned above, we also provide an\naccuracy validation method that is completely consistent with the on-board\nexecution. This can be done using the following method:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nThe quantization training toolchain integrated into HAT is primarily prepared\nfor Horizon's computing platform. Therefore, checking and compiling the\nquantized model is necessary. HAT provides an interface for model checking,\nallowing you to verify if the quantized model can run normally on the BPU:\n\n\n\nAfter model training is complete, the compile_perf_hbir script can compile the\nquantized model into an hbm file suitable for board deployment. This tool also\nestimates the model's performance on the BPU:\n\n\n\nThis outlines the entire process from data preparation to generating a\ndeployable quantized model.\n\n\nTraining Details#\n\nIn this section, we outline some important considerations for model training,\nmainly focusing on certain config settings.\n\n\nModel Buiding#\n\n\n\nHere, the \"type\" under \"model\" specifies the name of the defined model, while\nthe remaining variables represent other components of the model. The advantage\nof this model definition is that it allows us to easily replace the structure as\nneeded. For example, if we want to train a model with a ResNet50 backbone, we\nonly need to replace the \"backbone\" under \"model\".\n\n\nData Augmentation#\n\nSimilar to the model definition, the data augmentation process is implemented by\ndefining two dicts in the config file: data_loader and val_data_loader, which\ncorrespond to the processing workflows for the training and validation datasets,\nFor detailed information, see\n\"configs/occ/flashocc_henet_lss_occ3d_nuscenes.py\".\n\n\nTraining Strategy#\n\nA good training strategy is essential for training a high-accuracy model. For\neach training task, the corresponding training strategy is defined in the config\nfile, as indicated by the float_trainer variable.\n\n\n\nfloat_trainer defines our training approach from a broad perspective, including\nusing multi-card distributed training (distributed_data_parallel_trainer), the\nnumber of epochs for model training, and the choice of optimizer. Meanwhile,\ncallbacks reflect the smaller strategies and operations you wish to implement\nduring training. This includes learning rate adjustment methods (e.g.,\nCosineAnnealingLrUpdater), validating model metrics during training\n(Validation), and saving model checkpoints (Checkpoint). If you have additional\noperations you'd like to implement during training, you can also add them in\nthis dict format.\n\nFrom the above introduction, you should have a clear understanding of the config\nfile's functionality. Using the training scripts mentioned earlier, you can\ntrain a high-precision floating-point detection model. Of course, training a\ngood detection model is not our final goal; it serves as a pretrain for training\nthe fixed-point model later.\n\n\nQuantized Model Training#\n\nOnce we have the floating-point model, we can start training the corresponding\nfixed-point model. Similar to floating-point training, you can obtain a\npseudo-quantized model by running the following scripts, which will use\ncalibration and qat to achieve the target:\n\n\n\nAs you can see, our config file remains unchanged, only the stage type has been\naltered. The calibration process can provide better initialization parameters\nfor QAT quantization training. We use Float2Calibration and Float2QAT to convert\nthe model into calibration and QAT models, respectively.\n\n\n\n\nQuantization Training#\n\nWhen training a quantized model, you need to set quantize=True. At this point,\nthe corresponding floating-point model will be converted into a quantized model.\nThe relevant code is as follows:\n\n\n\nFor key steps in quantitative training, e.g., preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantitative\nnodes, setting quantitative parameters, and operator fusion, etc., please read\nthe Quantized Awareness Training (QAT) section.\n\n\nDifferent Training Strategies#\n\nAs previously mentioned, the quantitative training is actually the finetuning\nbased on the pure floating-point training. Therefore, in the quantitative\ntraining, set the initial learning rate is to one-tenth of the floating-point\ntraining, and the number of epochs of the training will greatly decrease as\nwell. The most important thing is that when defining model, we need to set\npretrained to the address of the trained pure floating-point model.\n\nAfter these simple adjustments, we can start training our quantitative model.","routePath":"/en/guide/advanced_content/hat/examples/flashocc","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":490},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":863},{"text":"Model Training","id":"model-training","depth":3,"charIndex":2033},{"text":"Exporting the Quantized Model","id":"exporting-the-quantized-model","depth":3,"charIndex":2530},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2699},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3389},{"text":"On-Board Accuracy Validation in Simulation","id":"on-board-accuracy-validation-in-simulation","depth":3,"charIndex":3508},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3762},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4419},{"text":"Model Buiding","id":"model-buiding","depth":3,"charIndex":4562},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":4949},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5293},{"text":"Quantized Model Training","id":"quantized-model-training","depth":3,"charIndex":6500},{"text":"Quantization Training","id":"quantization-training","depth":3,"charIndex":7096},{"text":"Different Training Strategies","id":"different-training-strategies","depth":3,"charIndex":7598}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":10,"title":"GaNet Lane Line Detection Model Training","content":"#\n\nThis tutorial focuses on showing how to train a GaNet model from scratch on the\nlane line dataset CuLane using HAT, including floating-point, quantitative, and\nfixed-point models.\n\nCuLane is one of the most commonly used datasets for lane line detection, and\nmany advanced lane line detection studies are preferentially based on this\ndataset for good validation.\n\nPrior to the model training, we need to prepare the dataset first. Here we\ndownload the official dataset and the corresponding labeled data CuLaneDataset.\nNote that the annotations_new.tar.gz file must be extracted at the end.\n\nThe structure of the data directory after unzipping is as follows:\n\n\n\nWhere list/train.txt contains the path to the training data and list/test.txt\ncontains the path to the test data.\n\n\nTraining Process#\n\nIf you just want to simply train the GaNet model, then you can read this section\nfirst.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nTo improve the training speed, we packed the original dataset and converted it\nto the LMDB format.\n\nSimply run the following script to complete the conversion:\n\n\n\nThe above two commands correspond to the transformation of the training dataset\nand the verification dataset, respectively. After packaging, the file structure\nin the ${target-data-dir} directory should look as follows:\n\n\n\ntrain_lmdb and test_lmdb are the packaged training dataset and verification\ndataset. You can then start training the model.\n\n\nModel Training#\n\nBefore the network starts the training, you can first calculate the number of\nnetwork operations and parameters using the following command:\n\n\n\nThe next step is to start the training. Training can also be done with the\nfollowing script. Before training, you need to make sure that the dataset path\nspecified in the configuration has already been changed to the path of the\npackaged dataset.\n\n\n\nSince the HAT algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\nwhich, respectively, indicates the training of the floating-point model and the\nquantitative model, and the conversion of the quantitative model to the\nfixed-point model, where the training of the quantitative model depends on the\nfloating-point model produced by the previous floating-point training.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model, and quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\nThe network structure of GaNet can be found in the Paper, which is not described\nin detail here.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nIn addition to backbone , the model also has neck , head , targets ,\npost_process and losses modules.\n\nIn GaNet , backbone is mainly to extract the features of the image, neck is\nmainly for feature enhancement, and head is mainly to get the predicted fraction\nand offset of key points of lane lines from the features.\n\ntargets are training targets from gt and post_process is mainly the\npost-processing part, which is used in inference.\n\nThe losses part uses LaneFastFocalLoss and L1Loss from the paper as the training\nlosses and loss_weight is the weight of the corresponding loss.\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining train_data_loader and val_data_loader in the config file, which\ncorrespond to the processing of the training sets and verification sets,\nrespectively. Taking train_data_loader as an example, the data enhancement uses\nFixedCrop, RandomFlip, Resize, RandomSelectOne, RGBShift, HueSaturationValue,\nJPEGCompress, MeanBlur, MedianBlur, RandomBrightnessContrast, ShiftScaleRotate\nand RandomResizedCrop to increase the diversity of training data and enhance the\ngeneralization ability of the model.\n\n\n\nSince the final model running on BPU uses a YUV444 image input, and the general\ntraining image input is in the RGB format, HAT provides BgrToYuv444 data\nenhancement to convert RGB to YUV444. To optimize the training process, HAT uses\nbatch_processor, which allows some enhancement processes to be placed in\nbatch_processor to optimize the training:\n\n\n\nIn which loss_collector is a function that gets the loss of the current batch\ndata.\n\nThe data transformation of the validation set is relatively simpler, as follows:\n\n\n\n\n\n\nTraining Strategy#\n\nTrain the floating-point model on the CuLane dataset using the Cosine learning\nstrategy with Warmup, as well as applying L2 norm to the weight parameters. The\nfloat_trainer, calibration_trainer, and int_trainer in the\nconfigs/lane_pred/ganet/ganet_mixvargenet_culane.py file correspond to the\ntraining strategies for floating-point, quantitative, and fixed-point models,\nrespectively.\n\nThe following is an example of float_trainer training strategy:\n\n\n\n\nQuantization Training#\n\nFor key steps in quantitative training, such as preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantization\nnodes, setting quantitative parameters, and operator fusion, please read the\nQuantized Awareness Training (QAT) section. Here we focus on how to define and\nuse the quantization models in lane prediction of HAT.\n\nIf the model is ready, and some existing modules are quantized, HAT uses the\nfollowing script in the training script to map the floating-point model to the\nfixed-point model uniformly.\n\n\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately. Due to the existence of the floating-point pre-training\nmodel, the learning rate Lr for quantitative training can be very small.\nGenerally, you can start from 0.001 or 0.0001, and you can do one or two times\nLr adjustments of scale=0.1 with StepLrUpdater without prolonging the training\ntime. In addition, weight decay will also have some effect on the training\nresults.\n\nThe quantitative training strategy for the GaNet example model can be found in\nthe configs/lane_pred/ganet/ganet_mixvargenet_culane.py file.","routePath":"/en/guide/advanced_content/hat/examples/ganet","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":780},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":1104},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1638},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2809},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2983},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3676},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3798},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3996},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4688},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4838},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":5651},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":6775},{"text":"Quantization Training","id":"quantization-training","depth":3,"charIndex":7249}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":11,"title":"Quantization Model Training","content":"lidar Fusion MultiTask Model Training#\n\nThis tutorial primarily guides you on how to use HAT to train a Lidar fusion\nmulti-task perception model on the Nuscenes autonomous driving dataset,\nincluding floating-point, quantized, and fixed-point models. The following\nexample\nconfigs/lidar_bevfusion/bevfusion_pointpillar_henet_multisensor_multitask_nuscen\nes.py illustrates how to configure and train a Lidar fusion multi-task\nperception model.\n\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes is a multimodal,\nmulti-task autonomous driving perception model. It takes inputs from two\nmodalities, camera and lidar, and outputs dynamic object 3D bounding boxes and\n3D occupancy grids.\n\n\nTraining Process#\n\nIf you want to quickly train the\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes model, you can start\nby reading this chapter. Like other tasks, for all training and evaluation\ntasks, HAT uniformly uses the tools + config approach to complete them. After\npreparing the original dataset, you can easily follow the steps below to\ncomplete the entire training process.\n\n\nDataset Preparation#\n\nHere, using the NuScenes dataset as an example, you can download the dataset\nfrom https://www.nuscenes.org/nuscenes. For the occupancy prediction task,\nyou'll also need to download the OCC ground truth (GT) from\nhttps://github.com/CVPR2023-3D-Occupancy-Prediction/CVPR2023-3D-Occupancy-Predic\ntion.\n\nIt's recommended to unzip the downloaded datasets into the occ3d/gts folder\nwithin the NuScenes dataset directory.\n\nThen, you can run the following commands to package lidar, images, and OCC GT\ndata into LMDB format:\n\n\n\nThese two commands correspond to converting the training dataset and validation\ndataset, respectively. After packaging, the file structure in the data directory\nshould look like this:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training and validation datasets, which\nare also the final datasets read by the network. meta contains information for\nmetrics initialization, copying from nuscenes.\n\nAfter executing the above command, the following file directory is generated:\n\n\n\nThe train_lmdb and val_lmdb are the training and validation datasets,\nrespectively, that have been packaged into LMDB format. These datasets are the\nfinal ones that the network reads for training and validation purposes.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the floating-point\nlidarMultiTask network. Before the network training starts, you can test the\ncomputation and the number of parameters of the network by using the following\ncommand.\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nThe commands above separately train the pretrained float-point model,\nfloating-point model and the fixed-point (quantized) model. The training of the\nfixed-point model needs to be based on the previously trained floating-point\nmodel. For more details, please refer to the Quantization-Aware Training\nsection.\n\n\nExporting the Quantized Model#\n\nAfter completing quantization training, you can start exporting the quantized\nmodel. You can use the following command to export it:\n\n\n\nSimilar to when training the model, when the parameter after --stage is \"float\"\nor \"calibration\", you can validate the trained floating-point or quantized\nmodels, respectively.\n\nYou can also validate the accuracy of the fixed-point model using the command\nbelow, but it’s important to note that you must first export the HBIR:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results of the\nquantized model:\n\n\n\n\nOn-Board Accuracy Validation in Simulation#\n\nIn addition to the model validation mentioned above, we also provide an accuracy\nvalidation method that is completely consistent with the on-board execution.\nThis can be done using the following method:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nThe quantization training toolchain integrated into HAT is primarily prepared\nfor Horizon's computing platform. Therefore, checking and compiling the\nquantized model is necessary. HAT provides an interface for model checking,\nallowing you to verify if the quantized model can run normally on the BPU:\n\n\n\nAfter model training is complete, the compile_perf_hbir script can compile the\nquantized model into an hbm file suitable for board deployment. This tool also\nestimates the model's performance on the BPU:\n\n\n\nThis outlines the entire process from data preparation to generating a\ndeployable quantized model.\n\n\nTraining Details#\n\nIn this section, we outline some important considerations for model training,\nmainly focusing on certain config settings.\n\n\nModel Buiding#\n\n\n\nIn this context, the type under model indicates the name of the defined model,\nwhile the remaining variables represent other components of the model. The\nadvantage of defining the model this way is that it allows us to easily swap out\nstructures as needed.\n\nThe bevfusion_pointpillar_henet_multisensor_multitask_nuscenes model mainly\nconsists of three parts: lidar network, camera_network, and bev_decoders. The\nbev_decoders contain two modules: BEVFormerDetDecoder and\nBevformerOccDetDecoder, which output 3D detection boxes and 3D occupancy grid\npredictions, respectively.\n\n\nData Augmentation#\n\nSimilar to the model definition, the data augmentation process is implemented by\ndefining two dicts in the config file: data_loader and val_data_loader, which\ncorrespond to the processing workflows for the training and validation datasets,\nrespectively. For example, the training set definition is as follows:\n\n\n\n\nTraining Strategy#\n\nA good training strategy is essential for training a high-accuracy model. For\neach training task, the corresponding training strategy is defined in the config\nfile, as indicated by the float_trainer variable.\n\nFor this multi-network fusion model, we suggest training the lidar input model\nfirst, followed by the camera input model, and finally training the fusion model\nwith both lidar + camera inputs to achieve better results. The lidar input model\ncan refer to the training of centerpoint. The camera input model\nbevformer_henet_camera_multitask_nuscenes could be trained with\nconfigs/lidar_bevfusion/bevformer_henet_camera_multitask_nuscenes_pretrain.py,\nand the command is:\n\n\n\nThe configuration for the float_trainer is as follows. You can see that we have\nloaded the pre-trained models for both the camera input and the lidar input\nseparately.\n\n\n\nfloat_trainer defines our training approach from a broad perspective, including\nusing multi-card distributed training (distributed_data_parallel_trainer), the\nnumber of epochs for model training, and the choice of optimizer.\n\nMeanwhile, callbacks reflect the smaller strategies and operations you wish to\nimplement during training. This includes learning rate adjustment methods (e.g.,\nCosineAnnealingLrUpdater), validating model metrics during training\n(Validation), and saving model checkpoints (Checkpoint). If you have additional\noperations you'd like to implement during training, you can also add them in\nthis dict format.\n\nFrom the above introduction, you should have a clear understanding of the config\nfile's functionality. Using the training scripts mentioned earlier, you can\ntrain a high-precision floating-point detection model. Of course, training a\ngood detection model is not our final goal; it serves as a pretrain for training\nthe fixed-point model later.\n\n\nQuantization Model Training#\n\nOnce we have the floating-point model, we can start training the corresponding\nfixed-point model. Similar to floating-point training, you can obtain a\npseudo-quantized model by running the following scripts, which will use\ncalibration and qat to achieve the target:\n\n\n\nAs you can see, our config file remains unchanged, only the stage type has been\naltered. The calibration process can provide better initialization parameters\nfor QAT quantization training.\n\nWe configure Float2Calibration and Float2QAT to convert the model into\ncalibration and QAT models, respectively. The specific configurations for\ncalibration and QAT are as follows:\n\n\n\nFor key steps in quantization training, such as preparing the floating-point\nmodel, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please refer to the\nQuantization-Aware Training section.","routePath":"/en/guide/advanced_content/hat/examples/lidar_fusion","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":692},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":1090},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":2331},{"text":"Exporting the Quantized Model","id":"exporting-the-quantized-model","depth":3,"charIndex":2996},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3495},{"text":"On-Board Accuracy Validation in Simulation","id":"on-board-accuracy-validation-in-simulation","depth":3,"charIndex":3614},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3866},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4523},{"text":"Model Buiding","id":"model-buiding","depth":3,"charIndex":4666},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":5261},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5595}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":12,"title":"lidarMultiTask Model Training","content":"#\n\nThis tutorial focuses on how to use HAT to train a lidarMultiTask model on the\nlidar point cloud dataset nuscenes from scratch, including floating-point, QAT\nand quantized models.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\ndownload the full dataset (v1.0) and nuScenes-lidarseg from nuscenes dataset.\n\nAfter downloading, unzip and organize the folder structure as follows which can\nrefer to nuscenes Tutorials\n\n\n\nTo improve the speed of training, we did a package of data information files to\nconvert them into lmdb format datasets from nuscenes lidar files. The conversion\ncan be successfully achieved by simply running the following script:\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. After the packing is completed, the file\nstructure in the data directory should look as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training and validation datasets, which\nare also the final datasets read by the network. meta contains information for\nmetrics initialization, copying from nuscenes.\n\nAlso, for nuscenes point cloud data training, it is necessary to generate\ndatabase files for each individual training target in the dataset and store it\nusing a .bin format file in tmp_nuscenes/lidar/nuscenes_gt_database. The path\ncan be modified as needed. At the same time, a file containing these database\ninformation in .pkl format needs to be generated. In addition, we need to\ncollect category information of all samples in train dataset and resample the\nwhole dataset, thus we can generate these information and save them in .pkl\nformat to accelerate training. Then, create these files by running the following\ncommand:\n\n\n\nAfter executing the above command, the following file directory is generated:\n\n\n\nnuscenes_gt_database and nuscenes_dbinfos_train.pkl are the samples that are\nused for sampling during training, and nuscenes_infos_train.pkl can be used to\naccelerate train dataset initialization.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the floating-point\nlidarMultiTask network. Before the network training starts, you can test the\ncomputation and the number of parameters of the network by using the following\ncommand.\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nSince the HAT algorithm package uses an ingenious registration mechanism, each\ntraining task can be started in the form of this train.py plus the config\nconfiguration file. The train.py is the unified training script has nothing to\ndo with the task. What kind of task we need to train, what kind of dataset to\nuse, and training-related hyperparameter settings are all in the specified\nconfig configuration file. The config file provides key dicts such as model\nbuilding and data reading.\n\n\nModel Building#\n\nThe network structure of lidarMultiTask can refer to CenterPoint model with\nmodified neck and additional segmentation head. You can find details in the\nconfig file. We can easily define and modify the model by defining a dict-type\nvariable such as model in the config configuration file.\n\n\n\nAmong them, the type under model means the name of the defined model, and the\nremaining variables mean the other components of the model. The advantage of\ndefining the model this way is that we can easily replace the structure we want.\nAfter starting, the training script calls the build_model interface to convert\nsuch a model of type dict into a model of type torch.nn.Module.\n\n\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts data_loader and val_data_loader in the config file, which\ncorrespond to the processing of the training and validation sets, respectively.\nHere we take data_loader as an example:\n\n\n\nHere, the type directly uses the interface torch.utils.data.DataLoader that\ncomes with pytorch, which represents the combination of batch_size size samples\ntogether. Here you may only need to pay attention the dataset variable, and the\ndata_path path is the path we mentioned in the first part of the dataset\npreparation. transforms contains a series of data augmentations. In\nval_data_loader, only point cloud reading, segmentation labels generation and\ndata reformat are available. You can also implement your desired data\naugmentation operations by inserting a new dict in transforms.\n\n\nTraining Strategy#\n\nTo train a model with high accuracy, a good training strategy is essential. For\neach training task, the corresponding training strategy is also defined in the\nconfig file, which can be seen from the variable float_trainer.\n\n\n\nThe float_trainer defines our training approach from the big picture, including\nthe use of distributed_data_parallel_trainer, the number of epochs for model\ntraining, and the choice of optimizer. At the same time, callbacks reflects the\nsmall strategies used by the model in the training process and the operations\nthat you want to implement, including the transformation method of the learning\nrate (CyclicLrUpdater), the indicator (Validation), and save (Checkpoint) the\noperation of the model. Of course, if you have operations that you want the\nmodel to implement during training, you can also add it in this dict way.\nfloat_trainer is responsible for concatenating the entire training logic, which\nis also responsible for model pretraining.\n\nNote\n\nIf you need to reproduce the accuracy, it is best not to modify the training\nstrategy in the config. Otherwise, unexpected training situations may arise.\n\nThrough the above introduction, you should have a clear understanding of the\nrole of the config file. Then, through the training script mentioned above, a\nhigh-precision pure floating-point model can be trained. Of course, training a\ngood model is not our ultimate goal, it is just a pre-training for our future\ntraining of quantitative models.\n\n\nQuantitative Model Training#\n\nWhen we have a floating-point model, we can start training the corresponding QAT\nmodel. In the same way as floating-point training, we can train a QAT model just\nby running the following script: BTW, it is recommended to add a calibration\nstage before quantization aware training. Calibration can provide better\ninitialization parameters for QAT.\n\n\n\nAs you can see, our configuration file has not changed, only the type of stage\nhas been changed. At this point, the training strategy we use comes from the\nqat_trainer in the config file.\n\n\n\n\nWith Different model_convert_pipeline Parameters#\n\nBy setting model_convert_pipeline when training quantitative models, the\ncorresponding floating-point model can be converted into a quantitative model,\nas below:\n\n\n\nFor the key steps in quantitative training, such as preparing floating-point\nmodels, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please read the Quantized\nAwareness Training (QAT) section.\n\n\nWith Different Training Strategies#\n\nAs we said before, the quantitative training is actually finetue on the basis of\npure floating-point training. Therefore, when quantized training, our initial\nlearning rate is set to half of the floating-point training, the number of\nepochs for training is also reduced, and most importantly, when model is\ndefined, our pretrained needs to be set to the address of the pure\nfloating-point model that has been trained or the calibration model.\n\nAfter making these simple adjustments, we can start training our quantitative\nmodel.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Validation#\n\nAfter the model is trained, we can also validate the performance of the trained\nmodel. Since we provide two stages of training process, float and qat, we can\nvalidate the performance of the model trained in these two stages. Run the\nfollowing two commands:\n\n\n\nAt the same time, we also provide a performance test of the quantization model,\njust run the following command, but it should be noted that hbir must be\nexported first:\n\n\n\nThe displayed accuracy is the real accuracy of the final int8 model. Of course,\nthis accuracy should be very close to the accuracy of the qat verification\nstage.\n\n\nSimulation of On-board Accuracy Validation#\n\nIn addition to the model validation described above, we offer an accuracy\nvalidation method that is identical to the board side, you can refer to the\nfollowing:\n\n\n\n\nModel Inference and Results Visualization#\n\nIf you want to see the detection effect of the trained model for a lidar point\ncloud file, we also provide point cloud prediction and visualization scripts in\nour tools folder, you just need to run the following script.\n\n\n\n\nModel Checking and Compilation#\n\nAfter training, the quantized model can be compiled into an hbm file that can be\nrun on the board by using the compile_perf_hbir tool. At the same time, the tool\ncan also estimate the running performance on the BPU. The following scripts can\nbe used:\n\n","routePath":"/en/guide/advanced_content/hat/examples/lidar_multitask","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":184},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":2038},{"text":"Model Building","id":"model-building","depth":3,"charIndex":2882},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":3573},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":4459},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":5960},{"text":"With Different model_convert_pipeline Parameters","id":"with-different-model_convert_pipeline-parameters","depth":3,"charIndex":6532},{"text":"With Different Training Strategies","id":"with-different-training-strategies","depth":3,"charIndex":7020},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":7588},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":7762},{"text":"Simulation of On-board Accuracy Validation","id":"simulation-of-on-board-accuracy-validation","depth":3,"charIndex":8377},{"text":"Model Inference and Results Visualization","id":"model-inference-and-results-visualization","depth":3,"charIndex":8587},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":8855}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":13,"title":"MapTROE Training","content":"#\n\nThe MapTROE reference algorithm is developed based on Horizon Algorithm Toolkit\n(HAT, Horizon's own deep learning algorithm toolkit). The training config for\nMapTROE is located under configs/map/ path. The following part takes\nconfigs/map/maptroe_henet_tinym_bevformer_nuscenes.py as an example to describe\nhow to configure and train MapTROE model.\n\n\nTraining Process#\n\nIf you just want to simply train the MapTROE model, then you can read this\nsection first.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nHere is an example of the nuscense dataset, which can be downloaded from\nhttps://www.nuscenes.org/nuscenes . Also, in order to improve the speed of\ntraining, we have done a packing of the original jpg format dataset to convert\nit to lmdb format. Just run the following script and it will be successful to\nachieve the conversion.\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. In addition, the MapTROE model also requires\nstandand definition map (SD map) for map fusion to improve model accuracy, which\ncan be obtained from the Open Street Map(OSM) https://www.openstreetmap.org.\nAfter the packing is completed, the file structure in the data directory should\nlook as follows.\n\n\n\nThe train_lmdb and val_lmdb are the training and validation datasets after\npackaging, and are the datasets that the network will eventually read. metas\ncontains the map information used during model training and validation. osm\ncontains the sd map information used in map fusion.\n\n\nModel Training#\n\nBefore the network starts the training, you can first calculate the number of\nnetwork operations and parameters using the following command:\n\n\n\nThe next step is to start the training. Training can be done with the following\nscript. Before training, you need to make sure that the dataset path specified\nin the configuration has already been changed to the path of the packaged\ndataset.\n\n\n\nSince the HAT algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be float, calibration,\nqat, which, respectively, indicates the training of the float model, the\ncalibration model and the qat model. Calibration model training depends on float\nmodel checkpoint, while qat model training depends on calibration model\ncheckpoint. Please read the Quantized Awareness Training (QAT) section.\n\n\nExport Quantization Model#\n\nOnce you've completed your qat training, you can start exporting your\nquantization model. You can export it with the following command:\n\n\n\n\nModel Validation#\n\nAfter completing the training, we get the trained float, calibration, and qat\nmodel. Similar to the training method, we can use the same method to complete\nmetrics validation on the trained model and get the metrics of Float,\nCalibration, and QAT respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by float,\ncalibration, or qat to validate the specific model respectively.\n\nThe following command can be used to verify the accuracy of a quantization\nmodel, but it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nquantization model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nQuantization Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\n\n\nWhere type under model indicates the name of the defined model, and the\nremaining variables indicate the other components of the model. The advantage of\ndefining the model this way is that we can easily replace the structure we want.\nFor example, if we want to train a model with a backbone of resnet18, we just\nneed to replace backbone under model .\n\n\nData Augmentation#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts data_loader and val_data_loader in the config file as\nfollows, corresponding to training set and the processing flow of the validation\nset.\n\n\n\nWhere type directly uses the interface torch.utils.data.DataLoader that comes\nwith pytorch, which represents the combination of batch_size size images\ntogether. The only thing to be concerned about here is probably the dataset\nvariable, data_path means the packed lmdb dataset path, map_path is the map\ninformation path, and sd_map_path is the sd map information path as we mentioned\nin the first part of the dataset preparation. transforms contains a series of\ndata enhancements. You can also achieve your own data augmentation by inserting\na new dict in transforms.\n\n\nTraining Strategies#\n\nIn order to train a model with high accuracy, a good training strategy is\nessential. For each training task, the corresponding training strategy is\ndefined in the config file as well, which can be seen from the variable\nfloat_trainer.\n\n\n\nThe float_trainer defines our training approach in the big picture, including\nthe use of distributed_data_parallel_trainer, the number of epochs for model\ntraining, and the choice of optimizer. Also, the callbacks reflect the small\nstrategies used by the model during training and the operations that you want to\nimplement, including the way to transform the learning rate\n(CosineAnnealingLrUpdater), the metrics to validate the model during training\n(Validation), and the operations to save (Checkpoint) the model. Of course, if\nyou have operations that you want the model to implement during training, you\ncan also add them in this dict way.\n\nNote\n\nIf reproducibility accuracy is needed, the training strategy in config is best\nnot modified. Otherwise, unexpected training situations may occur.\n\n\nQuantization Model Training#\n\nBy using float_trainer, we can get a float model with high accuracy. Thus we can\nstart to train the corresponding calibration and qat model. The corresponding\ntraining strategies are defined as follows.\n\n\n\nQuantization training is in fact finetuning based on float training, so during\nquantization training, our qat learning rate is much smaller than the float\nlearning rate. The number of epochs for quantization training is largely\nreduced, most importantly, when defining the model , our pretrained needs to be\nset to the path of the previous model checkpoint.\n\n\nThe Settings of Qconfig#\n\nBefore we begin quantization training, we need to set qconfig of the model.\nFloat model will be converted into a quantization model by the settings of\nqconfig as follows.\n\n\n\ncali_qconfig_setter and qat_qconfig_setter are qconfig settings for calibration\nmodel and qat model specifically. For qconfig settings, such as default\ntemplates and sensitivity templates, please read the Quantized Awareness\nTraining (QAT) Qconfig in Detail section.","routePath":"/en/guide/advanced_content/hat/examples/maptroe","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":353},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":679},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1731},{"text":"Export Quantization Model","id":"export-quantization-model","depth":3,"charIndex":2889},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":3057},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3618},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3741},{"text":"Quantization Model Checking and Compilation","id":"quantization-model-checking-and-compilation","depth":3,"charIndex":3939},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4578},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4728},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":5104},{"text":"Training Strategies","id":"training-strategies","depth":3,"charIndex":5933},{"text":"Quantization Model Training","id":"quantization-model-training","depth":3,"charIndex":6992},{"text":"The Settings of Qconfig","id":"the-settings-of-qconfig","depth":3,"charIndex":7588}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":14,"title":"Motr Multiple Object Track Model Training","content":"#\n\nThis tutorial focuses on showing how to train a Motr model from scratch on the\nlane line dataset MOT17 using HAT, including floating-point, quantitative, and\nfixed-point models.\n\nMOT17 is one of the most commonly used datasets for multiple object track, and\nmany advanced multiple object track studies are preferentially based on this\ndataset for good validation.\n\nPrior to the model training, we need to prepare the dataset first. Here we\ndownload the official dataset and the corresponding labeled data MOT17DATASET .\n\nThe structure of the data directory after unzipping is as follows:\n\n\n\nwhere MOT17-02-DPM is the name of video, det.txt is the result of detector, like\nthe MOT17-02-DPM is the result of DPM, the gt.txt under of gt is the labels, and\nall images under of img1.\n\n\nTraining Process#\n\nIf you just want to simply train the Motr model, then you can read this section\nfirst.\n\nSimilar to other tasks, HAT performs all training tasks and evaluation tasks in\nthe form of tools + config.\n\nAfter preparing the original dataset, take the following process to complete the\nwhole training process.\n\n\nDataset Preparation#\n\nSince the official test set does not have GT, we split the training set into\nhalf, with the first half of each video frame as the training set and the last\nhalf as the validation set.\n\nHAT provides a script to split the training set, just run the script below:\n\n\n\nAfter running the above script, a folder similar to the following structure will\nbe generated:\n\n\n\nIn order to improve the training speed, we made a package of the original\ndataset and converted it into a dataset in the format of LMDB. Just run the\nscript below:\n\n\n\nThe above two commands correspond to the transformation training dataset and the\nvalidation dataset respectively.\n\nTo measure accuracy, we need to use the label data to verify the dataset, so we\nmake a soft join, as follows:\n\n\n\nafter packaging and soft join, the file structure in the ${target-data-dir}\ndirectory should be as follows:\n\n\n\ntrain_lmdb and test_lmdb are packaged training datasets and validation datasets,\ntest_gt contains the label data of the validation set, and then you can start\ntraining the model.\n\n\nModel Training#\n\nSince the qim module input in the Motr model relies on some post-processing, we\nsplit the entire Motr model into two configs.\n\nExcept for the config that generates the fixed-point model, compiles, model\nchecker, and the model calculation amount needs to use qim , the rest use config\nof motr , and the detailed usage can be found in the following section.\n\nIn the following description, the first model is the base module of Motr , the\nsecond model is the qim module, and if not specifically specified, it is a model\nin which two modules are concatenated.\n\nBefore the network starts training, you can use the following command to\ncalculate the amount of computation and the number of parameters for the\nnetwork:\n\n\n\nThe next step is to start training. Training can also be done through the\nfollowing script, and you need to confirm whether the dataset path in the\nconfiguration has been switched to the packaged dataset path before training.\n\n\n\nSince the HAT algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\n\"qat\", which, respectively, indicates the training of the floating-point model\nand the quantitative model, and the conversion of the quantitative model to the\nfixed-point model, where the training of the quantitative model depends on the\nfloating-point model produced by the previous floating-point training.\n\nFor this model, the conversion of quantization model to fixed-point model\nrequires the use of config of the qim module.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration , QAT, and Quantized, which are floating-point, quantitative,\nand fully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", or \"qat\" to validate the trained floating-point model,\nquantitative model, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\nThe network structure of Motr can be found in the Paper, which is not described\nin detail here.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nIn addition to backbone , the model also has head , criterion , post_process and\ntrack_embed modules.\n\nIn Motr , backbone is mainly to extract the features of the image, head is\nmainly to get the predicted category, location, and feature from the features.\n\ncriterion is the module that computes loss at training time, post_process is\nmainly the post-processing part, and track_embed is the module used to update\nthe target query on the tracked (i.e. the qim module).\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining data_loader and val_data_loader in the config file, which correspond to\nthe processing of the training sets and verification sets, respectively. Taking\ndata_loader as an example, the data enhancement uses SeqRandomFlip,\nRandomSelectOne, SeqResize, SeqRandomSizeCrop, SeqToTensor, and SeqNormalize to\nincrease the diversity of training data and enhance the generalization ability\nof the model.\n\nSince the final model running on BPU uses a YUV444 image input, and the general\ntraining image input is in the RGB format, HAT provides SeqBgrToYuv444 data\nenhancement to convert RGB to YUV444.\n\n\n\nIn which loss_collector is a function that gets the loss of the current batch\ndata.\n\n\n\nThe data transformation of the validation set is relatively simpler, as follows:\n\n\n\n\n\n\nTraining Strategy#\n\nTraining a floating-point model on the Mot17 dataset using the Stepdecay\nlearning strategy, and impose L2 norm on the weight parameter.\n\nThe float_trainer, calibration_trainer, qat_trainer, and int_trainer in the\nconfigs/track_pred/motr_efficientnetb3_mot17.py file correspond to the training\nstrategies for floating-point, quantitative, and fixed-point models,\nrespectively.\n\nThe following is an example of float_trainer training strategy:：\n\n\n\n\nQuantization Training#\n\nFor key steps in quantitative training, such as preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantization\nnodes, setting quantitative parameters, and operator fusion, please read the\nQuantized Awareness Training (QAT) section. Here we focus on how to define and\nuse the quantization models in multiple object track of HAT.\n\nIf the model is ready, and some existing modules are quantized, HAT uses the\nfollowing script in the training script to map the floating-point model to the\nfixed-point model uniformly.\n\n\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately. Due to the existence of the floating-point pre-training\nmodel, the learning rate Lr for quantitative training can be very small.\nGenerally, you can start from 0.001 or 0.0001, and you can do one or two times\nLr adjustments of scale=0.1 with StepLrUpdater without prolonging the training\ntime. In addition, weight decay will also have some effect on the training\nresults.\n\nThe quantitative training strategy for the Motr example model can be found in\nthe configs/track_pred/motr_efficientnetb3_mot17.py file.","routePath":"/en/guide/advanced_content/hat/examples/motr","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":783},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":1106},{"text":"Model Training","id":"model-training","depth":3,"charIndex":2177},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":4026},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":4200},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":4904},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":5026},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":5224},{"text":"Training Details","id":"training-details","depth":2,"charIndex":5916},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":6066},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":6763},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":7633},{"text":"Quantization Training","id":"quantization-training","depth":3,"charIndex":8099}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":15,"title":"Multi-machine Instructions","content":"#\n\n\nEnvironment Preparation#\n\nPrepare two development PCs and make sure that both of them can be logged in\ndirectly without typing passwords.\n\nMake sure that the file directories and contents of the code to run on both\nmachines are identical.\n\n\nLaunching Docker#\n\nStart the docker on the first PC.\n\n\n\nWhere 096e26686644 is the image name of the container started in the first step,\nwhich can be viewed using docker ps.\n\nStart the docker on the second PC.\n\n\n\nNote that the mapping port number for SSH is changed here. The first PC is 10022\nand the second PC is 10023. Make sure these two PCs use different port numbers.\n\nMake sure the code directories inside the docker on both PCs are identical. You\ncan mount the same development directory using -v.\n\nIn addition, you can test whether the two docker containers can log into each\nother without passwords: Use the ifconfig command to get the IP address of the\ncontainer on one machine (PC1). Suppose the PC1's IP is 172.17.0.12, go to the\ncontainer on the other machine (PC2), run ssh -p 22 172.17.0.12 to check whether\nPC1 can be logged in. If the login is ok, then you can proceed to the next step.\n\n\nStarting Multi-machine Multi-card Training Script with Torchrun#\n\n\n\nhostip: IP address of the container on PC1, use ifconfig to check it.\n\n--nnodes 2: 2 is the total number of PCs.\n\n--nproc_per_node 4: 4 means the number of GPUs on each PC (you may need to\nmanually change this number to 4 in\nconfigs/classification/mobilenetv1_imagenet.py).\n\nRun this command to see the multi-machine multi-card instance running properly.","routePath":"/en/guide/advanced_content/hat/examples/multi_machine","lang":"en","toc":[{"text":"Environment Preparation","id":"environment-preparation","depth":2,"charIndex":3},{"text":"Launching Docker","id":"launching-docker","depth":2,"charIndex":244},{"text":"Starting Multi-machine Multi-card Training Script with Torchrun","id":"starting-multi-machine-multi-card-training-script-with-torchrun","depth":2,"charIndex":1151}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":16,"title":"PointPillars Detection Model Training","content":"#\n\nThis tutorial focuses on how to use HAT to train a PointPillars model on the\nradar point cloud dataset KITTI-3DObject from scratch, including floating-point,\nquantitative, and fixed-point models.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\ndownload the 3DObject dataset.\n\nThe following 4 files are included:\n\n 1. Left color images of object dataset\n 2. Velodyne point clouds\n 3. Camera calibration matrices of object dataset\n 4. Training labels of object dataset\n\nAfter downloading the above 4 files, unzip and organize the folder structure as\nfollows:\n\n\n\nIn order to create KITTI point cloud data, you need to load the original point\ncloud data and generate the associated data annotation file containing the\ntarget labels and annotation boxes.\n\nIt is also necessary to generate the point cloud data for each individual\ntraining target for the KITTI dataset and store it in a .bin file in\ndata/kitti/gt_database.\n\nIn addition, you need to generate a .pkl file containing data information for\nthe training data or validation data. Then, create the KITTI data by running the\nfollowing commands:\n\n\n\nThe above commands generate the following file directory:\n\n\n\nAlso, to improve the training speed, we packed and convert the data files into\nLMDB format datasets.\n\nSimply run the following commands to complete the conversion:\n\n\n\nThe first command: transforming the training dataset. The second command:\ntransforming the validation dataset.\n\nWhen the packaging completes, the file structure in the data directory should\nlook as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training dataset and validation\ndataset, which are also the final datasets read by the network.\n\nkitti3d_gt_database and kitti3d_dbinfos_train.pkl are the samples that are used\nfor sampling during training.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the floating-point\nPointPillars detection network. Before the network training starts, you can test\nthe number of operations and the number of parameters of the network by using\nthe following command:\n\n\n\nIf you simply want to start such a training task, just run the following\ncommand:\n\n\n\nThe HAT algorithm package uses an ingenious registration mechanism that allows\neach training task to be started in the form of train.py plus a config file.\n\ntrain.py is a uniform training script and independent of the task. The task we\nneed to train, the dataset we need to use, and the hyperparameters we need to\nset for the training are all in the specified config file.\n\nThe config file provides key dicts such as model building and data reading.\n\n\nModel Building#\n\nThe network structure of PointPillars can be found in Paper and here we will\nskip the details.\n\nWe can easily define and modify the model by defining a dict-type variable such\nas model in the config file.\n\n\n\nIn which, the type under model means the name of the defined model, and the\nremaining variables mean the other components of the model. The advantage of\ndefining the model in this way is that the structure can be easily replaced.\n\nAfter starting, the training script calls the build_model interface to convert\nsuch a model of the dict type into a model of the torch.nn.Module type.\n\n\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts (data_loader and val_data_loader) in the config file,\ncorresponding to the processing workflow of the training set and validation set,\nrespectively. Here we tak data_loader as an example:\n\n\n\nHere, type directly uses the interface torch.utils.data.DataLoader that comes\nwith pytorch, which means to combine the images with the size of batch_size\ntogether.\n\nHere you may only need to pay attention to the dataset variable. The path\ndata_path is the path we mentioned in the first part of the dataset preparation.\n\nThe transforms contains a series of data enhancements, while val_data_loader\ncontains only point cloud Pillarization (Voxelization) and Reformat.\n\nYou can also implement your desired data enhancement operations by inserting a\nnew dict in transforms.\n\n\nTraining Strategy#\n\nTo train a model with high accuracy, a good training strategy is essential.\n\nFor each training task, the corresponding training strategy is also defined in\nthe config file, which can be seen from the variable float_trainer.\n\n\n\nThe float_trainer defines our training approach in general, including the use of\ndistributed_data_parallel_trainer, the number of epochs for model training, and\nthe choice of optimizer.\n\nAlso, callbacks reflects the small strategies used by the model in the training\nprocess and the operations that you want to implement, including the\ntransformation method of the learning rate (CyclicLrUpdater), and the indicator\n(Validation), and the save operation (Checkpoint) of the model. Of course, if\nyou have operations that you want the model to implement during the training,\nyou can also add it in this way (dict).\n\nThe float_trainer is responsible for concatenating the entire training logic,\nwhich is also responsible for model pretraining.\n\nNote\n\nIf you need to reproduce the accuracy, it is best not to modify the training\nstrategy in the config file. Otherwise, unexpected training situations may\narise.\n\nThrough the above introductions, you should have a clear understanding of the\nrole of the config file. Then, through the training script mentioned above, a\nhigh-precision pure floating-point detection model can be trained.\n\nOf course, training a good detection model is not our ultimate goal, it is just\na pre-training for our future training of fixed-point models.\n\n\nQuantitative Model Training#\n\nWhen we have a floating-point model, we can start training the corresponding\nfixed-point model. In the same way as the floating-point training, we can train\na fixed-point model just by running the following script:\n\n\n\nAs you can see, our configuration file has not changed, except the stage type.\nAt this point, the training strategy we use comes from the qat_trainer and\ncalibration_trainer in the config file.\n\n\n\n\nValue of Quantize Parameter is Different#\n\nWhen we train the quantitative model, we need to set quantize=True. At this\nmoment, the corresponding floating-point model will be converted into a\nquantitative model. The code is as follows:\n\n\n\nFor the key steps in quantitative training, such as preparing floating-point\nmodels, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please read the Quantized\nAwareness Training (QAT) section.\n\n\nDifferent Training Strategies#\n\nAs we said before, quantitative training is actually the finetuning on the basis\nof pure floating-point training. Therefore, in quantitative training, the\ninitial learning rate is set to one-tenth of the floating-point training, the\nnumber of epochs for training is also greatly reduced, and most importantly,\nwhen model is defined, the pretrained parameter needs to be set to the address\nof the pure floating-point model that has been trained.\n\nAfter making these simple adjustments, we can start training our quantitative\nmodel.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter the model is trained, we can also verify the performance of the trained\nmodel. Since we provide two stages of training process, Float, Calibration and\nQAT, we can verify the performance of the model trained in these stages.\n\nSimply run the following two commands:\n\n\n\nAdditionally, the following command can be used to verify the accuracy of a\nfixed point model, but it should be noted that hbir must be exported first:\n\n\n\nThe displayed accuracy is the real accuracy of the final int8 model. Of course,\nthis accuracy should be very close to the accuracy of the QAT verification\nstage.\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the model validation described above, we offer an accuracy\nvalidation method that is identical to the board side, you can refer to the\nfollowing:\n\n\n\n\nModel Inference and Results Visualization#\n\nHAT provides infer_hbir.py which can visualizes the inference results of the\ntrained models at each stage:\n\n\n\n\nModel Checking and Compilation#\n\nAfter the training, the quantitative model can be compiled into an HBM file that\ncan be run on the board by using the compile_perf_hbir tool.\n\nThe tool can also estimate the running performance on the BPU. The following\nscripts can be used:\n\n","routePath":"/en/guide/advanced_content/hat/examples/pointpillars","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":200},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":1845},{"text":"Model Building","id":"model-building","depth":3,"charIndex":2668},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":3279},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":4158},{"text":"Quantitative Model Training","id":"quantitative-model-training","depth":2,"charIndex":5680},{"text":"Value of Quantize Parameter is Different","id":"value-of-quantize-parameter-is-different","depth":3,"charIndex":6126},{"text":"Different Training Strategies","id":"different-training-strategies","depth":3,"charIndex":6636},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":7201},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":7375},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":7988},{"text":"Model Inference and Results Visualization","id":"model-inference-and-results-visualization","depth":3,"charIndex":8194},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":8349}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":17,"title":"PointPillars Detection Model Training (No config)","content":"#\n\nThis tutorial focuses on how to use HAT to train a PointPillars model on the\nradar point cloud dataset KITTI-3DObject from scratch, including floating-point,\nquantized and fixed-point models.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\ndownload the 3DObject dataset . The following 4 files are included:\n\n 1. Left color images of object data set\n 2. Velodyne point clouds\n 3. Camera calibration matrices of object data set\n 4. Training labels of object data set\n\nAfter downloading the above 4 files, unzip and organize the folder structure as\nfollows:\n\n\n\nIn order to create KITTI point cloud data, the original point cloud data needs\nto be loaded and the associated data annotation file containing the target\nlabels and annotation boxes needs to be generated. It is also necessary to\ngenerate the point cloud data for each individual training target for the KITTI\ndataset and store it in a .bin format file in data/kitti/gt_database. In\naddition, a file containing data information in .pkl format needs to be\ngenerated for either the training data or the validation data. Then, create the\nKITTI data by running the following command:\n\n\n\nAfter executing the above command, the following file directory is generated:\n\n\n\nAlso, to improve the speed of training, we did a package of data information\nfiles to convert them into lmdb format datasets. The conversion can be\nsuccessfully achieved by simply running the following script:\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. After the packing is completed, the file\nstructure in the data directory should look as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training and validation datasets, which\nare also the final datasets read by the network. kitti3d_gt_database and\nkitti3d_dbinfos_train.pkl are the samples that are used for sampling during\ntraining.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the PointPillars detection\nnetwork.\n\n\nModel Building#\n\nThe network structure of PointPillars can refer to Paper , this is not described\nin detail here.\n\nThe entire process from model training to compilation is roughly as follows:\n\n\n\nFrom the above figure, we know that three stages of models are mainly used,\nnamely, Float Model , QAT Model, and Quantized Model: Where,\n\n * Float Model : General floating-point model.\n * QAT Model : Model for quantization aware training.\n * Quantized Model : Quantized model, with the parameter of INT8 type.\n\nIn addition, models with different structures (or states) are used in training,\ncompilation, and other processes:\n\n * model: A complete model structure, including pre processing, network\n   structure, and post processing of the model. It is mainly used for training\n   and evaluation.\n * deploy_model : Only contains the network structure (which can be compiled\n   into hbm), excluding pre processing and post processing. It is mainly used\n   for compilation.\n\nWe define a PointPillarsModel class to define all the content related to the\nmodel structure, including the above three stages: Float Model, QAT Model, and\nQuantized Model, as well as two states: model and deploy_model:\n\n\n\nAll content related to the model has been defined in the PointPillarsModel in\nthe above code. When using it, you can easily obtain the corresponding model\nstructure through PointPillarsModel.xxx().\n\nAfter completing the definition of the network structure, we can use the\nfollowing command to first check FLOPs and Params of the network:\n\n\n\n\nData Augmentation#\n\nSimilar to the Model building section, we define the DataHelper to implement\ndata related content, including transforms and data_Loader, etc.:\n\n\n\n\nTraining Strategy#\n\nTo train a model with high accuracy, a good training strategy is essential. For\neach training task, the training strategies for models at different stages\n(float, QAT) may vary slightly, so we also define the training strategy content\n(such as, optimizer and lr_schedule) in PointPillarsModel:\n\n\n\nNote\n\nIf you need to reproduce the accuracy, it is best not to modify the training\nstrategy in the sample code. Otherwise, unexpected training situations may\narise.\n\nThrough the above introduction, we have completed the definition of all modules\nrelated to model training. Of course, training a good detection model is not our\nultimate goal, it is just a pre-training for our future training of fixed-point\nmodels. If you simply want to start such a training task, just run the following\ncommand:\n\n\n\n\nQuantized Model Training#\n\nWhen we have a floating-point model, we can start training the corresponding\nfixed-point model. In the same way as floating-point training, we can train a\nfixed-point model just by running the following script:\n\n\n\n\nThe Value of the Quantize Parameter is Different#\n\nWhen we train the quantized model, we need to set quantize=True. At this time,\nthe corresponding floating-point model will be converted into a quantized model.\nThe code is as follows:\n\n\n\nFor the key steps in quantization training, such as preparing floating-point\nmodels, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please read the Quantized\nAwareness Training (QAT) section.\n\n\nDifferent Training Strategies#\n\nAs we said before, quantization training is actually finetue on the basis of\npure floating-point training. Therefore, when quantized training, our initial\nlearning rate is set to one-tenth of the floating-point training, the number of\nepochs for training is also greatly reduced, and most importantly, when model is\ndefined, our pretrained needs to be set to the address of the pure\nfloating-point model that has been trained.\n\nAfter making these simple adjustments, we can start training our quantized\nmodel.\n\n\nModel Validation#\n\nAfter the model is trained, we can also verify the performance of the trained\nmodel. Since we provide two stages of training process, float and qat, we can\nverify the performance of the model trained in these two stages. It is only\nnecessary to run the following two commands accordingly:\n\n\n\nAt the same time, we also provide a performance test of the quantization model,\njust run the following command:\n\n\n\nThe displayed accuracy is the real accuracy of the final int8 model. Of course,\nthis accuracy should be very close to the accuracy of the qat verification\nstage.\n\n\nAlign BPU Validation#\n\nIn addition to the model validation described above, we offer an accuracy\nvalidation method that is identical to the board side, you can refer to the\nfollowing:\n\n\n\n\nResults Visualization#\n\nIf you want to see the detection effect of the trained model for a single frame\nradar point cloud, we also provide point cloud prediction and visualization\nscripts in our tools folder, you just need to run the following script.\n\n\n\n\nModel Checking and Compilation#\n\nAfter training, the quantized model can be compiled into an hbm file that can be\nrun on the board by using the compile tool. At the same time, the tool can also\nestimate the running performance on the BPU. The following scripts can be used:\n\n","routePath":"/en/guide/advanced_content/hat/examples/pointpillars_v2","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":196},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":1946},{"text":"Model Building","id":"model-building","depth":3,"charIndex":2066},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":3598},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":3765},{"text":"Quantized Model Training","id":"quantized-model-training","depth":2,"charIndex":4583},{"text":"The Value of the Quantize Parameter is Different","id":"the-value-of-the-quantize-parameter-is-different","depth":3,"charIndex":4825},{"text":"Different Training Strategies","id":"different-training-strategies","depth":3,"charIndex":5335},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":5879},{"text":"Align BPU Validation","id":"align-bpu-validation","depth":3,"charIndex":6469},{"text":"Results Visualization","id":"results-visualization","depth":3,"charIndex":6657},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":6913}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":18,"title":"PointPillars Detection Model Training (No config and HAT Trainer)","content":"#\n\nThis tutorial aims to introduce in detail how to use HAT as a model library, and\nuse it in combination with a user-defined training framework to train the\nPointPillars model on the radar point cloud dataset KITTI-3DObject. In the\ntutorial, we will show how to integrate the model in HAT into your training\nframework, including key steps such as data preprocessing and model building, to\nhelp you successfully integrate HAT into your training framework, and train\nfloating-point, QAT and quantized models of PointPillars.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\ndownload the 3DObject dataset . The following 4 files are included:\n\n 1. left color images of object data set\n 2. velodyne point clouds\n 3. camera calibration matrices of object data set\n 4. taining labels of object data set\n\nAfter downloading the above 4 files, unzip and organize the folder structure as\nfollows:\n\n\n\nIn order to create KITTI point cloud data, the original point cloud data needs\nto be loaded and the associated data annotation file containing the target\nlabels and annotation boxes needs to be generated. It is also necessary to\ngenerate the point cloud data for each individual training target for the KITTI\ndataset and store it in a .bin format file in data/kitti/gt_database. In\naddition, a file containing data information in .pkl format needs to be\ngenerated for either the training data or the validation data. Then, create the\nKITTI data by running the following command:\n\n\n\nAfter executing the above command, the following file directory is generated:\n\n\n\nAlso, to improve the speed of training, we did a package of data information\nfiles to convert them into lmdb format datasets. The conversion can be\nsuccessfully achieved by simply running the following script:\n\n\n\nThe above two commands correspond to transforming the training dataset and the\nvalidation dataset, respectively. After the packing is completed, the file\nstructure in the data directory should look as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packaged training and validation datasets, which\nare also the final datasets read by the network. kitti3d_gt_database and\nkitti3d_dbinfos_train.pkl are the samples that are used for sampling during\ntraining.\n\n\nFloating-point Model Training#\n\nOnce the dataset is ready, you can start training the PointPillars detection\nnetwork.\n\n\nModel Building#\n\nThe network structure of PointPillars can refer to Paper , this is not described\nin detail here.\n\nThe entire process from model training to compilation is roughly as follows:\n\n\n\nFrom the above figure, we know that three stages of models are mainly used,\nnamely, Float Model , QAT Model, and Quantized Model: Where,\n\n * Float Model : General floating-point model.\n * QAT Model : Model for quantization aware training.\n * Quantized Model : Quantized model, with the parameter of INT8 type.\n\nIn addition, models with different structures (or states) are used in training,\ncompilation, and other processes:\n\n * model: A complete model structure, including pre processing, network\n   structure, and post processing of the model. It is mainly used for training\n   and evaluation.\n * deploy_model : Only contains the network structure (which can be compiled\n   into hbm), excluding pre processing and post processing. It is mainly used\n   for compilation.\n\nWe define a PointPillarsModel class to define all the content related to the\nmodel structure, including the above three stages: Float Model, QAT Model, and\nQuantized Model, as well as two states: model and deploy_model:\n\n\n\nAll content related to the model has been defined in the PointPillarsModel in\nthe above code. When using it, you can easily obtain the corresponding model\nstructure through PointPillarsModel.xxx().\n\nAfter completing the definition of the network structure, we can use the\nfollowing command to first check FLOPs and Params of the network:\n\n\n\n\nData Augmentation#\n\nSimilar to the Model building section, we define the DataHelper to implement\ndata related content, including transforms and data_Loader, etc.:\n\n\n\n\nTraining Strategy#\n\nTo train a model with high accuracy, a good training strategy is essential. For\neach training task, the training strategies for models at different stages\n(float, QAT) may vary slightly, so we also define the training strategy content\n(such as, optimizer and lr_schedule) in PointPillarsModel:\n\n\n\nNote\n\nIf you need to reproduce the accuracy, it is best not to modify the training\nstrategy in the sample code. Otherwise, unexpected training situations may\narise.\n\nThrough the above introduction, we have completed the definition of all modules\nrelated to model training. However, before starting training, we also need to\nhave a model training framework (for example, we can use the built-in training\nframework of HAT, or any other framework to complete the training). For example,\nthe following example code is based on Pytorch official tutorial and Pytorch\nopen source code , with a few modifications, a \"training framework\" that simply\nsupports single machine multi-gpu DDP is built:\n\n\n\nAt this point, we have completed the required content for model training and\nevaluation, and readers can view the complete code in the example. Next, you can\ntrain a high-precision pure floating-point detection model. Of course, training\na good detection model is not our ultimate goal, it is just a pre-training for\nour future training of fixed-point models.\n\n\n\n\nQuantized Model Training#\n\nWhen we have a floating-point model, we can start training the corresponding\nfixed-point model. In the same way as floating-point training, we can train a\nfixed-point model just by running the following script:\n\n\n\nWhen building the model structure section above, we have learned that there are\nsome differences between the Float Model and the QAT Model, mainly reflected in\nthe following aspects:\n\n\nThe Value of the Quantize Parameter is Different#\n\nWhen we train the quantized model, we need to set quantize=True. At this time,\nthe corresponding floating-point model will be converted into a quantized model.\nThe code is as follows:\n\n\n\nFor the key steps in quantization training, such as preparing floating-point\nmodels, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please read the Quantized\nAwareness Training (QAT) section.\n\n\nDifferent Training Strategies#\n\nAs we said before, quantization training is actually finetue on the basis of\npure floating-point training. Therefore, when quantized training, our initial\nlearning rate is set to one-tenth of the floating-point training, the number of\nepochs for training is also greatly reduced, and most importantly, when model is\ndefined, our pretrained needs to be set to the address of the pure\nfloating-point model that has been trained.\n\nAfter making these simple adjustments, we can start training our quantized\nmodel.\n\n\nModel Validation#\n\nAfter the model is trained, we can also verify the performance of the trained\nmodel. Since we provide two stages of training process, float and qat, we can\nverify the performance of the model trained in these two stages. It is only\nnecessary to run the following two commands accordingly:\n\n\n\nAt the same time, we also provide a performance test of the quantization model,\njust run the following command:\n\n\n\nThe displayed accuracy is the real accuracy of the final int8 model. Of course,\nthis accuracy should be very close to the accuracy of the qat verification\nstage.\n\n\nAlign BPU Validation#\n\nIn addition to the model validation described above, we offer an accuracy\nvalidation method that is identical to the board side, you can refer to the\nfollowing:\n\n\n\n\nResults Visualization#\n\nIf you want to see the detection effect of the trained model for a single frame\nradar point cloud, we also provide point cloud prediction and visualization\nscripts in our tools folder, you just need to run the following script.\n\n\n\n\nModel Checking and Compilation#\n\nAfter training, the quantized model can be compiled into an hbm file that can be\nrun on the board by using the compile tool. At the same time, the tool can also\nestimate the running performance on the BPU. The following scripts can be used:\n\n","routePath":"/en/guide/advanced_content/hat/examples/pointpillars_v2_no_hat_trainer","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":525},{"text":"Floating-point Model Training","id":"floating-point-model-training","depth":2,"charIndex":2274},{"text":"Model Building","id":"model-building","depth":3,"charIndex":2394},{"text":"Data Augmentation","id":"data-augmentation","depth":3,"charIndex":3926},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":4093},{"text":"Quantized Model Training","id":"quantized-model-training","depth":2,"charIndex":5466},{"text":"The Value of the Quantize Parameter is Different","id":"the-value-of-the-quantize-parameter-is-different","depth":3,"charIndex":5892},{"text":"Different Training Strategies","id":"different-training-strategies","depth":3,"charIndex":6402},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":6946},{"text":"Align BPU Validation","id":"align-bpu-validation","depth":3,"charIndex":7536},{"text":"Results Visualization","id":"results-visualization","depth":3,"charIndex":7724},{"text":"Model Checking and Compilation","id":"model-checking-and-compilation","depth":3,"charIndex":7980}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":19,"title":"PwcNet Optical Flow Prediction Model Training","content":"#\n\nThis tutorial focuses on how to train a PwcNet model from scratch on the optical\nflow dataset FlyingChairs using HAT, including floating-point, quantitative, and\nfixed-point models.\n\nFlyingChairs is the most used dataset in optical flow prediction, and many\nstate-of-the-art optical flow prediction studies are primarily based on this\ndataset for validation.\n\nBefore starting the model training, the first step is to prepare the dataset.\nHere we use the official dataset FlyingChairs.zip as the training and validation\nsets. Meanwhile, we need corresponding label data FlyingChairs_train_val.txt.\n\nThe extracted directory structure is as follows:\n\n\n\n\nTraining Process#\n\nIf you just want to train the PwcNet model, you can read this section first.\nSimilar to other tasks, HAT uses the tools + config format for all the training\nand evaluation tasks. After preparing the raw dataset, we can easily complete\nthe training process by taking the following procedure.\n\n\nDataset Preparation#\n\nTo improve the training speed, we packed the original dataset and converted it\nto the LMDB format. The conversion can be done by running the following script:\n\n\n\nThe above two commands are for the transformation of the training dataset and\nthe validation dataset respectively. After the packing, the file structure in\nthe directory should be as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packed training dataset and validation dataset.\nNext you can start training the model.\n\n\nModel Training#\n\nBefore the network training starts, you can first test the amount of operations\nand parameters of the network using the following commands:\n\n\n\nThe next step is to start training. Training can also be done with the following\nscript. Before the training, you need to make sure that the dataset path in the\nconfiguration has already been changed to the packed dataset path.\n\n\n\nSince the HAT algorithm toolkit uses an ingenious registration mechanism, each\ntraining task can be started in the form of train.py plus a config file.\n\ntrain.py is a uniform training script and independent of the task. The task we\nneed to train, the dataset we need to use, and the hyperparameters we need to\nset for the training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\n\"qat\" to train the floating-point model, the quantitative model, and the\nconversion of the quantitative model to the fixed-point model, where the\ntraining of the quantitative model depends on the floating-point model produced\nby the previous floating-point training.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Validation#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can complete metrics\nvalidation on the trained model in the same way and get the metrics of Float,\nCalibration, QAT, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to model training, --stage followed by \"float\", \"calibration\", \"qat\" can\nbe used to validate the trained floating-point model, quantitative model,\nrespectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results of the\nfixed-point models:\n\n\n\n\nSimulation On-board Accuracy Validation#\n\nIn addition to the above model validation, we provide accuracy validation method\nexactly the same as the on-board environment, which can be done by the\nfollowing:\n\n\n\n\nFixed-point Model Check and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon processors, it is a must to check and compile the quantitative model.\n\nWe provide an interface for model checking in the training script and you can\nfirst define a quantitative model and then check whether it works properly on\nthe BPU:\n\n\n\nAfter the model is trained, use the compile_perf_hbir script to compile the\nquantitative model into an HBM file that supports on-board running. This tool\ncan also be used to predict the model performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to generating a\nquantitative and deployable model.\n\n\nTraining Details#\n\nWe will illustrate some things that need to be aware of for model training,\nwhich mainly includes config related settings.\n\n\nModel Building#\n\nThe network structure of PwcNet can be found in the Paper and Community\nTensorFlow Version and here we will kip the details.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nIn addition to backbone, the model also has head and losses modules. In PwcNet,\nbackbone is mainly used to extract the features of two images, where head is\nmainly used to get the predicted optical flow map from the features while losses\nsamples LnNormLoss from the paper as the training loss. loss_weights represents\nthe loss weight.\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining two dicts, data_loader and val_data_loader, in the config file,\ncorresponding to the processing of the training and validation sets,\nrespectively.\n\nTaking data_loader as an example, the data enhancement uses RandomCrop,\nRandomFlip, SegRandomAffine, and FlowRandomAffineScale.\n\n\n\nSince the final model running on the BPU uses YUV444 as image input, while\ntraining image input is generally in the RGB format, HAT provides BgrToYuv444\ndata enhancement to convert RGB to YUV444.\n\nTo optimize the training process, some enhancement can be processed in\nbatch_processor to optimize the training.\n\n\n\nIn which loss_collector is the function to get the loss of the current batch\ndata.\n\nThe data conversion for the validation set is relatively simpler, as follows:\n\n\n\n\n\n\nTraining Strategy#\n\nThe floating-point model is trained on the FlyingChairs dataset using the Cosine\nlearning strategy with Warmup and imposing L2 norm on the parameter weight.\n\nThe float_trainer, calibration_trainer, qat_trainer, and int_trainer in the file\nconfigs/opticalflow_pred/pwcnet/pwcnet_pwcnetneck_flyingchairs.py refer to the\ntraining strategies for floating-point, quantitative, and fixed-point models,\nrespectively.\n\nTake the training strategy of float_trainer as an example:\n\n\n\n\nQuantitative Training#\n\nFor key steps in quantitative training, e.g., preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantitative\nnodes, setting quantitative parameters, and operator fusion, etc., please read\nthe Quantized Awareness Training (QAT) section.\n\nWhen the model is ready and some existing modules are quantized, HAT uses the\nfollowing script in the training script to map the floating-point model to the\nfixed-point model.\n\n\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately.\n\nBecause there is a floating-point pre-training model, the learning rate Lr of\nquantitative training can be rather small, usually starting from 0.001 or\n0.0001, and can perform Lr adjustments of scale=0.1 for 1 or 2 times with\nStepLrUpdater; at the same time, the training length does not need to be long.\n\nIn addition, weight decay will also have some influence on the training results.\n\nThe quantitative training strategy for the sample model of PwcNet can be found\nin the configs/opticalflow_pred/pwcnet/pwcnet_pwcnetneck_flyingchairs.py file.","routePath":"/en/guide/advanced_content/hat/examples/pwcnet","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":653},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":965},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1465},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2576},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":2750},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3438},{"text":"Simulation On-board Accuracy Validation","id":"simulation-on-board-accuracy-validation","depth":3,"charIndex":3560},{"text":"Fixed-point Model Check and Compilation","id":"fixed-point-model-check-and-compilation","depth":3,"charIndex":3769},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4463},{"text":"Model Building","id":"model-building","depth":3,"charIndex":4607},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":5196},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":6061},{"text":"Quantitative Training","id":"quantitative-training","depth":3,"charIndex":6555}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":20,"title":"QCNet Trajectory Prediction Model Training","content":"#\n\nThis tutorial primarily guides you on training a QCNet model from scratch on the\nArgoverse 2 dataset, including floating-point, quantized, and fixed-point\nmodels. QCNet is a trajectory prediction model, which can be referenced in the\npaperQuery-Centric Trajectory Prediction.\n\n\nTraining Process#\n\nIf you want to quickly train the QCNet model, start by reading this chapter.\nSimilar to other tasks, all training and evaluation tasks in HAT are\naccomplished using a tools + config approach. After preparing the raw dataset,\nyou can conveniently complete the entire training process by following the steps\nbelow.\n\n\nDataset Preparation#\n\nThe first step before training the model is to prepare the dataset. You can\ndownload it from the Argoverse 2 Dataset. Download the following: Training,\nValidation, and Test.\n\nAfter downloading, extract and organize the folders as follows:\n\n\n\nTo speed up the training process, we packed the dataset information files into\nlmdb format. You can convert the dataset by running the following script:\n\n\n\nThese commands respectively convert the training and validation datasets. After\npacking, the target-data-dir directory structure should be as follows:\n\n\n\ntrain_lmdb and val_lmdb are the packed training and validation datasets. Now you\ncan start training the model.\n\n\nModel Training#\n\nNext, you can start training. Training can be done using the following scripts.\nBefore training, ensure that the dataset path in the configuration has been\nswitched to the packed dataset path.\n\n\n\nThese commands respectively complete the training of the floating-point model\nand the fixed-point model. Training the fixed-point model requires a pre-trained\nfloating-point model. For more details, please read the chapter on\nQuantization-Aware Training.\n\n\nExporting the Fixed-Point Model#\n\nAfter completing quantization training, you can start exporting the fixed-point\nmodel. You can export it using the following command:\n\n\n\nSince QCNet uses relative spatio-temporal encoding, introducing spatio-temporal\ninvariance, we can reuse the features encoded from historical frames during\nmodel deployment. To facilitate model evaluation, we use an additional model to\ngenerate features from the historical frames. Similarly, you can use the\nfollowing command to export the model:\n\n\n\n\nModel Validation#\n\nAfter completing the training, we can obtain the trained floating-point and\nquantized models. Similar to the training method, we can use the same approach\nto evaluate the metrics of the trained models. This will yield metrics for\nFloat, Calibration, and Qat. The former provides the metrics for the\nfloating-point model, while the latter two provide the metrics for the models\nobtained through quantization calibration and quantization-aware training,\nrespectively.\n\n\n\nSimilar to training models, using \"float\" or \"calibration\" as the parameter\nafter --stage completes the validation of the trained floating-point model and\nquantized model respectively.\n\nFor fixed-point model accuracy validation, use the following command, but note\nthat it must be exported as hbir first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results of the\nfixed-point model:\n\n\n\n\nSimulation Onboard Accuracy Validation#\n\nIn addition to the above model validation, we also provide an accuracy\nvalidation method that is completely consistent with the onboard method:\n\n\n\n\nFixed-Point Model Check and Compilation#\n\nThe quantization training toolchain integrated in HAT is mainly prepared for\nHorizon's computing platform. Therefore, checking and compiling the quantized\nmodel is necessary. We provide an interface for model checking in HAT, allowing\nyou to define a quantized model and then check if it can run normally on BPU:\n\n\n\nAfter training the model, you can compile the quantized model into an hbm file\nthat can be run on board using the compile_perf_hbir script. This tool can also\nestimate the performance on BPU:\n\n\n\nThis completes the entire process from data preparation to generating a\ndeployable quantized model.\n\n\nTraining Details#\n\nIn this section, we explain some key points to note when training the model,\nmainly related to the config settings.\n\n\nModel Construction#\n\nThe QCNet network structure can be referenced from the paper. We define and\nmodify the model conveniently by defining a dict variable named model in the\nconfig file.\n\n\n\n\nData Loading#\n\nSimilar to the definition of model, the data_loader and val_data_loader for the\ntraining and validation stages are defined in the config configuration file\nusing the data_loader and val_data_loader dictionaries, respectively, which\ncorrespond to the processing pipelines for the training and validation datasets.\n\nSince QCNet does not include complex data augmentation (transforms=None),\ncollate_fn defines how individual data items are grouped into batches, including\nalignment operations.\n\n\n\nAdditionally, the config defines batch_processor for processing batches:\n\n\n\nIn batch_processor, a loss_collector function is passed to collect the losses\nfor the current batch of data, as shown below:\n\n\n\n\nTraining Strategy#\n\nFirst, let's introduce the strategy for training the floating-point model of\nQCNet on the Argoverse2 dataset. We use AdamW as the optimizer with settings\nlr=5e-4 and weight_decay=1e-4. However, we do not apply weight decay to bias ,\nnn.Embedding , and normalization layers. Therefore, using custom_param_optimizer\nhere easily achieves this setup. We employ a Cosine learning rate schedule, set\nthe warmup length to 1 epoch, and train the model for a total of 64 epochs.\nBelow is a complete configuration example for the float_trainer in the config\nfile:\n\n\n\n\nQuantization-Aware Training#\n\nFor key steps in quantization training, such as preparing the floating-point\nmodel, operator replacement, inserting quantization and dequantization nodes,\nsetting quantization parameters, and operator fusion, please refer to the\nQuantized Awareness Training (QAT) section. Here we focus on defining and using\nthe quantized model.\n\nThe quantization training strategy for the QCNet example model can be found in\nthe configs/traj_pred/qcnet_oe_argoverse2.py file, mainly divided into the\nquantization calibration calibration and quantization-aware training qat phases.\n\nQuantization Calibration Configuration:\n\n\n\nQuantization-Aware Training Configuration:\n\n\n\nHere, float2calibration and float2qat respectively define the conversion process\nfrom floating-point to calibration model and from floating-point to\nquantization-aware training model. The cali_qconfig_setter and\nqat_qconfig_setter provide the qconfig settings for the calibration model and\nthe qat model respectively. For more details on setting and debugging qconfig,\nsuch as default qconfig settings and quantization-sensitive operator settings,\nplease refer to the Quantization-Aware Training - Qconfig Details section.\n\n\nQuantization Sensitivity Operator Sorting#\n\nDuring quantization training, we need to set certain quantization-sensitive\noperators to int16 to meet the quantization precision requirements of the model.\nThe sorting of quantization-sensitive operators can be obtained by running the\nfollowing command:\n\nWe provide \"output_prob_L1_sensitive_ops.pt\" and\n\"output_pred_L1_sensitive_ops.pt\"， and you can follow these steps to generate\nthe sensitive tables:\n\nFirst, you should train a calibration model with\ndefault_calibration_qconfig_setter:\n\n\n\nThen you should rename saved \"calibration-checkpoint-last.pth.tar“ to\n\"calibration-checkpoint-best-defaultQconfig.pth.tar\", and run following command\nto generate sensitive analysis files in analysis folder.\n\n\n\nThis command corresponds to following command：\n\n\n\nFor the key steps, please refer to the Quantization-Aware Training - Accuracy\nTuning Tool Guide section.","routePath":"/en/guide/advanced_content/hat/examples/qcnet","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":280},{"text":"Dataset Preparation","id":"dataset-preparation","depth":3,"charIndex":614},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1301},{"text":"Exporting the Fixed-Point Model","id":"exporting-the-fixed-point-model","depth":3,"charIndex":1771},{"text":"Model Validation","id":"model-validation","depth":3,"charIndex":2294},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3091},{"text":"Simulation Onboard Accuracy Validation","id":"simulation-onboard-accuracy-validation","depth":3,"charIndex":3212},{"text":"Fixed-Point Model Check and Compilation","id":"fixed-point-model-check-and-compilation","depth":3,"charIndex":3401},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4056},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4193},{"text":"Data Loading","id":"data-loading","depth":3,"charIndex":4384},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5098},{"text":"Quantization-Aware Training","id":"quantization-aware-training","depth":3,"charIndex":5676},{"text":"Quantization Sensitivity Operator Sorting","id":"quantization-sensitivity-operator-sorting","depth":3,"charIndex":6887}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":21,"title":"Executing the Script","content":"#\n\nIn HAT, the functions that you can use and modify directly mainly include tools\nand configs The tools are mainly the core functional modules, including training\nand validation visualization, while configs mainly contains the options and\nparameters that can be configured during the execution of the functional\nmodules.\n\nThis tutorial shows you the core functions included in the tools, as well as the\ndevelopment specifications and the usage of configs.\n\nIn most cases, for the execution of tools, a config input is needed, except for\nsome tools related to datasets or single image visualization. Therefore, the\ngeneral execution paradigm can be summarized as follows:\n\n\n\nHere we mainly introduce the core functions and external interfaces of the\ntools.\n\n\nTools#\n\nThe current tools have some python scripts, each for a different function.\n\ntrain.py is a training tool with the following major parameters:\n\nPARAMETER                                  DESCRIPTION\n--stage {float, calibration, qat}          Different training and prediction stages.\n--config CONFIG, -c CONFIG                 Path to the config file.\n--device-ids DEVICE_IDS, -ids DEVICE_IDS   List of running GPUs.\n--dist-url DIST_URL                        Server address for multi-computer operations, auto by\n                                           default.\n--launcher {torch}                         Launch mode for multi-computer operations.\n--pipeline-test                            Whether to run the pipeline test.\n--opts                                     Modify config options using the command-line.\n--opts-overwrite                           Whether to allow modify config.\n--level                                    Default logging level for other rank except rank0.\n\npredict.py is a predicting tool with the following major parameters:\n\nPARAMETER                                  DESCRIPTION\n--stage {float, calibration, qat}          Different training and prediction stages.\n--config CONFIG, -c CONFIG                 Path to the config file.\n--device-ids DEVICE_IDS, -ids DEVICE_IDS   List of running GPUs.\n--dist-url DIST_URL                        Server address for multi-computer operations, auto by\n                                           default.\n--backend                                  The backend of communication methods of multiple nodes or\n                                           GPUs.\n--launcher {torch}                         Launch mode for multi-computer operations.\n--ckpt                                     The ckpt file for predict model.\n--pipeline-test                            Whether to run the pipeline test.\n\nmodel_checker.py is a checker tool for checking model executable on the BPU.\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n\nvalidation_hbir.py is an accuracy validation tool that provides fixed-point\naccuracy and fully aligned results with the on-board situations, with the\nfollowing major parameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n--stage {align_bpu}          Different prediction stages.\n\ncalops.py is the network ops calculation tool, with the following major\nparameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n--input-shape                Input shape.\n\ncompile_perf_hbir.py is the compilation and performance tool, with the following\nmajor parameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Directory of the config file.\n--opt {0,1,2}                Compilation-time optimization options.\n--jobs JOBS                  Number of threads for compilation.\n--model_path MODEL_PATH      The path for qat hbir.\n\ninfer_hbir.py is used to perform single image prediction, with the following\nmajor parameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n--model-inputs               The specified model inputs.\n--save-path                  The path where the visualization results are saved.\n--use-dataset                Whether to use sample of dataset for infer.\n\ncreate_data.py is used to pre-process Kitti3D lidar dataset, with the following\nmajor parameters:\n\nPARAMETER    DESCRIPTION\n--dataset    Name of dataset.\n--root-dir   Path of dataset.\n\nexport_onnx.py is used to export the model to onnx (only for visualization and\ndoes not support inference), with the following major parameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n\nexport_hbir.py is used to export hbir model, with the following major\nparameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n--save-path                  Path of save hbir model.\n\ngen_camera_param_nusc.py is used to get camera internal and external parameters\nfrom nuscenes, with the following major parameters:\n\nPARAMETER        DESCRIPTION\n--data-path      Path of dataset.\n--save-path      Path of save output.\n--save-by-city   Whether saved according to the city.\n--version        The version of dataset.\n\ngen_reference_points_nusc.py is used to get model input reference points from\nnuscenes, with the following major parameters:\n\nPARAMETER                    DESCRIPTION\n--data-path                  Path of dataset.\n--save-path                  Path of save output.\n--save-by-city               Whether saved according to the city.\n--version                    The version of dataset.\n--config CONFIG, -c CONFIG   Path to the config file.\n\nhomography_generator.pyis used to get ego2img matrix, with the following major\nparameters:\n\nPARAMETER                  DESCRIPTION\n--sensor2ego-translation   Translation matrix from sensor to ego coordinate system.\n--sensor2ego-rotation      Rotation matrix from sensor to ego coordinate system.\n--camera-intrinsic         camera intrinsic.\n--save-path                Path of save output.\n\nreference_points_generator.py is used to calculate model input reference points\nfrom homography matrix, with the following major parameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n--homography                 Path to the homography file。\n--save-path                  Path of save output.\n\nquant_analysis.py is used to analysis qat training, with the following major\nparameters:\n\nPARAMETER                    DESCRIPTION\n--config CONFIG, -c CONFIG   Path to the config file.\n\nThe datasets directory is for dataset-related packaging and visualization tools.","routePath":"/en/guide/advanced_content/hat/examples/scripts","lang":"en","toc":[{"text":"Tools","id":"tools","depth":2,"charIndex":758}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":22,"title":"UNet Segmentation Model Training","content":"#\n\nAs a sample of the HAT segmentation task, this tutorial demonstrates how to\ntrain a state-of-the-art floating-point and fixed-point model on the Cityscapes\ndataset using HAT.\n\nCityscapes is an image dataset of urban driving scenes, containing 5000 images\nwith pixel-level annotations, and the objects in the images are divided into 19\ncategories. The segmentation task is relatively complex and requires a high\nlevel of model capability, and it is not easy to achieve good metrics on the\nsegmentation task using small models.\n\nThis tutorial will elaborate on how to train a state-of-the-art segmentation\nmodel on the Cityscapes dataset using HAT and conduct quantitative training on\ntop of the floating-point model to finally get a fixed-point model from scratch.\n\n\nTraining Process#\n\n\nDownloading Dataset#\n\nTo download the Cityscapes dataset, you first need to register an account on the\nofficial website.\n\nAfter that, you can download the dataset files from the Download page. Here we\nonly need two files: gtFine_trainvaltest.zip and leftImg8bit_trainvaltest.zip.\n\nAdditionally, the official Cityscapes dataset provides a script for downloading\nand processing the data, see Github.\n\nFirst install the official tools using the following command:\n\n\n\nThen use the official tool to log into the account registered above and download\nthe required dataset files.\n\n\n\nFinally, unpack the downloaded files (optional):\n\n\n\n\nPacking Dataset#\n\nTo improve data reading efficiency, we recommend pre-packing the dataset into\nthe LMDB format. HAT provides the cityscapes_packer.py script for you to easily\nconvert the dataset from the original public format to numpy.ndarray or\ntorch.Tensor. Use msgpack to wrap the data and eventually pack it into LMDB\nfiles.\n\nThe command to pack the dataset is as follows:\n\n\n\nThe generated LMDB files are saved under ${data-dir}/train_lmdb and\n${data-dir}/val_lmdb.\n\n\nModel Training#\n\nAfter packing the dataset into LMDB files, you can start training the model. HAT\nprovides the training script train.py to facilitate model training together with\nconfig files.\n\nBefore starting the training, please make sure that the dataset path\n(data_rootdir) in the configuration file unet_mobilenetv1_cityscapes.py is set\nto the LMDB file path.\n\nThe commands for model training are as follows:\n\n\n\nThe above two commands are respectively for the training of the floating-point\nmodel and the fixed-point model. The fixed-point model training needs to be\nbased on the trained fixed-point model. For details on this, please read the\nQuantized Awareness Training (QAT) section.\n\n\nMetrics Validation#\n\nAfter training, the script will automatically validate the metrics of the\ntrained model.\n\nIn addition, HAT provides a script to validate the metrics of the trained\nfloating-point and fixed-point models, which is also configured by using the\nconfig file.\n\n\n\n\nSimulation of On-board Accuracy Validation#\n\nIn addition to the above model validation, we also provide the exact same\naccuracy validation method simulating the on-board conditions, as below:\n\n\n\n\nModel Inference#\n\nHAT provides the infer.py script to visualize the inference results of the\ntrained models at each phase:\n\n\n\n\nModel Compilation#\n\nThe quantization algorithms used in the HAT integrated quantitative training\nframework are specifically designed for Horizon processors, so fixed-point\nmodels trained with HAT can be compiled using the tools provided by HBDK to\ngenerate fixed-point models that can run on the computing platform. We provided\nthe script compile_perf.py to facilitate the compilation process.\n\n\n\nThus, we have got a segmentation task fixed-point model that can run on the\nHorizon processors from scratch.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, and Quantized, which are floating-point, quantitative, and\nfully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", to validate the trained floating-point model, and quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\n\nModel Structure#\n\n\n\nThe segmentation model mainly consists of backbone, neck, and head. Here we use\nMobileNetV1_0.25 as the backbone, which is a lightweight and efficient network\nstructure. The neck uses the Unet structure, which can combine the featuremap at\neach scale and preserve the fine spatial information. The head is a\nconvolutional layer responsible for the output of the final segmentation\nresults.\n\nWe use FocalLoss as the loss function. FocalLoss can be regarded as a\ncross-entropy loss function with dynamic weights, which can better solve the\nproblem of training difficulties caused by category imbalance.\n\nThe hierarchical structure of Unet is consistent with the idea of FPN and can be\ntreated with the same training method, that is, constructing an output on each\nscale of Unet, and construct a loss function with this output and the ground\ntruth with corresponding size for training, which supervises each network scale,\nprovides richer reference information for network training, reduces the training\ndifficulty, and improves the training speed and the final accuracy.\n\nMeanwhile, considering that the final result we need is the max-sized network\noutput (scale=4), to avoid the over large gradient of other sizes affecting the\naccuracy of the max-sized output, we add weights to the loss function according\nto scale. A layer with larger scale has smaller weight.\n\nUsually, after defining a model, especially for some public models, we would\nhave the need to check the FLOPs. HAT provides calops.py to calculate the\noperations of the model, which is implemented as below:\n\n\n\nSuch ops-counting tool can support both floating-point and fixed-point models at\nthe same time.\n\n\nData Pre-processing#\n\nFirst, we use LabelRemap to re-map the data labels to the interval [0, 18].\n\nFor the training set, SegRandomAffine can perform a random affine transformation\non the image for data enhancement, and we only configure a random scaling\nwithout any rotation operation.\n\nSince the training uses an FPN-like approach, we need to scale the labels to\ndifferent sizes to train the models with different scales.\n\nThe final model running on the BPU uses the YUV444 format as the image input,\nwhile regular trainings use the RGB format. Therefore, HAT provides\nImageBgrToYuv444 data enhancement to convert RGB data to the YUV444 format.\n\nFinally, normalization is necessary for deep learning model training.\n\nNote that here we use MultiBatchProcessor for the task training. This Processor\nsupports data pre-processing on the GPU in batches. Since the data preprocessing\nof the segmentation task is relatively complex, bottlenecks will occur if we use\nCPU for the processing. However, using GPU will increase the video memory usage,\nresulting in a lower maximum batch size. That said, the final training speed is\nconsiderably improved.\n\n\n\nCompared with the training set, the data preprocessing of the validation set\ndoes not require random affine transformation and multi-scale scaling. The other\nstages are the same and will not be repeated.\n\n\nTraining Strategies#\n\nThe segmentation task can be regarded as a pixel-level classification task, so\nthe training strategy is highly similar to that of the classification task.\n\nFirst, the training speed can be increased by increasing the learning rate as\nmuch as possible while ensuring convergence.\n\nWhen training with a certain learning rate until the accuracy no longer grows,\nyou can reduce the learning rate appropriately, the model will continue to\nconverge, and the accuracy can be further improved.\n\nAfter the final training, compare the accuracy of the test set with the training\nset, and if the accuracy of the training set is too much higher than the test\nset, the model is considered to be overfitted. In this case, increasing the\nweight decay can enhance the generalization ability of the model, reduce the\noverfitting, and obtain a higher accuracy of the test set.\n\n\n\n\nQuantitative Training Strategies#\n\nThe purpose of quantitative training is to simulate the process of fix-point\ncomputation by performing simulated data quantization based on the trained\nfloating-point model to minimize the accuracy loss during the quantitative to\nfixed-point model conversion.\n\nSince the floating-point model has converged to a good state after sufficient\ntraining, the model usually needs only a little fine-tuning in the quantitative\ntraining. The learning rate should not be set too large. You can start trying\nfrom 1e-4 order of magnitude, leaving other parameters the same as the\nfloating-point training. Similarly, as the model fine-tuning requires less\namount of training, usually a few dozen training epochs is enough.\n\nSince the model already has a good accuracy before quantitative training, room\nfor the accuracy improvement is small. The simulated quantization of the data\nwill also lead to large fluctuations in the training process. At this point, we\nneed to patiently and carefully observe the trend from the fluctuations and\nadjust the parameters appropriately to get the best results.\n\n\nModel Checking & Compilation#\n\nFor HAT, the significance of the quantitative model is that it can be run\ndirectly on the BPU. Therefore, it is necessary to check and compile the\nquantitative model.\n\nThe compile_perf.py script provided by HAT first checks the model to ensure that\nit can run properly on the BPU. Then it compiles the model to make it runnable\non the BPU. After that, the script will also test the compiled fixed-point model\nto predict its performance on the BPU. Run the command as below:\n\n\n\n\nPre-trained Model#\n\nHAT has provided pre-trained models for this example, which are all included in\nthe release package.","routePath":"/en/guide/advanced_content/hat/examples/segmentation","lang":"en","toc":[{"text":"Training Process","id":"training-process","depth":2,"charIndex":768},{"text":"Downloading Dataset","id":"downloading-dataset","depth":3,"charIndex":788},{"text":"Packing Dataset","id":"packing-dataset","depth":3,"charIndex":1417},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1891},{"text":"Metrics Validation","id":"metrics-validation","depth":3,"charIndex":2586},{"text":"Simulation of On-board Accuracy Validation","id":"simulation-of-on-board-accuracy-validation","depth":3,"charIndex":2865},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3061},{"text":"Model Compilation","id":"model-compilation","depth":3,"charIndex":3188},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":3696},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":3870},{"text":"Model Inference","id":"model-inference-1","depth":3,"charIndex":4563},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":4685},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":4883},{"text":"Training Details","id":"training-details","depth":2,"charIndex":5571},{"text":"Model Structure","id":"model-structure","depth":3,"charIndex":5591},{"text":"Data Pre-processing","id":"data-pre-processing","depth":3,"charIndex":7284},{"text":"Training Strategies","id":"training-strategies","depth":3,"charIndex":8637},{"text":"Quantitative Training Strategies","id":"quantitative-training-strategies","depth":3,"charIndex":9521},{"text":"Model Checking & Compilation","id":"model-checking--compilation","depth":3,"charIndex":10643},{"text":"Pre-trained Model","id":"pre-trained-model","depth":2,"charIndex":11152}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":23,"title":"StereoNet Binocular Depth Estimation Model Training","content":"#\n\nThis tutorial mainly shows you how to use HAT to train a StereoNet model from\nscratch on the dataset SceneFlow, including floating-point, quantized, and\nfixed-point models.\n\n\nDataset Preparation#\n\nBefore starting to train the model, the first step is to prepare the dataset,\nwhich can be downloaded in the SceneFlow dataset . At the same time, you need to\nprepare the list of files corresponding to the training data and validation\ndataset, and you can download SceneFlow_finalpass_train.txt and\nSceneFlow_finalpass_test.txt from here .\n\nAfter downloading, unzip and organize the folder structure as follows:\n\n\n\nIn order to improve the speed of training, we made a package of data information\nfiles and converted them into a dataset in LMDB format. Just run the script\nbelow to successfully achieve the conversion:\n\n\n\nThe above two commands correspond to the transformation training dataset and the\nvalidation dataset respectively, after packaging, the file structure in the\n${target-data-dir} directory should be as follows:\n\n\n\ntrain_lmdb and test_lmdb are packaged training datasets and validation datasets,\nand then you can start training the model.\n\n\nModel Training#\n\nBefore the network starts training, you can use the following command to\ncalculate the amount of computation and the number of parameters for the\nnetwork:\n\n\n\nThe next step is to start training. Training can also be done through the\nfollowing script, and you need to confirm whether the dataset path in the\nconfiguration has been switched to the packaged dataset path before training.\n\n\n\nSince the HAT algorithm package uses the registration mechanism, it allows each\ntraining task to be started in the form of train.py plus a config file. The\ntrain.py is a uniform training script and independent of the task, and the tasks\nwe need to train, the datasets we need to use, and the hyperparameter settings\nrelated to training are all in the specified config file.\n\nThe parameters after --stage in the above command can be \"float\", \"calibration\",\n\"qat\", which, respectively, indicates the training of the floating-point model\nand the quantitative model, and the conversion of the quantitative model to the\nfixed-point model, where the training of the quantitative model depends on the\nfloating-point model produced by the previous floating-point training.\n\n\nExport FixedPoint Model#\n\nOnce you've completed your quantization training, you can start exporting your\nfixed-point model. You can export it with the following command:\n\n\n\n\nModel Verification#\n\nAfter completing the training, we get the trained floating-point, quantitative,\nor fixed-point model. Similar to the training method, we can use the same method\nto complete metrics validation on the trained model and get the metrics of\nFloat, Calibration, QAT, and Quantized, which are floating-point, quantitative,\nand fully fixed-point metrics, respectively.\n\n\n\nSimilar to the model training, we can use --stage followed by \"float\",\n\"calibration\", \"qat\", to validate the trained floating-point model, quantitative\nmodel, respectively.\n\nThe following command can be used to verify the accuracy of a fixed-point model,\nbut it should be noted that hbir must be exported first:\n\n\n\n\nModel Inference#\n\nHAT provides the infer_hbir.py script to visualize the inference results for the\nfixed-point model:\n\n\n\n\nSimulation Board Accuracy Verification#\n\nIn addition to the above model validation, we provide an accuracy validation\nmethod identical to the on-board environment, which can be accomplished by:\n\n\n\n\nFixed-point Model Checking and Compilation#\n\nAs the quantitative training toolchain integrated in HAT is mainly prepared for\nHorizon's processors, it is a must to check and compile the quantitative models.\n\nWe provide an interface for model checking in HAT, which allows you to define a\nquantitative model and then check whether it can work properly on the BPU first.\n\n\n\nAfter the model is trained, you can use the compile_perf_hbir script to compile\nthe quantitative model into an HBM file that supports on-board running. The tool\ncan also predict the performance on the BPU.\n\n\n\nThe above is the whole process from data preparation to the generation of\nquantitative and deployable models.\n\n\nTraining Details#\n\nIn this note, we explain some things that need to be considered for model\ntraining, mainly including settings related to config.\n\n\nModel Construction#\n\nThe network structure of StereoNet can be found in the Paper, which is not\ndescribed in detail here.\n\nWe can easily define and modify the model by defining a dict type variable like\nmodel in the config file.\n\n\n\nIn addition to the backbone, the model also has head, post_process, losses\nmodules. Among them, backbone is mainly to extract the features of the image,\nand head is mainly used by the features to obtain the predicted parallax value.\nThe post_process is mainly the post-processing part, and the losses module uses\nthe SmoothL1Loss in the paper as the training loss, and the loss_weights is the\nweight of the corresponding loss.\n\n\nData Enhancement#\n\nLike the definition of model, the data enhancement process is implemented by\ndefining data_loader and val_data_loader in the config file, which correspond to\nthe processing of the training sets and verification sets, respectively. Taking\ndata_loader as an example, the data enhancement uses RandomCrop, ToTensor, and\nNormalize to increase the diversity of training data and enhance the\ngeneralization ability of the model.\n\nSince the final model running on BPU uses a YUV444 image input, and the general\ntraining image input is in the RGB format, HAT provides BgrToYuv444 data\nenhancement to convert RGB to YUV444.\n\n\n\nA loss_collector function is passed in batch_processor to get the loss for the\ncurrent batch data, as follows:\n\n\n\nThe data transformation of the validation set is relatively simpler, as follows:\n\n\n\n\n\n\nTraining Strategy#\n\nTraining a floating-point model on the SceneFlow dataset uses the learning\nstrategy of Cosine with Warmup, and impose L2 norm on the weight parameter.\n\nThe float_trainer, calibration_trainer, qat_trainer, and int_trainer in the\nconfigs/disparity_pred/stereonet/stereonet_stereonetneck_sceneflow.py file\ncorrespond to the training strategies for floating-point, quantitative, and\nfixed-point models, respectively.\n\nThe following is an example of float_trainer training strategy:\n\n\n\n\nQuantization Training#\n\nFor key steps in quantitative training, such as preparing the floating-point\nmodel, operator substitution, inserting quantization and inverse quantization\nnodes, setting quantitative parameters, and operator fusion, please read the\nQuantized Awareness Training (QAT) section. Here we focus on how to define and\nuse the quantization models in binocular depth estimation of HAT.\n\nIf the model is ready, and some existing modules are quantized, HAT uses the\nfollowing script in the training script to map the floating-point model to the\nfixed-point model uniformly.\n\n\n\nThe overall strategy of quantitative training can directly follow the strategy\nof floating-point training, but the learning rate and training length need to be\nadjusted appropriately. Due to the existence of the floating-point pre-training\nmodel, the learning rate Lr for quantitative training can be very small.\nGenerally, you can start from 0.001 or 0.0001, and you can do one or two times\nLr adjustments of scale=0.1 with StepLrUpdater without prolonging the training\ntime. In addition, weight decay will also have some effect on the training\nresults.\n\nThe quantitative training strategy for the StereoNet example model can be found\nin the configs/disparity_pred/stereonet/stereonet_stereonetneck_sceneflow.py\nfile.","routePath":"/en/guide/advanced_content/hat/examples/stereonet","lang":"en","toc":[{"text":"Dataset Preparation","id":"dataset-preparation","depth":2,"charIndex":177},{"text":"Model Training","id":"model-training","depth":3,"charIndex":1157},{"text":"Export FixedPoint Model","id":"export-fixedpoint-model","depth":3,"charIndex":2328},{"text":"Model Verification","id":"model-verification","depth":3,"charIndex":2502},{"text":"Model Inference","id":"model-inference","depth":3,"charIndex":3203},{"text":"Simulation Board Accuracy Verification","id":"simulation-board-accuracy-verification","depth":3,"charIndex":3325},{"text":"Fixed-point Model Checking and Compilation","id":"fixed-point-model-checking-and-compilation","depth":3,"charIndex":3523},{"text":"Training Details","id":"training-details","depth":2,"charIndex":4215},{"text":"Model Construction","id":"model-construction","depth":3,"charIndex":4365},{"text":"Data Enhancement","id":"data-enhancement","depth":3,"charIndex":5026},{"text":"Training Strategy","id":"training-strategy","depth":3,"charIndex":5864},{"text":"Quantization Training","id":"quantization-training","depth":3,"charIndex":6366}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":24,"title":"Execution Engine","content":"#\n\nAs described in the previous section introducing the framework, all the Data,\nModel, Callback and other sub-modules will eventually be put into the Engine for\nexecution after being built. As the execution engine of the whole HAT, the\nEngine has great importance.\n\nIn HAT, Engine defines the entire Pipeline for the training and prediction. For\nany deep-learning project, it is necessary to complete the training and\nprediction tasks for a given model. Therefore, this section focuses on the\nimplementation of the Engine module in HAT.\n\n\nEngine's Execution Workflow#\n\nThe most basic PipeBase of the entire HAT Engine defines all the Callbacks\noperable running phases, while LoopBase defines the basic execution flow of all\nEngines. As shown in the figure above, the entire Engine execution flow is\ncomposed of these parts: a variety of Callbacks and the processing operations of\nthe Processor associated with the model.\n\nThe Callbacks operable running phases include eight phases: on_loop_begin,\non_epoch_begin, on_step_begin, on_batch_begin, on_batch_end, on_step_end,\non_epoch _end, and on_loop_end. The execution order is shown in the above\nfigure, and you can use different Callbacks at different stages or use the same\nCallback at different stages as per their needs. For example, the common\nLrUpdater can be used in both on_epoch_begin and on_step_begin phases, and\nsimilarly, the scope of other parts of the Callback is shown in the figure.\n\nBatchProcessor is responsible for how data and models in the current Batch are\nrun, including basic operations commonly seen in model processing such as\nforward and backward. In addition, some of the grad update operations are also\ndefined here. Note that some complex training tasks also require BatchProcessor\nto complete more iterations and richer grad operations.\n\n\nEngine Structure#\n\nBased on LoopBase, a rich set of execution engines can be derived, as shown in\nthe Engine relationship diagram above.\n\nFrom the functional point of view, Trainer that focuses on training and\nPredictor that focuses on prediction can be derived from LoopBase.\n\n * Trainer: Responsible for all the training-related processes, which are\n   generally needed by deep-learning related training tasks.\n\n * Predictor: Responsible for prediction related processes, and is commonly used\n   in scenarios such as Validation.\n\nDepending on the execution method, different training methods can generate\ndifferent Trainers, such as DistibutedDataParallelTrainer based on\ntorch.nn.parallel.DistributedDataParallel and DataParallelTrainer based on\ntorch.nn.DataParallel, etc. Meanwhile, different execution methods require\ndifferent launching methods. For related details, refer to launcher in different\nTrainers.","routePath":"/en/guide/advanced_content/hat/framework/engine","lang":"en","toc":[{"text":"Engine's Execution Workflow","id":"engines-execution-workflow","depth":2,"charIndex":539},{"text":"Engine Structure","id":"engine-structure","depth":2,"charIndex":1820}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":25,"title":"Framework","content":"#\n\n\nCore Modules#\n\nThe above figure shows the abstract flowchart of the overall organization of the\nHAT framework. You can see that the training and verification process of HAT is\ncomposed of four core modules, namely Data, Model, Callback, and Engine. Here is\na brief introduction to each of these core modules.\n\n * Data: Responsible for all the data producing processes in HAT, including\n   Dataset (iterative output), Transforms (data enhancement for tasks), Collate\n   (data concatenation and batch packing), and Sampler (data sampling process).\n   All the data producing processes are finally organized in a unified way\n   through the Dataloader interface.\n\n * Model: Responsible for the building process of all the models in HAT. In HAT,\n   the model is generally divided into sub-modules such as backbone, neck, head,\n   task module, etc., and a unified structure is used to link all the\n   sub-modules to build the final model. Besides the common tasks, structure\n   also uses a GraphModel to specially handle the multitasking related model\n   structure building.\n\n * Callback: Responsible for dynamically adjusting the training state during the\n   execution of the Engine. Similar to the Hooks of the model in Torch, it can\n   perform dynamic adjustments in the specified modifiable positions according\n   to the training state provided by Engine without modifying the Engine code.\n   The modifiable positions of the whole Engine mainly include:\n   on_loop_begin(end), on_epoch_begin(end), on_step_begin(end), and\n   on_batch_begin(end).\n\n * Engine: Mainly responsible for building and executing the training or\n   prediction process, including the training module Trainer and the prediction\n   module Precitor. All other modules, such as Data, Model, Callback, etc., will\n   be fed to Engine after building, and Engine will implement unified scheduling\n   to complete the whole process of the training or prediction.\n\nIn addition to the four core modules, there are other supporting modules such as\nProfiler, Metric, Visualize, etc.\n\n * Profiler: As the prof tool of HAT, it mainly helps to locate the speed\n   bottleneck during the training or validation.\n\n * Metric: Mainly used for metrics validation during dataset training or\n   testing, which is actually a special case of Model and strongly bound to\n   specific datasets.\n\n * Visualize: Mainly used for the visualization of relevant datasets.\n\n\nTraining Building Process#\n\n1.For any dataset, build all the submodules required by Data.\n\nFirst build Dataset for iterative output, and process the data with Transform in\nthe iterative output, e.g., data enhancement operations in training, data\npreprocessing in testing, etc. Then use Sampler to control the output order of\nDataset, and use Collate to concatenate the data one by one, and finally pack a\nBatch of training data. The DataLoader unifies the scheduling of all processes\nand feeds the training data of the Batch into the training process as\nstructures.\n\n2.For any model, build all the submodules required by Model, e.g., Backbone,\nNeck, etc.\n\nUse Structure to link all the sub-modules together to form a complete model with\ntraining states, which will also be fed to the training process as a training\nobject.\n\n3.For the training task, select or define a suitable Callback to dynamically\nadjust the training state during the training.\n\nFor example, in each training session, output training results at regular\nintervals or dynamically adjust the learning rate of the training. Although the\ndefinition of Callback and Engine are separate, the execution process is\nembedded in the complete process of Engine.\n\n4.For the training environment, build a suitable Engine as the training engine.\n\nFor example, you can choose DistributedDataParallelTrainer or\nDataParallelTrainer for common multi-card training environments. The Engine can\norganize all the already-built modules together to complete all the\nenvironmental initialization needed by the training, including Data, Model, or\nother modules such as Callback, Metric, Profiler. Note that not all the modules\nin Engine are required.\n\n5.Finally, uniformly use the selected fit interface in Engine to complete the\nwhole training process.\n\nThe above is the overall structure of the HAT framework and the abstract flow of\nthe training. The diagram at the beginning of this section not only reflects the\ndata flow of the construction, but also includes invocation relationships\nbetween modules. For training, Engine is the core part. A comprehensive\nunderstanding of the operation flow of Engine will allow us to understand the\nentire data flow of HAT.","routePath":"/en/guide/advanced_content/hat/framework/framework","lang":"en","toc":[{"text":"Core Modules","id":"core-modules","depth":2,"charIndex":3},{"text":"Training Building Process","id":"training-building-process","depth":2,"charIndex":2411}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":26,"title":"FAQ","content":"#\n\n\nUnable to import hat in Docker container?#\n\nIf you get a GPU RuntimeError when running an example in GPU Docker, or a\nModuleNotFoundError: No module named 'hat', Segmentation fault (core dumped), or\nsomething like that when loading a hat, you can go in the following two\ndirections troubleshooting:\n\n 1. Start the GPU Docker container using the run_docker.sh script provided in\n    the OE package.\n 2. Execute the following script in the container to check if torch and cuda are\n    working. If CUDA cannot be called, you can use nvidia-smi to check if the\n    driver version meets the requirement. Because the OE package has upgraded\n    the torch environment to 1.1.30+cu116 since v1.1.60, the corresponding\n    driver requirements can be found in: Environment Deployment - Docker\n    Container Deployment.\n\n\n\n\n\n\nWhy the config prints correctly but launch trainer fails?#\n\nFor example, the following error requires you to change device_ids in config to\nthe currently available GPU resources; if the error contains \"CUDA out of\nmemory\", you need to change batch_size_per_gpu to the batch size that the\ncurrent resources can support.\n\n\n\n\nWhy the configuration of the device on the data run does not take effect?#\n\nPlease check whether to_cuda succeeds in hat/engine/processors/processor.py, it\nmay not because the data is not of the tensor type and the device is not called.\n\n\nWhy the configuration of the trainer's runtime device does not take effect?#\n\nThe device and device_id configurations are prioritized: if the trainer is\ninvoked using the default launch of train.py, only the device-ids will work, and\nthe device configuration will not work. If the trainer (predictor, loop base,\netc.) is called directly, the device can work directly.\n\n\nHow to set the validation/printing frequency?#\n\nIn the config file there is a parameter for the callback frequency, the\nauthentication frequency can be modified in the following way, and all other\ncallback related frequencies are set in the form of \"xxx_frep\":\n\n\n\n\nWhy the plt.show image is not displayed?#\n\nThis may be due to the fact that there is no graphical interface in the current\nLinux system, you can add plt.savefig('. /test.png',dpi=300) in the\ncorresponding code of each dataset under the path of hat/visualize, save the\nfile locally and check it again.\n\n\nHow to new dataset?#\n\ndataloader internal class call flow chart is shown below (take Coco dataset as\nan example), in which all classes and interfaces are called in config, data\nloading is realized by calling dataloader, dataloader loaded dataset is returned\nby Coco class, Coco class reads lmdb format packaged data, which is realized by\ncalling packer interface. Packed data in lmdb format, which is realized by\ncalling packer interface, implementation flow: packer calls CocoDetectionPacker\nclass to pack the data returned by CocoDetection (image, label...) into the set\nformat (lmdb, mxrecord).\n\nTo implement a dataset requires an override of the classes involved above,\noverriding __getitem()__ based on the dataset's format (which can also be\nadapted to the format of the original dataset).\n\n\nHow to resume an unexpectedly interrupted training session?#\n\nIt is possible to recover unexpectedly interrupted training, or to recover only\nthe optimizer for fine-tuning, by configuring the resume_optimizer and\nresume_epoch_or_step fields in {stage}_trainer of config. For example, add the\nfollowing fields to trainer in the config file:\n\n\n\n\nHow to visualize the model structure?#\n\nSupports visualization via the hb_model_info tool -v parameter.\n\n\nHow to change the method of calibration?#\n\nThe calibration_trainer field in config can be modified to add a new\nqconfig_params dictionary as referenced below:\n\n\n\n\nWhether the DETR model supports deformable conv?#\n\nThis operator is mainly used in deformable structures in deformable-DETR.\nHorizon is not yet supported, but the gridsample operator, which is already\nsupported, can be used instead.\n\n\nWhy is Lss less accuray than IPM in the BEV model?#\n\nThe input resolution of the Lss is 256x704, which is lower than the 512x960 of\nthe IPM, so the accuracy is even lower.\n\n\nWhat are the constraints on the Lss inputs in the BEV model?#\n\nThe Lss does dimension folding before grid_sample, so there are operator\ncompilation constraints on h, w of input_feature, currently: H,W ∈ [1, 1024] and\nH*W ≤ 720*1024.\n\n\nHow to solve the time consuming mul(depth,feature) of Lss in BEV model?#\n\nThe featuremap H and W can be converted to [128,128] by the grid_sample operator\nbefore doing the mul calculation.\n\n\nHow is voxelpooling implemented for Lss in BEV model? How many points are\nselected?#\n\nIn order not to lose the point cloud features located in the same voxel, we will\nsample each voxel 10 times and sum each point cloud feature to get a 128x128x64\nBEV feature map, corresponding to the following code:\n\n\n\n\n\n\nHow are reference points selected for Lss of the BEV model?#\n\nThe generation of points in _gen_reference_point sets invalid points outside the\nfeature range to the larger value. In order not to take invalid points, the\nfirst 10 points with smaller values are aggregated using topk (k=10 for faster\ntraining).\n\n\nHow does the Lss for the BEV model handle the case where the gridsample input is\nlarge?#\n\nThe Lss model is prone to exceed the 720*1024 BPU operator constraint limit for\nH*W because it collapses 3 dimensions into 1 before the gridsample operator. At\nthis point it is recommended that before dimension folding (i.e. dfeat =\ndfeat.view(B, 1, -1, H * W)), the dimensions that may be overrun are split, the\ngridsample is computed separately, and the results are finally superimposed.\n\n\nWhy is the BEV model's gkt accuracy metric lower? What are the advantages?#\n\nThe main reason why the accuracy of Transformer-based gkt is lower than that of\nIPM is that the amount of data in the open source dataset is not enough, and we\ntry to add some business data to get a higher accuracy than that of IPM.\nMoreover, the gkt model is more robust and can solve the accuracy effect caused\nby camera offset.\n\n\nWhether the transfomer of gkt is global in the BEV model?#\n\nIt's not global, it's a kernel (3x3) feature that is selected for attention, so\nit's a kernel-transformer.\n\n\nWhether the BEV model supports the public version of bevformer?#\n\nJ6 already supports bevformer optimized version.\n\n\nWhether the accuracy is constrained between BEV model detection and segmentation\ntasks?#\n\nThere will be an impact because the accuracy of the detection task in\nmultitasking will be degraded, which needs to be balanced by the training\nstrategy.\n\n\nWhat are the components of latency in the PointPillars model?#\n\nThe current latency of Pointpillars consists of two parts: the first part is the\npre-processing (voxelization), i.e., the time from the point cloud input to the\nhead; the second part is the post-processing time.\n\n\nHow does the amount of point cloud affect the performance of the PointPillars\nmodel?#\n\nThe higher the number of valid point clouds, the longer the pre-processing time,\nas specified in the Voxelization cfg parameter set:\n\n\n\n\nWhat operations are included in the preprocessing of a PointPillars model?#\n\nThe pre-processing is pillarization (voxel), the corresponding stage is voxeliza\ntation, and the flow is shown schematically below:\n\n\nHow many types of task detection does the PointPillars model support? How can it\nbe extended?#\n\nCurrently the PointPillars_Kitti_Car model only supports the car class of\ndetection, but we also provide a multi-category detection model\ncenterpoint_pointpillar_nuscenes.\n\n\nHow is the input data for PointPillars' board-side hbm model generated? How is\nit preprocessed?#\n\nThe data preprocessing includes reshape and padding (padding to (1,1,150000,4)\nwhere 150000 is the maximum amount of point cloud in the bin file), the\nreference code is shown below:\n\n\n\n\nWhy does the board side perf data for the PointPillars model not match the\nofficial metrics?#\n\nWhen using the hrt_model_exec perf tool on the board side to evaluate\nperformance, please specify a real input_file, otherwise the tool will use\nrandomly generated point cloud data, which may result in inaccurate perf\nmetrics.\n\n\nWhether the PTQ program support conversion to Lidar models in the PointPillars\nmodel?#\n\nIt is supported on the functional link to export ONNX models from the training\nframework for PTQ conversion checking. However, from the established experience,\nthe Lidar point cloud model is more risky in terms of the accuracy of\nquantization by going to the PTQ scheme, mainly due to the following reasons:\nthe point cloud is relatively sparse, and the data distribution situation is not\nfriendly to the quantization.\n\n\nCan centerpoint and pointpillars be used interchangeably?#\n\nThe two models can be used interchangeably, and only the relevant configurations\ninvolved in the dataset need to be modified correspondingly, e.g., point cloud\nrange, prediction category, and anchor, target, etc. for post-processing.","routePath":"/en/guide/advanced_content/hat/hat_faq","lang":"en","toc":[{"text":"Unable to import hat in Docker container?","id":"unable-to-import-hat-in-docker-container","depth":2,"charIndex":3},{"text":"Why the config prints correctly but launch trainer fails?","id":"why-the-config-prints-correctly-but-launch-trainer-fails","depth":2,"charIndex":818},{"text":"Why the configuration of the device on the data run does not take effect?","id":"why-the-configuration-of-the-device-on-the-data-run-does-not-take-effect","depth":2,"charIndex":1141},{"text":"Why the configuration of the trainer's runtime device does not take effect?","id":"why-the-configuration-of-the-trainers-runtime-device-does-not-take-effect","depth":2,"charIndex":1380},{"text":"How to set the validation/printing frequency?","id":"how-to-set-the-validationprinting-frequency","depth":2,"charIndex":1750},{"text":"Why the plt.show image is not displayed?","id":"why-the-pltshow-image-is-not-displayed","depth":2,"charIndex":2015},{"text":"How to new dataset?","id":"how-to-new-dataset","depth":2,"charIndex":2318},{"text":"How to resume an unexpectedly interrupted training session?","id":"how-to-resume-an-unexpectedly-interrupted-training-session","depth":2,"charIndex":3116},{"text":"How to visualize the model structure?","id":"how-to-visualize-the-model-structure","depth":2,"charIndex":3460},{"text":"How to change the method of calibration?","id":"how-to-change-the-method-of-calibration","depth":2,"charIndex":3566},{"text":"Whether the DETR model supports deformable conv?","id":"whether-the-detr-model-supports-deformable-conv","depth":2,"charIndex":3729},{"text":"Why is Lss less accuray than IPM in the BEV model?","id":"why-is-lss-less-accuray-than-ipm-in-the-bev-model","depth":2,"charIndex":3964},{"text":"What are the constraints on the Lss inputs in the BEV model?","id":"what-are-the-constraints-on-the-lss-inputs-in-the-bev-model","depth":2,"charIndex":4138},{"text":"How to solve the time consuming mul(depth,feature) of Lss in BEV model?","id":"how-to-solve-the-time-consuming-muldepthfeature-of-lss-in-bev-model","depth":2,"charIndex":4373},{"text":"How is voxelpooling implemented for Lss in BEV model? How many points are selected?","id":"how-is-voxelpooling-implemented-for-lss-in-bev-model-how-many-points-are-selected","depth":2,"charIndex":-1},{"text":"How are reference points selected for Lss of the BEV model?","id":"how-are-reference-points-selected-for-lss-of-the-bev-model","depth":2,"charIndex":4871},{"text":"How does the Lss for the BEV model handle the case where the gridsample input is large?","id":"how-does-the-lss-for-the-bev-model-handle-the-case-where-the-gridsample-input-is-large","depth":2,"charIndex":-1},{"text":"Why is the BEV model's gkt accuracy metric lower? What are the advantages?","id":"why-is-the-bev-models-gkt-accuracy-metric-lower-what-are-the-advantages","depth":2,"charIndex":5664},{"text":"Whether the transfomer of gkt is global in the BEV model?","id":"whether-the-transfomer-of-gkt-is-global-in-the-bev-model","depth":2,"charIndex":6074},{"text":"Whether the BEV model supports the public version of bevformer?","id":"whether-the-bev-model-supports-the-public-version-of-bevformer","depth":2,"charIndex":6243},{"text":"Whether the accuracy is constrained between BEV model detection and segmentation tasks?","id":"whether-the-accuracy-is-constrained-between-bev-model-detection-and-segmentation-tasks","depth":2,"charIndex":-1},{"text":"What are the components of latency in the PointPillars model?","id":"what-are-the-components-of-latency-in-the-pointpillars-model","depth":2,"charIndex":6606},{"text":"How does the amount of point cloud affect the performance of the PointPillars model?","id":"how-does-the-amount-of-point-cloud-affect-the-performance-of-the-pointpillars-model","depth":2,"charIndex":-1},{"text":"What operations are included in the preprocessing of a PointPillars model?","id":"what-operations-are-included-in-the-preprocessing-of-a-pointpillars-model","depth":2,"charIndex":7108},{"text":"How many types of task detection does the PointPillars model support? How can it be extended?","id":"how-many-types-of-task-detection-does-the-pointpillars-model-support-how-can-it-be-extended","depth":2,"charIndex":-1},{"text":"How is the input data for PointPillars' board-side hbm model generated? How is it preprocessed?","id":"how-is-the-input-data-for-pointpillars-board-side-hbm-model-generated-how-is-it-preprocessed","depth":2,"charIndex":-1},{"text":"Why does the board side perf data for the PointPillars model not match the official metrics?","id":"why-does-the-board-side-perf-data-for-the-pointpillars-model-not-match-the-official-metrics","depth":2,"charIndex":-1},{"text":"Whether the PTQ program support conversion to Lidar models in the PointPillars model?","id":"whether-the-ptq-program-support-conversion-to-lidar-models-in-the-pointpillars-model","depth":2,"charIndex":-1},{"text":"Can centerpoint and pointpillars be used interchangeably?","id":"can-centerpoint-and-pointpillars-be-used-interchangeably","depth":2,"charIndex":8706}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":27,"title":"Overview","content":"#\n\nHorizon-Torch-Samples is an algorithm tool based on the Pytorch and Pytorch\nplugin interfaces, which is an efficient and user-friendly algorithm toolkit for\nHorizon BPUs.\n\nPyTorch, on which Horizon-Torch-Samples relies, is a tensor library optimized\nfor deep learning by using GPUs and CPUs, which is now one of the most popular\ndeep learning frameworks. The Pytorch plugin is a set of quantization algorithm\ntools developed based on Pytorch. Focusing on the implementation of quantization\nfunctions close to the computing platform, its quantization algorithms are\ndeeply coupled with Horizon computing platforms, and the quantization models\ntrained with this tool can be compiled and run normally on Horizon BPUs.\n\nAs the basic framework of algorithm package developed by Horizon Robotics,\nHorizon-Torch-Samples is open to all algorithm users, developers, and\nresearchers. Its quantization training is closely related to the Horizon\nprocessors and contains a complete process: Floating point training --> QAT\ntraining --> Fixed-point transformation prediction --> Model check compilation\n(for Horizon BPU) --> On-board accuracy simulation verification. It also\nprovides state-of-the-art (SOTA) deep-learning models for common image tasks\nincluding classification, detection, segmentation, etc.\n\n\nFeatures#\n\n * Based on Pytorch and horizon_plugin_pytorch.\n * Include a complete process from Floating point training to On-board accuracy\n   simulation verification.\n * Include SOTA models for common image tasks such as classification, detection,\n   and segmentation. All samples are compatible with Horizon BPUs.\n\n\nSample Models#\n\nHorizon-Torch-Samples currently includes the following deep learning models:\n\nClassification Model\n\n * mobilenetv1_imagenet\n\n * mobilenetv2_imagenet\n\n * resnet18_imagenet\n\n * resnet50_imagenet\n\n * vargnetv2_imagenet\n\n * efficientnet_imagenet\n\n * horizon_swin_transformer_imagenet\n\n * mixvargenet_imagenet\n\n * efficientnasnetm_imagenet\n\n * efficientnasnets_imagenet\n\n * vit_small_imagenet\n\n * henet_tinye_imagenet\n\n * henet_tinym_imagenet\n\nDetection model\n\n * fcos_efficientnetb0_mscoco\n\n * fcos_efficientnetb1_mscoco\n\n * fcos_efficientnetb2_mscoco\n\n * fcos_efficientnetb3_mscoco\n\n * detr_resnet50_mscoco\n\n * detr_efficientnetb3_mscoco\n\n * deform_detr_resnet50_mscoco\n\n * fcos3d_efficientnetb0_nuscenes\n\n * pointpillars_kitti_car\n\n * centerpoint_pointpillar_nuscenes\n\nSegmentation model\n\n * deeplabv3plus_efficientnetm0_cityscapes\n * deeplabv3plus_efficientnetm1_cityscapes\n * deeplabv3plus_efficientnetm2_cityscapes\n * fastscnn_efficientnetb0tiny_cityscapes\n * unet_mobilenetv1_cityscapes\n\nOptical flow model\n\n * pwcnet_pwcnetneck_flyingchairs\n\nLane detection model\n\n * ganet_mixvargenet_culane\n\nMultiple Object Track\n\n * motr_efficientnetb3_mot17\n\nBinocular Depth Estimation\n\n * stereonet_stereonetneck_sceneflow\n * stereonetplus_mixvargenet_sceneflow\n\nBev Multi-task Model\n\n * bev_ipm_efficientnetb0_multitask_nuscenes\n * bev_lss_efficientnetb0_multitask_nuscenes\n * bev_gkt_mixvargenet_multitask_nuscenes\n * bev_ipm_4d_efficientnetb0_multitask_nuscenes\n * detr3d_efficientnetb3_nuscenes\n * petr_efficientnetb3_nuscenes\n * bevformer_tiny_resnet50_detection_nuscenes\n * bev_cft_efficientnetb3_nuscenes\n * bev_sparse_henet_tinym_nuscenes\n\nKeypoints Detection Model\n\n * keypoint_efficientnetb0_carfusion\n\nLidar Multi-task Model\n\n * centerpoint_mixvargnet_multitask_nuscenes\n\nTrajectory Prediction Model\n\n * densetnt_vectornet_argoverse1\n * qcnet_oe_argoverse2\n\nOccupancy Prediction Model\n\n * flashocc_henet_lss_occ3d_nuscenes\n\nOnline Map Construction\n\n * maptroe_henet_tinym_bevformer_nuscenes\n\nLidar Fusion Multi-task Model\n\n * bevfusion_pointpillar_henet_multisensor_multitask_nuscenes\n\nIn the above model, 'resnet18_imagenet', 'resnet50_imagenet',\n'vargnetv2_imagenet', 'efficientnasnetm_imagenet', 'efficientnasnets_imagenet',\n'efficientnet_imagenet', 'mixvargenet_imagenet', 'vargnetv2_imagenet',\n'ganet_mixvargenet_culane', 'deeplabv3plus_efficientnetm0_cityscapes',\n'deeplabv3plus_efficientnetm1_cityscapes',\n'deeplabv3plus_efficientnetm2_cityscapes',\n'fastscnn_efficientnetb0tiny_cityscapes',\n'bev_gkt_mixvargenet_multitask_nuscenes',\n'bev_ipm_efficientnetb0_multitask_nuscenes',\n'bev_lss_efficientnetb0_multitask_nuscenes',\n'flashocc_henet_lss_occ3d_nuscenes', 'detr3d_efficientnetb3_nuscenes' and\n'keypoint_efficientnetb0_carfusion' only need to do calibration quantization\naccuracy to achieve the goal, detailed accuracy reference model_zoo.","routePath":"/en/guide/advanced_content/hat/introduction","lang":"en","toc":[{"text":"Features","id":"features","depth":2,"charIndex":1299},{"text":"Sample Models","id":"sample-models","depth":2,"charIndex":1616}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":28,"title":"Modelzoo","content":"#\n\n\nClassification#\n\nNETWORK                             FLOAT   QAT     QUANTIZATION   DATASET    INPUT SHAPE   BPU LATENCY (MS)   FPS\nmobilenetv1_imagenet                74.12   73.92   73.61          ImageNet   1x3x224x224   0.42               5069.83\nmobilenetv2_imagenet                72.65   72.51   72.11          ImageNet   1x3x224x224   0.45               4541.88\nresnet18_imagenet                   72.04   72.03   72.03          ImageNet   1x3x224x224   0.65               2346.22\nresnet50_imagenet                   77.37   76.99   76.94          ImageNet   1x3x224x224   1.12               1117.93\nvargnetv2_imagenet                  73.94   73.56   73.64          ImageNet   1x3x224x224   0.50               3763.68\nefficientnet_imagenet               74.31   74.23   74.18          ImageNet   1x3x224x224   0.49               3887.20\nhorizon_swin_transformer_imagenet   80.24   80.15   80.05          ImageNet   1x3x224x224   4.19               257.265367\nmixvargenet_imagenet                71.33   71.23   71.04          ImageNet   1x3x224x224   0.44               4728.13\nefficientnasnetm_imagenet           80.24   79.99   79.94          ImageNet   1x3x280x280   0.99               1305.20\nefficientnasnets_imagenet           76.63   76.23   76.03          ImageNet   1x3x300x300   0.56               2943.99\nvit_small_imagenet                  79.50   79.40   77.86          ImageNet   1x3x224x224   2.35               472.41\nhenet_tinye_imagenet                77.68   77.22   76.92          ImageNet   1x3x224x224   0.60               2713.99\nhenet_tinym_imagenet                78.38   77.95   77.62          ImageNet   1x3x224x224   0.48(J6M)          3451.43(J6M)\n\n\nDetection#\n\nFCOS\n\nNETWORK                      BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\nfcos_efficientnetb0_mscoco   efficientnetb0   36.26   35.79   35.59          MS COCO   1x3x512x512   -                  -\nfcos_efficientnetb1_mscoco   efficientnetb1   41.37   41.21   40.71          MS COCO   1x3x640x640   2.44               487.89\nfcos_efficientnetb2_mscoco   efficientnetb2   45.35   45.10   45.00          MS COCO   1x3x768x768   3.46               325.84\nfcos_efficientnetb3_mscoco   efficientnetb3   48.03   47.65   47.58          MS COCO   1x3x896x896   5.75               187.30\n\nDETR\n\nNETWORK                      BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE    BPU LATENCY (MS)   FPS\ndetr_resnet50_mscoco         resnet50         35.70   31.42   31.31          MS COCO   1x3x800x1333   28.63              35.27\ndetr_efficientnetb3_mscoco   efficientnetb3   37.21   35.95   35.99          MS COCO   1x3x800x1333   22.20              45.64\n\nDeform DETR\n\nNETWORK                       BACKBONE   FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE    BPU LATENCY (MS)   FPS\ndeform_detr_resnet50_mscoco   resnet50   44.34   44.65   44.80          MS COCO   1x3x800x1333   222.42             4.51\n\nFCOS3D\n\nNETWORK                          BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET    INPUT SHAPE   BPU LATENCY (MS)   FPS\nfcos3d_efficientnetb0_nuscenes   efficientnetb0   30.60   30.27   30.31          nuscenes   1x3x512x896   3.03               409.89\n\n\nSegmentation#\n\nUNet\n\nNETWORK                       BACKBONE      FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\nunet_mobilenetv1_cityscapes   MobileNetV1   68.02   67.56   67.53          Cityscapes   1x3x1024x2048   1.64               783.06\n\nDeeplab\n\nNETWORK                                   BACKBONE          FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\ndeeplabv3plus_efficientnetm0_cityscapes   EfficientNet-M0   76.30   76.22   76.12          Cityscapes   1x3x1024x2048   4.83               218.18\ndeeplabv3plus_efficientnetm1_cityscapes   EfficientNet-M1   77.94   77.64   77.65          Cityscapes   1x3x1024x2048   9.21               111.76\ndeeplabv3plus_efficientnetm2_cityscapes   EfficientNet-M2   78.82   78.65   78.63          Cityscapes   1x3x1024x2048   13.77              74.04\n\nFastScnn\n\nNETWORK                                  BACKBONE              FLOAT   QAT     QUANTIZATION   DATASET      INPUT SHAPE     BPU LATENCY (MS)   FPS\nfastscnn_efficientnetb0tiny_cityscapes   EfficientNet-B0lite   69.97   69.90   69.88          Cityscapes   1x3x1024x2048   2.27               493.85\n\n\nOpticalFlow#\n\nPwcNet\n\nNETWORK                          BACKBONE   FLOAT    QAT      QUANTIZATION   DATASET        INPUT SHAPE   BPU LATENCY (MS)   FPS\npwcnet_pwcnetneck_flyingchairs   PwcNet     1.4117   1.4112   1.4075         FlyingChairs   1x6x384x512   -                  -\n\n\nLidar#\n\nPointPillars\n\nNETWORK                  BACKBONE               FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\npointpillars_kitti_car   SequentialBottleNeck   77.31   76.86   76.76          KITTI3D   150000x4      185.42             28.12\n\nCenterPoint\n\nNETWORK                            BACKBONE               FLOAT                   QAT                     QUANTIZATION            DATASET        INPUT SHAPE             BPU LATENCY (MS)   FPS\ncenterpoint_pointpillar_nuscenes   SequentialBottleNeck   58.32(NDS) 48.04(MAP)   58.11(NDS) 47.85(MAP)   58.14(NDS) 47.81(MAP)   nuscenes det   1x5x20x40000, 40000x4   55.26              100.62\n\nLidarMultiTask\n\nNETWORK                                     BACKBONE      FLOAT                                  QAT                                    QUANTIZATION                           DATASET               INPUT SHAPE             BPU LATENCY (MS)   FPS\ncenterpoint_mixvargnet_multitask_nuscenes   MixVarGENet   58.09(NDS) 47.27(MAP) 91.28(MeanIOU)   57.72(NDS) 46.76(MAP) 91.18(MeanIOU)   57.53(NDS) 46.28(MAP) 91.21(MeanIOU)   nuscenes det && seg   1x5x20x40000, 40000x4   53.56              125.52\n\nNote\n\nThe indicator for PointPillars is the Box3d Moderate item.\n\n\nLane Detection#\n\nGaNet\n\nNETWORK                    BACKBONE      FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE   BPU LATENCY (MS)   FPS\nganet_mixvargenet_culane   MixVarGENet   79.49   78.72   78.72          CuLane    1x3x320x800   0.94               1445.91\n\n\nMultiple Object Track#\n\nMotr\n\nNETWORK                     BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET   INPUT SHAPE                                       BPU LATENCY (MS)   FPS\nmotr_efficientnetb3_mot17   efficientnetb3   58.02   57.62   57.76          Mot17     1x3x800x1422, 1x256x2x128, 1x1x1x256, 1x4x2x128   14.99              68.31\n\n\nBinocular depth estimation#\n\nStereoNet\n\nNETWORK                               BACKBONE      FLOAT    QAT      QUANTIZATION   DATASET     INPUT SHAPE   BPU LATENCY (MS)   FPS\nstereonet_stereonetneck_sceneflow     StereoNeck    1.1270   1.1677   1.1685         SceneFlow   1x6x540x960   -                  -\nstereonetplus_mixvargenet_sceneflow   MixVarGENet   1.1270   1.1329   1.1351         SceneFlow   2x3x544x960   5.12               208.22\n\n\nBev#\n\nBev\n\nNETWORK                                        BACKBONE         FLOAT                                  QAT                                    QUANTIZATION                           DATASET               INPUT SHAPE                                                  BPU LATENCY (MS)   FPS\nbev_ipm_efficientnetb0_multitask_nuscenes      efficientnetb0   30.54(NDS) 21.70(MAP) 51.45(MeanIOU)   30.80(NDS) 21.66(MAP) 51.47(MeanIOU)   30.26(NDS) 21.56(MAP) 50.99(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x128x128x2                                     9.52               112.59\nbev_lss_efficientnetb0_multitask_nuscenes      efficientnetb0   30.06(NDS) 20.62(MAP) 51.80(MeanIOU)   30.10(NDS) 20.51(MAP) 51.78(MeanIOU)   30.08(NDS) 20.46(MAP) 51.47(MeanIOU)   nuscenes det && seg   6x3x256x704, 10x128x128x2, 10x128x128x2                      6.90               160.88\nbev_gkt_mixvargenet_multitask_nuscenes         MixVarGENet      28.10(NDS) 19.91(MAP) 48.52(MeanIOU)   27.98(NDS) 19.99(MAP) 48.42(MeanIOU)   27.90(NDS) 20.00(MAP) 48.35(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2,     16.31              64.30\n                                                                                                                                                                                                           6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2, 6x64x64x2\nbev_ipm_4d_efficientnetb0_multitask_nuscenes   efficientnetb0   37.21(NDS) 22.00(MAP) 52.87(MeanIOU)   37.32(NDS) 22.21(MAP) 53.83(MeanIOU)   37.34(NDS) 22.15(MAP) 53.87(MeanIOU)   nuscenes det && seg   6x3x512x960, 6x128x128x2, 1x64x128x128, 1x128x128x2          9.83               108.87\ndetr3d_efficientnetb3_nuscenes                 efficientnetb3   33.04(NDS) 27.52(MAP)                  32.84(NDS) 27.14(MAP)                  32.81(NDS) 27.06(MAP)                  nuscenes det          6x3x512x1408                                                 37.63              26.92\npetr_efficientnetb3_nuscenes                   efficientnetb3   37.65(NDS) 30.38(MAP)                  37.26(NDS) 29.29(MAP)                  37.40(NDS) 29.33(MAP)                  nuscenes det          6x3x512x1408                                                 77.10              13.05\nbevformer_tiny_resnet50_detection_nuscenes     resnet50         37.12(NDS) 26.79(MAP)                  37.16(NDS) 26.50(MAP)                  37.15(NDS) 26.59(MAP)                  nuscenes det          6x3x480x800, 1x2500x256, 1x50x50x2, 6x20x32x2, 1x100x50x2,   28.29(J6M)         35.90(J6M)\n                                                                                                                                                                                                           6x640x4x2, 1x2500x1\nbev_cft_efficientnetb3_nuscenes                efficientnetb3   32.79(NDS) 24.79(MAP)                  32.50(NDS) 24.47(MAP)                  32.42(NDS) 24.46(MAP)                  nuscenes det          6x3x512x1408                                                 36.50              27.75\nbev_sparse_henet_tinym_nuscenes                henet_tinym      54.19(NDS) 20.62(MAP)                  52.23(NDS) 20.62(MAP)                  -                                      nuscenes det          6x3x256x704, 6x4x4, 1x384x11, 1x384x256                      12.92(J6M)         79.68(J6M)\n\n\nKeypoint Detection#\n\nHeatmapKeypointModel\n\nNETWORK                             BACKBONE         FLOAT   QAT     QUANTIZATION   DATASET     INPUT SHAPE   BPU LATENCY (MS)   FPS\nkeypoint_efficientnetb0_carfusion   efficientnetb0   94.33   94.30   94.31          carfusion   1x3x128x128   0.45               4550.72\n\n\nTrajectory Prediction#\n\nDenseTNT\n\nNETWORK                         BACKBONE    FLOAT    QAT      QUANTIZATION   DATASET       INPUT SHAPE                                                   BPU LATENCY (MS)   FPS\ndensetnt_vectornet_argoverse1   vectornet   1.2974   1.2989   1.3038         argoverse 1   30x9x19x32, 30x11x9x64, 30x1x1x96, 30x2x1x2048, 30x1x1x2048   11.30              144.725382\n\nQCNet\n\nNETWORK               BACKBONE   FLOAT   QAT     QUANTIZATION   DATASET       INPUT SHAPE   BPU LATENCY (MS)   FPS\nqcnet_oe_argoverse2   -          83.03   82.19   -              argoverse 2   输入见下方list     11.42(J6M)         189.39\n\nNote\n\nThe indicator for QCNet is the HitRate item.\n\nThe input shape of the qcnet_oe_argoverse2 model is:\n\n1x30x10, 1x10x30x30, 1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x1, 1x1x30x80,\n1x1x30x80, 1x1x30x80, 1x1x30x6, 1x1x30x6, 1x1x30x6, 1x1x30x6, 1x1x30x30,\n1x1x30x30, 1x1x30x30,1x30x5x128, 1x30x2x128, 1x80, 1x80, 1x1x80x80, 1x1x80x80,\n1x1x80x80,1x1x80x50, 1x1x80x50, 1x1x80x50, 1x80x50, 1x80x50, 1x80x50,\n1x80x50,1x30x30, 1x30x1, 1x80x80\n\n\nOccupancy Prediction#\n\nFlashOcc\n\nNETWORK                             BACKBONE               FLOAT    QAT      QUANTIZATION   DATASET          INPUT SHAPE                               BPU LATENCY (MS)   FPS\nflashocc_henet_lss_occ3d_nuscenes   henet_tinym_imagenet   0.3674   0.3661   0.3656         occ3d_nuscenes   6x3x512x960, 10x128x128x2, 10x128x128x2   8.39(J6M)          119.23(J6M)\n\n\nOnline Map Construction#\n\nMapTROE\n\nNETWORK                                  BACKBONE               FLOAT    QAT      QUANTIZATION   DATASET    INPUT SHAPE                                         BPU LATENCY (MS)   FPS\nmaptroe_henet_tinym_bevformer_nuscenes   henet_tinym_imagenet   0.6632   0.6577   0.6387         nuscenes   6x3x480x800, 1x1x50x100, 6x20x100x2, 1x100x100x2,   11.03(J6M)         93.77(J6M)\n                                                                                                            6x2000x4x2, 1x5000x1\n\n\nLidar Fusion#\n\nLidarFusion\n\nNETWORK                                                      BACKBONE               FLOAT                               QAT                                 QUANTIZATION   DATASET                 INPUT SHAPE                                                    BPU LATENCY (MS)   FPS\nbevfusion_pointpillar_henet_multisensor_multitask_nuscenes   henet_tinym_imagenet   64.28(NDS) 58.09(MAP) 51.77(MIOU)   62.91(NDS) 57.48(MAP) 52.51(MIOU)   -              nuscenes det && occ3d   1x5x20x40000, 40000x4, 6x3x512x960, 1x256x128x2, 6x5120x2x2,   24.87(J6M)         41.21(J6M)\n                                                                                                                                                                                                   1x16384x1","routePath":"/en/guide/advanced_content/hat/model_zoo","lang":"en","toc":[{"text":"Classification","id":"classification","depth":2,"charIndex":3},{"text":"Detection","id":"detection","depth":2,"charIndex":1691},{"text":"Segmentation","id":"segmentation","depth":2,"charIndex":3249},{"text":"OpticalFlow","id":"opticalflow","depth":2,"charIndex":4425},{"text":"Lidar","id":"lidar","depth":2,"charIndex":4705},{"text":"Lane Detection","id":"lane-detection","depth":2,"charIndex":5958},{"text":"Multiple Object Track","id":"multiple-object-track","depth":2,"charIndex":6226},{"text":"Binocular depth estimation","id":"binocular-depth-estimation","depth":2,"charIndex":6578},{"text":"Bev","id":"bev","depth":2,"charIndex":7023},{"text":"Keypoint Detection","id":"keypoint-detection","depth":2,"charIndex":10417},{"text":"Trajectory Prediction","id":"trajectory-prediction","depth":2,"charIndex":10732},{"text":"Occupancy Prediction","id":"occupancy-prediction","depth":2,"charIndex":11810},{"text":"Online Map Construction","id":"online-map-construction","depth":2,"charIndex":12201},{"text":"Lidar Fusion","id":"lidar-fusion","depth":2,"charIndex":12740}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":29,"title":"How to Turn on AMP","content":"#\n\nWhen AMP (Automatic Mixed Precision) is enabled, Pytorch can automatically\ncompute some operators (such as convolution and full concatenation) by using\nfloat16 during model execution to increase computing speed and reduce memory\nusage. See the Official Pytorch Documentation for more details.\n\nAMP is prepared and ready to use in HAT. You just need to set the enable_amp to\nTrue when defining the batch_processor field in the config file.\n\nNote\n\nIn order to get accurate metrics during model validation, it is not necessary to\nturn on the AMP. To turn it off, set the enable_amp parameter to False when\ndefining the val_batch_processor field.\n\n","routePath":"/en/guide/advanced_content/hat/tutorials/amp","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":30,"title":"Calibration","content":"#\n\nIn QAT, an important stage is to calculate the quantitative parameter scale. A\ngood scale can significantly improve the accuracy of the model training results\nand speed up the convergence of the model. The calibration process is to run a\nfew batch data on floating-point models (only run the forward process, not\nbackward), count the distribution histogram and get min_value and max_value.\nThen, use the min_value and max_value to calculate the scale. When the QAT\naccuracy is low, calibrating the quantitative parameters like this before QAT\ncan provide better quantitative initilization parameters.\n\n\nHow to Define Calibration Model#\n\n * No need to change existing models by default\n   \n   Similar to the setting of QAT QConfig when defining a quantitative model, it\n   is also necessary to set Calibration QConfig when defining a calibration\n   model. However, the latter is simpler as the HAT has already implemented the\n   default settings for Calibration QConfig for direct use without any\n   modification to the model.\n\n * Define submodule Calibration QConfig\n   \n   By default, Calibration QConfig is set for all modules (inherited from\n   nn.Module) of the model. Therefore, the calibration will count the feature\n   distribution of all modules. If you have special needs, you can customize the\n   implementation of the set_calibration_qconfig method in the model:\n   \n   \n\n\nRun Calibration on Floating-point Models#\n\nHAT comes with built-in calibration functions, whose commands are similar to\nthose in normal training. Simply run the following command:\n\n\n\nSee the calibration_trainer settings in the config file:\n\n\n\n1. Setting Dataset:\n\nThe datasets for the calibration cannot be testing datasets (can be training\ndatasets or others). There is no definite conclusions for now about the\ntransforms for data enhancement, you can try either using the transforms\nconsistent with those in normal training or validation, or using the customized\ntransforms.\n\n2. Number of images to be iterated in calibration (for reference):\n\n * classification: 500~1500 images\n * segmentation && detection: 100~300 images\n\nNote\n\nThe number of images is not fixed either, the suggestions above are only\nexperiences summarized from existing experiments, which can be adjusted\naccording to the actual situation.\n\n\nQAT with Calibration Model#\n\n\n\nSetting of averaging_constant:\n\nIn the QAT, the update rule for the scale parameter is scale = (1 -\naveraging_constant) * scale + averaging_constant * current_scale .\n\nIt is found in some existing experiments that it may lead to higher accuracy by\nfixing the scale of the activation after calibration, i.e., by setting\naveraging_constant=0 of the activation and averaging_constant=1 of the weight.\n\nNote\n\nThe setting isn't suitable for all tasks, it can be adjusted according to the\nactual situation. E.g., Fixing scale in a LiDAR task may result in low accuracy.\n\nNext, you only need to start the training by executing the normal QAT command:\n\n","routePath":"/en/guide/advanced_content/hat/tutorials/calibration","lang":"en","toc":[{"text":"How to Define Calibration Model","id":"how-to-define-calibration-model","depth":2,"charIndex":605},{"text":"Run Calibration on Floating-point Models","id":"run-calibration-on-floating-point-models","depth":2,"charIndex":1386},{"text":"QAT with Calibration Model","id":"qat-with-calibration-model","depth":2,"charIndex":2302}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":31,"title":"FLOPs Tool","content":"#\n\n\nDefinition of FLOPs#\n\nFLOPs is a common tool used to evaluate the size of neural networks. Common\nFLOPs tools generally calculate two types of operations, one is convolutional\nlayer related operations, and the other is fully-connected layer related\noperations. Both are related to multiplication and addition.\n\nTake the common torch.nn.Conv2d as an example, the shape of the input data is bs\n* c_in * w_in * h_in, the shape of the output data is bs * c_out * w_out *\nh_out, and the size of the convolution kernel is f. Then the FLOPs of this\nConv2d are 2 * bs * f * f * c_in * c_out * w_out * h_out. Here 2 denotes half of\nthe addition operations and half of the multiplication operations.\n\nIn the case of torch.nn.Linear, the number of neurons in the input data is c_in,\nand the number of neurons in the output data is c_out. In fact, this\nfully-connected layer can be used as a special convolutional layer, where the\nsizes of both input and output are 1x1 and the size of the convolutional kernel\nis also 1x1. Then the FLOPs of the fully connected layer is bs * (c_in * c_out +\nc_out). Note that the ops of multiplication and addition are not exactly the\nsame here.\n\nAttention\n\nThe quantitative and QAT models are identical to the corresponding\nfloating-point models in terms of FLOPs.\n\n\nThe Use of FLOPs Tool#\n\nThe current FLOPs tool supports the calculation of three types of operations,\ni.e., torch.nn.Conv2d, torch.nn.Linear and torch.nn.ConvTranspose2d.\n\n\n\nIn config , the main keys affecting the op counting are test_model (or model)\nand test_inputs. The model determines the model to be checked by the tool, while\ntest_inputs determines the input size. The input shape can also be determined by\nthe input parameters of -input-shape B,C,H,W .\n\nNote that the calops tool can also support matmul by selecting fx for method.\nHowever, this method requires that the model itself can support fx.\n\n\nFLOPs of Common Classification Models (Input Size: 1x3x224x224)#\n\nNETWORK                   OPS(G)\nmobilenetv1 (alpha=1.0)   0.57\nmobilenetv2 (alpha=1.0)   0.31\nresnet18                  1.81\nresnet50                  3.86\nvargnetv2                 0.36\nefficientnetb0            0.39\n\n\nDescriptions#\n\nIn general, there are two cases when using calops, which may cause incorrect\nresults or calculation errors.\n\n 1. When hook is selected for method, but the model contains matmul or other\n    operators that are explicitly not covered.\n\n 2. When fx is selected for method, but the model itself does not support fx, or\n    contains other operators that are explicitly not covered.","routePath":"/en/guide/advanced_content/hat/tutorials/calops","lang":"en","toc":[{"text":"Definition of FLOPs","id":"definition-of-flops","depth":2,"charIndex":3},{"text":"The Use of FLOPs Tool","id":"the-use-of-flops-tool","depth":2,"charIndex":1293},{"text":"FLOPs of Common Classification Models (Input Size: `1x3x224x224`)","id":"flops-of-common-classification-models-input-size-1x3x224x224","depth":2,"charIndex":-1},{"text":"Descriptions","id":"descriptions","depth":2,"charIndex":2190}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":32,"title":"Model Compilation","content":"#\n\nHAT is capable of compiling fixed-point models to make the compiled Pytorch\nmodels suitable for on-board running. You can choose to compile the model as\ndescribed in the next section.\n\n\nConfig File as Input#\n\nThis method uses the script tools/compile_perf_hbir.py with the following\ncommand.\n\n\n\nThis command will get the compilation-related configurations from the\nhbir_compiler field in the config file, as illustrated by the following example:\n\n\n\nWhen the compilation is complete, the following files are generated.\n\n * tmp_compile/model.hbm: Compiled model file.","routePath":"/en/guide/advanced_content/hat/tutorials/compile","lang":"en","toc":[{"text":"Config File as Input","id":"config-file-as-input","depth":2,"charIndex":188}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":33,"title":"Config File","content":"#\n\nTraining a model using the HAT algorithm toolkit is usually done with a single\ncommand:\n\n\n\nIn which, /PATH/TO/CONFIG is the config file for model training, which defines\nthe model structure, dataset loading, and the entire training process.\n\nThis section introduces some fixed global keywords in the config file and their\nconfiguration descriptions, giving you an overview of the config file.\n\n\nGlobal Keywords#\n\n * training_stage: Stages of model training, including float, qat, and\n   int_infer.\n\n * device_ids: List of GPUs used for model training.\n\n * cudnn_benchmark: Whether to turn on CUDNN benchmark, usually defaults to\n   True.\n\n * seed: Whether to set the random number seed. usually defaults to None.\n\n * log_rank_zero_only: Simplifies the log printing in multi-card training by\n   outputting logs only on card 0. Usually defaults to True.\n\n * model: The structure of the model participating in the training process. type\n   is the type of the model, e.g., Classifier, Segmentor, RetinaNet, etc.,\n   corresponding to a type of models in classification, segmentation, and\n   detection, respectively. It will be built into a specific class in the\n   process, and other parameters are all used to initialize this class.\n\n * deploy_model: The model structure that participates in the deploy process,\n   mainly used for model compilation. Compared to model, in most cases you only\n   need to set the loss function and the post-processing part to None.\n\n * deploy_inputs: Simulated inputs for the deploy procedure. Values do not\n   matter here, just make sure the format meets the input requirements.\n\n * data_loader: Dataset loading process in the training phase. Its type is a\n   specific class torch.utils.data.DataLoader, and other parameters are all used\n   to initialize this class. You can also read the interface documents on the\n   Pytorch website to learn these parameters. Here dataset means to read a\n   specific dataset, e.g., ImageNet, MSCOCO, VOC, etc., and transforms means\n   data enhancement operations added when reading the data.\n\n * val_data_loader: The dataset loading process in the phase of validating model\n   performance. Different from data_loader, its data_path is different and the\n   processes of transforms and sample are removed.\n\n * batch_processor: Operations performed by the model at each iteration stage\n   during the training, including forward propagation, backward propagation,\n   parameter update, etc. The batch_transforms parameter, if included, indicates\n   that some data enhancement operations are performed on the GPU, which can\n   greatly speed up the training.\n\n * val_batch_processor: The operations performed by the model at each iteration\n   stage during the validation process, containing only forward propagation.\n\n * metric_updater: Metric updating method of the model during model training,\n   which is used to verify whether the performance of the training model is\n   improving. It is usually used together with train_metrics under\n   float_trainer. train_metrics is the specific form of the metric, while\n   metric_updater just provides an updating method.\n\n * val_metric_updater: Metric updating method of the trained model during the\n   performance validation process, which is used to verify the final performance\n   of the trained model. Similar to metric_updater, it is usually used together\n   with val_metrics under float_trainer.\n\n * float_trainer: Configuration of the floating-point model training process.\n   Its type is distributed_data_parallel_trainer, which means distributed\n   training is supported. Other parameters define the model, dataset loading,\n   optimizer, training epoch length, etc., where callbacks represents the\n   operations performed in the training, such as model saving, learning rate\n   update, precision validation, etc. It is a variable directly called by the\n   tools/train.py file.\n\n * qat_trainer: Configuration for the QAT model training process. This parameter\n   basically means the same as float_trainer. It is a variable directly called\n   by the tools/train.py file.\n\n * int_infer_trainer: With no training processes included, it is only used to\n   verify the accuracy of the fixed-point model. It is a variable directly\n   called by the tools/train.py file.\n\n * compile_cfg: Compile-related configuration. out_dir is the output path of the\n   compiled HBM file (deployment model).\n\nThe reason why these variables are called global keywords is that they are\ndefined in almost every config file and basically carry the same functions. By\nreading this document, you can get a general idea of what a config file can do.\n\n\nConfiguration#\n\nThis section describes the configuration of the global keyword for the data type\ndict.\n\nGlobal keywords of the dict type can be further divided into the following two\ntypes:\n\n 1. Those with type, such as model, data_loader, float_trainer, etc.\n\n 2. Those without type, such as compile_cfg, etc.\n\nThe difference is that a global keyword that contains type is essentially a\nclass whose type value can be either a string variable or a specific class, and\neven if it is a string, it will eventually be built into a corresponding class\nat runtime. The values of all the keys in the dict except type are used to\ninitialize this class. Similar to global keywords, these keys can be either a\nnumeric value or a dict containing a type variable, such as the dataset property\nin data_loader, and the transforms property under this dataset.\n\nFor a global keyword without a type variable, it is a regular dict variable, and\nthe code will get the corresponding values from its keys during runtime.\n\nHint\n\nAll provided configurations are guaranteed to work properly and reproduce the\naccuracy. If you need to modify the configuration due to the environment or\ntraining time, then you may need to change the training strategy as well.\nDirectly modifying individual configurations in the config file sometimes may\nnot lead to desired results.","routePath":"/en/guide/advanced_content/hat/tutorials/config","lang":"en","toc":[{"text":"Global Keywords","id":"global-keywords","depth":2,"charIndex":397},{"text":"Configuration","id":"configuration","depth":2,"charIndex":4633}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":34,"title":"Writing Specifications of set_qconfig and Customization of qconfig","content":"#\n\n\nWriting Specifications of set_qconfig#\n\nWhen defining the model to be quantified, the model set_qconfig method needs to\nbe implemented to configure the quantization method.\n\nThe current QConfig interface is provided by hat.utils.qconfig_manager. Call\nhat.utils.qconfig_manager in set_qconfig to implement the setting of the module\nQconfig, e.g.:\n\n\n\n\nCustomize QAT QConfig Parameters#\n\nUsing custom QConfig in QAT training is supported in HAT, simply configure the\nqconfig_params parameter in the qat_solver of the config file:\n\n\n\nqconfig_params has five main parameter configuration items: dtype,\nactivation_fake_quant, weight_fake_quant, activation_qkwargs and weight_qkwargs.\n\n * dtype: dtype is the quantization bit type, supporting \"qint8\" (default).\n\n * activation_fake_quant: Quantifier for activation, supporting \"fake_quant\"\n   (default), \"lsq\", and \"pact\".\n\n * weight_fake_quant: Quantifier for weight. Supporting \"fake_quant\" (default),\n   \"lsq\", and \"pact\".\n\n * activation_qkwargs: Parameters of activation quantifier.\n   \n   * When activation_fake_quant is \"fake_quant\", activation_qkwargs can be set\n     as below:\n   \n   \n   \n   * When activation_fake_quant is \"lsq\", activation_qkwargs can be set as\n     below:\n   \n   \n   \n   * When activation_fake_quant is \"pact\", activation_qkwargs can be set as\n     below:\n   \n   \n\n * weight_qkwargs: Specifies the parameters for the weight quantifier. Except\n   that the default observer for weight_qkwargs is\n   MovingAveragePerChannelMinMaxObserver, other parameters and usage are the\n   same as activation_qkwargs.\n\nNote: Generally you can just use the default configurations without changing\nactivation_qkwargs and weight_qkwargs. However, when performing the QAT training\nafter the calibration, you may need to modify averaging_constant.","routePath":"/en/guide/advanced_content/hat/tutorials/custom_qconfig","lang":"en","toc":[{"text":"Writing Specifications of set_qconfig","id":"writing-specifications-of-set_qconfig","depth":2,"charIndex":3},{"text":"Customize QAT QConfig Parameters","id":"customize-qat-qconfig-parameters","depth":2,"charIndex":353}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":35,"title":"Config Construction of FCOS-EfficientNetB0","content":"#\n\nTo help you better understand the config file, this section takes the\nFCOS-EfficientNetB0 model as an example, and adds a brief comment to each of its\nmodules for your reference, as follows:\n\n","routePath":"/en/guide/advanced_content/hat/tutorials/detailed_config","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":36,"title":"Start-up Method","content":"#\n\ntools+configs is the most basic training method of HAT at the moment. But in\nmany cases, we need to deal with multi-card or distributed environments. These\nenvironments need to rely on some third-party libraries in order to be able to\norganize the basic training approach in multiple environments in an efficient\nway.\n\nIn a multi-card or distributed environment, the common boot methods are torchrun\nand so on. Here we briefly talk about the differences between these boot methods\nusing commands.\n\nHere is an example of float training with resnet18, and all the current HAT\nsupported startup methods are listed for reference.\n\n\nThe Simplest Mode#\n\nThis mode supports a range of single-machine single-card and single-machine\nmulti-card two modes and does not support multi-machine multi-card, whose\nconfiguration only involves the modification of the index of device_ids in the\nconfigs.\n\nNote that the mode of single-machine multi-card is actually implemented with the\nhelp of torch.multiprocess, which effectively manages all the processes inside\nthe single machine, which is why it does not support multiple machines.\n\n\n\n\nTorchrun#\n\nNote that torchrun requires a version of torch greater than or equal to 1.10.0,\nas those lower than that use torch.distributed.launch, which is not supported by\nHAT, and is not recommended to use.\n\nTorchrun is the start-up tool provided by the torch framework to allow you to\neasily and quickly handle various environment variables inside the distributed\nenvironment.\n\nFor details on Torchrun, see Pytorch Community Documents.\n\n\n\nFinally, note that both Python multiprocess and Torchrun are process managers,\nand the method of communication between processes relies on the initialization\nmethod of the process group inside Torch, so the differences in management\nprograms do not affect the training efficiency.\n\nThe most important difference among different process management programs is the\nprocess management methods they use: for example, when a process quits\nabnormally, whether it can get the error messages of all nodes from the main\nprocess. Or, when a single process throws an exception, whether it can make sure\nthat all the processes exit completely. As for other aspects, such as internal\ndevelopment modes, the difference are not that big.","routePath":"/en/guide/advanced_content/hat/tutorials/launcher","lang":"en","toc":[{"text":"The Simplest Mode","id":"the-simplest-mode","depth":2,"charIndex":630},{"text":"Torchrun","id":"torchrun","depth":2,"charIndex":1125}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":37,"title":"Overriding Config Parameters Using Commands","content":"#\n\nHAT supports modifying the config parameters from the command line by adding\n(key, value) after --opts.\n\n\n\nWhere the key is the name of the parameter to be modified and the value is the\nvalue to be passed in, as in the config.py file:\n\n\n\nNote\n\nExamples in this document focus on the formats of different parameters when\nusing --opts. In your projects, the parameters and field configurations may\ndiffer from the example.\n\n * key supports multi-level parameter modification:\n   \n   \n   \n   The above command changes the value of the model.backbone field in config to\n   \"resnet50\".\n\n * value can be number, str, list, or tuple, but dict is not supported.\n   \n   For example, the following command can change the value of the\n   model.num_classes field in config to 10.\n   \n   \n   \n   However, because of the parsing mechanism, you need to quote the value to be\n   passed if its type is tuple or list, for example:\n   \n   \n   \n   If the value type is str, you need to add extra quotes, for example:\n   \n   ","routePath":"/en/guide/advanced_content/hat/tutorials/opts","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":38,"title":"Quantized Training","content":"#\n\nThis document only describes the operations needed to perform quantization\ntraining in HAT. For the basic principles of quantization and its implementation\nin the training framework, refer to the documentation of horizon_plugin_pytorch\n.\n\nIn quantized training, the conversion process from a floating-point model to a\nfixed-point model is as follows:\n\nMost of these steps are already integrated in the HAT training pipeline, and you\nonly needs to pay attention to implementing the fuse_model method to complete\nthe model fusion when adding a custom model and the set_qconfig method to\nconfigure the quantization method. The following points need to be noted when\nwriting the models.\n\n * HAT will only call the fuse_model method of the outermost module, so the\n   implementation of fuse_model is responsible for the fuse of all submodules.\n\n * Preference should be given to the base modules provided in\n   hat.models.base_modules, which has already implemented the fuse_model method\n   to reduce the effort and development difficulties.\n\n * Model registration, all the modules in HAT use the registration mechanism,\n   only when the defined model is registered in the corresponding registration\n   item, can the model be used in the config file as dict(type={$class_name},\n   ...) .\n\n * The set_qconfig method needs to be implemented in the outermost module. If\n   there is a special layer in a submodule that needs a separate QConfig\n   setting, the set_qconfig method needs to be implemented in that submodule as\n   well, details of which can be found in the Writing Specifications of\n   set_qconfig and Customization of qconfig sections.\n\nIn addition, to make the model transferable to a quantized model, some\nconditions need to be met, as described in the documentation for\nhorizon_plugin_pytorch.\n\n\nIntroduction to Quantized Training Process#\n\n\nAdding Custom Models#\n\n\n\n\nAdd The Config File#\n\n\n\n\nTraining#\n\nYou only need to simply specify the training phases in order when using the\ntools/train.py script, and the corresponding solver will be called automatically\naccording to the training phase to execute the training process.\n\n\n\n * float：normal floating-point training.\n * qat：QAT training (Quantized Awareness Training), this stage first initializes\n   a floating-point model, loads the trained floating-point model weights, and\n   then converts this floating-point model into a QAT model for training.\n * int_infer：Fixed-point transformation prediction, this stage first initializes\n   a floating-point model, converts the floating-point model into a QAT model\n   and loads the trained QAT model weights, and then converts the QAT model into\n   a fixed-point model. The converted fixed-point model cannot be trained and\n   can only perform validation to obtain the final fixed-point model accuracy.\n\n\nResume Training#\n\nUnexpectedly interrupted training can be resumed by configuring the\nresume_optimizer and resume_epoch_or_step fields in {stage}_trainer of config,\nor by resuming only the optimizer for fine-tuning. For example:\n\n\n\nTraining recovery has three scenarios:\n\n 1. Full Recovery: This scenario is to resume the training that was unexpectedly\n    interrupted, and will restore all the states of the previous checkpoint,\n    including optimizer, LR, epoch, step, and so on. In this scenario, you only\n    need to configure the resume_optimizer field.\n\n 2. Resume Optimizer for Fine-tuning: This scenario will only restore the state\n    of optimizer and LR, with epoch and step reset to 0 for the fine-tuning of\n    certain tasks. In this scenario, you need to configure both resume_optimizer\n    and resume_epoch_or_step=False.\n\n 3. Load Model Parameters Only: This scenario loads only model parameters and\n    does not restore any other state (optimizeizer, epoch, step, or LR). In this\n    scenario, you need to configure LoadCheckpoint in model_convert_pipeline,\n    resume_optimizer=False, and resume_epoch_or_step=False.\n\n\nQAT Mode#\n\n\nEffects#\n\nQat_mode is used to set whether to perform the quantization training with BN in\nthe QAT phase. With the help of the FuseBN interface provided by HAT, it can\nalso control whether to perform the training with BN throughout the whole\nprocess or with BN being gradually absorbed midway.\n\n\nOptional Definitions#\n\nThe following three settings are available for qat_mode：\n\n\n\n\nPrinciples#\n\nFuse BN#\n\nQAT Phase without BN (default quantization training method of HAT)\n\nBy setting qat_mode to fuse_bn, in the op fusion process of the floating-point\nmodel, the weight and bias of BN are absorbed into that of Conv, and the\noriginal combination of Conv + BN will be left with only Conv, and this\nabsorption process is theoretically error-free.\n\nWith BN#\n\nQAT Phase with BN\n\nBy setting qat_mode to with_bn, when the floating-point model is converted to\nQAT model, BN is not absorbed into Conv, but exists in the quantized model as a\nfused quantized op in the QAT phase in the form of Conv + BN + Output Quantized\nNode. Finally, at the end of quantization training, in the step where the model\nis converted to quantized (also called int infer), the weight and bias of BN\nwill be automatically absorbed into the quantization parameters of Conv, where\nthe quantized op obtained after the absorption remains consistent with the\noriginal QAT op calculation result.\n\nIn this mode, you can also choose to absorb the BN into Conv in the middle of\nQAT. The reason why the forward results of the QAT model before and after\nmanually absorbing the BN are inconsistent is that after the BN weight is\nabsorbed into the Conv weight, the quantized parameter conv_weight_scale\ncalculated in the previous quantization training is no longer applicable to the\ncurrent conv_weight and will lead to large errors in the quantization of\nconv_weight, which requires more quantization training and more adjustments on\nquantization parameters.\n\nWith BN Reverse Fold#\n\nQAT Phase with BN\n\nThe difference between this mode and with_bn is that, in this mode, the BN\nweight is considered when calculating conv_weight_scale in the quantization\ntraining phase before the BN is absorbed (calculations are not detailed here),\nso that after absorbing the BN weight, the conv_weight_scale is still applicable\nto the new conv_weight.\n\nThis mode is intended to provide a lossless way of absorbing BNs step by step:\nabsorbing BNs in the middle of the quantization training, the forward result of\nthe model is theoretically identical before and after the absorption, and you\ncan gradually absorb all the BNs in the model before the end of quantization\ntraining and ensure that the loss will not fluctuate too much after each\nabsorption.\n\nIn this mode, if there are BNs not absorbed at the end of the quantization\ntraining, they will be automatically absorbed when the model is converted from\nQAT to quantized. In theory, such absorption is lossless.\n\n\nUsage#\n\nSet qat_mode#\n\nOnly needs to set qat_mode in model_convert_pipeline.\n\nFor example:\n\n\n\nView Current qat_mode#\n\n\n\nSet Progressive Absorption BN#\n\nIn both with_bn and with_bn_reverse_fold modes, you can set FuseBN as a callback\nfunction to absorb the BN in the specified module at the specified epoch or\nstep.\n\nFuseBN definition:\n\n\n\nUse the FuseBN example in the config file:\n\n\n\n\nQat_mode Summary#\n\nQAT MODE               BN ABSORBED TIME                                            BN ABSORBING METHOD                                         FORWARD RESULT CHANGES AFTER ABSORPTION (THEORETICALLY )?\nfuse_bn                Must be in the floating-point model op fusion process       Absorbed after executing fuse_module                        No Changes\nwith_bn                Can be in the middle of quantized training process          By setting a callback function to absorb in the specified   Yes\n                                                                                   epoch or batch\nwith_bn                Can be in the conversion process of the model from QAT to   Auto completes with the model conversion                    Yes\n                       quantized\nwith_bn_reverse_fold   Can be in the middle of quantized training process          By setting a callback function to absorb in the specified   Yes\n                                                                                   epoch or batch\nwith_bn_reverse_fold   Can be in the conversion process of the model from QAT to   Auto completes with the model conversion                    Yes\n                       quantized\n\nIn general, a training process starts from the floating-point training, and when\nthe desired accuracy is met, move on to the quantization training, where only\nfuse_bn is used. Only when the floating-point training is skipped, i.e., it\nstarts with the quantization training, the quantized training mode with BN is\nneeded to ensure the model converges.\n\nNote\n\nThe reason why we say \"theoretically lossless before and after absorption\" or\n\"no change\" in this document is that because there is a low probability that the\nresults of the two floating-point calculations before and after the absorption\nwill not match at the later decimal places in the actual calculation. The small\nvariation combined with the quantization operation may result in an absolute\nerror in the output scale of some values of Conv after absorbing BN compared to\nthe output of Conv + BN before absorbing.","routePath":"/en/guide/advanced_content/hat/tutorials/quantization","lang":"en","toc":[{"text":"Introduction to Quantized Training Process","id":"introduction-to-quantized-training-process","depth":2,"charIndex":1805},{"text":"Adding Custom Models","id":"adding-custom-models","depth":3,"charIndex":1851},{"text":"Add The Config File","id":"add-the-config-file","depth":3,"charIndex":1877},{"text":"Training","id":"training","depth":2,"charIndex":1902},{"text":"Resume Training","id":"resume-training","depth":2,"charIndex":2812},{"text":"QAT Mode","id":"qat-mode","depth":2,"charIndex":3949},{"text":"Effects","id":"effects","depth":3,"charIndex":3961},{"text":"Optional Definitions","id":"optional-definitions","depth":3,"charIndex":4256},{"text":"Principles","id":"principles","depth":3,"charIndex":4340},{"text":"Fuse BN","id":"fuse-bn","depth":4,"charIndex":4353},{"text":"With BN","id":"with-bn","depth":4,"charIndex":4704},{"text":"With BN Reverse Fold","id":"with-bn-reverse-fold","depth":4,"charIndex":5876},{"text":"Usage","id":"usage","depth":3,"charIndex":6868},{"text":"Set qat_mode","id":"set-qat_mode","depth":4,"charIndex":6876},{"text":"View Current qat_mode","id":"view-current-qat_mode","depth":4,"charIndex":6962},{"text":"Set Progressive Absorption BN","id":"set-progressive-absorption-bn","depth":4,"charIndex":6988},{"text":"Qat_mode Summary","id":"qat_mode-summary","depth":3,"charIndex":7253}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":39,"title":"Registration Mechanism","content":"#\n\nThe registration mechanism is an important part of HAT, which plays an important\nrole in building config.\n\nThis section illustrates how to add a new module using the registration\nmechanism and how to use it in config by using a custom module as an example.\n\n\nModule Customization#\n\nHere we use backbone as an example to demonstrate how to develop a new module\n(e.g., mobilenet).\n\n\nDefining a New Backbone (e.g., MobileNet):#\n\nCreate a new file: hat/models/backbones/mobilenet.py.\n\n\n\n\nImporting Newly Defined Module#\n\nYou can import the newly defined module by adding the following line in\nhat/models/backbones/__init__.py:\n\n\n\n\nUsing the New Backbone in Config#\n\n\n\nSimilarly, You can register and use other registrable module by using this\nmethod.","routePath":"/en/guide/advanced_content/hat/tutorials/registry","lang":"en","toc":[{"text":"Module Customization","id":"module-customization","depth":2,"charIndex":261},{"text":"Defining a New Backbone (e.g., MobileNet):","id":"defining-a-new-backbone-eg-mobilenet","depth":3,"charIndex":383},{"text":"Importing Newly Defined Module","id":"importing-newly-defined-module","depth":3,"charIndex":486},{"text":"Using the New `Backbone` in `Config`","id":"using-the-new-backbone-in-config","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":40,"title":"HBDK Tool API Reference","content":"#\n\n\nhbdk api introduction#\n\nGeneral restrictions:\n\nThe maximum number of model inputs/outputs should not exceed 256.\n\nThe dimensions of all tensors in the model should not exceed 10.\n\nThe name of the input and output of the model must be unique.\n\nThe data types we support are ui8/si8/si16/ui32/si32/si64/float/bool. For\nspecific data types supported by a certain operator, please refer to the\noperator constraint document.\n\n\nModule: hbdk4.compiler.onnx#\n\nexport(proto onnx.ModelProto, *, name Optional[str] = None) -> Module#\n\n\n\nstatistics(proto onnx.ModelProto)#\n\n\n\n\nModule: hbdk4.compiler.torch#\n\nexport( jit torch.jit.ScriptModule, example_input Any, *, name Optional[str] =\nNone, input_names List[str] = None, output_names List[str] = None,\nlower_non_tensor bool = True) -> Module#\n\n\n\nstatistics(jit torch.jit.ScriptModule, example_input Any)#\n\n\n\n\nModule: hbdk4.compiler.apis#\n\nload(path str) -> Module#\n\n\n\nsave(m Module, path str) -> None#\n\n\n\nconvert( m Module, march Union[MarchBase, str], advice=False, advice_path=\"\",\n**kwargs) -> Module#\n\n\n\nstatistics(m Module) -> list#\n\n\n\nlink(hbo_list List[Hbo], output_path str, desc Optional[str] = None)#\n\n\n\ncompile( m Module, path str, march Union[MarchBase, str], opt int = 2, jobs int\n= 4, max_time_per_fc float = 0.0, debug bool = False, hbdk3_compatible_mode bool\n= False, progress_bar bool = False, advice float = 0.0, balance int = 100,\ninput_no_padding bool = False, output_no_padding bool = False) -> Union[Hbm,\nHbo]#\n\n\n\nvisualize( m Module, onnx_file Optional[str] = None, use_netron Optional[bool] =\nFalse)#\n\n\n\n\nModule: hbdk4.compiler.hbm_tools#\n\nhbm_extract_desc(model str) -> dict#\n\n\n\nhbm_update_desc(model str, desc_dict dict)#\n\n\n\nhbm_perf(model str, output_dir str = None)#\n\n\n\n\nClass: hbdk4.compiler.overlay.Argument#\n\nis_removable(self) -> Tuple#\n\n\n\nget_attached_op(self) -> List[Operation]#\n\n\n\nremove_attached_op(self)#\n\n\n\nerase(self)#\n\n\n\ninsert_transpose(self, permutes List[int])#\n\n\n\ninsert_image_convert(self, mode str = \"nv12\")#\n\n\n\ninsert_image_preprocess( self, mode str, divisor int, mean List[float], std\nList[float], is_signed bool = True)#\n\n\n\ninsert_roi_resize( self, mode str, interp_mode=\"bilinear\", pad_mode=\"constant\",\npad_value Optional[tuple] = (0, -128))#\n\n\n\ninsert_split(self, dim int)#\n\n\n\n\nClass: hbdk4.compiler.overlay.Module#\n\nfunctions(self) -> List[Function]#\n\n\n\ngraphs(self) -> List[Function]#\n\n\n\n\nClass: hbdk4.compiler.overlay.Function#\n\nremove_io_op(self, op_types=None, op_names=None)#\n\n\n\n\nhbdk api example usage#\n\n\nexport model and view model information#\n\nonnx#\n\n\n\n\nserialize model#\n\n\n\n\nfixed point model#\n\n\n\n\ncompile model#\n\ncompile the model exported using PTQ/QAT#\n\n\n\n\npackage multiple models into one HBM#\n\n\n\n\nmodel static perf#\n\n\n\n\nmodel inference#\n\n\n\n\nHBIR model modify#\n\ntensor name modify#\n\n\n\nmodel desc modify#\n\n\n\ninsert nodes#\n\nNote: To avoid the new insertion operator not running in some conversion passes,\nit is recommended to call the insert_xxx api before the convert stage\n\ninsert pyramid input#\n\n\n\ninsert image preprocess#\n\n\n\ninsert roi resize#\n\n\n\ninsert transpose#\n\n\n\ninsert split#\n\n\n\nremove adjacent nodes in the input and output of the model#\n\n\n\n\nHBM modify#\n\n","routePath":"/en/guide/advanced_content/hbdk_api_reference","lang":"en","toc":[{"text":"hbdk api introduction","id":"hbdk-api-introduction","depth":2,"charIndex":3},{"text":"Module: hbdk4.compiler.onnx","id":"module-hbdk4compileronnx","depth":3,"charIndex":425},{"text":"`export`(proto onnx.ModelProto, *, name Optional[str] = None) -> Module","id":"exportproto-onnxmodelproto--name-optionalstr--none---module","depth":4,"charIndex":-1},{"text":"`statistics`(proto onnx.ModelProto)","id":"statisticsproto-onnxmodelproto","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.torch","id":"module-hbdk4compilertorch","depth":3,"charIndex":568},{"text":"`export`( jit torch.jit.ScriptModule, example_input Any, *, name Optional[str] = None, input_names List[str] = None, output_names List[str] = None, lower_non_tensor bool = True) -> Module","id":"export-jit-torchjitscriptmodule-example_input-any--name-optionalstr--none-input_names-liststr--none-output_names-liststr--none-lower_non_tensor-bool--true---module","depth":4,"charIndex":-1},{"text":"`statistics`(jit torch.jit.ScriptModule, example_input Any)","id":"statisticsjit-torchjitscriptmodule-example_input-any","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.apis","id":"module-hbdk4compilerapis","depth":3,"charIndex":852},{"text":"`load`(path str) -> Module","id":"loadpath-str---module","depth":4,"charIndex":-1},{"text":"`save`(m Module, path str) -> None","id":"savem-module-path-str---none","depth":4,"charIndex":-1},{"text":"`convert`( m Module, march Union[MarchBase, str], advice=False, advice_path=\"\", **kwargs) -> Module","id":"convert-m-module-march-unionmarchbase-str-advicefalse-advice_path-kwargs---module","depth":4,"charIndex":-1},{"text":"`statistics`(m Module) -> list","id":"statisticsm-module---list","depth":4,"charIndex":-1},{"text":"`link`(hbo_list List[Hbo], output_path str, desc Optional[str] = None)","id":"linkhbo_list-listhbo-output_path-str-desc-optionalstr--none","depth":4,"charIndex":-1},{"text":"`compile`( m Module, path str, march Union[MarchBase, str], opt int = 2, jobs int = 4, max_time_per_fc float = 0.0, debug bool = False, hbdk3_compatible_mode bool = False, progress_bar bool = False, advice float = 0.0, balance int = 100, input_no_padding bool = False, output_no_padding bool = False) -> Union[Hbm, Hbo]","id":"compile-m-module-path-str-march-unionmarchbase-str-opt-int--2-jobs-int--4-max_time_per_fc-float--00-debug-bool--false-hbdk3_compatible_mode-bool--false-progress_bar-bool--false-advice-float--00-balance-int--100-input_no_padding-bool--false-output_no_padding-bool--false---unionhbm-hbo","depth":4,"charIndex":-1},{"text":"`visualize`( m Module, onnx_file Optional[str] = None, use_netron Optional[bool] = False)","id":"visualize-m-module-onnx_file-optionalstr--none-use_netron-optionalbool--false","depth":4,"charIndex":-1},{"text":"Module: hbdk4.compiler.hbm_tools","id":"module-hbdk4compilerhbm_tools","depth":3,"charIndex":1571},{"text":"`hbm_extract_desc`(model str) -> dict","id":"hbm_extract_descmodel-str---dict","depth":4,"charIndex":-1},{"text":"`hbm_update_desc`(model str, desc_dict dict)","id":"hbm_update_descmodel-str-desc_dict-dict","depth":4,"charIndex":-1},{"text":"`hbm_perf`(model str, output_dir str = None)","id":"hbm_perfmodel-str-output_dir-str--none","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Argument","id":"class-hbdk4compileroverlayargument","depth":3,"charIndex":1741},{"text":"`is_removable`(self) -> Tuple","id":"is_removableself---tuple","depth":4,"charIndex":-1},{"text":"`get_attached_op`(self) -> List[Operation]","id":"get_attached_opself---listoperation","depth":4,"charIndex":-1},{"text":"`remove_attached_op`(self)","id":"remove_attached_opself","depth":4,"charIndex":-1},{"text":"`erase`(self)","id":"eraseself","depth":4,"charIndex":-1},{"text":"`insert_transpose`(self, permutes List[int])","id":"insert_transposeself-permutes-listint","depth":4,"charIndex":-1},{"text":"`insert_image_convert`(self, mode str = \"nv12\")","id":"insert_image_convertself-mode-str--nv12","depth":4,"charIndex":-1},{"text":"`insert_image_preprocess`( self, mode str, divisor int, mean List[float], std List[float], is_signed bool = True)","id":"insert_image_preprocess-self-mode-str-divisor-int-mean-listfloat-std-listfloat-is_signed-bool--true","depth":4,"charIndex":-1},{"text":"`insert_roi_resize`( self, mode str, interp_mode=\"bilinear\", pad_mode=\"constant\", pad_value Optional[tuple] = (0, -128))","id":"insert_roi_resize-self-mode-str-interp_modebilinear-pad_modeconstant-pad_value-optionaltuple--0--128","depth":4,"charIndex":-1},{"text":"`insert_split`(self, dim int)","id":"insert_splitself-dim-int","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Module","id":"class-hbdk4compileroverlaymodule","depth":3,"charIndex":2273},{"text":"`functions`(self) -> List[Function]","id":"functionsself---listfunction","depth":4,"charIndex":-1},{"text":"`graphs`(self) -> List[Function]","id":"graphsself---listfunction","depth":4,"charIndex":-1},{"text":"Class: hbdk4.compiler.overlay.Function","id":"class-hbdk4compileroverlayfunction","depth":3,"charIndex":2386},{"text":"`remove_io_op`(self, op_types=None, op_names=None)","id":"remove_io_opself-op_typesnone-op_namesnone","depth":4,"charIndex":-1},{"text":"hbdk api example usage","id":"hbdk-api-example-usage","depth":2,"charIndex":2481},{"text":"export model and view model information","id":"export-model-and-view-model-information","depth":3,"charIndex":2507},{"text":"onnx","id":"onnx","depth":4,"charIndex":2549},{"text":"serialize model","id":"serialize-model","depth":3,"charIndex":2559},{"text":"fixed point model","id":"fixed-point-model","depth":3,"charIndex":2580},{"text":"compile model","id":"compile-model","depth":3,"charIndex":2603},{"text":"compile the model exported using PTQ/QAT","id":"compile-the-model-exported-using-ptqqat","depth":4,"charIndex":2619},{"text":"package multiple models into one HBM","id":"package-multiple-models-into-one-hbm","depth":3,"charIndex":2665},{"text":"model static perf","id":"model-static-perf","depth":3,"charIndex":2707},{"text":"model inference","id":"model-inference","depth":3,"charIndex":2730},{"text":"HBIR model modify","id":"hbir-model-modify","depth":3,"charIndex":2751},{"text":"tensor name modify","id":"tensor-name-modify","depth":4,"charIndex":2771},{"text":"model desc modify","id":"model-desc-modify","depth":4,"charIndex":2794},{"text":"insert nodes","id":"insert-nodes","depth":4,"charIndex":2816},{"text":"remove adjacent nodes in the input and output of the model","id":"remove-adjacent-nodes-in-the-input-and-output-of-the-model","depth":4,"charIndex":3096},{"text":"HBM modify","id":"hbm-modify","depth":3,"charIndex":3160}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":41,"title":"hmct.api.build_model","content":"#\n\n\nInterface Description#\n\nThe model conversion function provided by HMCT, inputs an onnx model and outputs\nthe quantized model after model conversion and quantization.\n\n\nInterface Form#\n\n\n\n\nReturn Value#\n\nOutput a quantized onnx model that can be used for quantized model accuracy\nevaluation, as well as compiled into a deployment model via hbdk.\n\n\nParameter#\n\n\nGenerator Description#\n\nGENERATOR NAME               DESCRIPTION\noriginal_float_model.onnx    The output after the original model conversion, the\n                             conversion of this stage includes: opset, ir version\n                             conversion, input_shape modification operation.\noptimized_float_model.onnx   The output of the model optimization phase, which consists\n                             of conversion such as: constant folding, operator fusion,\n                             useless operator removal, operator replacement, and operator\n                             splitting.\ncalibrated_model.onnx        Output from the model calibration phase, which involves\n                             quantization: inserting calibration nodes, counting data\n                             distributions, and calculating quantization parameters.\nptq_model.onnx               Output from the quantization phase of the model, which\n                             consists of: tuning and converting the quantization\n                             parameters based on the specified march.","routePath":"/en/guide/advanced_content/hmct_api_reference/build_model","lang":"en","toc":[{"text":"Interface Description","id":"interface-description","depth":2,"charIndex":3},{"text":"Interface Form","id":"interface-form","depth":3,"charIndex":171},{"text":"Return Value","id":"return-value","depth":3,"charIndex":191},{"text":"Parameter","id":"parameter","depth":2,"charIndex":350},{"text":"Generator Description","id":"generator-description","depth":2,"charIndex":363}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":42,"title":"hmct.api.check_model","content":"#\n\n\nInterface Description#\n\nThe quick check function for the transformation process of the model is provided\nby HMCT, where the input onnx model is checked with some random quantization\nparameters to see if the model conversion and the process is successful.\n\n\nInterface Form#\n\n\n\n\nReturn Value#\n\nGenerate an onnx model with stochastic parameter quantization.\n\n\nParameter#\n\nThe value ranges and definitions of the same parameters in check_model as in\nbuild_model are identical.","routePath":"/en/guide/advanced_content/hmct_api_reference/check_model","lang":"en","toc":[{"text":"Interface Description","id":"interface-description","depth":2,"charIndex":3},{"text":"Interface Form","id":"interface-form","depth":3,"charIndex":260},{"text":"Return Value","id":"return-value","depth":3,"charIndex":280},{"text":"Parameter","id":"parameter","depth":2,"charIndex":360}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":43,"title":"hmct.api.load_model","content":"#\n\nThe model loading function is provided by HMCT to load an onnx model with a\nspecified path for subsequent model conversion.\n\n\nInterface Form#\n\n\n\n\nParameter#\n\nonnx_model_file, string data type, mandatory, onnx model file path.\n\n\nReturn Value#\n\nThe onnx ModelProto object.","routePath":"/en/guide/advanced_content/hmct_api_reference/load_model","lang":"en","toc":[{"text":"Interface Form","id":"interface-form","depth":2,"charIndex":128},{"text":"Parameter","id":"parameter","depth":2,"charIndex":148},{"text":"Return Value","id":"return-value","depth":2,"charIndex":230}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":44,"title":"hmct.api.ORTExecutor","content":"#\n\n\nInterface Description#\n\nThe model inference function is provided by HMCT to support inference on\nintermediate models generated during the model conversion process.\n\n\nInterface Form#\n\n\n\n\nMember Function#","routePath":"/en/guide/advanced_content/hmct_api_reference/ortexecutor","lang":"en","toc":[{"text":"Interface Description","id":"interface-description","depth":2,"charIndex":3},{"text":"Interface Form","id":"interface-form","depth":3,"charIndex":169},{"text":"Member Function","id":"member-function","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":45,"title":"Common Abbreviations","content":"#\n\nA\n\nB\n\nC\n\nD\n\nF\n\nH\n\nI\n\nL\n\nP\n\nQ\n\nR\n\nS","routePath":"/en/guide/appendix/common_abbreviations","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":46,"title":"Common Image Format","content":"#\n\n\nIntroduction#\n\nWith the development of artificial intelligence, deep neural networks “blossom”\nin the field of vision. In order to fulfill the needs of different scenarios, we\nare exposed to a variety of image data formats. This subsection provides a\ndetailed introduction to several image data formats commonly used in deep\nlearning scenarios: RGB, BGR, YUV, NV12 and Gray.\n\n\nRGB#\n\nRGB, a common color image format. Each pixel point of the image stores the\nluminance values (0 ~ 255, UINT8) for the red (Red), green (Green), and blue\n(Blue) color channels.\n\nBased on this, if recorded as (R, G, B), then (255,0,0), (0,255,0), (0,0,255)\ncan represent the purest red, green, and blue, respectively. If the values of\nthe three RGB channels are all 0, the black color is obtained; if the values of\nthe three channels are all maxed out at 255, the white color is obtained.\n\nRGB can represent up to 256x256x256≈16.77 million colors, which is far more than\nthe human eye can perceive (about 10 million), so RGB is used in a wide variety\nof display fields and is closely related to daily life.\n\nHowever, RGB has a feature that each pixel must store the R, G, and B channel\nvalues at the same time, i.e., each pixel needs 3 bytes of storage space, which\nis very unfriendly to the storage and transmission of video scenes, and will\ntake up a lot of space and bandwidth.\n\n\nBGR#\n\nThe BGR image format is similar to RGB, except that the red, green and blue\nchannels are arranged in a different order. In the RGB format, the channel order\nof pixel points is red, green, and blue, while in the BGR format, the channel\norder of pixel points is blue, green, and red.\n\nThe BGR format is commonly used in computer vision libraries such as OpenCV and\nis the default image format for some software and hardware, with which it has\nbetter compatibility.\n\nBGR, like RGB, has a large amount of data and is not suitable for the storage\nand transmission of video scenes. Therefore, we also need other image formats to\nreplace RGB/BGR for video.\n\n\nYUV#\n\n\nIntroduction#\n\nYUV, a color image format, where Y denotes Luminance, which is used to specify\nthe brightness of a pixel (which can be interpreted as the degree of black and\nwhite), and U and V denote Chrominance or Chroma, which are used to specify the\ncolor of a pixel, and each of these values is expressed using UINT8, as shown\nbelow.\n\nThe YUV format uses luminance-chrominance separation, which means that only U\nand V are involved in the color representation, which is different from RGB.\n\nEven without the U and V components, we can “recognize” the basic content of an\nimage based on the Y component alone, except that it is presented as a black and\nwhite image. And the U and V components give color to these basics, and the\nblack and white image evolves into a color image. This means that we can\nminimize the sampling of the U and V components while retaining the Y-component\ninformation in order to minimize the amount of data, which is of great benefit\nto the storage and transmission of video data, which is why YUV is more suitable\nfor the video processing field compared to RGB.\n\n\nYUV Common Format#\n\nAccording to research, the human eye is more sensitive to luminance information\nthan color information. YUV downsampling is based on the characteristics of the\nhuman eye, the human eye is relatively insensitive to the color information for\ncompressed sampling, to get a relatively small file for playback and\ntransmission. According to the percentage of Y and UV, the three commonly used\nYUV formats are: YUV444, YUV422, and YUV420.\n\nThree graphs are used to visualize the percentage of Y and UV for different\nacquisition methods.\n\n\n\n * YUV444: Each Y component corresponds to a pair of UV components, occupying 3\n   bytes per pixel (Y + U + V = 8 + 8 + 8 = 24bits).\n * YUV422: Every two Y components share a pair of UV components, occupying 2\n   bytes per pixel (Y + 0.5U + 0.5V = 8 + 4 + 4 = 16bits)。\n * YUV420: Every four Y components share a pair of UV components, occupying 1.5\n   bytes per pixel (Y + 0.25U + 0.25V = 8 + 2 + 2 = 12bits).\n\nAt this point to understand the 4 in YUV4xx, this 4, in fact, expresses the\nmaximum sharing unit! That is, up to 4 Y's share a pair of UV's.\n\n\nYUV420 in Detail#\n\nIn YUV420, a pixel point corresponds to a Y and a small 4X4 square corresponds\nto a U and V. Each pixel occupies 1.5 bytes. YUV420 can also be categorized into\ntwo formats, YUV420P and YUV420SP, based on different UV component arrangements.\n\nThe YUV420P is arranged as shown below by storing the U first and then the V:\n\n\n\nYUV420SP is stored alternately with UV and UV, arranged as shown below:\n\n\n\nAt this point, I believe you can understand why the length of YUV420 data in\nmemory is width * height * 3 / 2.\n\n\nNV12#\n\nThe NV12 image format belongs to the YUV420SP format in the YUV color space,\nwhere every four Y components share a set of U and V components, with Y stored\nconsecutively and U and V stored crosswise.\n\nNV12 has half the amount of data of formats such as RGB/BGR while maintaining\nthe image brightness information, which can reduce the time for the model to\nload the input data, so the embedded side usually selects the NV12 image as the\nimage data input during deployment.\n\n\nGray#\n\nGray image format, also known as grayscale image format, is a single-channel\nimage format. In a Gray image, each pixel contains only one luminance value, and\neach value is represented using the UINT8 type, which is an integer between 0\nand 255. This brightness value indicates how bright or dark each pixel in the\nimage, with larger values indicating brighter pixels and smaller values\nindicating darker pixels.\n\nGray image format is also a common format for other color image formats (such as\nRGB, YUV, etc.) when converting to a single-channel image, which contains only\nthe luminance information of the image, and the image data is relatively small.\nTherefore, for some scenes that are less sensitive to image color information,\nit still has important application value.\n\n\nConvert between image formats#\n\nIn terms of image acquisition and display, RGB is mainly used, but in terms of\nimage storage, processing and transmission, YUV is selected. In a complete\napplication scenario, different image formats may need to be used.\n\nHow to realize the conversion between image formats? Can be simply understood\nthat there is a “standard”, based on this standard, through certain mathematical\noperations can be completed between different image formats. The following\ncomputer vision library opencv encapsulated function as an example, see how to\nrealize the image format conversion:\n\n\n\n\n\nWe provide in the OE package between the common image format conversion source\ncode (eg: RGB2NV12, BGR2RGB, etc.), image processing commonly used transformer\ndescription of the document please refer to the user manual Image Processing\nTransformer , The corresponding source code is located under the\nsamples/ai_toolchain/horizon_model_convert_sample/01_common/python/data path of\nthe OE development kit.\n\nDifferent image formats have different performances and advantages and\ndisadvantages, and in practice, you can personalize the selection of image\nformats according to your needs.\n\n\nReference Links#\n\n * https://blog.csdn.net/onion2007/article/details/46805335\n\n * https://zhuanlan.zhihu.com/p/538058910\n\n * https://zhuanlan.zhihu.com/p/248116694\n\n * https://blog.csdn.net/zego_0616/article/details/126658494\n\n * https://blog.csdn.net/luoyingxing/article/details/108516163","routePath":"/en/guide/appendix/community_articles/common_image_format","lang":"en","toc":[{"text":"Introduction","id":"introduction","depth":2,"charIndex":3},{"text":"RGB","id":"rgb","depth":2,"charIndex":380},{"text":"BGR","id":"bgr","depth":2,"charIndex":1366},{"text":"YUV","id":"yuv","depth":2,"charIndex":2024},{"text":"Introduction","id":"introduction-1","depth":3,"charIndex":2031},{"text":"YUV Common Format","id":"yuv-common-format","depth":3,"charIndex":3126},{"text":"YUV420 in Detail","id":"yuv420-in-detail","depth":3,"charIndex":4234},{"text":"NV12","id":"nv12","depth":2,"charIndex":4764},{"text":"Gray","id":"gray","depth":2,"charIndex":5245},{"text":"Convert between image formats","id":"convert-between-image-formats","depth":2,"charIndex":6028},{"text":"Reference Links","id":"reference-links","depth":2,"charIndex":7223}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":47,"title":"OE Development Kit Samples Introduction","content":"#\n\nThe algorithmic toolchain covers the key steps of model training (floating-point\nand quantization training, optional), conversion, performance/accuracy\nvalidation, deployment and inference. To facilitate your quick experience and\nlearning, rich and comprehensive samples are provided in the OE Development Kit.\nTo facilitate your understanding and use of these samples, this article will\nprovide a detailed introduction to these samples.\n\nFirst of all, after you obtain the OE development package, the directory\nstructure of the unzipped samples package is shown below:\n\n\n\nThe samples directory provides model training sample, floating-point to\nfixed-point model sample, and UCP related sample in the Unify Compute Platform.\nBelow is a distribution of general processes and samples used at each stage of\nthe toolchain:\n\nThe samples correspond to different stages of use in the flow of the toolchain\nshown above:\n\n * The model conversion sample contains the processes of model checking,\n   calibration data processing, model quantization compilation, single image\n   inference, and accuracy evaluation in the model PTQ conversion phase.\n\n * The model training sample contains dataset packing scripts, model config\n   files, training scripts, and other tool scripts for the floating-point model\n   training (optional) and QAT quantization training (optional) phases, so that\n   the QAT quantization training strategy can be attempted when the expected\n   accuracy is not achieved by applying model accuracy optimization tools.\n\n * UCP board-side deployment samples, including visual processing, model\n   inference, high-performance operator library, custom operator and object\n   detection full process sample. The model inference sample provides\n   compilation scripts, run scripts and source code for the model deployment\n   stage.\n\n\nModel Conversion Sample (horizon_model_convert_sample)#\n\nThe toolchain provides model conversion samples in the\nsamples/ai_toolchain/horizon_model_convert_sample folder, and the sample package\ndirectory structure is shown below:\n\n\n\nThe OE package not only provides PTQ model conversion samples, but additionally\ncontains one-click run scripts for model checking, calibration data\npreprocessing, conversion compilation, and inference.\n\nTake 03_resnet50 in the horizon_model_convert_sample/03_classification directory\nas an example of the relevant script:\n\n\n\nFor a tutorial on how to use the PTQ model conversion sample, please refer to\nPTQ Model Conversion Samples section.\n\n\nModel Training Sample (horizon_model_train_sample)#\n\nThe toolchain provides model training samples in the\nsamples/ai_toolchain/horizon_model_train_sample directory, and the sample\npackage structure is shown below:\n\n\n\nModel training is usually achieved by simply using the following commands:\n\n\n\n * $ needs to be configured as the path to the config file corresponding to\n   model training in the configs folder, which defines the model structure,\n   dataset loading, and the whole set of training procedures, and the sample\n   provides models including classification, detection, segmentation, and\n   optical flow estimation tasks.\n\n * The tools folder provides execution scripts containing dataset processing,\n   model training, conversion compilation, computation statistics, etc. The\n   functions of the relevant scripts are as follows.\n   \n   * calops.py: Network modeling computational volume statistics tool.\n   \n   * compile_perf_hbir.py: Compilation and perf tool.\n   \n   * create_data.py: Dataset preprocessing tool for preprocessing Kitti3D radar\n     dataset.\n   \n   * dataset_converters: Conversion script for different dataset formats are\n     provided under the folder.\n   \n   * datasets: Dataset package and data visualization script are provided under\n     the folder.\n   \n   * export_hbir.py: HBIR model export tool.\n   \n   * export_onnx.py: ONNX model export tool, exported ONNX model can only be\n     used for visualization, does not support inference.\n   \n   * gen_camera_param_nusc.py: Script to get camera internal and external\n     parameters from nuscenes.\n   \n   * homography_generator.py: Script to compute the ego2img matrix.\n   \n   * reference_points_generator.py: Script to calculate model input reference\n     points from the homography matrix.\n   \n   * gen_reference_points_nusc.py: Script to get model input reference points\n     from nuscenes.\n   \n   * infer_hbir.py: Single image prediction tool.\n   \n   * model_checker.py: Model checking tool.\n   \n   * predict.py: Prediction tool.\n   \n   * quant_analysis.py: Accuracy debug tool.\n   \n   * train.py: Model training script, support floating-point model training,\n     quantization training function.\n   \n   * validation_hbir.py: Accuracy verification tool to provide fully aligned\n     results on the upper board.\n\nFor a tutorial on using the model training samples, please refer to the\nintroduction in the Horizon Torch Samples section.\n\n\nUnify Compute Platform Sample (ucp_tutorial)#\n\nThe toolchain provides sample source code and running scripts for the Unify\nCompute Platform UCP in the samples/ai_toolchain/ucp_tutorial directory, and the\nsample package structure is shown below:\n\n\n\n * all-round: Object detection full process sample, for detailed introduction\n   and usage tutorial, please refer to the introduction in the section Object\n   Detection Full Process Sample.\n\n * custom_operator: Custom operator sample, including the DSP sample and\n   development sample for calling board-side GPU based on OpenCL interface, for\n   detailed introduction and usage tutorial, please refer to the introduction in\n   the section UCP Custom Operator.\n\n * deps_aarch64: AArch64 public dependency directory, containing UCP dependency\n   library and header file, etc.\n\n * deps_x86: X86 emulation public dependency directory\n\n * dnn: DNN samples, containing as below:\n   \n   * ai_benchmark provides performance and accuracy evaluation samples of common\n     models for embedded application development. For detailed introduction and\n     usage tutorial, please refer to the introduction in the section AI\n     Benchmark User Guide.\n   \n   * basic_samples provides model inference related to the use of shallow and\n     deep samples, designed to help you familiarize and learn the model\n     inference related to the interface and a variety of advanced features,\n     detailed introduction and use of the tutorial, please refer to the\n     introduction in the section Basic Sample User Guide.\n\n * hpl: HPL samples including sample source code and minimal executable\n   environment, detailed introduction and use of the tutorial, please refer to\n   the introduction in the section High Performance Library - Sample\n\n * vp: VP samples including sample source code and minimal executable\n   environment, detailed introduction and use of the tutorial, please refer to\n   the introduction in the section Visual Process - Sample.\n\n * tools: The Unify Compute Platform UCP provides tools.","routePath":"/en/guide/appendix/community_articles/oe_package_sample","lang":"en","toc":[{"text":"Model Conversion Sample (horizon_model_convert_sample)","id":"model-conversion-sample-horizon_model_convert_sample","depth":2,"charIndex":1836},{"text":"Model Training Sample (horizon_model_train_sample)","id":"model-training-sample-horizon_model_train_sample","depth":2,"charIndex":2511},{"text":"Unify Compute Platform Sample (ucp_tutorial)","id":"unify-compute-platform-sample-ucp_tutorial","depth":2,"charIndex":4935}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":48,"title":"Dataset Download","content":"#\n\nThe dataset used for the samples can be accessed from the following:\n\nDATASET        DOWNLOAD ADDRESS                                               DOWNLOAD STRUCTURE\nImageNet       https://www.image-net.org/download.php                         For download structure, please refer to Dataset Reference\n                                                                              Structure.\nCOCO           https://cocodataset.org/                                       For download structure, please refer to Dataset Reference\n                                                                              Structure.\nVOC            http://host.robots.ox.ac.uk/pascal/VOC/                        Need to download both versions 2007 and 2012, for download\n                                                                              structure, please refer to Dataset Reference Structure.\nCityscapes     https://github.com/mcordts/cityscapesScripts                   For download structure, please refer to Dataset Reference\n                                                                              Structure .\nCIFAR-10       http://www.cs.toronto.edu/~kriz/cifar.html                     For download structure, please refer to Dataset Reference\n                                                                              Structure .\nFlyingChairs   https://lmb.informatik.uni-freiburg.de/resources/datasets/Fl   For the download structure, please refer to the relevant\n               yingChairs.en.html                                             description in Data Pre-processing .\nKITTI3D        https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_be   For the download structure, please refer to the relevant\n               nchmark=3d                                                     description in Data Pre-processing .\nCULane         https://xingangpan.github.io/projects/CULane.html              For the download structure, please refer to the relevant\n                                                                              description in Data Pre-processing .\nNuscenes       https://www.nuscenes.org/nuscenes                              For the download structure, please refer to the relevant\n                                                                              description in Data Pre-processing .\nMot17          https://opendatalab.com/MOT17                                  For the download structure, please refer to the relevant\n                                                                              description in Data Pre-processing .\nCarfusion      http://www.cs.cmu.edu/~ILIM/projects/IM/CarFusion/cvpr2018/i   For the download structure, please refer to the relevant\n               ndex.html                                                      description in Data Pre-processing .\nArgoverse 1    https://www.argoverse.org/av1.html                             For the download structure, please refer to the relevant\n                                                                              description in Data Pre-processing .\nSceneFlow      https://lmb.informatik.uni-freiburg.de/resources/datasets/Sc   For the download structure, please refer to the relevant\n               eneFlowDatasets.en.html                                        description in Data Pre-processing .\n\nIf you have problems with the data preparation process, please contact Horizon.","routePath":"/en/guide/appendix/dataset_link","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":49,"title":"HBIR Operator Definition","content":"#\n\n\nhbir.abs (::mlir::hbdk::hbir::AbsOp)#\n\nHBIR tensor abs.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.acos (::mlir::hbdk::hbir::AcosOp)#\n\nHBIR tensor acos.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.acosh (::mlir::hbdk::hbir::AcoshOp)#\n\nHBIR tensor acosh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.add (::mlir::hbdk::hbir::AddOp)#\n\nHBIR tensor addition.\n\nApplies addition operator element-wise, $y_i=lhs_i+rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch add.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.asin (::mlir::hbdk::hbir::AsinOp)#\n\nHBIR tensor asin.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.asinh (::mlir::hbdk::hbir::AsinhOp)#\n\nHBIR tensor asinh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.atan (::mlir::hbdk::hbir::AtanOp)#\n\nHBIR tensor atan.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.atanh (::mlir::hbdk::hbir::AtanhOp)#\n\nHBIR tensor atanh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.avg_pool (::mlir::hbdk::hbir::AvgPoolOp)#\n\nHBIR n-D average pooling(only support 1d and 2d currently).\n\nApplies a nD average pooling over input.\n\nIn the simplest case, the output value of the operator with input size $(N, H,\nW, C)$, output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h}, ker_{w})$\ncan be precisely described as:\n\n$ out(N_i, h, w, C_j) = \\frac{1} { ker_h *ker_w } \\sum_{m = 0} ^ { ker_h - 1\n}\\sum_{n = 0} ^ { ker_w - 1 } input(N_i, stride[0]\\times h + m, stride[1] \\times\nw + n, C_j) $\n\nwhere $h,w$ respectively represent the size of H and W.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * parameters has the same manner as the Conv2D operator, the same goes for the\n   output size.\n\n * ceilMode controls output's compute is mode of floor or ceil, it's default\n   value is false.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in},\n   W_{in})$\n\n * Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(, H_{out},\n   W_{out}, C)$, where $$ represents any number of dimension.\n\n$ H_{out} = \\lfloor {\\frac{H_{in} + padding[0] + padding[2] - kernel[0]}\n{stride[0]} + 1}\\rfloor $\n\n$ W_{out} = \\lfloor {\\frac{W_{in} + padding[1] + padding[3] - kernel[1]}\n{stride[1]} + 1}\\rfloor $\n\nif ceilMode = true, please use ceil replace floor in the above output formula.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch avg_pool.\n\nTraits: CommonVerifier, PoolLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nkernel      ::mlir::ArrayAttr   64-bit integer array attribute\nstride      ::mlir::ArrayAttr   64-bit integer array attribute\npad         ::mlir::ArrayAttr   64-bit integer array attribute\ndilation    ::mlir::ArrayAttr   64-bit integer array attribute\nceilMode    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.batchnorm (::mlir::hbdk::hbir::BatchNormOp)#\n\nHbir Batch Normalize\n\nApplies Batch Normalization over each dimension of input. This compute can be\nprecisely described as:\n\n$ y = \\frac{x-mean[x]}{\\sqrt{Var[x]+\\epsilon}}*weight+bias $\n\nThis mean and standard-deviation are calculated per-dimension over the batches\nand weight and bias are learnable parameter vectors of the input size.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * eps - a value added to the denominator for numerical stability.\n * $mean(x)$ and $Var[x]$'s shape are $(C)$.\n * weight and bias are learnable scalar.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H,W,C)$ or $(N,M,H,W,C)$ or $(H,W,C)$ or $(,H,W,C)$, where $$\n   reprensent any number of dimension.\n * Output: same shape as input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch BatchNorm.\n\nTraits: CommonVerifier, Misc, SameVariadicOperandSize\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\neps         ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nmean      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nvar       1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nweight    1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.bev_pool_v2 (::mlir::hbdk::hbir::BevPoolV2Op)#\n\nHBIR bev_pool_v2 op, cpu operator, from mmdet3d, no corresponding operator in\ntorch/onnx\n\nConvert several planar image inputs into bev image outputs, thus providing\nsupport for data processing under bird's eye view.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * depth: the depth tensor.\n * feat: the feat tensor.\n * ranks_depth: Stores the index value of depth.\n * ranks_feat: Stores the index value of feat.\n * ranks_bev: Stores the Voxel index value of the valid bev space.\n * interval_starts: Each element marks the starting point of each \"continuation\n   segment\" of the ranks_bev feat.\n * interval_lengths: Each element identifies the length of each \"continuous\n   segment\" of the ranks_bev feat.\n * bev_feat_shape: output's shape. Aligned with the public version of cudu\n   kernel, no permute(0, 4, 1, 2, 3) operation is performed in the kernel. And\n   can support rank>=4.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * depth: (B, N, D, fH, fW)\n\n * feat: (B, N, fH, fW, C)\n\n * ranks_depth: (N_points, )\n\n * ranks_feat: (N_points, )\n\n * ranks_bev: (N_points, )\n\n * interval_starts: (N_pillar, )\n\n * interval_lengths: (N_pillar, )\n\n * output: shape same as bev_feat_shape, (B, D_Z, D_Y, D_X, C)\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE        MLIR TYPE           DESCRIPTION\nbev_feat_shape   ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND            DESCRIPTION\ndepth              tensor of 64-bit float or 32-bit float or 16-bit float or\n                   bfloat16 type or 8-bit signed integer or 16-bit signed\n                   integer or 32-bit signed integer or 64-bit signed integer or\n                   8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer or values or\n                   none type\nfeat               tensor of 64-bit float or 32-bit float or 16-bit float or\n                   bfloat16 type or 8-bit signed integer or 16-bit signed\n                   integer or 32-bit signed integer or 64-bit signed integer or\n                   8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer or values or\n                   none type\nranks_depth        tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\nranks_feat         tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\nranks_bev          tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\ninterval_starts    tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\ninterval_lengths   tensor of 8-bit signed integer or 16-bit signed integer or\n                   32-bit signed integer or 64-bit signed integer or 8-bit\n                   unsigned integer or 16-bit unsigned integer or 32-bit\n                   unsigned integer or 64-bit unsigned integer values or none\n                   type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cast_type (::mlir::hbdk::hbir::CastTypeOp)#\n\nelemental type cast operation\n\nData are actually moved. Traits: CommonVerifier, Misc, NaiveRoiInfer,\nNaiveTiling, SameOperandsAndResultShape\n\nInterfaces: CastOpInterface, HBTLExecutable, HbdkExecutorInterface,\nHbdkInferType, MoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE          DESCRIPTION\nforceSaturate   ::mlir::BoolAttr   bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.ceil (::mlir::hbdk::hbir::CeilOp)#\n\nHBIR tensor ceil.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.clip (::mlir::hbdk::hbir::ClipOp)#\n\nHBIR Clip Op.\n\nClamps all elements in input into the range $[min, max] $.Assume min_value and\nmax_value be min and max, respectively, this performs:\n\n$ y_i = min(max(x_i, min_value_i), max_value_i) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * min(min_value): lower-bound of the range to be clamped to\n * max(max_value): upper-bound of the range to be clamped to\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch hardtanh.\n\nTraits: CommonVerifier, LutLike, MoveF16CastLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nmin         ::mlir::Attribute   64-bit float attribute or 64-bit signless integer attribute\nmax         ::mlir::Attribute   64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.concat (::mlir::hbdk::hbir::ConcatOp)#\n\nConcatenates tensors along one dimension\n\nConcatenates the given sequence of seq tensors in the given dimension. No\nelemental type conversion.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * dim - the dimension over which the tensors are concatenated.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch cat.\n\nTraits: CommonVerifier, MoveLike, NaiveTiling, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, PortAccess, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninputs    variadic of tensor of 64-bit float or 32-bit float or 16-bit\n          float or bfloat16 type or 8-bit signed integer or 16-bit\n          signed integer or 32-bit signed integer or 64-bit signed\n          integer or 8-bit unsigned integer or 16-bit unsigned integer\n          or 32-bit unsigned integer or 64-bit unsigned integer or or\n          values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.constant (::mlir::hbdk::hbir::ConstantOp)#\n\nHBIR constant generation op.\n\nGenerate a constant with specified type and value Traits: CommonVerifier,\nConstant, NoFuseFp16TypeLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, NoMemoryEffect\n(MemoryEffectOpInterface), NonBatchAxesInfer, Perf, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE              DESCRIPTION\nvalues      ::mlir::ElementsAttr   constant vector/tensor attribute\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.conv2d (::mlir::hbdk::hbir::Conv2dOp)#\n\nHBIR 2-D convolution.\n\nApplies a 2D convolution over an input signal composed of several input\nchannels.\n\nIn the simplest case, the output value of the layer with input size $(N, H_{in},\nW_{in}, C_{in})$ and output $(N, H_{out}, W_{out}, C_{out})$ can be precisely\ndescibed as:\n\n$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in} -\n1}weight(C_{out_j},k) \\star input(N_i,k) $\n\nwhere $\\star$ is the valid 2D cross-correlation operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with 2\n   elements, default value is (1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end),\n   default value is (0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 2\n   elements, default value is (0,0). It's harder to describe, but this link has\n   a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH\n   represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or\n   $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where * represent\n   any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[0] + padding[2] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, NoFuseFp16TypeLike, StencilLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nNoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable, SchedInterface,\nSchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nstride      ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation    ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv2dtranspose (::mlir::hbdk::hbir::Conv2dTransposeOp)#\n\nHBIR transposed conv2d op.\n\nInverse operation of Conv2dTranspose in shape.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor\n * weight: the deconvolution kernel\n * stride: same as conv2d's stride, [s_h, s_w]\n * pad: pad information for clipping output [h_top,w_left,h_bottom,w_right]\n * dilation: same as conv2d's dilation, [d_h, d_w]\n * group: same as conv2d's group\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*, h, w, in_channel)$\n * weight: $(in_channel, kh, kw, out_channel / group)$\n * output: $(*, ho, wo, out_channel)$\n * bias: $out_channel$\n\nwhere:\n\n$ ho = (h - 1) * stride[0] - (pad[0] + pad[2]) + dilation[0] * (kh - 1) + 1 $ $\nwo = (w - 1) * stride[1] - (pad[1] + pad[3]) + dilation[1] * (kw - 1) + 1 $\n\nTraits: CommonVerifier, ConvLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE             DESCRIPTION\nstride          ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad             ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation        ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum        ::mlir::IntegerAttr   64-bit signless integer attribute\nillegalWeight   ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv3d (::mlir::hbdk::hbir::Conv3dOp)#\n\nHBIR 3-D convolution.\n\nApplies a 3D convolution over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size $(N, C_{in},\nD, H, W)$ and output $(N, C_{out}, D_{out}, H_{out}, W_{out})$ can be precisely\ndescribed as:\n\n$ out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1}\nweight(C_{out_j}, k) \\star input(N_i, k) $\n\nwhere $\\star$ is the valid 3D [cross-correlation] operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $D$, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with 3\n   elements, default value is (1,1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 5 elements, the padding sequences is\n   (d_begin,h_begin,w_begin,d_end,h_end,w_end), default value is (0,0,0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 3\n   elements, default value is (0,0,0). It's harder to describe, but this link\n   has a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight: $(C_{out}, KD, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and\n   KH represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,D_{in},H_{in},W_{in},C_{in})$ or $(D_{in},H_{in},W_{in},C_{in})$\n   or $(*,D_{in},H_{in},W_{in},C_{in})$, where * represent any number of\n   dimension.\n\n * Output: $(N,D_{out},H_{out},W_{out},C_{out})$ or\n   $(D_{out},H_{out},W{out},C_{out})$ or $(*,D_{out},H_{out},W_{out},C_{out})$.\n\n$ D_{out}=\\lfloor \\frac{D_{in} + padding[0] + padding[3] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[1] + padding[4] -\ndilation[1]\\times(kernel[1]-1)-1}{stride[1]}+1\\rfloor $\n\n$ W_{out}=\\lfloor \\frac{W_{in} + padding[2] + padding[5] -\ndilation[2]\\times(kernel[2]-1)-1}{stride[2]}+1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nstride      ::mlir::ArrayAttr     64-bit integer array attribute with exactly 3 elements\npad         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 6 elements\ndilation    ::mlir::ArrayAttr     64-bit integer array attribute with exactly 3 elements\ngroupNum    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    5D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.conv (::mlir::hbdk::hbir::ConvOp)#\n\nHBIR convolution.\n\nApplies a convolution over an input signal composed of several input planes.\n\nrank in convolution is an integer greater than or equal to 1.\n\nIn the simplest case, for 2D convolution, rank=2 the output value of the layer\nwith input size $(N, H_{in}, W_{in}, C_{in})$ and output $(N, H_{out}, W_{out},\nC_{out})$ can be precisely descibed as:\n\n$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in} -\n1}weight(C_{out_j},k) \\star input(N_i,k) $\n\nwhere $\\star$ is the valid 2D cross-correlation operation, $N$ is the batch\nsize, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * stride controls the stride for the cross-correlation, an integer array with\n   rank elements, default value is (1,1,...).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 2*rank elements, when rank=2, the padding sequences is\n   (h_begin,w_begin,h_end,w_end), default value is (0,0,0,0), when n=3, the\n   padding sequences is (d_begin,h_begin,w_begin,d_end,h_end,w_end), default\n   value is (0,0,0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with\n   rank elements, default value is (0,0,...). It's harder to describe, but this\n   link has a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * Weight for 2D: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW\n   and KH represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape for 2D:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or\n   $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where * represent\n   any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor \\frac{H_{in} + padding[0] + padding[2] -\ndilation[0]\\times(kernel[0]-1)-1}{stride[0]}+1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, ConvLike, NoFuseFp16TypeLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE     MLIR TYPE             DESCRIPTION\nstride        ::mlir::ArrayAttr     64-bit integer array attribute\npad           ::mlir::ArrayAttr     64-bit integer array attribute\ndilation      ::mlir::ArrayAttr     64-bit integer array attribute\ngroupNum      ::mlir::IntegerAttr   64-bit signless integer attribute\nchannelLast   ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.cos (::mlir::hbdk::hbir::CosOp)#\n\nHBIR tensor cos.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cosh (::mlir::hbdk::hbir::CoshOp)#\n\nHBIR tensor cosh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.cumsum (::mlir::hbdk::hbir::CumSumOp)#\n\nHBIR cumsum.\n\nPerforms cumulative sum of the input elements along the given axis. By default,\nit will do the sum inclusively meaning the first element is copied as is.\nThrough an exclusive attribute, this behavior can change to exclude the first\nelement. It can also perform summation in the opposite direction of the axis.\nFor that, set reverse attribute to 1.\n\nArgs: input (Tensor): the input tensor. output (Tensor): Output tensor of the\nsame type as input with cumulative sums of the input elements Attribute: axis\n(int): Must be in the range [-rank(input), rank(input)-1]. Negative value means\ncounting dimensions from the back. exclusive (int): Must be 0 or 1, defaut is 0.\n0 means the first element is copied to output, 1 will not. reverse (int): Must\nbe 0 or 1, defaut is 0. 1 means performing summation in the opposite direction\nof the axis.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch cumsum.\n\nTraits: CommonVerifier, Misc, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\naxis        ::mlir::IntegerAttr   64-bit signless integer attribute\nexclusive   ::mlir::IntegerAttr   64-bit signless integer attribute\nreverse     ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.deform_conv2d (::mlir::hbdk::hbir::DeformConv2dOp)#\n\nHBIR deformable 2-D convolution.\n\nApplies a deformable 2D convolution over an input signal composed of several\ninput channels.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * offset controls the offset for the sampling locations in the convolution\n   kernel.\n\n * mask controls different weights to different positions in the convolution\n   kernel.\n\n * stride controls the stride for the cross-correlation, an integer array with 2\n   elements, default value is (1,1).\n\n * padding controls the amount of padding applied to the input, an integer array\n   with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end),\n   default value is (0,0,0,0).\n\n * dilation controls the spacing between kernel points, an integer array with 2\n   elements, default value is (0,0). It's harder to describe, but this link has\n   a nice visualization of what dilation does.\n\n * groups controls the connections between inputs and outputs, an integer\n   variable, default value is 1.\n\n * offsetGroup controls the connections between inputs and offset, an integer\n   variable, default value is 1.\n\n * weight: $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH\n   represent kernel's height and width, respectively.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where *\n   represent any number of dimension.\n\n * Offset:\n   $(N,H_{out},W_{out},2\\timesoffset_groups\\timeskernel[0]\\timeskernel[1])$ or\n   $(*,H_{out},W_{out},2\\timesoffset_groups\\timeskernel[0]\\timeskernel[1])$,\n   where * represent any number of dimension.\n\n * Mask: $(N,H_{out},W_{out},offset_groups\\timeskernel[0]\\timeskernel[1])$ or\n   $(*,H_{out},W_{out},offset_groups\\timeskernel[0]\\timeskernel[1])$, where *\n   represent any number of dimension.\n\n * Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or\n   $(*,H_{out},W_{out},C_{out})$\n\n$ H_{out}=\\lfloor\n\\frac{H_{in}+padding[0]+padding[2]-dilation[0]\\times(kernel[0]-1)-1}{stride[0]}+\n1\\rfloor $ $ W_{out}=\\lfloor\n\\frac{W_{in}+padding[1]+padding[3]-dilation[1]\\times(kernel[1]-1)-1}{stride[1]}+\n1\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch convolution.\n\nTraits: CommonVerifier, Misc, NoFuseFp16TypeLike, SameVariadicOperandSize,\nStencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE        MLIR TYPE             DESCRIPTION\nstride           ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\npad              ::mlir::ArrayAttr     64-bit integer array attribute with exactly 4 elements\ndilation         ::mlir::ArrayAttr     64-bit integer array attribute with exactly 2 elements\ngroupNum         ::mlir::IntegerAttr   64-bit signless integer attribute\noffsetGroupNum   ::mlir::IntegerAttr   64-bit signless integer attribute\nuseMask          ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    4D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\noffset    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nmask      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.div (::mlir::hbdk::hbir::DivOp)#\n\nHBIR tensor division.\n\nApplies division operator element-wise, $y_i=lhs_i\\div rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch div.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.equal (::mlir::hbdk::hbir::EqualOp)#\n\nHBIR tensor equal.\n\nDetermines whether two tensors are equal element by element - wise, $y_i =\n(lhs_i == rhs_i) $. Traits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.exp (::mlir::hbdk::hbir::ExpOp)#\n\nHBIR tensor exp.\n\nApplies exponential operator element - wise, $y=e^{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the exponential of the elements of the input tensor\n   input.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.fake_cast (::mlir::hbdk::hbir::FakeCastOp)#\n\nfake elemental type cast operation\n\nCast float input to specified dtype, and then cast back to the same float type.\nTraits: CommonVerifier, Misc, SameElementType, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE          DESCRIPTION\ndtype       ::mlir::TypeAttr   any type attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 32-bit float or 16-bit float values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 32-bit float or 16-bit float values\n\n\nhbir.flip (::mlir::hbdk::hbir::FlipOp)#\n\nHBIR flip\n\nReverse the order of the input tensor along given axis in dims.\n\n--------------------------------------------------------------------------------\n\nParametes:\n\n * input: the input tensor.\n * dims: axis need to reverse.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: same as the input.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.floor (::mlir::hbdk::hbir::FloorOp)#\n\nHBIR tensor floor.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.gelu (::mlir::hbdk::hbir::GELUOp)#\n\nHBIR GELU activation.\n\nApplies gelu funciton element-wise. Gelu functon defined as:\n\n$ Gelu(x) = xP(X\\leq x) = x*\\phi(x) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Gelu is shorthand for Gaussian Error Linear Unit.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch gelu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.gather_elements (::mlir::hbdk::hbir::GatherElementsOp)#\n\nHBIR gather op for onnx GatherElements.\n\nHBIR gather op for onnx GatherElements. Traits: CommonVerifier, Misc,\nMoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer,\nPerf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.gather_nd (::mlir::hbdk::hbir::GatherNdOp)#\n\nHBIR gather_nd op for onnx gather_nd.\n\nHBIR gather_nd op for onnx gather_nd. Traits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer,\nPerf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nbatchDim    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.greater_equal (::mlir::hbdk::hbir::GreaterEqualOp)#\n\nHBIR tensor greater_equal.\n\nApplies greater_equal operator element - wise, $y_i = lhs_i > = rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.greater (::mlir::hbdk::hbir::GreaterOp)#\n\nHBIR tensor greater.\n\nApplies greater operator element - wise, $y_i = lhs_i > rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.grid_sample (::mlir::hbdk::hbir::GridSampleOp)#\n\nHBIR grid_sample.\n\nFrom the input and a flow-field grid, computes the output using input values and\npixel locations from the grid.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: input of shape $(*, H_{in}, W_{in}, C_{in}) $, where * represent any\n   number of dimension.\n * grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$\n * output: $(*, H_{out}, W_{out}, C_{in})$\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\nalignCorner     ::mlir::BoolAttr                      bool attribute\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\ngrid      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.index (::mlir::hbdk::hbir::IndexOp)#\n\nHBIR index op for aten::index_select\n\nReturns a new tensor which indexes the :attr:input tensor along dimension\n:attr:dim using the entries in :attr:index which is a LongTensor.\n\nThe returned tensor has the same number of dimensions as the original tensor\n(:attr:input). The :attr:dim\\ th dimension has the same size as the length of\n:attr:index; other dimensions have the same size as in the original tensor.\n\nArgs: input (Tensor): the input tensor. dim (int): the dimension in which we\nindex index (IntTensor or LongTensor): the tensor containing the indices to\nindex\n\nTraits: CommonVerifier, Misc, MoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindex     tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.layernorm (::mlir::hbdk::hbir::LayerNormOp)#\n\nHbir Layer Normalize\n\nApplies Layer Normalization over a mini - batch of inputs. This compute can be\nprecisely described as:\n\n$ y = \\frac{x - mean[x]} {\\sqrt{Var[x] +\\epsilon}} * weight + bias $\n\nThe Mean and standard-deviation are calculated over the last D dimensions, where\nD is the dimension of normalized_shape(dims).\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Unlike Batch Normalization, which compute mean and standard - deviation,\n   LayerNormalization compute these in single sample's different dimension.\n * dims controls normalized_shape.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, *)$\n * Output: $(N, *)$ (same shape as input)\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch layerNorm.\n\nTraits: CommonVerifier, Misc, SameVariadicOperandSize\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nNoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer, Perf, Quantizable,\nRoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\neps         ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nweight    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nbias      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.leaky_relu (::mlir::hbdk::hbir::LeakyReLUOp)#\n\nHBIR Leaky ReLU Op.\n\nApplies LeakyRelu funciton element - wise.LeakyRelu function defined as:\n\n$ LeakyRelu(x) = max(0, x) + slope * min(0, x) $\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * slope - Controls the angle of the negative slope.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch leaky_relu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling, SameElementType,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nslop        ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.less_equal (::mlir::hbdk::hbir::LessEqualOp)#\n\nHBIR tensor less_equal.\n\nApplies less_equal operator element - wise, $y_i = lhs_i < = rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.less (::mlir::hbdk::hbir::LessOp)#\n\nHBIR tensor less.\n\nApplies less operator element - wise, $y_i = lhs_i < rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.linear (::mlir::hbdk::hbir::LinearOp)#\n\nHBIR Linear.\n\nApplies a linear transformation to the incoming data: $y = xW ^ T + b$\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Weight's shape is $(C_{in},C_{out})$.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(, C_{in})$ where $$ represents any number of dimensions and $C_{in}$\n   = in_features.\n * Output: $(*, C_{out})$ where all but the last dimension are the same shape as\n   the input and $C_{out}$ = out_features.\n\n--------------------------------------------------------------------------------\n\nPrototype: pytorch linear.\n\nTraits: CommonVerifier, LinearLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nweight    2D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nbias      1D tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values\n\n\nhbir.log (::mlir::hbdk::hbir::LogOp)#\n\nHBIR tensor log.\n\nApplies natural logarithm operator element - wise, $y = \\log_e{(x)}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the natural logarithm of the elements of input.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.log_softmax (::mlir::hbdk::hbir::LogSoftmaxOp)#\n\nHBIR LogSoftmax Op.\n\nApplies the LogSoftmax function to an n - dimensional input Tensor rescaling\nthem so that the elements of the n-dimensional output. The output tensor has the\nsame dimension and shape as the input with values in the range [-inf, 0)\n\nLogSoftmax function is defined as:\n\n$ LogSoftmax(x_i) = log(\\frac{exp(x_i)} {\\sum_jexp(x_j)}) $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(*), where * means, any number of additional dimensions.\n * Output(*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch LogSoftmax.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.logical_and (::mlir::hbdk::hbir::LogicalAndOp)#\n\nHBIR tensor and.\n\nApplies 'logical and' operator element - wise, $y_i = lhs_i && rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike, SameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\nrhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.logical_not (::mlir::hbdk::hbir::LogicalNotOp)#\n\nHBIR tensor logical not, output bool.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape,\nSameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 8-bit signed integer or 16-bit signed integer or\n          16-bit float or or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.logical_or (::mlir::hbdk::hbir::LogicalOrOp)#\n\nHBIR tensor or.\n\nApplies 'logical or' operator element - wise, $y_i = lhs_i || rhs_i$. Traits:\nBroadcastable, CommonVerifier, EltwiseLike, SameOperandsElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\nrhs       tensor of 8-bit signed integer or 16-bit signed integer or\n          or 16-bit float or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.matmul (::mlir::hbdk::hbir::MatMulOp)#\n\nHBIR Matrix Multiplication.\n\nApplies matrix multiplication between two inputs: $C = A \\times B$, where\n$\\times$ means matrix multiplication.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * If both tensors are 1-dimensional, the dot product (scalar) is returned\n * If both arguments are 2-dimensional, the matrix-matrix product is returned\n * If the first argument is 1-dimensional and the second argument is\n   2-dimensional, the matrix-vector product is returned\n * If the first argument is 2-dimensional and the second argument is\n   1-dimensional, the matrix-vector product is returned\n * If both arguments are at least 1-dimensional and at least one argument is\n   N-dimensional (where N > 2), then a batched matrix multiply is returned\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * lhs: $(B_{l}, M, C)$, where $B_{l}$ represent any number of dimension.\n * rhs: $(B_{r}, C, N)$, where $B_{r}$ represent any number of dimension.\n * output: $(B_{o}, M, N)$, where $B_{o}$ represent represent the result of\n   broadcast between $B_{l}$ and $V_{r}$.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch Matmul.\n\nTraits: CommonVerifier, MatmulLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values\n\n\nhbir.max (::mlir::hbdk::hbir::MaxOp)#\n\nHBIR tensor max.\n\nApplies maximum operator element-wise, $y_i=max(lhs_i,rhs_i)$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.max_pool (::mlir::hbdk::hbir::MaxPoolOp)#\n\nHBIR n-D max pooling(only support 1d and 2d currently).\n\nApplies a n-D max Pooling over an input.\n\nIn the 2d case, for example, the output value of the operator with input size\n$(N, H, W, C)$, output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h},\nker_{w})$ can be precisely described as:\n\n$ out(N_i, h, w, C_j) = \\frac{1} { ker_h *ker_w }\\max_{m = 0} ^ { ker_h - 1\n}\\max_{n = 0} ^ { ker_w - 1 } input(N_i, stride[0] \\times h + m, stride[1]\n\\times w + n, C_j) $\n\nwhere $h,w$ respectively represent the size of H and W.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * parameters has the same manner as the Conv2D operator, the same goes for the\n   output size.\n * ceilMode controls output 's compute is mode of floor or ceil, it' s default\n   value is false.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in},\n   W_{in})$\n\n * Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(, H_{out},\n   W_{out}, C)$, where $$ represents any number of dimension.\n\n$ H_{out} = \\lfloor {\\frac{H_{in} + padding[0] + padding[2] - kernel[0]}\n{stride[0]} + 1}\\rfloor $\n\n$ W_{out} = \\lfloor {\\frac{W_{in} + padding[1] + padding[3] - kernel[1]}\n{stride[1]} + 1}\\rfloor $\n\nif ceilMode = true, please use ceil replace floor in the above ouput formula.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch max_pool.\n\nTraits: CommonVerifier, MoveF16CastLike, PoolLike, StencilLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nkernel      ::mlir::ArrayAttr   64-bit integer array attribute\nstride      ::mlir::ArrayAttr   64-bit integer array attribute\npad         ::mlir::ArrayAttr   64-bit integer array attribute\ndilation    ::mlir::ArrayAttr   64-bit integer array attribute\nceilMode    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or values or none type\n\n\nhbir.min (::mlir::hbdk::hbir::MinOp)#\n\nHBIR tensor min.\n\nApplies minimum operator element-wise, $y_i=min(lhs_i,rhs_i)$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.mod (::mlir::hbdk::hbir::ModOp)#\n\nHbir get modulo of two dividing tensors\n\nComputes the modulo function. It is equivalent to the operator x1 % x2\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * sameSignAsDividend: result has the same sign as dividend or divisor. Default\n   true means same sign as dividend.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE            MLIR TYPE          DESCRIPTION\nsameSignAsDividend   ::mlir::BoolAttr   bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.mul (::mlir::hbdk::hbir::MulOp)#\n\nHBIR tensor multiplication.\n\nApplies multiplication operator element-wise, $y_i=lhs_i\\times rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch mul.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.nan_to_num (::mlir::hbdk::hbir::NanToNumOp)#\n\nHBIR tensor nan_to_num.\n\nReplaces NaN, positive infinity, and negative infinity values in input with the\nvalues specified by nan, posinf, and neginf.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the replace value of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nnan         ::mlir::FloatAttr   64-bit float attribute\nposinf      ::mlir::FloatAttr   64-bit float attribute\nneginf      ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type values or none type\n\n\nhbir.neg (::mlir::hbdk::hbir::NegOp)#\n\nHBIR tensor neg.\n\nApplies negation operator element - wise, $y = x\\times - 1$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the negative of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.nonzero (::mlir::hbdk::hbir::NonZeroOp)#\n\nHBIR nonzero op for torch.nonzero\n\nFind all indices in tensor that are not 0\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer values or\n         none type\n\n\nhbir.pad (::mlir::hbdk::hbir::PadOp)#\n\nPad at both edges of Tensor.\n\nPadding at the begin and end position with constant / border value. Traits:\nCommonVerifier, Expansion, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                         DESCRIPTION\nbegin           ::mlir::ArrayAttr                 64-bit integer array attribute\nend             ::mlir::ArrayAttr                 64-bit integer array attribute\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr   mode to expand input feature on H/W\npadValue        ::mlir::Attribute                 64-bit float attribute or 64-bit signless integer attribute\nfoldable        ::mlir::BoolAttr                  bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.point_pillar_preprocess (::mlir::hbdk::hbir::PointPillarPreProcessOp)#\n\nHBIR point pillar preprocess op.\n\nHBIR point pillar preprocess.Voxelization and Normalization Traits:\nCommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE             DESCRIPTION\npcRanges            ::mlir::ArrayAttr     64-bit float array attribute\nnormRanges          ::mlir::ArrayAttr     64-bit float array attribute\nvoxelSizes          ::mlir::ArrayAttr     64-bit float array attribute\nmaxVoxelNum         ::mlir::IntegerAttr   64-bit signless integer attribute\nmaxPointsPerVoxel   ::mlir::IntegerAttr   64-bit signless integer attribute\nnormDims            ::mlir::ArrayAttr     64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\npoints    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\nvoxels   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\ncoords   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.pow (::mlir::hbdk::hbir::PowOp)#\n\nHBIR tensor pow.\n\nPow takes input data(lhs) and exponent Tensor(rhs), and produces one output data\nwhere the function $f(x) = x ^ {exponent}$, is applied to the data tensor\nelement - wise Traits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.prelu (::mlir::hbdk::hbir::PreluOp)#\n\nHBIR tensor prelu\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nslope     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.relu (::mlir::hbdk::hbir::ReLUOp)#\n\nHBIR ReLU activation.\n\nApplies the rectified linear unit function element - wise.Relu function is\ndefined as:\n\n$ ReLU(x) = (x) ^ + = max(0, x) $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(*), where * means any number of dimensions.\n * Output(*), same shapes as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch Relu.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling, SameElementType,\nSameOperandsAndResultShape\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reciprocal (::mlir::hbdk::hbir::ReciprocalOp)#\n\nHBIR tensor reciprocal.\n\nApplies reciprocal operator element - wise, $y = \\frac{1}{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the reciprocal of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_all (::mlir::hbdk::hbir::ReduceAllOp)#\n\nTests if all elements in input evaluate to True.\n\nReturn True if all elements in the row evaluate to True and False otherwise.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce all on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          or values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of values or none type\n\n\nhbir.reduce_argmax (::mlir::hbdk::hbir::ReduceArgmaxOp)#\n\nCalculate max on multiple axes and return its index.\n\nReturn the indices of the max elements of the input tensor's element along the\nprovided axis.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimension to perform reduce argmax on. Accepted range is [-r, r - 1]\n   where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output\n   shape will be 1x3x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceArgMax.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, MoveTransposeInterface, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.reduce_argmin (::mlir::hbdk::hbir::ReduceArgminOp)#\n\nCalculate min on multiple axes and return its index.\n\nReturn the indices of the min elements of the input tensor's element along the\nprovided axis.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimension to perform reduce argmin on. Accepted range is [-r, r - 1]\n   where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output\n   shape will be 1x3x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceArgMin.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, MoveTransposeInterface, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, RoiInfer, SchedInterface,\nSchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 8-bit signed integer or 16-bit signed integer or\n         32-bit signed integer or 64-bit signed integer or 8-bit\n         unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer values or none\n         type\n\n\nhbir.reduce_max (::mlir::hbdk::hbir::ReduceMaxOp)#\n\nCalculate max on multiple axes.\n\nReturn the max value of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce max on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMax.\n\nTraits: CommonVerifier, MoveF16CastLike, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_mean (::mlir::hbdk::hbir::ReduceMeanOp)#\n\nCalculate mean on multiple axes.\n\nReturn the mean of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce mean on. If it's a list, reduce over all\n   of them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMean.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_min (::mlir::hbdk::hbir::ReduceMinOp)#\n\nCalculate min on multiple axes.\n\nReturn the min value of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce min on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceMin\n\nTraits: CommonVerifier, MoveF16CastLike, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reduce_sum (::mlir::hbdk::hbir::ReduceSumOp)#\n\nCalculate sum on multiple axes.\n\nReturn the sum of all elements in the provided axes of the input tensor.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dims: dimensions to perform reduce sum on. If it's a list, reduce over all of\n   them. Accepted range is [-r, r - 1] where r = rank(input).\n * keepDim: keep the reduced dimensions or not. Default true means keep reduced\n   dimensions.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: $(*)$, where * represents any dimension.\n * output: if keepDim is True, same as input. Otherwise, all reduced dims will\n   be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output\n   shape will be 1x4).\n\n--------------------------------------------------------------------------------\n\nPrototype: ONNX ReduceSum.\n\nTraits: CommonVerifier, ReduceLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\nkeepDim     ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.reshape (::mlir::hbdk::hbir::ReshapeOp)#\n\nView a tensor as another shape\n\nReturns a tensor with the same data and number of elements as input, but with\nthe specified shape.When possible, the returned tensor will be a view of input.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * shape - the new shape.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch reshape.\n\nTraits: CommonVerifier, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nshape       ::mlir::ArrayAttr   64-bit integer array attribute\nfoldable    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.resize2d (::mlir::hbdk::hbir::Resize2dOp)#\n\nHBIR 2-D resizing.\n\nScale the input proportionally.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * ratio controls zoom size.*mode controls interpolation type, it's default\n   value is nearest.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(*, H_{in}, W_{in}, C)$\n * Output: $(*, H_{out}, W_{out}, C)$, where\n\n$ H_{out} = \\lfloor{H_{in} * ratio}\\rfloor $\n\n$ W_{out} = \\lfloor{W_{in} * ratio}\\rfloor $\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch upsample_nearest2d.\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nratio           ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\nsize            ::mlir::ArrayAttr                     64-bit integer array attribute with exactly 2 elements\nstep            ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\ninitialOffset   ::mlir::ArrayAttr                     64-bit float array attribute with exactly 2 elements\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.roll (::mlir::hbdk::hbir::RollOp)#\n\nRoll the tensor along the given dimensions\n\nRoll the tensor along the given dimension.Elements that are shifted beyond the\nlast position are re-introduced at the first position.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * shifts - The number of places by which the elements of the tensor are\n   shifted.\n * dims -Axis along which to roll.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch roll.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nshifts      ::mlir::ArrayAttr   64-bit integer array attribute\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.round (::mlir::hbdk::hbir::RoundOp)#\n\nHBIR tensor round.\n\nRounds elements of input to the nearest integer.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * This function implements the round half to even.\n\n * Returns a new tensor with the round of the elements of input.\n\nTraits: CommonVerifier, EltwiseLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndecimals    ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.rsqrt (::mlir::hbdk::hbir::RsqrtOp)#\n\nHBIR tensor rsqrt.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.scatter_elements (::mlir::hbdk::hbir::ScatterElementsOp)#\n\nHBIR scatter elements op. Same semantics as scatter elements in onnx. In\naddition, it supports the mean mode of torch scatter_reduce\n\nHBIR scatter elements scatter.The ONNX op link: https: //\ngithub.com/onnx/onnx/blob/main/docs/Operators.md#ScatterElements\n\nCopy the data to output, the specify a direction axis, use the values in updates\nto update the values in output at specific location according to indices.\n\nIn addition, it supports the mean mode of torch scatter_reduce.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE                             DESCRIPTION\naxis                ::mlir::IntegerAttr                   64-bit signless integer attribute\nscatterReduceMode   ::mlir::hbdk::ScatterReduceModeAttr   scatter reduce mode\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ndata      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values or none\n          type\nupdates   tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.scatter_nd (::mlir::hbdk::hbir::ScatterNDOp)#\n\nHBIR scatterND op. Same semantics as scatterND in onnx.\n\nHBIR scatterDN.The ONNX op link: https: //\ngithub.com/onnx/onnx/blob/main/docs/Operators.md#ScatterND\n\nCopy the data to output, then use the values in updates to update the values in\nthe output at some directions given by the indices.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE           MLIR TYPE                             DESCRIPTION\nscatterReduceMode   ::mlir::hbdk::ScatterReduceModeAttr   scatter reduce mode\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ndata      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer or 8-bit\n          unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer values\nupdates   tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.select (::mlir::hbdk::hbir::SelectOp)#\n\nselect a tensor from a bigger tensor on a specific dim and index\n\nSlices the input tensor along the selected dimension at the given index.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * dim - the dimension to slice.\n * index - the index to select with.\n * Select operator is equivalent to slicing.\n\n--------------------------------------------------------------------------------\n\nPrototype:Pytorch select.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, RoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nindex       ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.sigmoid (::mlir::hbdk::hbir::SigmoidOp)#\n\nHBIR Sigmoid activation.\n\nApplies the element - wise function.Sigmoid function is defined as:\n\n$ Sigmoid(x) = \\frac{1} {1 + exp(-x)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch sigmoid.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sign (::mlir::hbdk::hbir::SignOp)#\n\nHBIR tensor sign.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, EltwiseLike, MoveF16CastLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sin (::mlir::hbdk::hbir::SinOp)#\n\nHBIR tensor sin.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sinh (::mlir::hbdk::hbir::SinhOp)#\n\nHBIR tensor sinh.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.slice (::mlir::hbdk::hbir::SliceOp)#\n\nSlice a tensor out of a tensor\n\nSlicing like python's style means taking elements from one given index to\nanother given index.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * begin -the index start to pick(inclusive).\n * end -the index end to pick(exclusive).\n * step -the step interval of picking.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch slice.\n\nTraits: CommonVerifier, Foldable, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nbegin       ::mlir::ArrayAttr   64-bit integer array attribute\nend         ::mlir::ArrayAttr   64-bit integer array attribute\nstep        ::mlir::ArrayAttr   64-bit integer array attribute\nfoldable    ::mlir::BoolAttr    bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.slice_scatter (::mlir::hbdk::hbir::SliceScatterOp)#\n\nEmbeds the values of the src tensor into input at the given dimension\n\nEmbeds the values of the src tensor into input at the given dimension. This\nfunction returns a tensor with fresh storage; it does not create a view.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * src (Tensor) -the tensor to embed into input.\n * dim (int) -the dimension to insert the slice into.\n * start (int) -the start index of where to insert the slice.\n * end (int) -the end index of where to insert the slice.\n * step (int) -the how many elements to skip in.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch slice_scatter.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nstart       ::mlir::IntegerAttr   64-bit signless integer attribute\nend         ::mlir::IntegerAttr   64-bit signless integer attribute\nstep        ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nsrc       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.softmax (::mlir::hbdk::hbir::SoftmaxOp)#\n\nHBIR Softmax Op.\n\nApplies the Softmax function to an n - dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output. Tensor lie in the range $[0,\n1] $ and sum to 1.\n\nSoftmax function is defined as:\n\n$ Softmax(x_i) = \\frac{exp(x_i)} {\\sum_jexp(x_j)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input(), where $$ means, any number of additional dimensions.Output(), same\n   shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch softmax.\n\nTraits: CommonVerifier, Misc, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface),\nNonBatchAxesInfer, Perf, Quantizable, RoiInfer, SchedInterface, SchedTemp,\nShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.softplus (::mlir::hbdk::hbir::SoftplusOp)#\n\nHBIR Softplus Op.\n\nApplies the SoftPlus function element-wise. SoftPlus function defined as:\n\n$ SoftPlus(x) = \\frac{1}{\\beta}*log(1+exp(\\beta * x)) $\n\nSoftPlus is a smooth approximation to the ReLU function.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * beta - the $\\beta$ value for the Softplus formulation.\n * max - values is for numerical stability, when $\\beta *x > max,\n   SoftPlus(x)=x$.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch softplus.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\nbeta        ::mlir::FloatAttr   64-bit float attribute\nthreshold   ::mlir::FloatAttr   64-bit float attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.sort (::mlir::hbdk::hbir::SortOp)#\n\nHBIR tensor sort\n\nSorts the elements of the input tensor along a given dimension in ascending\norder by value.\n\n--------------------------------------------------------------------------------\n\nParameters:\n\n * input: the input tensor.\n * dim: the dimension to sort along.\n * descending: controls the sorting order (ascending or descending).\n * stable: makes the sorting routine stable, which guarantees that the order of\n   equivalent elements is preserved.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: $(N, *)$\n * Values: $(N, *)$ (same shape as input)\n * Indices: $(N, *)$ (same shape as input)\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch sort.\n\nTraits: CommonVerifier, Misc\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE    MLIR TYPE             DESCRIPTION\ndim          ::mlir::IntegerAttr   64-bit signless integer attribute\ndescending   ::mlir::BoolAttr      bool attribute\nstable       ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT    DESCRIPTION\nvalues    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer values or\n          none type\n\n\nhbir.sqrt (::mlir::hbdk::hbir::SqrtOp)#\n\nHBIR tensor sqrt.\n\nApplies square root operator element - wise, $y = \\sqrt{x}$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Returns a new tensor with the square root of the elements of input. If input\n   is negative, then it will return NaN.\n\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.stack (::mlir::hbdk::hbir::StackOp)#\n\nStack multiple tensors along one extra dimension\n\nConcatenates a sequence of tensors along a new dimension.No elemental type\nconversion.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * All tensors need to be of the same size.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch stack.\n\nTraits: CommonVerifier, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninputs    variadic of tensor of 64-bit float or 32-bit float or 16-bit\n          float or bfloat16 type or 8-bit signed integer or 16-bit\n          signed integer or 32-bit signed integer or 64-bit signed\n          integer or 8-bit unsigned integer or 16-bit unsigned integer\n          or 32-bit unsigned integer or 64-bit unsigned integer or or\n          values or none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.sub (::mlir::hbdk::hbir::SubOp)#\n\nHBIR tensor substraction.\n\nApplies substraction operator element-wise, $y_i=lhs_i-rhs_i$.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * Our arithmetic operator support broadcast.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: CalibOp, HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\nlhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\nrhs       tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.tan (::mlir::hbdk::hbir::TanOp)#\n\nHBIR tensor tan.\n\nReturn tensor after the operation, which has the same shape as the input.\nTraits: CommonVerifier, LutLike, SameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.tanh (::mlir::hbdk::hbir::TanhOp)#\n\nHBIR Tanh activation.\n\nApplies the tanh function element - wise.Tanh function is defined as:\n\n$ Tanh(x) =\\frac{exp(x) - exp(-x)} {exp(x) + exp(-x)} $\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * Input: (*), where * means any number of dimensions.\n * Output: (*), same shape as the input.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch tanh.\n\nTraits: CommonVerifier, LutLike, NaiveRoiInfer, NaiveTiling,\nSameOperandsAndResultShape\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nMoveTransposeInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf,\nQuantizable, SchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.tile (::mlir::hbdk::hbir::TileOp)#\n\nConstructs a tensor by tiling a given tensor.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE    MLIR TYPE           DESCRIPTION\nmultiplies   ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type\n\n\nhbir.topk (::mlir::hbdk::hbir::TopkOp)#\n\nHBIR tensor topk\n\nReturns the k largest elements of the given input tensor along a given\ndimension.\n\nIf dim is not given, the last dimension of the input is chosen.\n\nIf largest is False then the k smallest elements are returned.\n\nvalues, indices are returned in separate tensors, where the indices are the\nindices of the elements in the original input tensor.\n\nThe boolean option sorted if True, will make sure that the returned k elements\nare themselves sorted.\n\nTraits: CommonVerifier, Misc, MoveF16CastLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType,\nIndexOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Perf, Quantizable,\nSchedInterface, SchedTemp, ShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE             DESCRIPTION\nk           ::mlir::IntegerAttr   64-bit signless integer attribute\ndim         ::mlir::IntegerAttr   64-bit signless integer attribute\nlargest     ::mlir::BoolAttr      bool attribute\nsorted      ::mlir::BoolAttr      bool attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT    DESCRIPTION\nvalues    tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nindices   tensor of 8-bit signed integer or 16-bit signed integer or\n          32-bit signed integer or 64-bit signed integer values or\n          none type\n\n\nhbir.transpose (::mlir::hbdk::hbir::TransposeOp)#\n\nReverse or permute the dims of an array; returns the modified array.\n\nReturns a tensor that is a view of the original tensor input with its dimesions\npermuted.\n\n--------------------------------------------------------------------------------\n\nNote:\n\n * input: the input tensor.\n * dims: the desired ordering of dimensions.\n\n--------------------------------------------------------------------------------\n\nPrototype: Pytorch permute.\n\nTraits: CommonVerifier, MoveF16CastLike, MoveLike, SameElementType\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, Layout,\nNoMemoryEffect (MemoryEffectOpInterface), NonBatchAxesInfer, Perf, Quantizable,\nRoiInfer, SchedInterface, SchedTemp, ShapeInference, Tiling\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE   MLIR TYPE           DESCRIPTION\ndims        ::mlir::ArrayAttr   64-bit integer array attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or or values\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values\n\n\nhbir.warp (::mlir::hbdk::hbir::WarpOp)#\n\nHBIR warp.\n\nFrom the input, sample(bi - linear interpolation) pixels specified by grid.\n\n--------------------------------------------------------------------------------\n\nShape:\n\n * input: input of shape $(*, H_{in}, W_{in}, C_{in})$, where * represent any\n   number of dimension.\n * grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$\n * output: $(*, H_{out}, W_{out}, C_{in})$\n\nTraits: CommonVerifier, Expansion, Round, SampleLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nAttributes:#\n\nATTRIBUTE       MLIR TYPE                             DESCRIPTION\nmode            ::mlir::hbdk::InterpolationModeAttr   interpolation mode for all march\nexpansionMode   ::mlir::hbdk::ExpansionModeAttr       mode to expand input feature on H/W\npadValue        ::mlir::Attribute                     64-bit float attribute or 64-bit signless integer attribute\n\n\nOperands:#\n\nOPERAND   DESCRIPTION\ninput     tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\nmove      tensor of 64-bit float or 32-bit float or 16-bit float or\n          bfloat16 type or 8-bit signed integer or 16-bit signed\n          integer or 32-bit signed integer or 64-bit signed integer or\n          8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n          unsigned integer or 64-bit unsigned integer or values or\n          none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or values or\n         none type\n\n\nhbir.where (::mlir::hbdk::hbir::WhereOp)#\n\nHBIR where op\n\nReturn a tensor of elements selected from either lhs or rhs, depending on\ncondition.\n\nTraits: Broadcastable, CommonVerifier, EltwiseLike\n\nInterfaces: HBTLExecutable, HbdkExecutorInterface, HbdkInferType, NoMemoryEffect\n(MemoryEffectOpInterface), Perf, Quantizable, SchedInterface, SchedTemp,\nShapeInference\n\nEffects: MemoryEffects::Effect{}\n\n\nOperands:#\n\nOPERAND     DESCRIPTION\ncondition   tensor of values or none type\nlhs         tensor of 64-bit float or 32-bit float or 16-bit float or\n            bfloat16 type or 8-bit signed integer or 16-bit signed\n            integer or 32-bit signed integer or 64-bit signed integer or\n            8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n            unsigned integer or 64-bit unsigned integer or or values or\n            none type\nrhs         tensor of 64-bit float or 32-bit float or 16-bit float or\n            bfloat16 type or 8-bit signed integer or 16-bit signed\n            integer or 32-bit signed integer or 64-bit signed integer or\n            8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n            unsigned integer or 64-bit unsigned integer or or values or\n            none type\n\n\nResults:#\n\nRESULT   DESCRIPTION\noutput   tensor of 64-bit float or 32-bit float or 16-bit float or\n         bfloat16 type or 8-bit signed integer or 16-bit signed\n         integer or 32-bit signed integer or 64-bit signed integer or\n         8-bit unsigned integer or 16-bit unsigned integer or 32-bit\n         unsigned integer or 64-bit unsigned integer or or values or\n         none type","routePath":"/en/guide/appendix/hbir_op","lang":"en","toc":[{"text":"`hbir.abs` (::mlir::hbdk::hbir::AbsOp)","id":"hbirabs-mlirhbdkhbirabsop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands","depth":3,"charIndex":445},{"text":"Results:","id":"results","depth":3,"charIndex":842},{"text":"`hbir.acos` (::mlir::hbdk::hbir::AcosOp)","id":"hbiracos-mlirhbdkhbiracosop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-1","depth":3,"charIndex":1655},{"text":"Results:","id":"results-1","depth":3,"charIndex":2052},{"text":"`hbir.acosh` (::mlir::hbdk::hbir::AcoshOp)","id":"hbiracosh-mlirhbdkhbiracoshop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-2","depth":3,"charIndex":2868},{"text":"Results:","id":"results-2","depth":3,"charIndex":3265},{"text":"`hbir.add` (::mlir::hbdk::hbir::AddOp)","id":"hbiradd-mlirhbdkhbiraddop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-3","depth":3,"charIndex":4309},{"text":"Results:","id":"results-3","depth":3,"charIndex":5073},{"text":"`hbir.asin` (::mlir::hbdk::hbir::AsinOp)","id":"hbirasin-mlirhbdkhbirasinop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-4","depth":3,"charIndex":5889},{"text":"Results:","id":"results-4","depth":3,"charIndex":6286},{"text":"`hbir.asinh` (::mlir::hbdk::hbir::AsinhOp)","id":"hbirasinh-mlirhbdkhbirasinhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-5","depth":3,"charIndex":7102},{"text":"Results:","id":"results-5","depth":3,"charIndex":7499},{"text":"`hbir.atan` (::mlir::hbdk::hbir::AtanOp)","id":"hbiratan-mlirhbdkhbiratanop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-6","depth":3,"charIndex":8312},{"text":"Results:","id":"results-6","depth":3,"charIndex":8709},{"text":"`hbir.atanh` (::mlir::hbdk::hbir::AtanhOp)","id":"hbiratanh-mlirhbdkhbiratanhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-7","depth":3,"charIndex":9525},{"text":"Results:","id":"results-7","depth":3,"charIndex":9922},{"text":"`hbir.avg_pool` (::mlir::hbdk::hbir::AvgPoolOp)","id":"hbiravg_pool-mlirhbdkhbiravgpoolop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes","depth":3,"charIndex":12132},{"text":"Operands:","id":"operands-8","depth":3,"charIndex":12491},{"text":"Results:","id":"results-8","depth":3,"charIndex":12642},{"text":"`hbir.batchnorm` (::mlir::hbdk::hbir::BatchNormOp)","id":"hbirbatchnorm-mlirhbdkhbirbatchnormop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-1","depth":3,"charIndex":14050},{"text":"Operands:","id":"operands-9","depth":3,"charIndex":14165},{"text":"Results:","id":"results-9","depth":3,"charIndex":16018},{"text":"`hbir.bev_pool_v2` (::mlir::hbdk::hbir::BevPoolV2Op)","id":"hbirbev_pool_v2-mlirhbdkhbirbevpoolv2op","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-2","depth":3,"charIndex":17999},{"text":"Operands:","id":"operands-10","depth":3,"charIndex":18132},{"text":"Results:","id":"results-10","depth":3,"charIndex":20647},{"text":"`hbir.cast_type` (::mlir::hbdk::hbir::CastTypeOp)","id":"hbircast_type-mlirhbdkhbircasttypeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-3","depth":3,"charIndex":21493},{"text":"Operands:","id":"operands-11","depth":3,"charIndex":21606},{"text":"Results:","id":"results-11","depth":3,"charIndex":22006},{"text":"`hbir.ceil` (::mlir::hbdk::hbir::CeilOp)","id":"hbirceil-mlirhbdkhbirceilop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-12","depth":3,"charIndex":22804},{"text":"Results:","id":"results-12","depth":3,"charIndex":23201},{"text":"`hbir.clip` (::mlir::hbdk::hbir::ClipOp)","id":"hbirclip-mlirhbdkhbirclipop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-4","depth":3,"charIndex":24678},{"text":"Operands:","id":"operands-13","depth":3,"charIndex":24922},{"text":"Results:","id":"results-13","depth":3,"charIndex":25296},{"text":"`hbir.concat` (::mlir::hbdk::hbir::ConcatOp)","id":"hbirconcat-mlirhbdkhbirconcatop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-5","depth":3,"charIndex":26477},{"text":"Operands:","id":"operands-14","depth":3,"charIndex":26607},{"text":"Results:","id":"results-14","depth":3,"charIndex":27019},{"text":"`hbir.constant` (::mlir::hbdk::hbir::ConstantOp)","id":"hbirconstant-mlirhbdkhbirconstantop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-6","depth":3,"charIndex":27790},{"text":"Results:","id":"results-15","depth":3,"charIndex":27921},{"text":"`hbir.conv2d` (::mlir::hbdk::hbir::Conv2dOp)","id":"hbirconv2d-mlirhbdkhbirconv2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-7","depth":3,"charIndex":30754},{"text":"Operands:","id":"operands-15","depth":3,"charIndex":31151},{"text":"Results:","id":"results-16","depth":3,"charIndex":31535},{"text":"`hbir.conv2dtranspose` (::mlir::hbdk::hbir::Conv2dTransposeOp)","id":"hbirconv2dtranspose-mlirhbdkhbirconv2dtransposeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-8","depth":3,"charIndex":32845},{"text":"Operands:","id":"operands-16","depth":3,"charIndex":33315},{"text":"Results:","id":"results-17","depth":3,"charIndex":33686},{"text":"`hbir.conv3d` (::mlir::hbdk::hbir::Conv3dOp)","id":"hbirconv3d-mlirhbdkhbirconv3dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-9","depth":3,"charIndex":36408},{"text":"Operands:","id":"operands-17","depth":3,"charIndex":36805},{"text":"Results:","id":"results-18","depth":3,"charIndex":37189},{"text":"`hbir.conv` (::mlir::hbdk::hbir::ConvOp)","id":"hbirconv-mlirhbdkhbirconvop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-10","depth":3,"charIndex":40031},{"text":"Operands:","id":"operands-18","depth":3,"charIndex":40417},{"text":"Results:","id":"results-19","depth":3,"charIndex":40798},{"text":"`hbir.cos` (::mlir::hbdk::hbir::CosOp)","id":"hbircos-mlirhbdkhbircosop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-19","depth":3,"charIndex":41366},{"text":"Results:","id":"results-20","depth":3,"charIndex":41763},{"text":"`hbir.cosh` (::mlir::hbdk::hbir::CoshOp)","id":"hbircosh-mlirhbdkhbircoshop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-20","depth":3,"charIndex":42576},{"text":"Results:","id":"results-21","depth":3,"charIndex":42973},{"text":"`hbir.cumsum` (::mlir::hbdk::hbir::CumSumOp)","id":"hbircumsum-mlirhbdkhbircumsumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-11","depth":3,"charIndex":44656},{"text":"Operands:","id":"operands-21","depth":3,"charIndex":44922},{"text":"Results:","id":"results-22","depth":3,"charIndex":45322},{"text":"`hbir.deform_conv2d` (::mlir::hbdk::hbir::DeformConv2dOp)","id":"hbirdeform_conv2d-mlirhbdkhbirdeformconv2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-12","depth":3,"charIndex":48387},{"text":"Operands:","id":"operands-22","depth":3,"charIndex":48936},{"text":"Results:","id":"results-23","depth":3,"charIndex":49550},{"text":"`hbir.div` (::mlir::hbdk::hbir::DivOp)","id":"hbirdiv-mlirhbdkhbirdivop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-23","depth":3,"charIndex":50347},{"text":"Results:","id":"results-24","depth":3,"charIndex":51111},{"text":"`hbir.equal` (::mlir::hbdk::hbir::EqualOp)","id":"hbirequal-mlirhbdkhbirequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-24","depth":3,"charIndex":51942},{"text":"Results:","id":"results-25","depth":3,"charIndex":52706},{"text":"`hbir.exp` (::mlir::hbdk::hbir::ExpOp)","id":"hbirexp-mlirhbdkhbirexpop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-25","depth":3,"charIndex":53364},{"text":"Results:","id":"results-26","depth":3,"charIndex":53761},{"text":"`hbir.fake_cast` (::mlir::hbdk::hbir::FakeCastOp)","id":"hbirfake_cast-mlirhbdkhbirfakecastop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-13","depth":3,"charIndex":54607},{"text":"Operands:","id":"operands-26","depth":3,"charIndex":54716},{"text":"Results:","id":"results-27","depth":3,"charIndex":54821},{"text":"`hbir.flip` (::mlir::hbdk::hbir::FlipOp)","id":"hbirflip-mlirhbdkhbirflipop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-14","depth":3,"charIndex":55633},{"text":"Operands:","id":"operands-27","depth":3,"charIndex":55756},{"text":"Results:","id":"results-28","depth":3,"charIndex":56156},{"text":"`hbir.floor` (::mlir::hbdk::hbir::FloorOp)","id":"hbirfloor-mlirhbdkhbirfloorop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-28","depth":3,"charIndex":56979},{"text":"Results:","id":"results-29","depth":3,"charIndex":57376},{"text":"`hbir.gelu` (::mlir::hbdk::hbir::GELUOp)","id":"hbirgelu-mlirhbdkhbirgeluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-29","depth":3,"charIndex":58686},{"text":"Results:","id":"results-30","depth":3,"charIndex":59083},{"text":"`hbir.gather_elements` (::mlir::hbdk::hbir::GatherElementsOp)","id":"hbirgather_elements-mlirhbdkhbirgatherelementsop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-15","depth":3,"charIndex":59904},{"text":"Operands:","id":"operands-30","depth":3,"charIndex":60034},{"text":"Results:","id":"results-31","depth":3,"charIndex":60717},{"text":"`hbir.gather_nd` (::mlir::hbdk::hbir::GatherNdOp)","id":"hbirgather_nd-mlirhbdkhbirgatherndop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-16","depth":3,"charIndex":61508},{"text":"Operands:","id":"operands-31","depth":3,"charIndex":61638},{"text":"Results:","id":"results-32","depth":3,"charIndex":62321},{"text":"`hbir.greater_equal` (::mlir::hbdk::hbir::GreaterEqualOp)","id":"hbirgreater_equal-mlirhbdkhbirgreaterequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-32","depth":3,"charIndex":63130},{"text":"Results:","id":"results-33","depth":3,"charIndex":63888},{"text":"`hbir.greater` (::mlir::hbdk::hbir::GreaterOp)","id":"hbirgreater-mlirhbdkhbirgreaterop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-33","depth":3,"charIndex":64375},{"text":"Results:","id":"results-34","depth":3,"charIndex":65133},{"text":"`hbir.grid_sample` (::mlir::hbdk::hbir::GridSampleOp)","id":"hbirgrid_sample-mlirhbdkhbirgridsampleop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-17","depth":3,"charIndex":65946},{"text":"Operands:","id":"operands-34","depth":3,"charIndex":66388},{"text":"Results:","id":"results-35","depth":3,"charIndex":67146},{"text":"`hbir.index` (::mlir::hbdk::hbir::IndexOp)","id":"hbirindex-mlirhbdkhbirindexop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-18","depth":3,"charIndex":68419},{"text":"Operands:","id":"operands-35","depth":3,"charIndex":68549},{"text":"Results:","id":"results-36","depth":3,"charIndex":69232},{"text":"`hbir.layernorm` (::mlir::hbdk::hbir::LayerNormOp)","id":"hbirlayernorm-mlirhbdkhbirlayernormop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-19","depth":3,"charIndex":70846},{"text":"Operands:","id":"operands-36","depth":3,"charIndex":71024},{"text":"Results:","id":"results-37","depth":3,"charIndex":72143},{"text":"`hbir.leaky_relu` (::mlir::hbdk::hbir::LeakyReLUOp)","id":"hbirleaky_relu-mlirhbdkhbirleakyreluop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-20","depth":3,"charIndex":73508},{"text":"Operands:","id":"operands-37","depth":3,"charIndex":73623},{"text":"Results:","id":"results-38","depth":3,"charIndex":73997},{"text":"`hbir.less_equal` (::mlir::hbdk::hbir::LessEqualOp)","id":"hbirless_equal-mlirhbdkhbirlessequalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-38","depth":3,"charIndex":74813},{"text":"Results:","id":"results-39","depth":3,"charIndex":75571},{"text":"`hbir.less` (::mlir::hbdk::hbir::LessOp)","id":"hbirless-mlirhbdkhbirlessop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-39","depth":3,"charIndex":76046},{"text":"Results:","id":"results-40","depth":3,"charIndex":76804},{"text":"`hbir.linear` (::mlir::hbdk::hbir::LinearOp)","id":"hbirlinear-mlirhbdkhbirlinearop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-40","depth":3,"charIndex":77804},{"text":"Results:","id":"results-41","depth":3,"charIndex":78188},{"text":"`hbir.log` (::mlir::hbdk::hbir::LogOp)","id":"hbirlog-mlirhbdkhbirlogop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-41","depth":3,"charIndex":78907},{"text":"Results:","id":"results-42","depth":3,"charIndex":79304},{"text":"`hbir.log_softmax` (::mlir::hbdk::hbir::LogSoftmaxOp)","id":"hbirlog_softmax-mlirhbdkhbirlogsoftmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-21","depth":3,"charIndex":80724},{"text":"Operands:","id":"operands-42","depth":3,"charIndex":80854},{"text":"Results:","id":"results-43","depth":3,"charIndex":81228},{"text":"`hbir.logical_and` (::mlir::hbdk::hbir::LogicalAndOp)","id":"hbirlogical_and-mlirhbdkhbirlogicalandop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-43","depth":3,"charIndex":82066},{"text":"Results:","id":"results-44","depth":3,"charIndex":82338},{"text":"`hbir.logical_not` (::mlir::hbdk::hbir::LogicalNotOp)","id":"hbirlogical_not-mlirhbdkhbirlogicalnotop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-44","depth":3,"charIndex":82823},{"text":"Results:","id":"results-45","depth":3,"charIndex":82977},{"text":"`hbir.logical_or` (::mlir::hbdk::hbir::LogicalOrOp)","id":"hbirlogical_or-mlirhbdkhbirlogicalorop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-45","depth":3,"charIndex":83495},{"text":"Results:","id":"results-46","depth":3,"charIndex":83767},{"text":"`hbir.matmul` (::mlir::hbdk::hbir::MatMulOp)","id":"hbirmatmul-mlirhbdkhbirmatmulop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-46","depth":3,"charIndex":85380},{"text":"Results:","id":"results-47","depth":3,"charIndex":85646},{"text":"`hbir.max` (::mlir::hbdk::hbir::MaxOp)","id":"hbirmax-mlirhbdkhbirmaxop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-47","depth":3,"charIndex":86335},{"text":"Results:","id":"results-48","depth":3,"charIndex":87099},{"text":"`hbir.max_pool` (::mlir::hbdk::hbir::MaxPoolOp)","id":"hbirmax_pool-mlirhbdkhbirmaxpoolop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-22","depth":3,"charIndex":89332},{"text":"Operands:","id":"operands-48","depth":3,"charIndex":89691},{"text":"Results:","id":"results-49","depth":3,"charIndex":89842},{"text":"`hbir.min` (::mlir::hbdk::hbir::MinOp)","id":"hbirmin-mlirhbdkhbirminop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-49","depth":3,"charIndex":90544},{"text":"Results:","id":"results-50","depth":3,"charIndex":91308},{"text":"`hbir.mod` (::mlir::hbdk::hbir::ModOp)","id":"hbirmod-mlirhbdkhbirmodop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-23","depth":3,"charIndex":92346},{"text":"Operands:","id":"operands-50","depth":3,"charIndex":92469},{"text":"Results:","id":"results-51","depth":3,"charIndex":93227},{"text":"`hbir.mul` (::mlir::hbdk::hbir::MulOp)","id":"hbirmul-mlirhbdkhbirmulop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-51","depth":3,"charIndex":94292},{"text":"Results:","id":"results-52","depth":3,"charIndex":95056},{"text":"`hbir.nan_to_num` (::mlir::hbdk::hbir::NanToNumOp)","id":"hbirnan_to_num-mlirhbdkhbirnantonumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-24","depth":3,"charIndex":96107},{"text":"Operands:","id":"operands-52","depth":3,"charIndex":96332},{"text":"Results:","id":"results-53","depth":3,"charIndex":96480},{"text":"`hbir.neg` (::mlir::hbdk::hbir::NegOp)","id":"hbirneg-mlirhbdkhbirnegop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-53","depth":3,"charIndex":97212},{"text":"Results:","id":"results-54","depth":3,"charIndex":97609},{"text":"`hbir.nonzero` (::mlir::hbdk::hbir::NonZeroOp)","id":"hbirnonzero-mlirhbdkhbirnonzeroop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-54","depth":3,"charIndex":98358},{"text":"Results:","id":"results-55","depth":3,"charIndex":98758},{"text":"`hbir.pad` (::mlir::hbdk::hbir::PadOp)","id":"hbirpad-mlirhbdkhbirpadop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-25","depth":3,"charIndex":99430},{"text":"Operands:","id":"operands-55","depth":3,"charIndex":99931},{"text":"Results:","id":"results-56","depth":3,"charIndex":100308},{"text":"`hbir.point_pillar_preprocess` (::mlir::hbdk::hbir::PointPillarPreProcessOp)","id":"hbirpoint_pillar_preprocess-mlirhbdkhbirpointpillarpreprocessop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-26","depth":3,"charIndex":101102},{"text":"Operands:","id":"operands-56","depth":3,"charIndex":101610},{"text":"Results:","id":"results-57","depth":3,"charIndex":102010},{"text":"`hbir.pow` (::mlir::hbdk::hbir::PowOp)","id":"hbirpow-mlirhbdkhbirpowop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-57","depth":3,"charIndex":103188},{"text":"Results:","id":"results-58","depth":3,"charIndex":103952},{"text":"`hbir.prelu` (::mlir::hbdk::hbir::PreluOp)","id":"hbirprelu-mlirhbdkhbirpreluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-58","depth":3,"charIndex":104715},{"text":"Results:","id":"results-59","depth":3,"charIndex":105473},{"text":"`hbir.relu` (::mlir::hbdk::hbir::ReLUOp)","id":"hbirrelu-mlirhbdkhbirreluop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-59","depth":3,"charIndex":106704},{"text":"Results:","id":"results-60","depth":3,"charIndex":107101},{"text":"`hbir.reciprocal` (::mlir::hbdk::hbir::ReciprocalOp)","id":"hbirreciprocal-mlirhbdkhbirreciprocalop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-60","depth":3,"charIndex":108086},{"text":"Results:","id":"results-61","depth":3,"charIndex":108483},{"text":"`hbir.reduce_all` (::mlir::hbdk::hbir::ReduceAllOp)","id":"hbirreduce_all-mlirhbdkhbirreduceallop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-27","depth":3,"charIndex":110021},{"text":"Operands:","id":"operands-61","depth":3,"charIndex":110191},{"text":"Results:","id":"results-62","depth":3,"charIndex":110464},{"text":"`hbir.reduce_argmax` (::mlir::hbdk::hbir::ReduceArgmaxOp)","id":"hbirreduce_argmax-mlirhbdkhbirreduceargmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-28","depth":3,"charIndex":111803},{"text":"Operands:","id":"operands-62","depth":3,"charIndex":111973},{"text":"Results:","id":"results-63","depth":3,"charIndex":112370},{"text":"`hbir.reduce_argmin` (::mlir::hbdk::hbir::ReduceArgminOp)","id":"hbirreduce_argmin-mlirhbdkhbirreduceargminop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-29","depth":3,"charIndex":113948},{"text":"Operands:","id":"operands-63","depth":3,"charIndex":114118},{"text":"Results:","id":"results-64","depth":3,"charIndex":114515},{"text":"`hbir.reduce_max` (::mlir::hbdk::hbir::ReduceMaxOp)","id":"hbirreduce_max-mlirhbdkhbirreducemaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-30","depth":3,"charIndex":116087},{"text":"Operands:","id":"operands-64","depth":3,"charIndex":116257},{"text":"Results:","id":"results-65","depth":3,"charIndex":116654},{"text":"`hbir.reduce_mean` (::mlir::hbdk::hbir::ReduceMeanOp)","id":"hbirreduce_mean-mlirhbdkhbirreducemeanop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-31","depth":3,"charIndex":118286},{"text":"Operands:","id":"operands-65","depth":3,"charIndex":118456},{"text":"Results:","id":"results-66","depth":3,"charIndex":118853},{"text":"`hbir.reduce_min` (::mlir::hbdk::hbir::ReduceMinOp)","id":"hbirreduce_min-mlirhbdkhbirreduceminop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-32","depth":3,"charIndex":120501},{"text":"Operands:","id":"operands-66","depth":3,"charIndex":120671},{"text":"Results:","id":"results-67","depth":3,"charIndex":121068},{"text":"`hbir.reduce_sum` (::mlir::hbdk::hbir::ReduceSumOp)","id":"hbirreduce_sum-mlirhbdkhbirreducesumop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-33","depth":3,"charIndex":122694},{"text":"Operands:","id":"operands-67","depth":3,"charIndex":122864},{"text":"Results:","id":"results-68","depth":3,"charIndex":123261},{"text":"`hbir.reshape` (::mlir::hbdk::hbir::ReshapeOp)","id":"hbirreshape-mlirhbdkhbirreshapeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-34","depth":3,"charIndex":124449},{"text":"Operands:","id":"operands-68","depth":3,"charIndex":124619},{"text":"Results:","id":"results-69","depth":3,"charIndex":125019},{"text":"`hbir.resize2d` (::mlir::hbdk::hbir::Resize2dOp)","id":"hbirresize2d-mlirhbdkhbirresize2dop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-35","depth":3,"charIndex":126344},{"text":"Operands:","id":"operands-69","depth":3,"charIndex":127147},{"text":"Results:","id":"results-70","depth":3,"charIndex":127544},{"text":"`hbir.roll` (::mlir::hbdk::hbir::RollOp)","id":"hbirroll-mlirhbdkhbirrollop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-36","depth":3,"charIndex":128789},{"text":"Operands:","id":"operands-70","depth":3,"charIndex":128975},{"text":"Results:","id":"results-71","depth":3,"charIndex":129375},{"text":"`hbir.round` (::mlir::hbdk::hbir::RoundOp)","id":"hbirround-mlirhbdkhbirroundop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-37","depth":3,"charIndex":130382},{"text":"Operands:","id":"operands-71","depth":3,"charIndex":130512},{"text":"Results:","id":"results-72","depth":3,"charIndex":130909},{"text":"`hbir.rsqrt` (::mlir::hbdk::hbir::RsqrtOp)","id":"hbirrsqrt-mlirhbdkhbirrsqrtop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-72","depth":3,"charIndex":131725},{"text":"Results:","id":"results-73","depth":3,"charIndex":132122},{"text":"`hbir.scatter_elements` (::mlir::hbdk::hbir::ScatterElementsOp)","id":"hbirscatter_elements-mlirhbdkhbirscatterelementsop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-38","depth":3,"charIndex":133307},{"text":"Operands:","id":"operands-73","depth":3,"charIndex":133563},{"text":"Results:","id":"results-74","depth":3,"charIndex":134610},{"text":"`hbir.scatter_nd` (::mlir::hbdk::hbir::ScatterNDOp)","id":"hbirscatter_nd-mlirhbdkhbirscatterndop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-39","depth":3,"charIndex":135600},{"text":"Operands:","id":"operands-74","depth":3,"charIndex":135764},{"text":"Results:","id":"results-75","depth":3,"charIndex":136742},{"text":"`hbir.select` (::mlir::hbdk::hbir::SelectOp)","id":"hbirselect-mlirhbdkhbirselectop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-40","depth":3,"charIndex":137926},{"text":"Operands:","id":"operands-75","depth":3,"charIndex":138124},{"text":"Results:","id":"results-76","depth":3,"charIndex":138524},{"text":"`hbir.sigmoid` (::mlir::hbdk::hbir::SigmoidOp)","id":"hbirsigmoid-mlirhbdkhbirsigmoidop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-76","depth":3,"charIndex":139715},{"text":"Results:","id":"results-77","depth":3,"charIndex":140112},{"text":"`hbir.sign` (::mlir::hbdk::hbir::SignOp)","id":"hbirsign-mlirhbdkhbirsignop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-77","depth":3,"charIndex":140946},{"text":"Results:","id":"results-78","depth":3,"charIndex":141343},{"text":"`hbir.sin` (::mlir::hbdk::hbir::SinOp)","id":"hbirsin-mlirhbdkhbirsinop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-78","depth":3,"charIndex":142153},{"text":"Results:","id":"results-79","depth":3,"charIndex":142550},{"text":"`hbir.sinh` (::mlir::hbdk::hbir::SinhOp)","id":"hbirsinh-mlirhbdkhbirsinhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-79","depth":3,"charIndex":143363},{"text":"Results:","id":"results-80","depth":3,"charIndex":143760},{"text":"`hbir.slice` (::mlir::hbdk::hbir::SliceOp)","id":"hbirslice-mlirhbdkhbirsliceop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-41","depth":3,"charIndex":144998},{"text":"Operands:","id":"operands-80","depth":3,"charIndex":145294},{"text":"Results:","id":"results-81","depth":3,"charIndex":145694},{"text":"`hbir.slice_scatter` (::mlir::hbdk::hbir::SliceScatterOp)","id":"hbirslice_scatter-mlirhbdkhbirslicescatterop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-42","depth":3,"charIndex":147079},{"text":"Operands:","id":"operands-81","depth":3,"charIndex":147413},{"text":"Results:","id":"results-82","depth":3,"charIndex":148177},{"text":"`hbir.softmax` (::mlir::hbdk::hbir::SoftmaxOp)","id":"hbirsoftmax-mlirhbdkhbirsoftmaxop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-43","depth":3,"charIndex":149526},{"text":"Operands:","id":"operands-82","depth":3,"charIndex":149656},{"text":"Results:","id":"results-83","depth":3,"charIndex":150053},{"text":"`hbir.softplus` (::mlir::hbdk::hbir::SoftplusOp)","id":"hbirsoftplus-mlirhbdkhbirsoftplusop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-44","depth":3,"charIndex":151550},{"text":"Operands:","id":"operands-83","depth":3,"charIndex":151720},{"text":"Results:","id":"results-84","depth":3,"charIndex":152117},{"text":"`hbir.sort` (::mlir::hbdk::hbir::SortOp)","id":"hbirsort-mlirhbdkhbirsortop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-45","depth":3,"charIndex":153561},{"text":"Operands:","id":"operands-84","depth":3,"charIndex":153793},{"text":"Results:","id":"results-85","depth":3,"charIndex":154190},{"text":"`hbir.sqrt` (::mlir::hbdk::hbir::SqrtOp)","id":"hbirsqrt-mlirhbdkhbirsqrtop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-85","depth":3,"charIndex":155365},{"text":"Results:","id":"results-86","depth":3,"charIndex":155762},{"text":"`hbir.stack` (::mlir::hbdk::hbir::StackOp)","id":"hbirstack-mlirhbdkhbirstackop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-46","depth":3,"charIndex":156855},{"text":"Operands:","id":"operands-86","depth":3,"charIndex":156985},{"text":"Results:","id":"results-87","depth":3,"charIndex":157397},{"text":"`hbir.sub` (::mlir::hbdk::hbir::SubOp)","id":"hbirsub-mlirhbdkhbirsubop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-87","depth":3,"charIndex":158345},{"text":"Results:","id":"results-88","depth":3,"charIndex":159109},{"text":"`hbir.tan` (::mlir::hbdk::hbir::TanOp)","id":"hbirtan-mlirhbdkhbirtanop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-88","depth":3,"charIndex":159922},{"text":"Results:","id":"results-89","depth":3,"charIndex":160319},{"text":"`hbir.tanh` (::mlir::hbdk::hbir::TanhOp)","id":"hbirtanh-mlirhbdkhbirtanhop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-89","depth":3,"charIndex":161513},{"text":"Results:","id":"results-90","depth":3,"charIndex":161910},{"text":"`hbir.tile` (::mlir::hbdk::hbir::TileOp)","id":"hbirtile-mlirhbdkhbirtileop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-47","depth":3,"charIndex":162660},{"text":"Operands:","id":"operands-90","depth":3,"charIndex":162785},{"text":"Results:","id":"results-91","depth":3,"charIndex":163185},{"text":"`hbir.topk` (::mlir::hbdk::hbir::TopkOp)","id":"hbirtopk-mlirhbdkhbirtopkop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-48","depth":3,"charIndex":164352},{"text":"Operands:","id":"operands-91","depth":3,"charIndex":164648},{"text":"Results:","id":"results-92","depth":3,"charIndex":165045},{"text":"`hbir.transpose` (::mlir::hbdk::hbir::TransposeOp)","id":"hbirtranspose-mlirhbdkhbirtransposeop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-49","depth":3,"charIndex":166401},{"text":"Operands:","id":"operands-92","depth":3,"charIndex":166524},{"text":"Results:","id":"results-93","depth":3,"charIndex":166901},{"text":"`hbir.warp` (::mlir::hbdk::hbir::WarpOp)","id":"hbirwarp-mlirhbdkhbirwarpop","depth":2,"charIndex":-1},{"text":"Attributes:","id":"attributes-50","depth":3,"charIndex":167954},{"text":"Operands:","id":"operands-93","depth":3,"charIndex":168327},{"text":"Results:","id":"results-94","depth":3,"charIndex":169085},{"text":"`hbir.where` (::mlir::hbdk::hbir::WhereOp)","id":"hbirwhere-mlirhbdkhbirwhereop","depth":2,"charIndex":-1},{"text":"Operands:","id":"operands-94","depth":3,"charIndex":169875},{"text":"Results:","id":"results-95","depth":3,"charIndex":170707}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":50,"title":"ONNX Operator BPU Constraint List","content":"#\n\nIn the table below:\n\nlhs: left-hand side, the left-hand side in the operation.\n\nrhs: right-hand side, the right-hand side in the operation.","routePath":"/en/guide/appendix/supported_op_list/onnx_operator_support_list","lang":"en","toc":[],"domain":"","frontmatter":{"outline":false},"version":"3.0.22"},{"id":51,"title":"","content":"","routePath":"/en/guide/appendix/supported_op_list/onnx_operator_support_list_html","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":52,"title":"","content":"In the table below:\n\nlhs: left-hand side, the left-hand side in the operation.\n\nrhs: right-hand side, the right-hand side in the operation.","routePath":"/en/guide/appendix/supported_op_list/onnx_operator_support_list_remark","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":53,"title":"Torch Operator BPU Constraint List","content":"#\n\nAttention\n\nThe following alias substitutions are made by default below:\n\n\n\nIn the table below:\n\nlhs: left-hand side, the left-hand side in the operation.\n\nrhs: right-hand side, the right-hand side in the operation.","routePath":"/en/guide/appendix/supported_op_list/torch_operator_support_list","lang":"en","toc":[],"domain":"","frontmatter":{"outline":false},"version":"3.0.22"},{"id":54,"title":"","content":"","routePath":"/en/guide/appendix/supported_op_list/torch_operator_support_list_html","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":55,"title":"","content":"Attention\n\nThe following alias substitutions are made by default below:\n\n\n\nIn the table below:\n\nlhs: left-hand side, the left-hand side in the operation.\n\nrhs: right-hand side, the right-hand side in the operation.","routePath":"/en/guide/appendix/supported_op_list/torch_operator_support_list_remark","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":56,"title":"Benchmark of Model Performance","content":"#\n\n\nDescriptions#\n\n * Test Conditions:\n   \n   * Test Board: J6E.\n   \n   * Number of Test Cores: Latency single core, FPS dual core.\n   \n   * Frequency to obtain model performance data: Average of performance\n     parameters over a 5-minute period.\n   \n   * Python version: Python 3.10.\n   \n   * Models: Models under the samples/ucp_tutorial/dnn/ai_benchmark/j6 path in\n     the OE package.\n\n * Table Header Acronyms:\n   \n   * C = Computation, in GOPs (i.e., billion operations per second), obtained by\n     calling the hbm_perf interface.\n   \n   * FPS = Frame(s) Per Second, obtained by running the fps.sh script with\n     single thread of different models in the ai_benchmark sample package/script\n     on the dev board. Post-processing included.\n   \n   * ITC = Inference Time Consumption, in ms (millisecond), obtained by running\n     the latency.sh script with single thread of different models in the\n     ai_benchmark sample package/script on the dev board. Post-processing not\n     included.\n   \n   * TCPP = Postprocess Time Consumption, in ms (millisecond), obtained by\n     running the latency.sh script with single thread of different models in the\n     ai_benchmark sample package/script on the dev board.\n   \n   * RV = Read Volume in a single inference, in mb (Mbit), obtained by calling\n     the hbm_perf interface.\n   \n   * WV = Write Volume in a single inference, in mb (Mbit), obtained by calling\n     the hbm_perf interface.\n\n\nModel Key Performance Data#\n\n\nModel Full Performance Data#\n\n\nMobileNetv1#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 1.14\n\n * FPS: 5087.50\n\n * ITC(ms): 0.455\n\n * TCPP(ms): 0.034\n\n * RV(mb): 4.56\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7374(FLOAT)/0.7298(INT8)\n\n\nMobileNetv2#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 0.63\n\n * FPS: 4662.30\n\n * ITC(ms): 0.480\n\n * TCPP(ms): 0.034\n\n * RV(mb): 3.95\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7217(FLOAT)/0.7147(INT8)\n\n\nResNet50#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 7.72\n\n * FPS: 1115.10\n\n * ITC(ms): 1.165\n\n * TCPP(ms): 0.034\n\n * RV(mb): 26.08\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7703(FLOAT)/0.7678(INT8)\n\n\nGoogleNet#\n\n * INPUT SIZE: 1x3x224x224\n * C(GOPs): 3.00\n * FPS: 2627.10\n * ITC(ms): 0.687\n * TCPP(ms): 0.034\n * RV(mb): 6.93\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7018(FLOAT)/0.6992(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/GoogleNet\n\n\nEfficientNet_Lite0#\n\n * INPUT SIZE: 1x224x224x3\n * C(GOPs): 0.77\n * FPS: 3959.00\n * ITC(ms): 0.562\n * TCPP(ms): 0.034\n * RV(mb): 5.21\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7479(FLOAT)/0.7452(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite1#\n\n * INPUT SIZE: 1x240x240x3\n * C(GOPs): 1.20\n * FPS: 3057.00\n * ITC(ms): 0.637\n * TCPP(ms): 0.034\n * RV(mb): 6.11\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7652(FLOAT)/0.7612(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite2#\n\n * INPUT SIZE: 1x260x260x3\n * C(GOPs): 1.72\n * FPS: 2327.70\n * ITC(ms): 0.736\n * TCPP(ms): 0.034\n * RV(mb): 6.95\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7734(FLOAT)/0.7696(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite3#\n\n * INPUT SIZE: 1x280x280x3\n * C(GOPs): 2.77\n * FPS: 1755.20\n * ITC(ms): 0.883\n * TCPP(ms): 0.033\n * RV(mb): 9.39\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7917(FLOAT)/0.7891(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nEfficientNet_Lite4#\n\n * INPUT SIZE: 1x300x300x3\n * C(GOPs): 5.11\n * FPS: 1234.70\n * ITC(ms): 1.121\n * TCPP(ms): 0.034\n * RV(mb): 14.47\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.8063(FLOAT)/0.8047(INT8)\n * LINKS:\n   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/li\n   te\n\n\nVargconvnet#\n\n * INPUT SIZE: 1x3x224x224\n * C(GOPs): 9.06\n * FPS: 1438.00\n * ITC(ms): 0.996\n * TCPP(ms): 0.034\n * RV(mb): 10.44\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7793(FLOAT)/0.7764(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/VargConvNet\n\n\nEfficientnasnet_m#\n\n * INPUT SIZE: 1x3x300x300\n * C(GOPs): 4.53\n * FPS: 1418.00\n * ITC(ms): 0.999\n * TCPP(ms): 0.034\n * RV(mb): 13.76\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7935(FLOAT)/0.7920(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientnas\n   Net\n\n\nEfficientnasnet_s#\n\n * INPUT SIZE: 1x3x280x280\n * C(GOPs): 1.44\n * FPS: 3250.80\n * ITC(ms): 0.598\n * TCPP(ms): 0.034\n * RV(mb): 5.45\n * WV(mb): 0.0041\n * Dataset: ImageNet\n * ACCURACY: Top1: 0.7441(FLOAT)/0.7528(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientnas\n   Net\n\n\nResNet18#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 3.63\n\n * FPS: 2350.10\n\n * ITC(ms): 0.695\n\n * TCPP(ms): 0.034\n\n * RV(mb): 11.87\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7169(FLOAT)/0.7164(INT8)\n\n\nYOLOv2_Darknet19#\n\n * INPUT SIZE: 1x3x608x608\n * C(GOPs): 62.94\n * FPS: 224.52\n * ITC(ms): 4.746\n * TCPP(ms): 0.299\n * RV(mb): 51.86\n * WV(mb): 0.77\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.2760(FLOAT)/0.2700(INT8)\n * LINKS: https://pjreddie.com/darknet/yolo\n\n\nYOLOv3_Darknet53#\n\n * INPUT SIZE: 1x3x416x416\n * C(GOPs): 65.87\n * FPS: 203.94\n * ITC(ms): 5.238\n * TCPP(ms): 1.753\n * RV(mb): 69.24\n * WV(mb): 9.64\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.3370(FLOAT)/0.3360(INT8)\n * LINKS: https://github.com/ChenYingpeng/caffe-yolov3/\n\n\nyolov5x_v2.0#\n\n * INPUT SIZE: 1x3x672x672\n * C(GOPs): 243.86\n * FPS: 59.90\n * ITC(ms): 17.092\n * TCPP(ms): 5.873\n * RV(mb): 145.54\n * WV(mb): 51.90\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.4810(FLOAT)/0.4670(INT8)\n * LINKS: https://github.com/ultralytics/yolov5/releases/tag/v2.0\n\n\nSSD_MobileNetv1#\n\n * INPUT SIZE: 1x3x300x300\n * C(GOPs): 2.30\n * FPS: 2296.80\n * ITC(ms): 0.843\n * TCPP(ms): 0.199\n * RV(mb): 6.28\n * WV(mb): 0.25\n * Dataset: VOC\n * ACCURACY: mAP: 0.7345(FLOAT)/0.7262(INT8)\n * LINKS: https://github.com/chuanqi305/MobileNet-SSD\n\n\nYOLOv3_VargDarknet#\n\n * INPUT SIZE: 1x3x416x416\n * C(GOPs): 42.82\n * FPS: 295.50\n * ITC(ms): 3.725\n * TCPP(ms): 1.765\n * RV(mb): 46.53\n * WV(mb): 4.91\n * Dataset: COCO\n * ACCURACY: [IoU=0.50:0.95]= 0.3280(FLOAT)/0.3270(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Yolov3_VargD\n   arknet\n\n\nDeeplabv3plus_efficientnetb0#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 30.80\n * FPS: 148.04\n * ITC(ms): 7.106\n * TCPP(ms): 0.312\n * RV(mb): 18.86\n * WV(mb): 10.49\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7630(FLOAT)/0.7570(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nFastscnn_efficientnetb0#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 12.52\n * FPS: 247.44\n * ITC(ms): 4.384\n * TCPP(ms): 0.309\n * RV(mb): 14.20\n * WV(mb): 9.57\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.6997(FLOAT)/0.6911(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/FastSCNN\n\n\nDeeplabv3plus_efficientnetm1#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 77.08\n * FPS: 91.03\n * ITC(ms): 11.356\n * TCPP(ms): 0.303\n * RV(mb): 74.71\n * WV(mb): 54.00\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7794(FLOAT)/0.7754(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nDeeplabv3plus_efficientnetm2#\n\n * INPUT SIZE: 1x3x1024x2048\n * C(GOPs): 124.19\n * FPS: 64.83\n * ITC(ms): 15.794\n * TCPP(ms): 0.305\n * RV(mb): 142.30\n * WV(mb): 81.26\n * Dataset: Cityscapes\n * ACCURACY: mIoU: 0.7882(FLOAT)/0.7854(INT8)\n * LINKS:\n   https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Plu\n   s\n\n\nBev_gkt_mixvargenet_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points(0-8): 6x64x64x2\n\n * C(GOPs): 206.95\n\n * FPS: 60.16\n\n * ITC(ms): 17.934\n\n * TCPP(ms): 5.596\n\n * RV(mb): 150.79\n\n * WV(mb): 118.67\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.2809(FLOAT)/0.2790(INT8) MeanIOU: 0.4852(FLOAT)/0.4835(INT8)\n   mAP: 0.1990(FLOAT)/0.2000(INT8)\n\n\nBev_ipm_4d_efficientnetb0_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points: 6x128x128x2 prev_feat: 1x164x28x128\n   prev_point: 1x128x128x2\n\n * C(GOPs): 53.58\n\n * FPS: 106.12\n\n * ITC(ms): 10.929\n\n * TCPP(ms): 5.721\n\n * RV(mb): 68.76\n\n * WV(mb): 55.37\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3721(FLOAT)/0.3736(INT8) MeanIOU: 0.5287(FLOAT)/0.5387(INT8)\n   mAP: 0.2200(FLOAT)/0.2218(INT8)\n\n\nBev_ipm_efficientnetb0_multitask#\n\n * INPUT SIZE: image: 6x3x512x960 points: 6x128x128x2\n\n * C(GOPs): 52.97\n\n * FPS: 109.29\n\n * ITC(ms): 10.242\n\n * TCPP(ms): 5.626\n\n * RV(mb): 65.45\n\n * WV(mb): 53.27\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3055(FLOAT)/0.3029(INT8) MeanIOU: 0.5145(FLOAT)/0.5098(INT8)\n   mAP: 0.2170(FLOAT)/0.2163(INT8)\n\n\nDetr3d_efficientnetb3#\n\n * INPUT SIZE: coords(0-3): 6x4x256x2 image: 6x3x512x1408 masks: 1x4x256x24\n\n * C(GOPs): 225.28\n\n * FPS: 26.27\n\n * ITC(ms): 38.796\n\n * TCPP(ms): 1.114\n\n * RV(mb): 477.43\n\n * WV(mb): 308.98\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3304(FLOAT)/0.3281(INT8) mAP: 0.2752(FLOAT)/0.2705(INT8)\n\n\nPetr_efficientnetb3#\n\n * INPUT SIZE: image: 6x3x512x1408 pos_embed: 1x96x44x256\n\n * C(GOPs): 217.42\n\n * FPS: 13.06\n\n * ITC(ms): 77.263\n\n * TCPP(ms): 1.136\n\n * RV(mb): 1272.47\n\n * WV(mb): 1151.16\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3765(FLOAT)/0.3737(INT8) mAP: 0.3038(FLOAT)/0.2935(INT8)\n\n\nBevformer_tiny_resnet50_detection#\n\n * INPUT SIZE: img: 6x3x480x800 prev_bev: 1x2500x256 prev_bev_ref: 1x50x50x2\n   queries_rebatch_grid: 6x20x32x2 restore_bev_grid: 1x100x50x2\n   reference_points_rebatch: 6x640x4x2 bev_pillar_counts: 1x2500x1\n\n * C(GOPs): 385.94\n\n * FPS: 27.57\n\n * ITC(ms): 46.385\n\n * TCPP(ms): 1.404\n\n * RV(mb): 308.11\n\n * WV(mb): 214.28\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.3713(FLOAT)/0.3668(INT8) mAP: 0.2673(FLOAT)/0.2608(INT8)\n\n\nFlashocc_henet_lss_occ3d_nuscenes#\n\n * INPUT SIZE: img: 6x3x512x960 points: 10x128x128x2 points_depth: 10x128x128x2\n\n * C(GOPs): 126.80\n\n * FPS: 88.96\n\n * ITC(ms): 12.103\n\n * TCPP(ms): 40.872\n\n * RV(mb): 110.47\n\n * WV(mb): 68.86\n\n * Dataset: Nuscenes\n\n * ACCURACY: mIoU: 0.3674(FLOAT)/0.0738(INT8)\n\n\nHorizon_swin_transformer#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 8.98\n\n * FPS: 255.81\n\n * ITC(ms): 4.292\n\n * TCPP(ms): 0.034\n\n * RV(mb): 43.32\n\n * WV(mb): 4.82\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.8024(FLOAT)/0.7953(INT8)\n\n\nMixvargenet#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 2.07\n\n * FPS: 4944.60\n\n * ITC(ms): 0.463\n\n * TCPP(ms): 0.033\n\n * RV(mb): 2.51\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7075(FLOAT)/0.7063(INT8)\n\n\nVargnetv2#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 0.72\n\n * FPS: 3853.60\n\n * ITC(ms): 0.530\n\n * TCPP(ms): 0.034\n\n * RV(mb): 4.68\n\n * WV(mb): 0.0041\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7342(FLOAT)/0.7317(INT8)\n\n\nVit_small#\n\n * INPUT SIZE: 1x3x224x224\n\n * C(GOPs): 9.20\n\n * FPS: 477.07\n\n * ITC(ms): 2.370\n\n * TCPP(ms): 0.034\n\n * RV(mb): 31.35\n\n * WV(mb): 5.56\n\n * Dataset: ImageNet\n\n * ACCURACY: Top1: 0.7950(FLOAT)/0.7797(INT8)\n\n\nCenterpoint_pointpillar#\n\n * INPUT SIZE: points: 300000x5 voxel_feature: 1x5x20x40000 coors: 40000x4\n\n * C(GOPs): 127.76\n\n * FPS: 89.29\n\n * ITC(ms): 60.218\n\n * TCPP(ms): 12.967\n\n * RV(mb): 48.98\n\n * WV(mb): 24.78\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.5831(FLOAT)/0.5819(INT8) mAP: 0.4804(FLOAT)/0.4780(INT8)\n\n\nDetr_efficientnetb3#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 67.31\n\n * FPS: 45.88\n\n * ITC(ms): 22.138\n\n * TCPP(ms): 0.341\n\n * RV(mb): 270.02\n\n * WV(mb): 146.61\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.3721(FLOAT)/0.3591(INT8)\n\n\nDetr_resnet50#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 202.99\n\n * FPS: 35.99\n\n * ITC(ms): 28.139\n\n * TCPP(ms): 0.342\n\n * RV(mb): 399.17\n\n * WV(mb): 263.34\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.3569(FLOAT)/0.3140(INT8)\n\n\nFCOS3D_efficientnetb0#\n\n * INPUT SIZE: 1x3x512x896\n\n * C(GOPs): 19.94\n\n * FPS: 410.89\n\n * ITC(ms): 3.517\n\n * TCPP(ms): 2.721\n\n * RV(mb): 11.45\n\n * WV(mb): 4.14\n\n * Dataset: nuscenes\n\n * ACCURACY: NDS: 0.3060(FLOAT)/0.3029(INT8) mAP: 0.2133(FLOAT)/0.2079(INT8)\n\n\nGanet_mixvargenet#\n\n * INPUT SIZE: 1x3x320x800\n\n * C(GOPs): 10.74\n\n * FPS: 1436.60\n\n * ITC(ms): 0.998\n\n * TCPP(ms): 0.218\n\n * RV(mb): 2.18\n\n * WV(mb): 0.53\n\n * Dataset: CuLane\n\n * ACCURACY: F1Score: 0.7948(FLOAT)/0.7878(INT8)\n\n\nKeypoint_efficientnetb0#\n\n * INPUT SIZE: 1x3x128x128\n\n * C(GOPs): 0.58\n\n * FPS: 4734.00\n\n * ITC(ms): 0.491\n\n * TCPP(ms): 0.076\n\n * RV(mb): 4.62\n\n * WV(mb): 0.01\n\n * Dataset: Carfusion\n\n * ACCURACY: PCK(alpha=0.1): 0.9433(FLOAT)/0.9432(INT8)\n\n\nPointpillars_kitti_car#\n\n * INPUT SIZE: 150000x4\n\n * C(GOPs): 67.23\n\n * FPS: 24.49\n\n * ITC(ms): 221.340\n\n * TCPP(ms): 0.534\n\n * RV(mb): 51.20\n\n * WV(mb): 30.80\n\n * Dataset: Kitti3d\n\n * ACCURACY: APDet= 0.7732(FLOAT)/0.7676(INT8)\n\n\nDeformable_detr_resnet50#\n\n * INPUT SIZE: 1x3x800x1333\n\n * C(GOPs): 425.65\n\n * FPS: 4.47\n\n * ITC(ms): 224.570\n\n * TCPP(ms): 15.521\n\n * RV(mb): 3663.77\n\n * WV(mb): 2724.37\n\n * Dataset: MS COCO\n\n * ACCURACY: [IoU=0.50:0.95]= 0.4414(FLOAT)/0.4205(INT8)\n\n\nStereonetplus_mixvargenet#\n\n * INPUT SIZE: 2x3x544x960\n\n * C(GOPs): 48.59\n\n * FPS: 205.38\n\n * ITC(ms): 5.364\n\n * TCPP(ms): 1.956\n\n * RV(mb): 40.57\n\n * WV(mb): 34.81\n\n * Dataset: SceneFlow\n\n * ACCURACY: EPE: 1.1270(FLOAT)/1.1345(INT8)\n\n\nCenterpoint_mixvargnet_multitask#\n\n * INPUT SIZE: points: 300000x5 voxel_feature: 1x5x20x40000 coors: 40000x4\n\n * C(GOPs): 51.45\n\n * FPS: 90.71\n\n * ITC(ms): 57.852\n\n * TCPP(ms): 12.087\n\n * RV(mb): 39.20\n\n * WV(mb): 16.39\n\n * Dataset: Nuscenes\n\n * ACCURACY: NDS: 0.5809(FLOAT)/0.5754(INT8) MeanIOU: 0.9128(FLOAT)/0.9121(INT8)\n   mAP: 0.4727(FLOAT)/0.4629(INT8)\n\n\nUnet_mobilenetv1#\n\n * INPUT SIZE: 1x3x1024x2048\n\n * C(GOPs): 7.36\n\n * FPS: 751.49\n\n * ITC(ms): 1.748\n\n * TCPP(ms): 0.148\n\n * RV(mb): 16.49\n\n * WV(mb): 11.67\n\n * Dataset: Cityscapes\n\n * ACCURACY: mIoU: 0.6802(FLOAT)/0.6757(INT8)\n\n\nMotr_efficientnetb3#\n\n * INPUT SIZE: image: 1x800x1422x3 track_query: 1x2x128x156 ref_points:\n   1x2x128x4 mask_query: 1x1x256x1\n\n * C(GOPs): 82.01\n\n * FPS: 65.57\n\n * ITC(ms): 15.589\n\n * TCPP(ms): 4.947\n\n * RV(mb): 130.73\n\n * WV(mb): 52.07\n\n * Dataset: Mot17\n\n * ACCURACY: MOTA: 0.5805(FLOAT)/0.5754(INT8)\n\n\nDensetnt_vectornet#\n\n * INPUT SIZE: goals_2d: 30x1x2048x2 goals_2d_mask: 30x1x2048x1 instance_mask:\n   30x1x96x1 lane_feat: 30x9x64x11 traj_feat: 30x19x32x9\n\n * C(GOPs): 18.67\n\n * FPS: 144.18\n\n * ITC(ms): 11.123\n\n * TCPP(ms): 2.339\n\n * RV(mb): 79.04\n\n * WV(mb): 59.54\n\n * Dataset: Argoverse 1\n\n * ACCURACY: minFDA: 1.2975(FLOAT)/1.3044(INT8)\n\n\nMaptroe_henet-tinym_bevformer#\n\n * INPUT SIZE: img: 6x3x480x800 osm_mask: 1x1x50x100 queries_rebatch_grid:\n   6x20x100x2 restore_bev_grid: 1x100x100x2 reference_points_rebatch: 6x2000x4x2\n   bev_pillar_counts: 1x5000x1\n * C(GOPs): 133.06\n * FPS: 67.20\n * ITC(ms): 15.626\n * TCPP(ms): 0.258\n * RV(mb): 148.08\n * WV(mb): 46.42\n * Dataset: Nuscenes\n * ACCURACY: mAP: 0.6633(FLOAT)/0.6340(INT8)","routePath":"/en/guide/benchmark","lang":"en","toc":[{"text":"Descriptions","id":"descriptions","depth":2,"charIndex":3},{"text":"Model Key Performance Data","id":"model-key-performance-data","depth":2,"charIndex":1442},{"text":"Model Full Performance Data","id":"model-full-performance-data","depth":2,"charIndex":1472},{"text":"MobileNetv1","id":"mobilenetv1","depth":3,"charIndex":1503},{"text":"MobileNetv2","id":"mobilenetv2","depth":3,"charIndex":1725},{"text":"ResNet50","id":"resnet50","depth":3,"charIndex":1947},{"text":"GoogleNet","id":"googlenet","depth":3,"charIndex":2167},{"text":"EfficientNet_Lite0","id":"efficientnet_lite0","depth":3,"charIndex":2467},{"text":"EfficientNet_Lite1","id":"efficientnet_lite1","depth":3,"charIndex":2785},{"text":"EfficientNet_Lite2","id":"efficientnet_lite2","depth":3,"charIndex":3103},{"text":"EfficientNet_Lite3","id":"efficientnet_lite3","depth":3,"charIndex":3421},{"text":"EfficientNet_Lite4","id":"efficientnet_lite4","depth":3,"charIndex":3739},{"text":"Vargconvnet","id":"vargconvnet","depth":3,"charIndex":4058},{"text":"Efficientnasnet_m","id":"efficientnasnet_m","depth":3,"charIndex":4363},{"text":"Efficientnasnet_s","id":"efficientnasnet_s","depth":3,"charIndex":4682},{"text":"ResNet18","id":"resnet18","depth":3,"charIndex":5000},{"text":"YOLOv2_Darknet19","id":"yolov2_darknet19","depth":3,"charIndex":5220},{"text":"YOLOv3_Darknet53","id":"yolov3_darknet53","depth":3,"charIndex":5489},{"text":"yolov5x_v2.0","id":"yolov5x_v20","depth":3,"charIndex":5770},{"text":"SSD_MobileNetv1","id":"ssd_mobilenetv1","depth":3,"charIndex":6060},{"text":"YOLOv3_VargDarknet","id":"yolov3_vargdarknet","depth":3,"charIndex":6324},{"text":"Deeplabv3plus_efficientnetb0","id":"deeplabv3plus_efficientnetb0","depth":3,"charIndex":6652},{"text":"Fastscnn_efficientnetb0","id":"fastscnn_efficientnetb0","depth":3,"charIndex":6983},{"text":"Deeplabv3plus_efficientnetm1","id":"deeplabv3plus_efficientnetm1","depth":3,"charIndex":7299},{"text":"Deeplabv3plus_efficientnetm2","id":"deeplabv3plus_efficientnetm2","depth":3,"charIndex":7630},{"text":"Bev_gkt_mixvargenet_multitask","id":"bev_gkt_mixvargenet_multitask","depth":3,"charIndex":7963},{"text":"Bev_ipm_4d_efficientnetb0_multitask","id":"bev_ipm_4d_efficientnetb0_multitask","depth":3,"charIndex":8306},{"text":"Bev_ipm_efficientnetb0_multitask","id":"bev_ipm_efficientnetb0_multitask","depth":3,"charIndex":8701},{"text":"Detr3d_efficientnetb3","id":"detr3d_efficientnetb3","depth":3,"charIndex":9042},{"text":"Petr_efficientnetb3","id":"petr_efficientnetb3","depth":3,"charIndex":9357},{"text":"Bevformer_tiny_resnet50_detection","id":"bevformer_tiny_resnet50_detection","depth":3,"charIndex":9654},{"text":"Flashocc_henet_lss_occ3d_nuscenes","id":"flashocc_henet_lss_occ3d_nuscenes","depth":3,"charIndex":10113},{"text":"Horizon_swin_transformer","id":"horizon_swin_transformer","depth":3,"charIndex":10413},{"text":"Mixvargenet","id":"mixvargenet","depth":3,"charIndex":10646},{"text":"Vargnetv2","id":"vargnetv2","depth":3,"charIndex":10868},{"text":"Vit_small","id":"vit_small","depth":3,"charIndex":11088},{"text":"Centerpoint_pointpillar","id":"centerpoint_pointpillar","depth":3,"charIndex":11306},{"text":"Detr_efficientnetb3","id":"detr_efficientnetb3","depth":3,"charIndex":11621},{"text":"Detr_resnet50","id":"detr_resnet50","depth":3,"charIndex":11864},{"text":"FCOS3D_efficientnetb0","id":"fcos3d_efficientnetb0","depth":3,"charIndex":12102},{"text":"Ganet_mixvargenet","id":"ganet_mixvargenet","depth":3,"charIndex":12364},{"text":"Keypoint_efficientnetb0","id":"keypoint_efficientnetb0","depth":3,"charIndex":12592},{"text":"Pointpillars_kitti_car","id":"pointpillars_kitti_car","depth":3,"charIndex":12835},{"text":"Deformable_detr_resnet50","id":"deformable_detr_resnet50","depth":3,"charIndex":13066},{"text":"Stereonetplus_mixvargenet","id":"stereonetplus_mixvargenet","depth":3,"charIndex":13318},{"text":"Centerpoint_mixvargnet_multitask","id":"centerpoint_mixvargnet_multitask","depth":3,"charIndex":13554},{"text":"Unet_mobilenetv1","id":"unet_mobilenetv1","depth":3,"charIndex":13916},{"text":"Motr_efficientnetb3","id":"motr_efficientnetb3","depth":3,"charIndex":14146},{"text":"Densetnt_vectornet","id":"densetnt_vectornet","depth":3,"charIndex":14454},{"text":"Maptroe_henet-tinym_bevformer","id":"maptroe_henet-tinym_bevformer","depth":3,"charIndex":14798}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":57,"title":"","content":"","routePath":"/en/guide/doc_introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":58,"title":"Environment Deployment","content":"#\n\n\nForeword#\n\nHorizon OpenExplorer currently provides 2 sets of model quantization schemes at\nthe same time.\n\n * PTQ: Post-training Quantization.\n * QAT: Quantization aware training, which only supports the Pytorch framework\n   for now.\n\nAmong them:\n\n * Both solutions do not interfere with the training phase of the floating-point\n   model, which is your own responsibility. Horizon has also provided some\n   open-source implementations of the public version of Pytorch for efficient\n   models in classification, detection, segmentation, and other scenarios in the\n   samples/ai_toolchain/horizon_model_train_sample for reference, with the\n   support for training and replication on the host.\n\n * For PTQ scheme, you need to quantize the model in the host development\n   environment, and then copy the compiled .hbm model to the dev board\n   environment for subsequent deployment.\n\n * For the QAT scheme, you need to complete the QAT training of the model in the\n   host development environment, perform the quantization conversion, and then\n   copy the compiled .hbm model to the dev board environment for subsequent\n   deployment.\n\nFor both of the above quantization schemes and the development environment of\nthe efficient model, Horizon provides both local manual installation and Docker\ncontainers. We strongly recommend using Docker containers as they do not pollute\nthe local environment and is easy to use, descriptions of these two are included\nin the following sections.\n\n\nDevelopment Environment Deployment#\n\n\nDevelopment Machine Preparation#\n\nIn order to use the toolchain smoothly, we recommends that the development\nmachine you choose should meet the following requirements:\n\nFor more information about CUDA compatibility with graphics cards, refer to\nNVIDIA website information.\n\n\nDocker Container Deployment #\n\nDocker Base Environment#\n\nHorizon requires the following Docker base environment, please complete the\ninstallation on your host computer in advance.\n\n * Docker (20.10.10 and above, 20.10.10 is recommended), refer to the Docker\n   Installation Manual.\n * NVIDIA Container Toolkit (1.13.5 and above, 1.15.0 is recommended), refer to\n   installation manual NVIDIA Container Toolkit installation manual.\n\nAfter completing the installation of the Docker environment, remember to add\nnon-root users into Docker users group by running below command:\n\n\n\nPlease obtain the required Docker image from following:\n\n * Horizon OpenExplorer Docker Hub GPU Docker\n * Horizon OpenExplorer Docker Hub CPU Docker\n\nThe naming form is as follows:\n\n * GPU Docker: openexplorer/ai_toolchain_ubuntu_22_j6_gpu:{version}\n * CPU Docker: openexplorer/ai_toolchain_ubuntu_22_j6_cpu:{version}\n\nTip\n\nReplace the by the actual version number.\n\nDocker Image Usage #\n\nTo help you quickly use the toolchain, we provide a Docker image containing the\ncomplete development environment, which greatly simplifies the deployment\nprocess of the development environment.\n\nNote\n\nIf you have downloaded the offline image, you need to use the following command\nto load the image locally first.\n\n\n\nYou can start the Docker container corresponding to the current OE version by\nrunning the following script directly from the first level of the OE package(the\nscript will automatically pull the image from the official Docker hub if there\nis no local image):\n\n\n\nWhere data is the path to the evaluation dataset folder. Please create the path\nbefore running the command, otherwise loading problems may occur.\n\nIf you want to use the CPU version of the Docker image, you need to add the cpu\nparameter:\n\n\n\nThe download links for the relevant publicly available evaluation datasets that\nthe OE package samples rely on can be accessed by referring to the introduction\nin section Dataset Download.\n\nIf you want to start the Docker container manually, you can refer to the\nfollowing command, where {version} is the OE version number you are currently\nusing.\n\nNote\n\nFor your convenience, we provide CPU Docker and GPU Docker for you to choose on\ndemand.\n\nAttention\n\n 1. Since the environment variables PATH and LD_LIBRARY_PATH are configured\n    during the build process of the OE Docker image, not using the recommended\n    way (e.g., docker attach) to enter the container may result in the\n    environment variables not being loaded correctly, which may lead to the use\n    of abnormalities in tools such as Cmake, GCC, CUDA, and so on.\n\n 2. If you want the Docker container to exit without removing it, use the\n    command line docker run -it to start it manually, without the --rm option.\n\n 3. If you want the Docker container to run background after startup, add the -d\n    option after the command line docker run -it, the container ID will be\n    returned after the container is started, and then you can enter the\n    container again with the command docker exec -it {container ID} /bin/bash.\n\n\nLocal Manual Installation#\n\nThis section describes the local manual installation environment method, and\nintroduces the environment-related dependencies and descriptions for each of the\ntwo quantization schemes and Horizon open-source efficient model training. We\nrecommend prioritizing the easy-to-use PTQ quantization scheme after the\nfloating-point model training, and switch to the QAT quantization scheme only\nwhen accuracy issues cannot be solved.\n\nLocal Manual Installation Environment Method#\n\nTo manually install the environment locally, simply run the script below to\ncomplete the environment installation in one click.\n\n\n\nThe installation script will automatically check the environment. If there are\nmissing dependencies or configurations, it will interrupt the installation\nprocess and give suggestions to fix it, as shown below:\n\nRun the script again after adding the dependencies as suggested.\n\nNote\n\n * If you need to generate a board-side executeable program, use the\n   cross-compilation tool aarch64-none-linux-gnu-gcc and\n   aarch64-none-linux-gnu-g++. Its version is Arm GNU Toolchain 12.2.Rel1.\n\n * If you need to generate an X86 emulation environment executable program, use\n   the X86 gcc, if the script suggests that gcc/g++ versions are incorrect, you\n   need to recreate the gcc and g++ soft links as gcc-12.2.0 and g++-12.2.0\n   after the required versions are installed.\n\n * For host-side (x86) dependent libraries (isl, gmp, mpc, mpfr, etc.), if you\n   use lib/x86_64-linux-gnu, please specify them by LD_LIBRARY_PATH in the\n   compiled project if the compilation reports errors.\n\n * If there is a glibc library version conflict problem during compilation, for\n   example: the error of undefined symbols for xxx@GLIBC_xxx, please specify the\n   path to aarch64-none-linux-gnu/lib of the toolchain by using -rpath-link in\n   the compiled project, and at the same time add -lxxx to the compiled project,\n   for example: -lpthread.\n   \n   In addition, you should pay special attention to the variable SRCS (boxed\n   below) that records the source file. It is better to put it in front of the\n   ${LIBS} link library, otherwise it may also report undefined symbols.\n   \n   \n\n * The installation script will automatically check the environment. If there\n   are missing dependencies or configurations, it will interrupt the\n   installation process and you can run the script again after adding the\n   dependencies as suggested.\n\n * After the installation script is completed successfully, it will add path and\n   other information to the ~/.bashrc system environment variable(The\n   environment variable LD_LIBRARY_PATH is often used and it is recommended that\n   you check that the environment variable is as expected), run source ~/.bashrc\n   to make the current terminal configuration take effect.\n\n * The torch version should be 2.3.0+cu118 and the torchvision version should be\n   0.18.0.\n\nPTQ Quantization Environment Dependence#\n\nThe PTQ scheme has the following software dependencies on the base software of\nthe development machine operating environment:\n\n * Operating system: Ubuntu22.04\n * Python3.10\n * libpython3.10\n * python3-devel\n * python3-pip\n * gcc&g++: 12.2.1\n * graphviz\n\nQAT Quantization Environment Dependence #\n\nThe QAT quantization environment is installed in the local environment and you\nneed to ensure that the following basic environmental conditions are met.\n\nThe environmental dependencies required for the quantitative training tool to be\ntrained are listed below:\n\nHW/OS                        GPU                        CPU\nos                           Ubuntu22.04                Ubuntu22.04\ncuda                         11.8                       N/A\npython                       3.10                       3.10\ntorch                        2.3.0+cu118                2.3.0+cpu\ntorchvision                  0.18.0+cu118               0.18.0+cpu\nRecommended Graphics Cards   titan v/2080ti/v100/3090   N/A\n\nAfter completing the training of the QAT model, you can install the relevant\ntoolkits in the current training environment and complete the subsequent model\nconversion directly through the interface call.\n\nEfficient Model Floating-point Training Environment Instruction#\n\nHorizon provides the source code of several open-source efficient models in\nsamples/ai_toolchain/horizon_model_train_sample. For information on the\nfloating-point and QAT base environment, refer to section QAT Quantization\nEnvironment Deployment.\n\n\nRuntime Environment Deployment#\n\nOnce the model has been quantized, the compiled model can be deployed on the dev\nboard environment for inference and execution.\n\nTo deploy the runtime environment, you need to prepare a dev board with the\nsystem image programmed, and then copy the relevant supplementary files to the\ndev board.\n\n\nDev Board Preparation#\n\nAt this stage, you need to verify the usability of the dev board and program the\navailable system images to the board.\n\n\nBoard Side Tool Installation#\n\nSome of the supplementary tools of the toolchain are not included in the system\nimage, but can be copied to the dev board by running the installation script in\nthe OE package in the host environment, as follows:\n\n\n\nNote\n\nWhere ${board_ip} is the IP address you set for the dev board. Make sure that\nyou can successfully access this IP on the development PC.\n\nAfter the supplementary files are successfully installed, please restart the dev\nboard and execute hrt_model_exec --help on the dev board to verify if the\ninstallation is successful.","routePath":"/en/guide/env_install","lang":"en","toc":[{"text":"Foreword","id":"foreword","depth":2,"charIndex":3},{"text":"Development Environment Deployment","id":"development-environment-deployment","depth":2,"charIndex":1484},{"text":"Development Machine Preparation","id":"development-machine-preparation","depth":3,"charIndex":1522},{"text":"Docker Container Deployment","id":"docker-container-deployment","depth":3,"charIndex":-1},{"text":"Docker Base Environment","id":"docker-base-environment","depth":4,"charIndex":1828},{"text":"Docker Image Usage","id":"docker-image-usage","depth":4,"charIndex":-1},{"text":"Local Manual Installation","id":"local-manual-installation","depth":3,"charIndex":4874},{"text":"Local Manual Installation Environment Method","id":"local-manual-installation-environment-method","depth":4,"charIndex":5329},{"text":"PTQ Quantization Environment Dependence","id":"ptq-quantization-environment-dependence","depth":4,"charIndex":7793},{"text":"QAT Quantization Environment Dependence","id":"qat-quantization-environment-dependence","depth":4,"charIndex":-1},{"text":"Efficient Model Floating-point Training Environment Instruction","id":"efficient-model-floating-point-training-environment-instruction","depth":4,"charIndex":9043},{"text":"Runtime Environment Deployment","id":"runtime-environment-deployment","depth":2,"charIndex":9358},{"text":"Dev Board Preparation","id":"dev-board-preparation","depth":3,"charIndex":9688},{"text":"Board Side Tool Installation","id":"board-side-tool-installation","depth":3,"charIndex":9833}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":59,"title":"PTQ and QAT Introduction","content":"#\n\nAccording to whether to adjust the parameters after quantization, we can\nclassify the quantization methods into Quantization-aware Training (QAT) and\nPost-training Quantization (PTQ).\n\nThe difference in operations between these two methods is shown in the following\ndiagram (Left: PTQ; Right: QAT).\n\nThe PTQ uses a batch of calibration data to calibrate the trained model,\nconverting the trained FP32 model directly into a fixed-point computational\nmodel without any training of the original model. Only a few hyperparameters are\nadjusted to complete the quantization process, and the process is simple and\nfast, no training is required, so this method has been widely used in a large\nnumber of end-side and cloud-side deployment scenarios, we recommend trying the\nPTQ method first to see if it meets your deployment accuracy and performance\nrequirements .\n\nThe QAT is to quantize the trained model and then retrain it again. Since\nfixed-point values cannot be used for backward gradient calculation, the actual\nprocedure is to insert fake quantization nodes in front of some ops to obtain\nthe truncated values of the data flowing through this op during training, which\ncan be easily used when quantizing the nodes during the deployment of the\nquantization model. We need to obtain the best quantization parameters by\ncontinuously optimizing the accuracy during the training. As model training is\ninvolved, it requires higher level of skills for the developers.","routePath":"/en/guide/faststart/ptq_qat_overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":60,"title":"Algorithm Model PTQ + On-board Deployment Quick Start","content":"#\n\nTo help you quickly get started, this section introduces the basic workflow of\nthe PTQ scheme using ResNet50 as an example to illustrate the details.\n\nThe basic workflow is as follows:\n\nAttention\n\nBefore starting, make sure you have completed the environment installation on\nboth dev PC and dev board by following the section Environment Deployment.\n\n\nFloating-point Model Preparation#\n\nThe OE package provides you with rich PTQ model samples under the\nsamples/ai_toolchain/horizon_model_convert_sample path. The ResNet50 model\nsample is located under the 03_classification/03_resnet50 path.\n\nPlease first execute the 00_init.sh script to obtain the corresponding\ncalibration dataset and the original model of the sample. Please refer to\nsection Prepare Models for their model sources and related instructions.\n\nIf you need to convert a private model, refer to section Prepare Floating-point\nModel Preparation to prepare opset=10-19 onnx models in advance. The following\ntable shows reference schemes for converting different frameworks to ONNX model\nformats.\n\n\nModel Checking#\n\nAfter the floating-point model is ready, we recommend a quick checking of the\nmodel to ensure that it meets the support constraints of the computing platform.\nFor the ResNet50 model in ONNX format, we can complete the model checking by\ntyping the following command in the command line:\n\n\n\nIf your model is a multi-input model, you can refer to the following command:\n\n\n\nThe main parameters of the hb_compile tool are as follows, for more parameter\ndescriptions, refer to section Model Checking.\n\nTake the ResNet50 model as an example, you can execute the 01_check.sh script to\nquickly complete model checking.\n\nThe main contents of the 01_check.sh script file are as follows:\n\n\n\nIf the model checking fails, confirm the error messages and modification\nsuggestions according to the hb_compile.log file printed on the terminal or\ngenerated under the current path, please refer to section Model Checking for\nmore instructions.\n\n\nModel Conversion#\n\nAfter the model checking passes, you can use the hb_compile tool to covert the\nmodel, refer to the following command:\n\n\n\nAmong them, resnet50_config.yaml is the configuration file corresponding to the\nmodel conversion, for details please refer to section Configuration File\nTemplate.\n\nIn addition, the model quantization of the PTQ scheme also depends on a certain\nnumber of pre-processed samples for calibration, which is described in section\nPre-processing Calibration Data.\n\n\nYAML Configuration File #\n\nThe YAML configuration file contains 4 required parameter groups\n(model_parameters, input_parameters, calibration_parameters,\ncompiler_parameters) and 1 optional parameter group (custom_op).\n\nEach parameter group contains both required and optional parameters (optional\nparameters are hidden by default), you can refer to section Specific Parameter\nInformation for specific requirements and filling methods.\n\nNote\n\nFor ONNX model, you should configure the onnx_model parameter, instead of\nconfiguring the caffe_model and prototxt parameters in the `model_parameters``\nparameter group.\n\n * The input_type_rt and input_type_train parameters in the input_parameters\n   parameter group are used to specify the data type (e.g., NV12) that the model\n   will receive when it is actually deployed on the board and the data type\n   (e.g., RGB) for its own training, respectively.\n   \n   When the two data types are inconsistent, the conversion tool automatically\n   inserts a BPU-accelerated preprocessing node in the frontend of the model to\n   complete the corresponding color space conversion.\n   \n   Meanwhile, the mean_value, scale_value and std_value parameters in this\n   parameter group can also be used to configure the data normalization\n   operation of the image input model, which will be integrated into the\n   preprocessing node for BPU acceleration by the conversion tool after\n   configuration.\n   \n   The formula for data normalization is as follows:\n   \n   $data_norm = (data - mean_value) * scale_value$\n\n * The cal_data_dir parameter in the calibration_parameters parameter group\n   needs to be configured with the path of the preprocessed calibration data\n   folder, refer to the section Pre-processing Calibration Data for the\n   descriptions of the preprocessing method.\n\n\nPre-processing Calibration Data #\n\nAttention\n\n * Please note that before doing this step, make sure that you have already\n   finished obtaining the calibration dataset by executing the 00_init.sh script\n   in the corresponding sample directory.\n\n * If you are currently only concerned with model performance, you do not need\n   to configure cal_data_dir parameter in the yaml and skip this subsection. The\n   tool will perform pseudo calibration to facilitate quick verification.\n\nThe calibration data of the PTQ scheme is generally screened from the training\nset or verification set of about 100 (can be appropriately increased or\ndecreased) typical data, and should avoid very rare and unusual samples, such as\nsolid color images, images without any detection or classification targets, etc.\n\nThe filtered calibration data should also needs to be preprocessed in the same\nway as that before the model inference, and after processing, the data type\n(input_type_train), layout (input_layout_train) and size (input_shape) should\nstay the same as the original model.\n\nFor the preprocessing of calibration data, Horizon recommends directly using and\nmodifying the sample code. Take the ResNet50 model as an example, the\ncalibration_transformers function in the preprocess.py file contains the\npre-processing transformers for its calibration data, and the processed\ncalibration data is consistent with its YAML configuration file, that is:\n\n * input_type_train : 'rgb'\n\n * input_layout_train : 'NCHW'\n\n\n\nWhere transformers are defined in ../../../01_common/python/data/transformer.py\nfile, please refer to section Image Processing Transformer. You can choose to\nmodify and extend them as needed.\n\nAfter modifying the preprocess.py file, you can modify and execute the\n02_preprocess.sh script to complete the preprocessing of the calibration data.\n\n\n\nThe main contents of the 02_preprocess.sh script file is as follows:\n\n\n\nThe parameters of the data_preprocess.py file is described as follows:\n\n * src_dir: Path to the raw calibration data.\n\n * dst_dir: Storage path of the processed data, which can be customized.\n\n * pic_ext: File suffix of the processed data, which is mainly used to help\n   remember the data type and can be left unconfigured.\n\n * read_mode: Image reading mode, which can be configured as skimage, opencv or\n   PIL. Note that the format of the image read by skimage is RGB with the data\n   range of 0-1, the format of the image read by opencv is BGR with the data\n   range of 0-255, the format of the image read by PIL is RGB with the data\n   range of 0-255.\n\n * saved_data_type: Type of saved data after processing.\n\nIf you choose to write your own python code to pre-process the calibration data,\nyou can use the numpy.save command to save it as a npy file, which will be read\nby the toolchain calibration based on the numpy.load command.\n\n\nModel Conversion #\n\nAfter preparing the calibration data and YAML configuration file, you can\ncomplete the entire process of model parsing, graph optimization, calibration,\nquantization, and compilation conversion in one command.\n\nFor a detailed explanation of the internal process, please refer to section\nModel Conversion Interpretation.\n\nAfter conversion, the following outputs will be saved under the working_dir path\nspecified by the YAML file. For details, refer to section Interpret Conversion\nOutput.\n\n\n\nNote\n\nIf during model conversion there exists a situation of removing nodes at the\ninput/output, the *_quantized_removed_model.bc will also be saved. In this\nscenario, we recommend you to use this HBIR model to compare with the final\ngenerated hbm model if you need to do consistency comparison later.\n\n\nFast Performance Verification#\n\nFor the xxx.hbm model file generated by the conversion, Horizon supports both\nfirst estimating the static performance of the BPU part of the model on the dev\nPC first, and providing an executable tool on the board to quickly evaluate the\ndynamic performance without any coding.\n\nFor more detailed descriptions and performance tuning recommendations, refer to\nsections Model Performance Analysis and Model Performance Optimization.\n\n\nStatic Performance Evaluation#\n\nAs described in section Model Conversion, after model conversion, HTML and JSON\nfiles containing static performance evaluation information for the model will be\ngenerated under the working_dir path, both with the same content. But the HTML\nfile is more readable. The following is the HTML file generated by the ResNet50\nmodel conversion, where:\n\n * The Summary tab provides the performance of the BPU part of the model\n   predicted by the compiler.\n   \n   \n\n * The Temporal Statistics tab provides the bandwidth usage of the model during\n   the inference time of one frame.\n   \n   \n\n * In addition, the Layer Details tab provides the computation amount,\n   computation time, data handling time and the active time period of the\n   compiled layer (does not represent the execution time of the layer, usually\n   multiple layers alternate/execute in parallel) of each layer of BPU\n   operators.\n   \n   \n\n\nDynamic Performance Evaluation#\n\nOnce the static performance of the model meets expectations, we can further\nevaluate the dynamic performance of the model on the board, and the reference\nmethod is as follows:\n\n 1. Make sure you have completed the environment deployment of the dev board\n    according to section Environment Deployment.\n\n 2. Copy the xxx.hbm model generated by the conversion to any path in the\n    /userdata folder of the dev board.\n\n 3. Use the hrt_model_exec perf tool to quickly evaluate the time consumption\n    and frame rate of the model.\n\n\n\nThe main parameters of the hrt_model_exec tool are described as follows, refer\nto the section hrt_model_exec Tool Introduction for more instructions:\n\nNote\n\n * If you can't find the hrt_model_exec tool on the board side, you can run the\n   install.sh script under the package/board path in the OE package again.\n\n * When evaluating Latency, you can specify thread_num as 1 for single-threaded\n   serial reasoning.\n\n * When evaluating FPS, you usually use multi-threaded concurrent reasoning to\n   fill up BPU resources, so you can configure core_id to 0 and thread_num to be\n   multi-threaded.\n\n * When the model input is dynamic, please fill in the input_valid_shape and\n   input_stride parameters according to the actual input.\n\n * If you configure the profile_path parameter, the program needs to run\n   normally before the profiler.log log file and the profiler.csv file will be\n   generated, please do not use the Ctrl+C command to interrupt the program.\n\nWhen the dynamic performance of the model does not meet expectations, refer to\nthe section Model Performance Optimization for performance tuning.\n\n\nAccuracy Verification#\n\nOnce the performance of the model has been verified as expected, subsequent\naccuracy verification can be performed. Please first ensure that you have\nprepared the relevant evaluation datasets and mounted them in a Docker\ncontainer. The dataset used for the samples can be obtained by referring to the\nsection Dataset Download.\n\nAs described in the section Model Conversion, the model conversion generates two\nquantized models, xxx_quantized_model.bc and xxx.hbm, and the their outputs are\nkept numerically consistent.\n\nYou can also use the hb_verifier tool in the dev PC environment for consistency\nverification, the reference command is as follows, refer to section The\nhb_verifier Tool for detailed descriptions:\n\n\n\nCompared to xxx.hbm, Horizon recommends prioritizing the quantization accuracy\nof the xxx_quantized_model.bc in the Python environment on the dev PC as it is\nmuch easier and faster, refer to the section Development Machine Python\nEnvironment Verification.\n\nxxx.hbm is evaluated on the board side based on C++ code, refer to the section\nDevelopment Board C++ Environment Verification. For more detailed accuracy\nverification and optimization recommendations, refer to the sections Model\nAccuracy Analysis and PTQ Model Accuracy Optimization.\n\n\nDevelopment Machine Python Environment Verification #\n\nTake the ResNet50 model as an example, for the single inference and verification\nset accuracy evaluation example of the quantized model\nresnet50_224x224_nv12_quantized_model.bc, refer to the scripts 04_inference.sh\nand 05_evaluate.sh in the sample directory. The reference commands are as\nfollows:\n\n\n\nThe two scripts will perform the inference by calling ../../cls_inference.py and\n../../cls_evaluate.py accordingly. Taking the cls_inference.py file as an\nexample, the usage logic of main interfaces in the code is as follows:\n\n\n\nThe preprocessing operations of the infer_image_preprocess function come from\nthe preprocess.py file described in the section Pre-processing Calibration Data.\nCompared to the calibration_transformers function, there is an additional\nprocess of transforming the data to input_type_rt (for parameter descriptions,\nsee the section Yaml Configuration File). The data preparation for HBIR model\ninference (including input_type_rt to input_type_train color conversion,\nmean/scale processing, etc.) will be done internally, so there is no need to do\nadditional external normalization and other processing. The specific code is as\nfollows:\n\n\n\n\nDevelopment Board C++ Environment Verification #\n\nOn the development board side, Horizon provides the unify compute platform (UCP\nfor short) to help you quickly complete the deployment of models, and provide\nrelated samples. You can refer to section Model Deployment to learn the basics\nof model deployment and BPU SDK API interface, and then to section AI-Benchmark\nto learn the complete code framework of the sample model accuracy evaluation.\n\nModel Deployment #\n\nThe OE package provides a basic example of model deployment in the following\npath to facilitate you to learn how to use the Model Inference API interface of\nUCP. For details on the example, refer to the section Basic Sample User Guide.\n\nAttention\n\nPlease note that before you can deploy the model, you need to obtain the models\nused on the board.\n\n * Execute resolve_ai_benchmark_ptq.sh and resolve_ai_benchmark_qat.sh scripts\n   in the samples/ai_toolchain/model_zoo/runtime/ai_benchmark directory.\n\n * Execute resolve_runtime_sample.sh script in the\n   samples/ai_toolchain/model_zoo/runtime/basic_samples directory.\n\nAmong them, the code/00_quick_start/src/main.cc file in the sample directory\nprovides the complete flow code of the resnet50 model from preparing data to\nmodel inference, and then execution of post-processing to produce classification\nresults.\n\nThe main code logic in main.cc includes the following 6 steps. For instructions\non the API interfaces involved in the code, refer to the section BPU SDK API\nDOC.\n\n 1. Load the model and get the model handle.\n\n 2. Prepare the model input and output tensor and apply the corresponding BPU\n    memory space.\n\n 3. Read the model input data and put it into the requested input tensor.\n\n 4. Infer the model and get the model output.\n\n 5. Implement the model post-processing based on the data in the output tensor.\n\n 6. Release the related resources.\n\n\n\nThe sample running is referenced as follows:\n\n\n\nAI-Benchmark #\n\nThe OE package also provides sample packages for typical classification,\ndetection, segmentation, and optical flow sample model board-side performance\nand accuracy evaluation under the\nsamples/ai_toolchain/ucp_tutorial/dnn/ai_benchmark path, on top of which you can\ncontinue with further application development.\n\nFor more details, you can refer to the section AI Benchmark User Guide.\n\n\nApplication Development#\n\nWhen you are satisfied with the performance and accuracy of the model, you can\nthen continue with the development of the upper-level application by referring\nto the steps described in the section Embedded Application Development.","routePath":"/en/guide/faststart/ptq_quickstart","lang":"en","toc":[{"text":"Floating-point Model Preparation","id":"floating-point-model-preparation","depth":2,"charIndex":354},{"text":"Model Checking","id":"model-checking","depth":2,"charIndex":1064},{"text":"Model Conversion","id":"model-conversion","depth":2,"charIndex":2007},{"text":"YAML Configuration File","id":"yaml-configuration-file","depth":3,"charIndex":-1},{"text":"Pre-processing Calibration Data","id":"pre-processing-calibration-data","depth":3,"charIndex":-1},{"text":"Model Conversion","id":"model-conversion-1","depth":3,"charIndex":-1},{"text":"Fast Performance Verification","id":"fast-performance-verification","depth":2,"charIndex":7994},{"text":"Static Performance Evaluation","id":"static-performance-evaluation","depth":3,"charIndex":8459},{"text":"Dynamic Performance Evaluation","id":"dynamic-performance-evaluation","depth":3,"charIndex":9393},{"text":"Accuracy Verification","id":"accuracy-verification","depth":2,"charIndex":11067},{"text":"Development Machine Python Environment Verification","id":"development-machine-python-environment-verification","depth":3,"charIndex":-1},{"text":"Development Board C++ Environment Verification","id":"development-board-c-environment-verification","depth":3,"charIndex":-1},{"text":"Model Deployment","id":"model-deployment","depth":4,"charIndex":-1},{"text":"AI-Benchmark","id":"ai-benchmark","depth":4,"charIndex":-1},{"text":"Application Development","id":"application-development","depth":2,"charIndex":15903}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":61,"title":"Algorithm Model QAT + Deployment Quick Start","content":"#\n\nIn addition to PTQ scheme, we also provide you with a quick start introduction\nof algorithmic models from quantization to deployment by using QAT scheme,\nplease refer to section Algorithm Model QAT + Deployment Quick Start.","routePath":"/en/guide/faststart/qat_quickstart","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":62,"title":"Key Concepts","content":"#\n\nThis section provides you with some concepts that may appear frequently within\nthe following as well as some commonly used background knowledge.\n\n * Original floating-point model\n   \n   Available models obtained from the DL framework training like TensorFlow,\n   PyTorch, etc. This model is computed with a precision of float32.\n\n * Board-side Deployable Model(HBM model)\n   \n   A model format suitable for running on the Horizon computing platform. It can\n   support model execution on both ARM CPU and BPU. Since the operation speed on\n   the BPU will be much faster than that on the CPU, the operators will be\n   computed on the BPU as much as possible. For operators that are not supported\n   on the BPU at the moment, they will be computed on the CPU.\n\n * Operator\n   \n   Deep learning algorithm are composed of computational units, we call these\n   computational units as the Operator (also known as op). The operator is a\n   mapping from a function space onto a function space, the name of the operator\n   is unique in the same model, but more than one operator of the same type can\n   exist. For example, Conv1, Conv2, are two different operators with the same\n   operator type.\n\n * Model conversion\n   \n   Process of converting the original floating-point model or the\n   standard-compliant onnx model into a Horizon hybrid heterogeneous model.\n\n * Model quantization\n   \n   Currently one of the most effective model optimization methods in industry.\n   Quantization is to establish data mapping relationships between fixed-point\n   data and floating-point data to achieve inference performance gains with\n   little precision loss, which can be simply understood as using \"low-bit\"\n   numbers to represent FP32 or other types of values, e.g., FP32 --> INT8 can\n   achieve 4 times parameter compression, and faster calculations can be achived\n   while memory usage is reduced.\n   \n   * The Quantize node is used to quantize the input data of the model from the\n     [float] type to [int8] type, which uses the following formula:\n     \n     \n     \n     * round(x) rounds the floating point number.\n     * clamp(x) clamps the data to an integer value between -128 and 127.\n     * scale is the quantized scale factor.\n     * zero_point is the asymmetric quantization zero-point offset value. When\n       in symmetric quantization, zero_point = 0.\n     \n     The C++ reference implementation is as follows:\n     \n     \n   \n   * The Dequantize node is used to dequantize output data of the model from the\n     int8 or int32 type back to float or double type with the following formula:\n     \n     \n     \n     The C++ reference implementation is as follows.\n     \n     \n\n * PTQ\n   \n   PTQ conversion scheme, a quantization method that first trains a\n   floating-point model and then uses a calibration image to calculate\n   quantization parameters to convert the floating-point model into a quantized\n   model. For more details, refer to the PTQ and QAT Introduction section.\n\n * QAT\n   \n   QAT (quantization-aware training) scheme, which intervenes in the\n   floating-point model structure during the floating-point training to enable\n   the model to perceive the loss from quantization and reduce the quantization\n   loss accuracy. For more details, refer to the PTQ and QAT Introduction\n   section.\n\n * Tensor\n   \n   The Tensor is a multidimensional array with a uniform data type, as a\n   container for the data computed by the operator, it contains the input and\n   output data. The carrier of tensor specific information, contains the name,\n   shape, data layout, data type, etc. of the tensor data.\n\n * Data layout\n   \n   In the deep learning, multidimensional data is stored through the\n   multidimensional array (tensor), and the generic neural network featuremaps\n   are usually stored using the four-dimensional array (i.e., 4D) format, i.e.,\n   the following four dimensions:\n   \n   * N: The number of Batch, e.g. the number of images.\n   * H: Height, the height of the image。\n   * W: Width, the weight of the image。\n   * C: Channel, the number of channels of the image.\n   \n   However, the data can only be stored linearly, so the four dimensions have a\n   corresponding order, and different layout formats of the data will affect the\n   computational performance. The common data storage formats are NCHW and NHWC:\n   \n   * NCHW: It stores all the pixel values of the same channel in order.\n   * NHWC: It stores the pixel values of the same position of different channels\n     in order.\n   \n   As shown below:\n\n * Data type\n   \n   The image data types commonly used below include rgb, bgr, gray, yuv444,\n   nv12, and featuremap.\n   \n   * The rgb, bgr, and gray are commonly used image format. Note that each value\n     is represented using UINT8.\n   * The yuv444 is also a popular image format. Note that each value is\n     represented using UINT8.\n   * The NV12 is a popular YUV420 image format. note that each value is\n     represented using UINT8.\n   * The Featuremap is suitable for cases where the above listed formats failed\n     to meet your needs, and this type uses float32 for each value. For example,\n     this format is commonly used for model processing such as radar and speech.\n\n * Batch, Batch Size\n   \n   In the model training process, a set of training samples used in each\n   iteration is called a batch. The batch size refers to the number of samples\n   that the model processes in each iteration.\n\n * Cosine similarity\n   \n   One of the accuracy comparison algorithms, the computation result takes the\n   value range of [-1,1], if the result of the comparison is closer to 1, it\n   means that the value of the two is more similar, and the closer to -1 means\n   that the value of the two is more opposite.\n\n * Stride\n   \n   Stride is the actual size of the space occupied by each line of an image when\n   it is stored in memory. Most computer processors work with 32-bit or 64-bit,\n   so the processor will read the complete amount of data at a time, preferably\n   in multiples of 4 bytes or 8 bytes, if other values, the computer will need\n   to specialize in processing, which will lead to a reduction in efficiency. In\n   order to efficiently process the images by the computer, it is common to fill\n   in some extra data on top of the original data to achieve 4-byte or 8-byte\n   alignment. The operation of alignment is also called Padding, and the actual\n   alignment rules depend on the specific hardware and software system.\n   \n   Suppose we have an 8-bit deep grayscale image with a height (Height) of 20\n   pixels and a width (Width) of 30 pixels, then the effective data per line of\n   the image is 30 bytes, and if the computer's alignment rule is 8 bytes, then\n   the span of the image after alignment is 32 bytes, at which point the amount\n   of data that needs to be Padding per line is 2 bytes.\n\n * Calibration dataset\n   \n   The dataset used to do forward inference in the PTQ scenario. The\n   distribution of this dataset represents the distribution of all datasets and\n   should be representative when obtaining the calibration set. If the dataset\n   is not the model-matched dataset or is not representative enough, the\n   quantization factor computed from the calibration set performs poorly on the\n   full dataset, with high quantization loss and low accuracy after\n   quantization.\n\nFor more information about abbreviations in the documents, please refer to the\nsection Common Abbreviations.","routePath":"/en/guide/key_concept","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":63,"title":"The Practical Guide to Deploying Multi-Batch ResNet18 Model with Pyramid Input","content":"#\n\nThe overall process of using the PTQ link of the Horizon OpenExplorer toolchain\nincludes multiple phases such as model optimization, model calibration, model\nconversion to fixed-point model, model compilation and boarding. This section\ntakes the sample of a multi-Batch classification model with Pyramid input based\non the public version of ResNet18, and the step-by-step deployment practice is\ndemonstrated in use for your reference.\n\n\nPrepare the Floating Point Model#\n\nTo prepare the ResNet18 floating point model, here we use torchvision to export\nthe desired floating point model.\n\n\n\n\nCalibration Set Preparation#\n\nInformation about the public version of the ResNet18 model can be found in The\nResNet18 description within the Pytorch documentation, where it can be seen that\nthe data preprocessing flow for the ResNet18 model is:\n\n 1. The image short side deflates to 256.\n 2. Resize the image to 224x224 with center crop.\n 3. The data were normalized with mean taking the values [0.485, 0.456, 0.406]\n    and std taking the values [0.229, 0.224, 0.225].\n\nA sample data preprocessing code is shown below:\n\n\n\nTo support PTQ model calibration, we need to take a small batch dataset from the\nImageNet dataset, using the first 100 images as a sample here:\n\n\n\nThe catalog structure of the calibration set generated based on the data\npreprocessing code above is then as follows:\n\n\n\n\nGenerate Board-side Model #\n\nPTQ Conversion Link supports both command line tools and PTQ API for model\nquantization compilation to generate board-side models, the following is an\nintroduction to the use of the two ways.\n\n\nCommand-line Tool#\n\nThe command line tool approach only requires you to install horizon_tc_ui\n(pre-installed in the Docker environment) and create the corresponding yaml file\nbased on the model information configuration, here we take the yaml file\ncorresponding to the ResNet18 model (config.yaml) to show and explain.\n\n\n\nNote\n\nHere, input_name and input_shape are left empty because the tool supports the\nscenario of single input with no dynamic shape (i.e., the tool internally parses\nthe ONNX model and obtains the name and shape of the input).\n\nWhen the yaml file configuration is complete, you just need to call The\nhb_compile Tool to execute the command, the tool executes the command and the\nkey log is as follows:\n\n\n\nAfter the completion of the command execution, in the yaml file working_dir\nparameter configuration of the directory (model_output), will be generated as\nshown below each stage of the intermediate model, the final model on the board\nand the model information file, of which resnet18_224x224_rgb.hbm that is, the\nboard-side of the inference can be reasoned that the model file:\n\n\n\n\nPTQ API#\n\nCommand line tool provides high ease of use but also bring some flexibility\nreduction, therefore, when you have the flexibility needs, you can use the PTQ\nAPI way to complete the quantization compilation of the model, the following for\nyou to introduce the use of the API way to generate the specific process of the\nboard-side of the model.\n\nAttention\n\nPlease note that due to the large number of parameters in some interfaces, only\nthe necessary parameters are configured in the sample below to facilitate your\noverall practice verification, please refer to HMCT API Refernence and HBDK Tool\nAPI Reference for the full parameters of specific interfaces.\n\nModel Optimized Calibration#\n\nFirst, graph optimization and calibration quantization are performed on the\nfloating-point model, a process for which we use the HMCT API, as exemplified\nbelow:\n\n\n\nAfter build_model is executed correctly, the ONNX model for each phase will be\ngenerated in the working_dir directory, which has the following directory\nstructure:\n\n\n\nThe *ptq_model.onnx file here is the ONNX model file after the graph\noptimization, calibration and pre-compilation process. For a specific\ndescription of the ONNX model in the intermediate stages, please refer to the\nsection Post-Training Quantization(PTQ) - PTQ Conversion Steps - Model\nQuantization and Compilation - Interpret Conversion Output.\n\nModel Turning Fixed Point and Compilation#\n\nNext, we need to complete the PTQ model to fixed-point model and model\ncompilation operation, this process we need to complete through the compiler's\nAPI, the sample is as follows:\n\n\n\nAfter compilation, the working_dir directory will hold the intermediate stage\nand final model files that can be used on the board, with the following\ndirectory structure:\n\n\n\n\nBuilding Board-side Sample #\n\n 1. Dependency libraries for preparing board-side sample.\n\nTo build the boardside sample as quickly as possible, we recommend that you use\nsamples/ucp_tutorial/deps_aarch64 directly from the OE package.\n\nDirectory as dependent libraries, and the key header files and dynamic libraries\nthat the board-side running sample depends on are listed below:\n\n\n\n 2. Board-side sample development\n\nThe following sample shows the process of completing one board-side model\ninference and obtaining the classification result TOP1 based on the binary file\ninput and the board-side model.\n\n\n\n 3. Cross-compile to generate board-side executable program\n\nBefore cross-compiling, you need to prepare CMakeLists.txt and the sample files.\nCMakeLists.txt content is as follows, because the sample does not contain data\npreprocessing and other operations, so there are fewer dependencies, here is\nmainly on the compilation parameters of GCC, dependent header files and dynamic\nlibrary configuration. Where dnn board-side inference library and hbucp is used\nto do operations on tensor.\n\n\n\nThe environment directory structure for compilation is as follows:\n\n\n\nWhen the sample files and CMakeLists.txt are ready, you can compile them. A\nsample of the compile command is shown below:\n\nAttention\n\nNote that the compilation scripts have to be configured with CC and CXX as the\nactual paths to cross-compile GCC and G++.\n\n\n\nOnce compiled, the board-ready run_sample binary program is generated. At this\npoint, the board-side sample build process is complete.\n\n\nPreparation for Board-side Operation#\n\nWhen the executable program is compiled, the inputs to the model need to be\nprepared. Since the scene calibration set for this hands-on tutorial can be used\nas model input, we can just use the calibration set data directly as model input\nhere. Of course, you can also modify the board-side program according to the\npre-processing logic of the calibration set and give it to the model input (this\nprocess should be noted that the modified program needs to ensure that the same\npre-processing of the original image has been done as in the calibration).\n\nHere we simply convert the calibration set in npy format to a binary file, as\nshown in the following sample:\n\n\n\nIn addition to the preparation of the input data, you are also required to\nensure that the following are now ready before running at the board-side:\n\n * Horizon J6 development board for the actual execution of board-side program\n   runs.\n * A model that can be used for board-side inference, i.e., the output of\n   Generate Board-side Model.\n * The board-side program, the output of Building Board-side Sample.\n * The board-side program depends on libraries, and in order to reduce\n   deployment costs, you can directly use the contents of the OE package\n   samples/ucp_tutorial/deps_aarch64/ucp/lib folder.\n * Input file in binary format for model input when the board-side sample\n   performs inference.\n\n\nBoard-side Operation#\n\nAfter the above steps are completed, we switch to the board-side environment,\nwhere we integrate the program, model files, input data and dependency libraries\ntogether in the board-side environment with the following reference directory\nstructure:\n\n\n\nFinally, you can configure LD_LIBRARY_PATH and run the program as follows:\n\n\n\nAs you can see, the label: 65 printed in the log is exactly the label\ncorresponding to the ILSVRC2012_val_00000001 image in the ImageNet dataset,\ni.e., the classification result is correct.\n\nThis concludes the full process of practicing the PTQ deployment of the\nmulti-batch ResNet18 model with Pyramid input.","routePath":"/en/guide/model_deployment_guidance/pyramid_batch_resnet18_deployment_guidance","lang":"en","toc":[{"text":"Prepare the Floating Point Model","id":"prepare-the-floating-point-model","depth":2,"charIndex":439},{"text":"Calibration Set Preparation","id":"calibration-set-preparation","depth":2,"charIndex":592},{"text":"Generate Board-side Model","id":"generate-board-side-model","depth":2,"charIndex":-1},{"text":"Command-line Tool","id":"command-line-tool","depth":3,"charIndex":1607},{"text":"PTQ API","id":"ptq-api","depth":3,"charIndex":2713},{"text":"Model Optimized Calibration","id":"model-optimized-calibration","depth":4,"charIndex":3379},{"text":"Model Turning Fixed Point and Compilation","id":"model-turning-fixed-point-and-compilation","depth":4,"charIndex":4089},{"text":"Building Board-side Sample","id":"building-board-side-sample","depth":2,"charIndex":-1},{"text":"Preparation for Board-side Operation","id":"preparation-for-board-side-operation","depth":2,"charIndex":6053},{"text":"Board-side Operation","id":"board-side-operation","depth":2,"charIndex":7463}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":64,"title":"The Practical Guide to Deploying the ResNet18 Model with Pyramid Input","content":"#\n\nThe overall process of using the PTQ link of the Horizon OpenExplorer toolchain\nincludes multiple phases such as model optimization, model calibration, model\nconversion to fixed-point model, model compilation and boarding. This section\ntakes the Pyramid input classification model based on the public version of\nResNet18 as a sample, and the step-by-step deployment practice is demonstrated\nin use for your reference.\n\n\nPrepare the Floating Point Model#\n\nTo prepare the ResNet18 floating point model, here we use torchvision to export\nthe desired floating point model.\n\n\n\n\nCalibration Set Preparation#\n\nInformation about the public version of the ResNet18 model can be found in The\nResNet18 description within the Pytorch documentation, where it can be seen that\nthe data preprocessing flow for the ResNet18 model is:\n\n 1. The image short side deflates to 256.\n 2. Resize the image to 224x224 with center crop.\n 3. The data were normalized with mean taking the values [0.485, 0.456, 0.406]\n    and std taking the values [0.229, 0.224, 0.225].\n\nA sample data preprocessing code is shown below:\n\n\n\nTo support PTQ model calibration, we need to take a small batch dataset from the\nImageNet dataset, using the first 100 images as a sample here:\n\n\n\nThe catalog structure of the calibration set generated based on the data\npreprocessing code above is then as follows:\n\n\n\n\nGenerate Board-side Model #\n\nPTQ Conversion Link supports both command line tools and PTQ API for model\nquantization compilation to generate board-side models, the following is an\nintroduction to the use of the two ways.\n\n\nCommand-line Tool#\n\nThe command line tool approach only requires you to install horizon_tc_ui\n(pre-installed in the Docker environment) and create the corresponding yaml file\nbased on the model information configuration, here we take the yaml file\ncorresponding to the ResNet18 model (config.yaml) to show and explain.\n\n\n\nNote\n\nHere, input_name and input_shape are left empty because the tool supports the\nscenario of single input with no dynamic shape (i.e., the tool internally parses\nthe ONNX model and obtains the name and shape of the input).\n\nWhen the yaml file configuration is complete, you just need to call The\nhb_compile Tool to execute the command, the tool executes the command and the\nkey log is as follows:\n\n\n\nAfter the completion of the command execution, in the yaml file working_dir\nparameter configuration of the directory (model_output), will be generated as\nshown below each stage of the intermediate model, the final model on the board\nand the model information file, of which resnet18_224x224_rgb.hbm that is, the\nboard-side of the inference can be reasoned that the model file:\n\n\n\n\nPTQ API#\n\nCommand line tool provides high ease of use but also bring some flexibility\nreduction, therefore, when you have the flexibility needs, you can use the PTQ\nAPI way to complete the quantization compilation of the model, the following for\nyou to introduce the use of the API way to generate the specific process of the\nboard-side of the model.\n\nAttention\n\nPlease note that due to the large number of parameters in some interfaces, only\nthe necessary parameters are configured in the sample below to facilitate your\noverall practice verification, please refer to HMCT API Refernence and HBDK Tool\nAPI Reference for the full parameters of specific interfaces.\n\nModel Optimized Calibration#\n\nFirst, graph optimization and calibration quantization are performed on the\nfloating-point model, a process for which we use the HMCT API, as exemplified\nbelow:\n\n\n\nAfter build_model is executed correctly, the ONNX model for each phase will be\ngenerated in the working_dir directory, which has the following directory\nstructure:\n\n\n\nThe *ptq_model.onnx file here is the ONNX model file after the graph\noptimization, calibration and pre-compilation process. For a specific\ndescription of the ONNX model in the intermediate stages, please refer to the\nsection Post-Training Quantization(PTQ) - PTQ Conversion Steps - Model\nQuantization and Compilation - Interpret Conversion Output.\n\nModel Turning Fixed Point and Compilation#\n\nNext, we need to complete the PTQ model to fixed-point model and model\ncompilation operation, this process we need to complete through the compiler's\nAPI, the sample is as follows:\n\n\n\nAfter compilation, the working_dir directory will hold the intermediate stage\nand final model files that can be used on the board, with the following\ndirectory structure:\n\n\n\n\nBuilding Board-side Sample #\n\n 1. Dependency libraries for preparing board-side sample.\n\nTo build the boardside sample as quickly as possible, we recommend that you use\nsamples/ucp_tutorial/deps_aarch64 directly from the OE package.\n\nDirectory as dependent libraries, and the key header files and dynamic libraries\nthat the board-side running sample depends on are listed below:\n\n\n\n 2. Board-side sample development\n\nThe following sample shows the process of completing one board-side model\ninference and obtaining the classification result TOP1 based on the binary file\ninput and the board-side model.\n\n\n\n 3. Cross-compile to generate board-side executable program\n\nBefore cross-compiling, you need to prepare CMakeLists.txt and the sample files.\nCMakeLists.txt content is as follows, because the sample does not contain data\npreprocessing and other operations, so there are fewer dependencies, here is\nmainly on the compilation parameters of GCC, dependent header files and dynamic\nlibrary configuration. Where dnn board-side inference library and hbucp is used\nto do operations on tensor.\n\n\n\nThe environment directory structure for compilation is as follows:\n\n\n\nWhen the sample files and CMakeLists.txt are ready, you can compile them. A\nsample of the compile command is shown below:\n\nAttention\n\nNote that the compilation scripts have to be configured with CC and CXX as the\nactual paths to cross-compile GCC and G++.\n\n\n\nOnce compiled, the board-ready run_sample binary program is generated. At this\npoint, the board-side sample build process is complete.\n\n\nPreparation for Board-side Operation#\n\nWhen the executable program is compiled, the inputs to the model need to be\nprepared. Since the scene calibration set for this hands-on tutorial can be used\nas model input, we can just use the calibration set data directly as model input\nhere. Of course, you can also modify the board-side program according to the\npre-processing logic of the calibration set and give it to the model input (this\nprocess should be noted that the modified program needs to ensure that the same\npre-processing of the original image has been done as in the calibration).\n\nHere we simply convert the calibration set in npy format to a binary file, as\nshown in the following sample:\n\n\n\nIn addition to the preparation of the input data, you are also required to\nensure that the following are now ready before running at the board-side:\n\n * Horizon J6 development board for the actual execution of board-side program\n   runs.\n * A model that can be used for board-side inference, i.e., the output of\n   Generate Board-side Model.\n * The board-side program, the output of Building Board-side Sample.\n * The board-side program depends on libraries, and in order to reduce\n   deployment costs, you can directly use the contents of the OE package\n   samples/ucp_tutorial/deps_aarch64/ucp/lib folder.\n * Input file in binary format for model input when the board-side sample\n   performs inference.\n\n\nBoard-side Operation#\n\nAfter the above steps are completed, we switch to the board-side environment,\nwhere we integrate the program, model files, input data and dependency libraries\ntogether in the board-side environment with the following reference directory\nstructure:\n\n\n\nFinally, you can configure LD_LIBRARY_PATH and run the program as follows:\n\n\n\nAs you can see, the label: 65 printed in the log is exactly the label\ncorresponding to the ILSVRC2012_val_00000001 image in the ImageNet dataset,\ni.e., the classification result is correct.\n\nThis concludes the full process of practicing the PTQ deployment of the ResNet18\nmodel with Pyramid input.","routePath":"/en/guide/model_deployment_guidance/pyramid_resnet18_deployment_guidance","lang":"en","toc":[{"text":"Prepare the Floating Point Model","id":"prepare-the-floating-point-model","depth":2,"charIndex":422},{"text":"Calibration Set Preparation","id":"calibration-set-preparation","depth":2,"charIndex":575},{"text":"Generate Board-side Model","id":"generate-board-side-model","depth":2,"charIndex":-1},{"text":"Command-line Tool","id":"command-line-tool","depth":3,"charIndex":1590},{"text":"PTQ API","id":"ptq-api","depth":3,"charIndex":2696},{"text":"Model Optimized Calibration","id":"model-optimized-calibration","depth":4,"charIndex":3362},{"text":"Model Turning Fixed Point and Compilation","id":"model-turning-fixed-point-and-compilation","depth":4,"charIndex":4072},{"text":"Building Board-side Sample","id":"building-board-side-sample","depth":2,"charIndex":-1},{"text":"Preparation for Board-side Operation","id":"preparation-for-board-side-operation","depth":2,"charIndex":6036},{"text":"Board-side Operation","id":"board-side-operation","depth":2,"charIndex":7446}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":65,"title":"The Practical Guide to Deploying the ResNet18 Model with Resizer Input","content":"#\n\nThe overall process of using the PTQ link of the Horizon OpenExplorer toolchain\nincludes multiple phases such as model optimization, model calibration, model\nconversion to fixed-point model, model compilation and boarding. This section\ntakes the Resizer input classification model based on the public version of\nResNet18 as a sample, and the step-by-step deployment practice is demonstrated\nin use for your reference.\n\n\nPrepare the Floating Point Model#\n\nTo prepare the ResNet18 floating point model, here we use torchvision to export\nthe desired floating point model.\n\n\n\n\nCalibration Set Preparation#\n\nInformation about the public version of the ResNet18 model can be found in The\nResNet18 description within the Pytorch documentation, where it can be seen that\nthe data preprocessing flow for the ResNet18 model is:\n\n 1. The image short side deflates to 256.\n 2. Resize the image to 224x224 with center crop.\n 3. The data were normalized with mean taking the values [0.485, 0.456, 0.406]\n    and std taking the values [0.229, 0.224, 0.225].\n\nA sample data preprocessing code is shown below:\n\n\n\nTo support PTQ model calibration, we need to take a small batch dataset from the\nImageNet dataset, using the first 100 images as a sample here:\n\n\n\nThe catalog structure of the calibration set generated based on the data\npreprocessing code above is then as follows:\n\n\n\n\nGenerate Board-side Model #\n\nPTQ Conversion Link supports both command line tools and PTQ API for model\nquantization compilation to generate board-side models, the following is an\nintroduction to the use of the two ways.\n\n\nCommand-line Tool#\n\nThe command line tool approach only requires you to install horizon_tc_ui\n(pre-installed in the Docker environment) and create the corresponding yaml file\nbased on the model information configuration, here we take the yaml file\ncorresponding to the ResNet18 model (config.yaml) to show and explain.\n\n\n\nNote\n\nHere, input_name and input_shape are left empty because the tool supports the\nscenario of single input with no dynamic shape (i.e., the tool internally parses\nthe ONNX model and obtains the name and shape of the input).\n\nWhen the yaml file configuration is complete, you just need to call The\nhb_compile Tool to execute the command, the tool executes the command and the\nkey log is as follows:\n\n\n\nAfter the completion of the command execution, in the yaml file working_dir\nparameter configuration of the directory (model_output), will be generated as\nshown below each stage of the intermediate model, the final model on the board\nand the model information file, of which resnet18_224x224_nv12_resizer.hbm that\nis, the board-side of the inference can be reasoned that the model file:\n\n\n\n\nPTQ API#\n\nCommand line tool provides high ease of use but also bring some flexibility\nreduction, therefore, when you have the flexibility needs, you can use the PTQ\nAPI way to complete the quantization compilation of the model, the following for\nyou to introduce the use of the API way to generate the specific process of the\nboard-side of the model.\n\nAttention\n\nPlease note that due to the large number of parameters in some interfaces, only\nthe necessary parameters are configured in the sample below to facilitate your\noverall practice verification, please refer to HMCT API Refernence and HBDK Tool\nAPI Reference for the full parameters of specific interfaces.\n\nModel Optimized Calibration#\n\nFirst, graph optimization and calibration quantization are performed on the\nfloating-point model, a process for which we use the HMCT API, as exemplified\nbelow:\n\n\n\nAfter build_model is executed correctly, the ONNX model for each phase will be\ngenerated in the working_dir directory, which has the following directory\nstructure:\n\n\n\nThe *ptq_model.onnx file here is the ONNX model file after the graph\noptimization, calibration and pre-compilation process. For a specific\ndescription of the ONNX model in the intermediate stages, please refer to the\nsection Post-Training Quantization(PTQ) - PTQ Conversion Steps - Model\nQuantization and Compilation - Interpret Conversion Output.\n\nModel Turning Fixed Point and Compilation#\n\nNext, we need to complete the PTQ model to fixed-point model and model\ncompilation operation, this process we need to complete through the compiler's\nAPI, the sample is as follows:\n\n\n\nAfter compilation, the working_dir directory will hold the intermediate stage\nand final model files that can be used on the board, with the following\ndirectory structure:\n\n\n\n\nBuilding Board-side Sample #\n\n 1. Dependency libraries for preparing board-side sample.\n\nTo build the boardside sample as quickly as possible, we recommend that you use\nsamples/ucp_tutorial/deps_aarch64 directly from the OE package.\n\nDirectory as dependent libraries, and the key header files and dynamic libraries\nthat the board-side running sample depends on are listed below:\n\n\n\n 2. Board-side sample development\n\nThe following sample shows the process of completing one board-side model\ninference and obtaining the classification result TOP1 based on the binary file\ninput and the board-side model.\n\n\n\n 3. Cross-compile to generate board-side executable program\n\nBefore cross-compiling, you need to prepare CMakeLists.txt and the sample files.\nCMakeLists.txt content is as follows, because the sample does not contain data\npreprocessing and other operations, so there are fewer dependencies, here is\nmainly on the compilation parameters of GCC, dependent header files and dynamic\nlibrary configuration. Where dnn board-side inference library and hbucp is used\nto do operations on tensor.\n\n\n\nThe environment directory structure for compilation is as follows:\n\n\n\nWhen the sample files and CMakeLists.txt are ready, you can compile them. A\nsample of the compile command is shown below:\n\nAttention\n\nNote that the compilation scripts have to be configured with CC and CXX as the\nactual paths to cross-compile GCC and G++.\n\n\n\nOnce compiled, the board-ready run_sample binary program is generated. At this\npoint, the board-side sample build process is complete.\n\n\nPreparation for Board-side Operation#\n\nWhen the executable program is compiled, the inputs to the model need to be\nprepared. Since the scene calibration set for this hands-on tutorial can be used\nas model input, we can just use the calibration set data directly as model input\nhere. Of course, you can also modify the board-side program according to the\npre-processing logic of the calibration set and give it to the model input (this\nprocess should be noted that the modified program needs to ensure that the same\npre-processing of the original image has been done as in the calibration).\n\nHere we simply convert the calibration set in npy format to a binary file, as\nshown in the following sample:\n\n\n\nIn addition to the preparation of the input data, you are also required to\nensure that the following are now ready before running at the board-side:\n\n * Horizon J6 development board for the actual execution of board-side program\n   runs.\n * A model that can be used for board-side inference, i.e., the output of\n   Generate Board-side Model.\n * The board-side program, the output of Building Board-side Sample.\n * The board-side program depends on libraries, and in order to reduce\n   deployment costs, you can directly use the contents of the OE package\n   samples/ucp_tutorial/deps_aarch64/ucp/lib folder.\n * Input file in binary format for model input when the board-side sample\n   performs inference.\n\n\nBoard-side Operation#\n\nAfter the above steps are completed, we switch to the board-side environment,\nwhere we integrate the program, model files, input data and dependency libraries\ntogether in the board-side environment with the following reference directory\nstructure:\n\n\n\nFinally, you can configure LD_LIBRARY_PATH and run the program as follows:\n\n\n\nAs you can see, the label: 65 printed in the log is exactly the label\ncorresponding to the ILSVRC2012_val_00000001 image in the ImageNet dataset,\ni.e., the classification result is correct.\n\nThis concludes the full process of practicing the PTQ deployment of the ResNet18\nmodel with Resizer input.","routePath":"/en/guide/model_deployment_guidance/resizer_resnet18_deployment_guidance","lang":"en","toc":[{"text":"Prepare the Floating Point Model","id":"prepare-the-floating-point-model","depth":2,"charIndex":422},{"text":"Calibration Set Preparation","id":"calibration-set-preparation","depth":2,"charIndex":575},{"text":"Generate Board-side Model","id":"generate-board-side-model","depth":2,"charIndex":-1},{"text":"Command-line Tool","id":"command-line-tool","depth":3,"charIndex":1590},{"text":"PTQ API","id":"ptq-api","depth":3,"charIndex":2705},{"text":"Model Optimized Calibration","id":"model-optimized-calibration","depth":4,"charIndex":3371},{"text":"Model Turning Fixed Point and Compilation","id":"model-turning-fixed-point-and-compilation","depth":4,"charIndex":4081},{"text":"Building Board-side Sample","id":"building-board-side-sample","depth":2,"charIndex":-1},{"text":"Preparation for Board-side Operation","id":"preparation-for-board-side-operation","depth":2,"charIndex":6045},{"text":"Board-side Operation","id":"board-side-operation","depth":2,"charIndex":7455}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":66,"title":"The Practical Guide to Deploying ResNet18 Model with Dequantize Nodes Removed","content":"#\n\nThe overall process of using the PTQ link of the Horizon OpenExplorer toolchain\nincludes multiple phases such as model optimization, model calibration, model\nconversion to fixed-point model, model compilation and boarding. This section\ntakes the sample of removing dequantize node from the RGB input classification\nmodel based on the public version of ResNet18, and the step-by-step deployment\npractice is demonstrated in use for your reference.\n\n\nPrepare the Floating Point Model#\n\nTo prepare the ResNet18 floating point model, here we use torchvision to export\nthe desired floating point model.\n\n\n\n\nCalibration Set Preparation#\n\nInformation about the public version of the ResNet18 model can be found in The\nResNet18 description within the Pytorch documentation, where it can be seen that\nthe data preprocessing flow for the ResNet18 model is:\n\n 1. The image short side deflates to 256.\n 2. Resize the image to 224x224 with center crop.\n 3. The data were normalized with mean taking the values [0.485, 0.456, 0.406]\n    and std taking the values [0.229, 0.224, 0.225].\n\nA sample data preprocessing code is shown below:\n\n\n\nTo support PTQ model calibration, we need to take a small batch dataset from the\nImageNet dataset, using the first 100 images as a sample here:\n\n\n\nThe catalog structure of the calibration set generated based on the data\npreprocessing code above is then as follows:\n\n\n\n\nGenerate Board-side Model #\n\nPTQ Conversion Link supports both command line tools and PTQ API for model\nquantization compilation to generate board-side models, the following is an\nintroduction to the use of the two ways.\n\n\nCommand-line Tool#\n\nThe command line tool approach only requires you to install horizon_tc_ui\n(pre-installed in the Docker environment) and create the corresponding yaml file\nbased on the model information configuration, here we take the yaml file\ncorresponding to the ResNet18 model (config.yaml) to show and explain.\n\n\n\nNote\n\nHere, input_name and input_shape are left empty because the tool supports the\nscenario of single input with no dynamic shape (i.e., the tool internally parses\nthe ONNX model and obtains the name and shape of the input).\n\nWhen the yaml file configuration is complete, you just need to call The\nhb_compile Tool to execute the command, the tool executes the command and the\nkey log is as follows:\n\n\n\nAfter the completion of the command execution, in the yaml file working_dir\nparameter configuration of the directory (model_output), will be generated as\nshown below each stage of the intermediate model, the final model on the board\nand the model information file, of which resnet18_224x224_rgb_removed.hbm that\nis, the board-side of the inference can be reasoned that the model file:\n\n\n\n\nPTQ API#\n\nCommand line tool provides high ease of use but also bring some flexibility\nreduction, therefore, when you have the flexibility needs, you can use the PTQ\nAPI way to complete the quantization compilation of the model, the following for\nyou to introduce the use of the API way to generate the specific process of the\nboard-side of the model.\n\nAttention\n\nPlease note that due to the large number of parameters in some interfaces, only\nthe necessary parameters are configured in the sample below to facilitate your\noverall practice verification, please refer to HMCT API Refernence and HBDK Tool\nAPI Reference for the full parameters of specific interfaces.\n\nModel Optimized Calibration#\n\nFirst, graph optimization and calibration quantization are performed on the\nfloating-point model, a process for which we use the HMCT API, as exemplified\nbelow:\n\n\n\nAfter build_model is executed correctly, the ONNX model for each phase will be\ngenerated in the working_dir directory, which has the following directory\nstructure:\n\n\n\nThe *ptq_model.onnx file here is the ONNX model file after the graph\noptimization, calibration and pre-compilation process. For a specific\ndescription of the ONNX model in the intermediate stages, please refer to the\nsection Post-Training Quantization(PTQ) - PTQ Conversion Steps - Model\nQuantization and Compilation - Interpret Conversion Output.\n\nModel Turning Fixed Point and Compilation#\n\nNext, we need to complete the PTQ model to fixed-point model and model\ncompilation operation, this process we need to complete through the compiler's\nAPI, the sample is as follows:\n\n\n\nAfter compilation, the working_dir directory will hold the intermediate stage\nand final model files that can be used on the board, with the following\ndirectory structure:\n\n\n\n\nBuilding Board-side Sample #\n\n 1. Dependency libraries for preparing board-side sample.\n\nTo build the boardside sample as quickly as possible, we recommend that you use\nsamples/ucp_tutorial/deps_aarch64 directly from the OE package.\n\nDirectory as dependent libraries, and the key header files and dynamic libraries\nthat the board-side running sample depends on are listed below:\n\n\n\n 2. Board-side sample development\n\nThe following sample shows the process of completing one board-side model\ninference and obtaining the classification result TOP1 based on the binary file\ninput and the board-side model.\n\n\n\n 3. Cross-compile to generate board-side executable program\n\nBefore cross-compiling, you need to prepare CMakeLists.txt and the sample files.\nCMakeLists.txt content is as follows, because the sample does not contain data\npreprocessing and other operations, so there are fewer dependencies, here is\nmainly on the compilation parameters of GCC, dependent header files and dynamic\nlibrary configuration. Where dnn board-side inference library and hbucp is used\nto do operations on tensor.\n\n\n\nThe environment directory structure for compilation is as follows:\n\n\n\nWhen the sample files and CMakeLists.txt are ready, you can compile them. A\nsample of the compile command is shown below:\n\nAttention\n\nNote that the compilation scripts have to be configured with CC and CXX as the\nactual paths to cross-compile GCC and G++.\n\n\n\nOnce compiled, the board-ready run_sample binary program is generated. At this\npoint, the board-side sample build process is complete.\n\n\nPreparation for Board-side Operation#\n\nWhen the executable program is compiled, the inputs to the model need to be\nprepared. Since the scene calibration set for this hands-on tutorial can be used\nas model input, we can just use the calibration set data directly as model input\nhere. Of course, you can also modify the board-side program according to the\npre-processing logic of the calibration set and give it to the model input (this\nprocess should be noted that the modified program needs to ensure that the same\npre-processing of the original image has been done as in the calibration).\n\nHere we simply convert the calibration set in npy format to a binary file, as\nshown in the following sample:\n\n\n\nIn addition to the preparation of the input data, you are also required to\nensure that the following are now ready before running at the board-side:\n\n * Horizon J6 development board for the actual execution of board-side program\n   runs.\n * A model that can be used for board-side inference, i.e., the output of\n   Generate Board-side Model.\n * The board-side program, the output of Building Board-side Sample.\n * The board-side program depends on libraries, and in order to reduce\n   deployment costs, you can directly use the contents of the OE package\n   samples/ucp_tutorial/deps_aarch64/ucp/lib folder.\n * Input file in binary format for model input when the board-side sample\n   performs inference.\n\n\nBoard-side Operation#\n\nAfter the above steps are completed, we switch to the board-side environment,\nwhere we integrate the program, model files, input data and dependency libraries\ntogether in the board-side environment with the following reference directory\nstructure:\n\n\n\nFinally, you can configure LD_LIBRARY_PATH and run the program as follows:\n\n\n\nAs you can see, the label: 65 printed in the log is exactly the label\ncorresponding to the ILSVRC2012_val_00000001 image in the ImageNet dataset,\ni.e., the classification result is correct.\n\nThis concludes the full process of practicing the PTQ deployment of removing\ndequantize node with the ResNet18 model with RGB input.","routePath":"/en/guide/model_deployment_guidance/rgb_remove_resnet18_deployment_guidance","lang":"en","toc":[{"text":"Prepare the Floating Point Model","id":"prepare-the-floating-point-model","depth":2,"charIndex":450},{"text":"Calibration Set Preparation","id":"calibration-set-preparation","depth":2,"charIndex":603},{"text":"Generate Board-side Model","id":"generate-board-side-model","depth":2,"charIndex":-1},{"text":"Command-line Tool","id":"command-line-tool","depth":3,"charIndex":1618},{"text":"PTQ API","id":"ptq-api","depth":3,"charIndex":2732},{"text":"Model Optimized Calibration","id":"model-optimized-calibration","depth":4,"charIndex":3398},{"text":"Model Turning Fixed Point and Compilation","id":"model-turning-fixed-point-and-compilation","depth":4,"charIndex":4108},{"text":"Building Board-side Sample","id":"building-board-side-sample","depth":2,"charIndex":-1},{"text":"Preparation for Board-side Operation","id":"preparation-for-board-side-operation","depth":2,"charIndex":6072},{"text":"Board-side Operation","id":"board-side-operation","depth":2,"charIndex":7482}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":67,"title":"The Practical Guide to Deploying the ResNet18 Model with RGB Input","content":"#\n\nThe overall process of using the PTQ link of the Horizon OpenExplorer toolchain\nincludes multiple phases such as model optimization, model calibration, model\nconversion to fixed-point model, model compilation and boarding. This section\ntakes the RGB input classification model based on the public version of ResNet18\nas a sample , and the step-by-step deployment practice is demonstrated in use\nfor your reference.\n\n\nPrepare the Floating Point Model#\n\nTo prepare the ResNet18 floating point model, here we use torchvision to export\nthe desired floating point model.\n\n\n\n\nCalibration Set Preparation#\n\nInformation about the public version of the ResNet18 model can be found in The\nResNet18 description within the Pytorch documentation, where it can be seen that\nthe data preprocessing flow for the ResNet18 model is:\n\n 1. The image short side deflates to 256.\n 2. Resize the image to 224x224 with center crop.\n 3. The data were normalized with mean taking the values [0.485, 0.456, 0.406]\n    and std taking the values [0.229, 0.224, 0.225].\n\nA sample data preprocessing code is shown below:\n\n\n\nTo support PTQ model calibration, we need to take a small batch dataset from the\nImageNet dataset, using the first 100 images as a sample here:\n\n\n\nThe catalog structure of the calibration set generated based on the data\npreprocessing code above is then as follows:\n\n\n\n\nGenerate Board-side Model #\n\nPTQ Conversion Link supports both command line tools and PTQ API for model\nquantization compilation to generate board-side models, the following is an\nintroduction to the use of the two ways.\n\n\nCommand-line Tool#\n\nThe command line tool approach only requires you to install horizon_tc_ui\n(pre-installed in the Docker environment) and create the corresponding yaml file\nbased on the model information configuration, here we take the yaml file\ncorresponding to the ResNet18 model (config.yaml) to show and explain.\n\n\n\nNote\n\nHere, input_name and input_shape are left empty because the tool supports the\nscenario of single input with no dynamic shape (i.e., the tool internally parses\nthe ONNX model and obtains the name and shape of the input).\n\nWhen the yaml file configuration is complete, you just need to call The\nhb_compile Tool to execute the command, the tool executes the command and the\nkey log is as follows:\n\n\n\nAfter the completion of the command execution, in the yaml file working_dir\nparameter configuration of the directory (model_output), will be generated as\nshown below each stage of the intermediate model, the final model on the board\nand the model information file, of which resnet18_224x224_rgb.hbm that is, the\nboard-side of the inference can be reasoned that the model file:\n\n\n\n\nPTQ API#\n\nCommand line tool provides high ease of use but also bring some flexibility\nreduction, therefore, when you have the flexibility needs, you can use the PTQ\nAPI way to complete the quantization compilation of the model, the following for\nyou to introduce the use of the API way to generate the specific process of the\nboard-side of the model.\n\nAttention\n\nPlease note that due to the large number of parameters in some interfaces, only\nthe necessary parameters are configured in the sample below to facilitate your\noverall practice verification, please refer to HMCT API Refernence and HBDK Tool\nAPI Reference for the full parameters of specific interfaces.\n\nModel Optimized Calibration#\n\nFirst, graph optimization and calibration quantization are performed on the\nfloating-point model, a process for which we use the HMCT API, as exemplified\nbelow:\n\n\n\nAfter build_model is executed correctly, the ONNX model for each phase will be\ngenerated in the working_dir directory, which has the following directory\nstructure:\n\n\n\nThe *ptq_model.onnx file here is the ONNX model file after the graph\noptimization, calibration and pre-compilation process. For a specific\ndescription of the ONNX model in the intermediate stages, please refer to the\nsection Post-Training Quantization(PTQ) - PTQ Conversion Steps - Model\nQuantization and Compilation - Interpret Conversion Output.\n\nModel Turning Fixed Point and Compilation#\n\nNext, we need to complete the PTQ model to fixed-point model and model\ncompilation operation, this process we need to complete through the compiler's\nAPI, the sample is as follows:\n\n\n\nAfter compilation, the working_dir directory will hold the intermediate stage\nand final model files that can be used on the board, with the following\ndirectory structure:\n\n\n\n\nBuilding Board-side Sample #\n\n 1. Dependency libraries for preparing board-side sample.\n\nTo build the boardside sample as quickly as possible, we recommend that you use\nsamples/ucp_tutorial/deps_aarch64 directly from the OE package.\n\nDirectory as dependent libraries, and the key header files and dynamic libraries\nthat the board-side running sample depends on are listed below:\n\n\n\n 2. Board-side sample development\n\nThe following sample shows the process of completing one board-side model\ninference and obtaining the classification result TOP1 based on the binary file\ninput and the board-side model.\n\n\n\n 3. Cross-compile to generate board-side executable program\n\nBefore cross-compiling, you need to prepare CMakeLists.txt and the sample files.\nCMakeLists.txt content is as follows, because the sample does not contain data\npreprocessing and other operations, so there are fewer dependencies, here is\nmainly on the compilation parameters of GCC, dependent header files and dynamic\nlibrary configuration. Where dnn board-side inference library and hbucp is used\nto do operations on tensor.\n\n\n\nThe environment directory structure for compilation is as follows:\n\n\n\nWhen the sample files and CMakeLists.txt are ready, you can compile them. A\nsample of the compile command is shown below:\n\nAttention\n\nNote that the compilation scripts have to be configured with CC and CXX as the\nactual paths to cross-compile GCC and G++.\n\n\n\nOnce compiled, the board-ready run_sample binary program is generated. At this\npoint, the board-side sample build process is complete.\n\n\nPreparation for Board-side Operation#\n\nWhen the executable program is compiled, the inputs to the model need to be\nprepared. Since the scene calibration set for this hands-on tutorial can be used\nas model input, we can just use the calibration set data directly as model input\nhere. Of course, you can also modify the board-side program according to the\npre-processing logic of the calibration set and give it to the model input (this\nprocess should be noted that the modified program needs to ensure that the same\npre-processing of the original image has been done as in the calibration).\n\nHere we simply convert the calibration set in npy format to a binary file, as\nshown in the following sample:\n\n\n\nIn addition to the preparation of the input data, you are also required to\nensure that the following are now ready before running at the board-side:\n\n * Horizon J6 development board for the actual execution of board-side program\n   runs.\n * A model that can be used for board-side inference, i.e., the output of\n   Generate Board-side Model.\n * The board-side program, the output of Building Board-side Sample.\n * The board-side program depends on libraries, and in order to reduce\n   deployment costs, you can directly use the contents of the OE package\n   samples/ucp_tutorial/deps_aarch64/ucp/lib folder.\n * Input file in binary format for model input when the board-side sample\n   performs inference.\n\n\nBoard-side Operation#\n\nAfter the above steps are completed, we switch to the board-side environment,\nwhere we integrate the program, model files, input data and dependency libraries\ntogether in the board-side environment with the following reference directory\nstructure:\n\n\n\nFinally, you can configure LD_LIBRARY_PATH and run the program as follows:\n\n\n\nAs you can see, the label: 65 printed in the log is exactly the label\ncorresponding to the ILSVRC2012_val_00000001 image in the ImageNet dataset,\ni.e., the classification result is correct.\n\nThis concludes the full process of practicing the PTQ deployment of the ResNet18\nmodel with RGB input.","routePath":"/en/guide/model_deployment_guidance/rgb_resnet18_deployment_guidance","lang":"en","toc":[{"text":"Prepare the Floating Point Model","id":"prepare-the-floating-point-model","depth":2,"charIndex":419},{"text":"Calibration Set Preparation","id":"calibration-set-preparation","depth":2,"charIndex":572},{"text":"Generate Board-side Model","id":"generate-board-side-model","depth":2,"charIndex":-1},{"text":"Command-line Tool","id":"command-line-tool","depth":3,"charIndex":1587},{"text":"PTQ API","id":"ptq-api","depth":3,"charIndex":2693},{"text":"Model Optimized Calibration","id":"model-optimized-calibration","depth":4,"charIndex":3359},{"text":"Model Turning Fixed Point and Compilation","id":"model-turning-fixed-point-and-compilation","depth":4,"charIndex":4069},{"text":"Building Board-side Sample","id":"building-board-side-sample","depth":2,"charIndex":-1},{"text":"Preparation for Board-side Operation","id":"preparation-for-board-side-operation","depth":2,"charIndex":6033},{"text":"Board-side Operation","id":"board-side-operation","depth":2,"charIndex":7443}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":68,"title":"Model Performance Optimization","content":"#\n\nBased on previous performance analysis, you may find that the performance\nresults are less than expected. This section covers Horizon's recommendations\nand measures to improve model performance, including:\n\n * Check Those Performance-affecting YAML Configuration Parameters;\n\n * CPU OP Processing;\n\nNote\n\nChecking yaml configuration parameters and CPU OP Processing only apply to usage\nscenarios where the model is compiled by using hb_compile.\n\nBecause some optimizations may influence the parameter space of the original\nfloating-point model, in other words, it may cause model retraining. To prevent\nthe costs of repeated adjustments and retraining brought about by model\nperformance optimization, we suggest that you use random parameters to export\nmodels and validate performance before getting satisfactory model performance.\n\n\nCheck Those Performance-affecting YAML Configuration Parameters#\n\nSome parameters in the model conversion configuration file can affect model's\nfinal performance, you can check if they've been correctly specified as you\nexpected. The definitions and functions of all parameters please refer to the\nSpecific Parameter Information section.\n\n * The debug_mode parameter is used for accuracy debugging analysis. If\n   dump_all_layers_output is configured, a dequantized output node will be added\n   to each convolutional and matmul operator to dump the intermediate results in\n   model conversion. It will significantly reduce model's onboard performance.\n   Therefore, please remember to remove the dump_all_layers_output parameter\n   from debug_mode in performance evaluation.\n\n * The compile_mode parameter is used to select whether the optimization\n   direction is bandwidth or latency when compiling the model. If you are\n   concerned about performance, please configure it as latency.\n\n * The optimize_level parameter is used to select the optimization level of the\n   compiler. O0: No optimization, fastest compilation speed and lowest\n   optimization level. O1 to O2: As the optimization level increases, the\n   compiled model is expected to execute faster, but the compilation time is\n   also expected to be longer.\n\n * The max_time_per_fc parameter is used to control the execution time of the\n   function-call of the compiled model data instruction, thus implementing the\n   model priority preemption function. Setting this parameter to change the\n   execution time of the function-call of the preempted model will affect the\n   on-board performance of the model.\n\n\nCPU OP Processing#\n\nIf the evaluation of hrt_model_exec perf confirms that the apparent performance\nbottleneck is due to the current operator running on the CPU. Then in such case,\nwe suggest that you should confirm if the OPs which currently running on the CPU\ncan be supported by the BPU as described in the Toolchain Operator Support\nConstraint List-ONNX Operator Support List section.\n\nIf the operator parameters used are outside the constraints supported by the\nBPU, we suggest that you adjust the corresponding computing parameters of the\noriginal floating-point model back into the restricted range. To help you\nquickly find out the off-limits parameter(s), we suggest that you proceed the\nmodel check procedure as described in the Check the Model section, the tool will\nprint out the off-limits parameters at the console.\n\nNote\n\nNote that you'll need to handle the effect on model performance (if any) caused\nby modifying the original Floating-point model parameters. Take the\ninput_channel or output_channel of Convolution exceeding restrictions as classic\nexamples, by reducing number of channels to quickly enable the OP to be\nsupported by the BPU can also affect model accuracy.\n\nIf the operator does not have BPU support, you need to optimize it according to\nthe following conditions:\n\n * CPU operator at the middle of the model\n   \n   For cases where the CPU operator is in the middle of the model, it is\n   recommended that you try parameter adjustment, operator replacement or model\n   modification as a priority.\n\n * CPU operator at the beginning and end of the model\n   \n   For the case where the CPU operator is at the beginning and end of the model,\n   please refer to the following example, using the\n   quantization/anti-quantization nodes as an example.\n   \n   * For nodes connected to the model input and output, you can add the\n     remove_node_type parameter to the yaml file model_parameters configuration\n     group (model parameters group) and recompile the model.\n     \n     ","routePath":"/en/guide/performance_tune","lang":"en","toc":[{"text":"Check Those Performance-affecting YAML Configuration Parameters","id":"check-those-performance-affecting-yaml-configuration-parameters","depth":2,"charIndex":836},{"text":"CPU OP Processing","id":"cpu-op-processing","depth":2,"charIndex":2509}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":69,"title":"Eager Mode","content":"#\n\nThe horizon_plugin_pytorch currently supports quantization using the eager mode,\nhowever, we no longer recommend using this mode. For quantization in eager mode,\ncurrently you are still supported by our horizon_plugin_pytorch. The overall\nflow of the Eager mode is also based on PyTorch's quantization interface and\nideas, so we recommend that you read the Eager mode section of the PyTorch\nofficial document.\n\n\nDifference from fx mode#\n\nThe main differences between using eager mode and fx mode in\nhorizon_plugin_pytorch are:\n\n * The eager mode only supports module operators. Before carryout model\n   quantization, you need to manually replace the functional operators in the\n   floating-point model with Module-type operators in PyTorch or proprietary\n   operators defined in horizon_plugin_pytorch, including but not limited to:\n\nORIGINAL FLOATING-POINT OPERATOR   OPERATOR TO BE REPLACED\ntorch.nn.functional.relu           torch.nn.ReLU()\na + b / torch.add                  horizon.nn.quantized.FloatFunctional().add\nTensor.exp                         horizon.nn.Exp()\ntorch.nn.functional.interpolate    horizon.nn.Interpolate()\n\n * You must manually define the operators to be fused and explicitly call the\n   fusion function before carryout model quantization, and also specify the use\n   of the fuser_func provided in horizon_plugin_pytorch when calling it. As\n   shown below:\n\n","routePath":"/en/guide/plugin/advanced_tutorial/eager","lang":"en","toc":[{"text":"Difference from fx mode","id":"difference-from-fx-mode","depth":2,"charIndex":414}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":70,"title":"FX Quantization Principle","content":"#\n\nBefore reading this section, it is recommended to read the torch.fx — PyTorch\ndocumentation to get an initial understanding of torch's FX mechanism.\n\nFX uses the symbolic execution approach, which allows models to be graphically\nconstructed at the nn.Module or function level, allowing for automated fuse and\nother graph-based optimizations.\n\n\nQuantized Process#\n\n\nFuse (Optional)#\n\nFX can sense the computational graph, so you can automate the fusion of\noperators. You no longer need to manually specify the operators to be fused,\njust call the interface directly.\n\n\n\n * Note that there is no inplace parameter for fuse_fx, because internally you\n   need to do symbolic trace on the model to generate a GraphModule, so you\n   can't do inplace modification.\n * The fused_model and model will share almost all attributes (including\n   submodules, operators, etc.), so do not make any changes to the model after\n   fuse, as this may affect the fused_model.\n * It is not necessary to explicitly call the fuse_fx interface, as the\n   subsequent prepare interface integrates the fuse procedure internally.\n\n\nPrepare#\n\nThe global march must be set according to the target hardware platform before\ncalling the prepare interface, which internally performs a fuse procedure (even\nif the model has already been fused) and then replaces the eligible operators in\nthe model with implementations from horizon.nn.qat.\n\n * You can choose the appropriate qconfig (Calibtaion or QAT, note that you\n   can't mix the two qconfigs) if you want.\n * Similar to fuse_fx, this interface does not support the inplace parameter,\n   and please do not make any changes to the input model after prepare.\n\n\n\n\nEager Mode Compatibility#\n\nIn most cases, FX quantized interfaces can directly replace eager mode quantized\ninterfaces (prepare_qat -> prepare), but they cannot be mixed with eager mode\ninterfaces. Some models require some modifications to the code structure in the\nfollowing cases.\n\n * Operations not supported by FX: the symbolic trace of torch supports limited\n   operations, e.g., it does not support non-static variables as judgment\n   conditions, it does not support pkgs (e.g., numpy) other than torch by\n   default, etc., and unexecuted conditional branches will be discarded.\n * Operations that don't want to be processed by FX: If torch's ops are used in\n   the model's pre- and post-processing, FX will treat them as part of the model\n   when tracing, producing undesired behavior (e.g., replacing some of torch's\n   function calls with FloatFunctional).\n\nBoth of these cases can be avoided by using wrap, as illustrated by RetinaNet.\n\n","routePath":"/en/guide/plugin/advanced_tutorial/fx_quantization_explain","lang":"en","toc":[{"text":"Quantized Process","id":"quantized-process","depth":2,"charIndex":346},{"text":"Fuse (Optional)","id":"fuse-optional","depth":3,"charIndex":367},{"text":"Prepare","id":"prepare","depth":3,"charIndex":1105},{"text":"Eager Mode Compatibility","id":"eager-mode-compatibility","depth":3,"charIndex":1681}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":71,"title":"Operator Fusion","content":"#\n\nThe operator fusion supported by the training tool can be divided into two main\ncategories: 1. absorb BN. 2. fuse Add, ReLU(6).\n\n\nAbsorb BN#\n\nThe purpose of absorbing BN is to reduce the computing workload of the model.\nSince BN is a linear transformation process, the parameters of BN can be\nabsorbed into the parameters of Conv when both BN and Conv occur together, thus\neliminating the computation of BN from the deployed model.\n\nThe computation of absorption proceeds as follows:\n\nBy absorbing BN, Conv2d + BN2d can be simplified to Conv2d:\n\n\nFuse Add、ReLU(6)#\n\nDifferent from the CUDA Kernel Fusion that fuses the CUDA Kernel to increase the\ncomputation speed, the fusion supported by the training tool is more on the\nquantization level.\n\nThe BPU hardware provides optimization for common model basic structures, and\nwhen computing Conv -> Add -> ReLU operator combinations, it allows the data\ntransfer between operators to retain a high degree of accuracy, improving the\noverall numerical accuracy of the model. Therefore, when quantizing the model,\nwe can consider Conv -> Add -> ReLU as a whole.\n\nSince the training tool quantizes the model in terms of torch.nn.Module, in\norder to treat Conv -> Add -> ReLU as a whole during quantization, they need to\nbe merged into a single Module.\n\nThe operator fusion, in addition to allowing the intermediate results to retain\nthe high precision state, also eliminates the need to convert the intermediate\nresults to a low precision representation, and therefore the execution speed\nwill be faster compared to no fusion.\n\nSince operator fusion improves both model accuracy and model speed, it should\ngenerally be done for all fusable parts.\n\n\nImplementation Principle#\n\nThanks to FX's advantage of obtaining computational graphs, the training tool\ncan automate the analysis of the model's computational graphs, match the fusible\nparts according to the predefined fusion pattern, and implement the fusion\noperation by submodule substitution. An example is given below:\n\nThe absorption of BN and the fusion of Add, ReLU(6) can be accomplished by the\nsame mechanism, so no distinction is needed in fusion.\n\n\n\nAs you can see, after performing the operator fusion operation to the model, the\nBN is absorbed into the Conv, and the Conv, Add, and ReLU are fused into a\nModule (_generated_add_0). The original submodule is replaced with Identity and\nis not called in the forward code.\n\nFX automatically replaces the plus sign of x = x + y in the model with a Module\nform named _generated_add_0 to support operations related to operator fusion and\nquantization.\n\n\nOperators that can be Fused#\n\nThe currently supported combinations of fusable operators are shown in the\nfollowing function definitions:\n\n","routePath":"/en/guide/plugin/advanced_tutorial/op_fusion","lang":"en","toc":[{"text":"Absorb BN","id":"absorb-bn","depth":2,"charIndex":132},{"text":"Fuse Add、ReLU(6)","id":"fuse-addrelu6","depth":2,"charIndex":549},{"text":"Implementation Principle","id":"implementation-principle","depth":2,"charIndex":1692},{"text":"Operators that can be Fused","id":"operators-that-can-be-fused","depth":2,"charIndex":2604}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":72,"title":"Model Segmented Deployment","content":"#\n\n\nScenario#\n\nIn some scenarios, there may be a need to split the model trained as a whole\ninto multiple segments for on-board deployment. For example, for the two-stage\ndetection model in the below picture, if the DPP needs to be executed on the\nCPU, and the output (roi) of the DPP needs to be the input of RoiAlign, then the\nmodel needs to be split into Stage1 and Stage2 according to the annotation of\nthe dotted box and compiled separately for on-board deployment. When running on\nthe board, the fixed-point data output from the backbone will be directly used\nas the input to RoiAlign.\n\n\n\n\nUsage#\n\n\n\n 1. Model modification: As shown in the picture above, based on a model which\n    can normally be trained in quantized awareness, you need to insert the\n    QuantStub after the cutoff point of the model segments before prepare_qat.\n    Note that if horizon_plugin_pytorch.quantization.QuantStub is used, scale =\n    None must be set.\n\n 2. QAT training: train the modified model on quantized awareness as a whole,\n    the inserted QuantStub will record the scale of the input data of the Stage2\n    model into the buffer.\n\n 3. Fixed-point model conversion: convert the whole trained QAT model to a\n    fixed-point model using the convert interface.\n\n 4. Segmentation and compilation: segment the model according to the form after\n    deploying on the board, then export and compile the segmented model\n    respectively. Note that although the input of Stage2 is quantized data\n    during training, the example_input of Stage2 when exporting still needs to\n    be the floating-point form, the inserted QuantStub in Stage2 will configure\n    the correct scale for the data and quantize it.","routePath":"/en/guide/plugin/advanced_tutorial/segmented_deploy","lang":"en","toc":[{"text":"Scenario","id":"scenario","depth":2,"charIndex":3},{"text":"Usage","id":"usage","depth":2,"charIndex":595}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":73,"title":"","content":"class horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int =\n512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int =\n-1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nKL observer.\n\nKL observer based on histogram. Histogram is calculated online and won’t be\nsaved.\n\n * Parameters:\n * bins (int) – Number of histograms bins.\n * update_interval (int) – Interval of computing KL entropy and update min/max.\n   KLObserver will constantly collect histograms of activations, but only\n   perform KL calculation when update_interval is satisfied. if it is set to 1,\n   KL entropy will be computed every forward step. Larger interval guarantees\n   less time and does no harm to calibration accuracy. Set it to the total\n   calibration steps can achieve best performance. update_interval must be no\n   greater than total calibration steps, otherwise no min/max will be computed.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int =\n1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype |\nQuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min:\nint | None = None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMSE observer.\n\nObserver module for computing the quantization parameters based on the Mean\nSquare Error (MSE) between the original tensor and the quantized one.\n\nThis observer linear searches the quantization scales that minimize MSE.\n\n * Parameters:\n * stride (int) – Searching stride. Larger value gives smaller search space,\n   which means less computing time but possibly poorer accuracy. Default is 1.\n   Suggests no greater than 20.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMin max observer.\n\nThis observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average minimum\nand maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MixObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMix observer.\n\nThis observer computes the quantization parameters based on multiple calibration\nmethods and selects the quantization parameters with the smallest quantization\nerror.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver\n(percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01,\nch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nPercentile observer.\n\nPercentile observer based on histogram. Histogram is calculated online and won’t\nbe saved. The minimum and maximum are moving averaged to compute the\nquantization parameters.\n\n * Parameters:\n * percentile (float) – Index percentile of histrogram\n * bins (int) – Number of histograms bins.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\n\n\nclass horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver\n(averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric,\nquant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)\n\nMovingAverageMinMax Observer.\n\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving averages\nof minimums and maximums of the incoming tensors. The module records the average\nminimum and maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, only support per_tensor_symmetric\n   scheme\n * reduce_range – Reduces the range of the quantized data type by 1 bit\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – Whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver\n(averaging_constant=0.01, ch_axis=0, dtype=torch.qint8,\nqscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None,\nis_sync_quantize=False, factory_kwargs=None)\n\nMovingAveragePerChannelMinMax Observer.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * ch_axis – Channel axis\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, Only support per_channel_symmetric\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.","routePath":"/en/guide/plugin/api_processed/calibration_api","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":74,"title":"Environmental Dependence","content":"#\n\nDEPENDENCY    GPU            CPU\nos            Ubuntu22.04    Ubuntu22.04\ncuda          11.8           N/A\npython        3.10           3.10\ntorch         2.3.0+cu118    2.3.0+cpu\ntorchvision   0.18.0+cu118   0.18.0+cpu","routePath":"/en/guide/plugin/installation","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":75,"title":"Introduction","content":"#\n\nThe quantization indicates a technique for performing computations and storing\ntensors with the bit widths lower than the floating-point accuracy. Quantized\nmodels perform some or all operations on the tensor using integers rather than\nfloating-point values. Compared to typical FP32 models, taking INT8 quantization\nas an example, resulting in a 4x reduction in model size and a 4x reduction in\nmemory bandwidth requirements. The hardware support for INT8 computation is\ntypically 2 to 4 times faster than FP32 computation. The quantization is\nprimarily a technique to accelerate the inference, and the quantization\noperations are only supported for forward computation.\n\nhorizon_plugin_pytorch provides the BPU-adapted quantization operations and\nsupports quantization-aware training (QAT). The QAT uses fake-quantization\nmodules to model the quantization errors in forward computation and\nbackpropagation. Note that the computation process of the QAT is performed by\nusing floating-point operations. At the end of the QAT, horizon_plugin_pytorch\nprovides the conversion functions to convert the trained model to a fixed-point\nmodel, using a more compact model for representation and high-performance\nvectorization on the BPU.\n\nThis section gives you a detailed introduction to horizon_plugin_pytorch\nquantitative training tool developed on the basis of PyTorch.\n\nHorizon_plugin_pytorch is developed based on PyTorch, in order to reduce the\nlearning cost of you, we refer to the design of PyTorch on quantized awareness\ntraining. This doc doesn't repeat the contents which PyTorch doc already\ncontains, if you want to understand the details of the tool, we recommend that\nyou read the Official source code or the python source code of this tool. To\nensure a smooth experience, we recommend that you first read the PyTorch\ndocumentation and familiarize yourself with the quantized awareness training and\ndeployment tools provided by PyTorch.\n\nFor the purpose of brevity, the code in the documentation has been replaced with\nthe following aliases by default:\n\n","routePath":"/en/guide/plugin/introduce","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":76,"title":"quantization.hbdk4.export","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.export (model: Module, example_inputs:\nAny, *, name: str = 'forward', input_names: Any | None = None, output_names: Any\n| None = None, input_descs: Any | None = None, output_descs: Any | None = None)\n\nExport nn.Module to hbir model.\n\n * Parameters:\n * model (Module) – Input model.\n * example_inputs (Any) – Example input for tracing.\n * name (str) – The name of func in exported module. Users can get the func by\n   getattr(hbir_module, name).\n * input_names (Optional[Any]) – Set hbir inputs with given names, should have\n   the same structure with example_inputs.\n * output_names (Optional[Any]) – Set hbir outputs with given names, should have\n   the same structure with model output.\n * input_descs (Optional[Any]) – Set hbir inputs with given descriptions, should\n   have the same structure with example_inputs.\n * output_descs (Optional[Any]) – Set hbir outputs with given descriptions,\n   should have the same structure with model output.\n * Return type: Module\n * Returns: Hbir model wrapped with Module.","routePath":"/en/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_export","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":77,"title":"quantization.hbdk4.get_hbir_input_flattener","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.get_hbir_input_flattener (model:\nModule)\n\nGet an callable func to flatten model input into a flat tuple of Tensor.\n\n * Parameters: model (Module) – Hbir model.\n * Return type: callable","routePath":"/en/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_get_hbir_input_flattener","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":78,"title":"quantization.hbdk4.get_hbir_output_unflattener","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.hbdk4.get_hbir_output_unflattener (model:\nModule)\n\nGet an callable func to unflatten model output into origin format.\n\n * Parameters: model (Module) – Hbir model.\n * Return type: callable","routePath":"/en/guide/plugin/plugin_api_reference/export/horizon_plugin_pytorch_quantization_hbdk4_get_hbir_output_unflattener","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":79,"title":"Fake Quantize","content":"#\n\n\n\nclass horizon_plugin_pytorch.quantization.FakeQuantize (observer: type = ,\nsaturate: bool | None = None, in_place: bool = False, compat_mask: bool = True,\nchannel_len: int = 1, fast_training=True, **observer_kwargs)\n\nSimulate the quantize and dequantize operations in training time.\n\nThe output of this module is given by\n\nfake_quant_x = clamp(floor(x / scale + 0.5), quant_min, quant_max) * scale\n\n * scale defines the scale factor used for quantization.\n\n * zero_point specifies the quantized value to which 0 in floating point maps to\n\n * quant_min specifies the minimum allowable quantized value.\n\n * quant_max specifies the maximum allowable quantized value.\n\n * fake_quant_enabled controls the application of fake quantization on tensors,\n   note that statistics can still be updated.\n\n * observer_enabled controls statistics collection on tensors\n\n * dtype specifies the quantized dtype that is being emulated with\n   fake-quantization, the allowable values is qint8 and qint16. The values of\n   quant_min and quant_max should be chosen to be consistent with the dtype\n\n * Parameters:\n\n * observer (type) – Module for observing statistics on input tensors and\n   calculating scale and zero-point.\n\n * saturate (Optional[bool]) – Whether zero out the grad for value out of quanti\n   range.\n\n * in_place (bool) – Whether use in place fake quantize.\n\n * compat_mask (bool) – Whether pack the bool mask into bitfield when saturate =\n   True.\n\n * channel_len (int) – Size of data at channel dim.\n\n * fast_training – Whether use fast training mode. If True, computing scale and\n   fake quantization will be done in one step.\n\n * observer_kwargs – Arguments for the observer module\n\nobserver\n\nUser provided module that collects statistics on the input tensor and provides a\nmethod to calculate scale and zero-point.\n\nextra_repr()\n\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\nforward (x)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nset_qparams (scale: Tensor | Sequence | float, zero_point: Tensor | Sequence |\nint | None = None)\n\nSet qparams, default symmetric.\n\n * Parameters:\n * scale ( Tensor | Sequence | float)\n * zero_point ( Tensor | Sequence | int | None)\n\nclassmethod with_args (**kwargs)\n\nWrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances. Can be used in conjunction with\n_callable_args\n\nExample:\n\n","routePath":"/en/guide/plugin/plugin_api_reference/fake_quantize","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":80,"title":"bgr2centered_gray","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2centered_gray (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to centered gray\n\n * Parameters: input (Tensor) – input image in BGR format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: centered gray image of shape [N, 1, H, W], ranging -128 to 127\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2centered_gray","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":81,"title":"bgr2centered_yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2centered_yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from BGR format to centered YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in BGR format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: -112 to 107, U, V: -112 to\n   112). “full” for YUV full swing (Y, U, V: -128 to 127). default is “studio”\n * Returns: centered YUV image\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2centered_yuv","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":82,"title":"bgr2gray","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2gray (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to gray\n\n * Parameters: input (Tensor) – input image in BGR format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: gray image of shape [N, 1, H, W], ranging 0 to 255\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2gray","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":83,"title":"bgr2rgb","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2rgb (input: Tensor)\n\nConvert color space.\n\nConvert images from BGR format to RGB\n\n * Parameters: input (Tensor) – image in BGR format with shape [N, 3, H, W]\n * Returns: image in RGB format with shape [N, 3, H, W]\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2rgb","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":84,"title":"bgr2yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.bgr2yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from BGR format to YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in BGR format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: 16 to 235, U, V: 16 to 240).\n   “full” for YUV full swing (Y, U, V: 0 to 255). default is “studio”\n * Returns: YUV image\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_bgr2yuv","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":85,"title":"nn.BgrToYuv444","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.BgrToYuv444 (channel_reversal: bool = False)\n\nConvert image color format from bgr to yuv444.\n\n * Parameters: channel_reversal (bool) – Color channel order, set to True when\n   used on RGB input. Defaults to False.\n\nforward (input: Tensor)\n\nForward pass of BgrToYuv444.\n\n * Parameters: input ( Tensor)","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_BgrToYuv444","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":86,"title":"nn.Correlation","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.Correlation (kernel_size: int = 1,\nmax_displacement: int = 1, stride1: int = 1, stride2: int = 1, pad_size: int =\n0, is_multiply: bool = True)\n\nPerform multiplicative patch comparisons between two feature maps.\n\nCorrelation performs multiplicative patch comparisons between two feature maps.\nGiven two multi-channel feature maps $f_{1}, f_{2}$, with $w$, $h$, and $c$\nbeing their width, height, and number of channels, the correlation layer lets\nthe network compare each patch from $f_{1}$ with each patch from $f_{2}$.\n\nFor now we consider only a single comparison of two patches. The ‘correlation’\nof two patches centered at $x_{1}$ in the first map and $x_{2}$ in the second\nmap is then defined as:\n\n$c(x_{1}, x_{2}) = \\displaystyle\\sum_{o \\in [-k,k] \\times [-k,k]} $\n\nfor a square patch of size $K:=2k+1$.\n\nNote that the equation above is identical to one step of a convolution in neural\nnetworks, but instead of convolving data with a filter, it convolves data with\nother data. For this reason, it has no training weights.\n\nComputing $c(x_{1}, x_{2})$ involves $c * K^{2}$ multiplications. Comparing all\npatch combinations involves $w^{2}*h^{2}$ such computations.\n\nGiven a maximum displacement $d$, for each location $x_{1}$ it computes\ncorrelations $c(x_{1}, x_{2})$ only in a neighborhood of size $D:=2d+1$, by\nlimiting the range of $x_{2}$. We use strides $s_{1}, s_{2}$, to quantize\n$x_{1}$ globally and to quantize $x_{2}$ within the neighborhood centered around\n$x_{1}$.\n\nThe final output is defined by the following expression:\n\n$out[n, q, i, j] = c(x_{i, j}, x_{q})$\n\nwhere $i$ and $j$ enumerate spatial locations in $f_{1}$, and $q$ denotes the\n$q^{th}$ neighborhood of $x_{i,j}$.\n\n * Parameters:\n * kernel_size (int) – kernel size for Correlation must be an odd number\n * max_displacement (int) – Max displacement of Correlation\n * stride1 (int) – stride1 quantize data1 globally\n * stride2 (int) – stride2 quantize data2 within neighborhood centered around\n   data1\n * pad_size (int) – pad for Correlation\n * is_multiply (bool) – operation type is either multiplication or subduction,\n   only support True now\n\nforward (data1: Tensor | QTensor, data2: Tensor | QTensor)\n\nForward for Horizon Correlation.\n\n * Parameters:\n * data1 (Union[Tensor, QTensor]) – shape of [N,C,H,W]\n * data2 (Union[Tensor, QTensor]) – shape of [N,C,H,W]\n * Returns: output\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_Correlation","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":87,"title":"nn.MultiScaleDeformableAttention","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.MultiScaleDeformableAttention (embed_dims: int =\n256, num_heads: int = 8, num_levels: int = 4, num_points: int = 4, im2col_step:\nint = 64, dropout: float = 0.1, batch_first: bool = False, value_proj_ratio:\nfloat = 1.0, split_weight_mul: bool = False, split_batch: bool = False)\n\nAn attention module used in Deformable-Detr.\n\nDeformable DETR: Deformable Transformers for End-to-End Object Detection..\n\n * Parameters:\n * embed_dims (int) – The embedding dimension of Attention. Default: 256.\n * num_heads (int) – Parallel attention heads. Default: 8.\n * num_levels (int) – The number of feature map used in Attention. Default: 4.\n * num_points (int) – The number of sampling points for each query in each head.\n   Default: 4.\n * im2col_step (int) – The step used in image_to_column. Default: 64.\n * dropout (float) – A Dropout layer on inp_identity. Default: 0.1.\n * batch_first (bool) – Key, Query and Value are shape of (batch, n, embed_dim)\n   or (n, batch, embed_dim). Default to False.\n * value_proj_ratio (float) – The expansion ratio of value_proj. Default: 1.0.\n * split_weight_mul (bool) – Whether split attention weight mul onto each level\n   outputs. Enable this can reduce memory usage in qat training.\n * split_batch (bool) – Whether Compute each batch at a time. Enable this can\n   reduce memory usage in qat training.\n\nforward (query: Tensor | QTensor, key: Tensor | QTensor | None = None, value:\nTensor | QTensor | None = None, identity: Tensor | QTensor | None = None,\nquery_pos: Tensor | QTensor | None = None, key_padding_mask: Tensor | None =\nNone, reference_points: Tensor | QTensor | None = None, spatial_shapes: Tensor |\nNone = None)\n\nForward Function of MultiScaleDeformAttention.\n\n * Parameters:\n * query (Union[Tensor, QTensor]) – Query of Transformer with shape (num_query,\n   bs, embed_dims).\n * key (Union[Tensor, QTensor, None]) – The key tensor with shape (num_key, bs,\n   embed_dims).\n * value (Union[Tensor, QTensor, None]) – The value tensor with shape (num_key,\n   bs, embed_dims).\n * identity (Union[Tensor, QTensor, None]) – The tensor used for addition, with\n   the same shape as query. Default None. If None, query will be used.\n * query_pos (Union[Tensor, QTensor, None]) – The positional encoding for query.\n   Default: None.\n * key_padding_mask (Optional[Tensor]) – ByteTensor for query, with shape [bs,\n   num_key].\n * reference_points (Union[Tensor, QTensor, None]) – The normalized reference\n   points with shape (bs, num_query, num_levels, 2), all elements is range in\n   [0, 1], top-left (0,0), bottom-right (1, 1), including padding area. or (bs,\n   num_query, num_levels, 4), add additional two dimensions is (w, h) to form\n   reference boxes.\n * spatial_shapes (Optional[Tensor]) – Spatial shape of features in different\n   levels. int tensor with shape (num_levels, 2), last dimension represents (h,\n   w).\n * Returns: the same shape with query.\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_MultiScaleDeformableAttention","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":88,"title":"nn.PointPillarsScatter","content":"#\n\n\n\nclass horizon_plugin_pytorch.nn.PointPillarsScatter (output_shape=None)\n\nforward (voxel_features: Tensor, coords: Tensor, output_shape: Tensor | list |\ntuple | None = None)\n\nForward of Horizon PointPillarsScatter.\n\n * Parameters:\n * voxel_features (Tensor) – [M, …], dimention after M will be flattened.\n * coords (Tensor) – [M, (n, …, y, x)], only indices on N, H and W are used.\n * output_shape (Union[Tensor, list, tuple, None]) – Expected output shape.\n   Defaults to None.\n * Returns: The NCHW pseudo image.\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_PointPillarsScatter","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":89,"title":"nn.functional.filter","content":"#\n\n\n\nhorizon_plugin_pytorch.nn.functional.filter (*inputs: Tuple[Tensor] |\nTuple[QTensor], threshold: float, idx_range: Tuple[int, int] | None = None)\n\nFilter.\n\nThe output order is different with bpu, because that the compiler do some\noptimization and slice input following complex rules, which is hard to be done\nby plugin.\n\nAll inputs are filtered along HW by the max value within a range in channel dim\nof the first input. Each NCHW input is splited, transposed and flattened to\nList[Tensor[H * W, C]] first. If input is QTensor, the output will be\ndequantized.\n\n * Parameters:\n * inputs (Union[Tuple[Tensor], Tuple[QTensor]]) – Data in NCHW format. Each\n   input shold have the same size in N, H, W. The output will be selected\n   according to the first input.\n * threshold (float) – Threshold, the lower bound of output.\n * idx_range (Optional[Tuple[int, int]]) – The index range of values counted in\n   compare of the first input. Defaults to None which means use all the values.\n * Returns: A list with same length of batch size, and each element contains:\n\n * max_value: Flattened max value within idx_range in channel dim.\n * max_idx: Flattened max value index in channel dim.\n * coord: The original coordinates of the output data in the input data in the\n   shape of [M, (h, w)].\n * (multi) data: Filtered data in the shape of [M, C].\n\n * Return type: Union[List[List[Tensor]], List[List[QTensor]]]","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_functional_filter","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":90,"title":"nn.functional.point_pillars_preprocess","content":"#\n\n\n\nhorizon_plugin_pytorch.nn.functional.point_pillars_preprocess (points_list:\nList[Tensor], pc_range: Tensor, voxel_size: Tensor, max_voxels: int,\nmax_points_per_voxel: int, use_max: bool, norm_range: Tensor, norm_dims: Tensor)\n\nPreprocess PointPillars.\n\n * Parameters:\n * points_list (List[Tensor]) – [(M1, ndim), (M2, ndim),…], List of PointCloud\n   data.\n * pc_range (Tensor) – (6,), indicate voxel range, format: [x_min, y_min, z_min,\n   x_max, y_max, z_max]\n * voxel_size (Tensor) – (3,), xyz, indicate voxel size.\n * max_voxels (int) – Indicate maximum voxels.\n * max_points_per_voxel (int) – Indicate maximum points contained in a voxel.\n * use_max (bool) – Whether to use max_voxels, for deploy should be True.\n * norm_range (Tensor) – Feature range, like [x_min, y_min, z_min, …, x_max,\n   y_max, z_max, …].\n * norm_dims (Tensor) – Dims to do normalize.\n * Returns: (features, coords), encoded feature and coordinates in (idx, z, y,\n   x) format.\n * Return type: (Tensor, Tensor)","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_nn_functional_point_pillars_preprocess","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":91,"title":"rgb2bgr","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2bgr (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to BGR\n\n * Parameters: input (Tensor) – image in RGB format with shape [N, 3, H, W]\n * Returns: image in BGR format with shape [N, 3, H, W]\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2bgr","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":92,"title":"rgb2centered_gray","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2centered_gray (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to centered gray\n\n * Parameters: input (Tensor) – input image in RGB format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: centered gray image of shape [N, 1, H, W], ranging -128 to 127\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2centered_gray","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":93,"title":"rgb2centered_yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2centered_yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from RGB format to centered YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in RGB format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: -112 to 107, U, V: -112 to\n   112). “full” for YUV full swing (Y, U, V: -128 to 127). default is “studio”\n * Returns: centered YUV image\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2centered_yuv","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":94,"title":"rgb2gray","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2gray (input: Tensor)\n\nConvert color space.\n\nConvert images from RGB format to gray\n\n * Parameters: input (Tensor) – input image in RGB format of shape [N, 3, H, W],\n   ranging 0 to 255\n * Returns: gray image of shape [N, 1, H, W], ranging 0 to 255\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2gray","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":95,"title":"rgb2yuv","content":"#\n\n\n\nhorizon_plugin_pytorch.rgb2yuv (input: Tensor, swing: str = 'studio')\n\nConvert color space.\n\nConvert images from RGB format to YUV444 BT.601\n\n * Parameters:\n * input (Tensor) – input image in RGB format, ranging 0 to 255\n * swing (str) – “studio” for YUV studio swing (Y: 16 to 235, U, V: 16 to 240).\n   “full” for YUV full swing (Y, U, V: 0 to 255). default is “studio”\n * Returns: YUV image\n * Return type: Tensor","routePath":"/en/guide/plugin/plugin_api_reference/horizon_operator/horizon_plugin_pytorch_rgb2yuv","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":96,"title":"March","content":"#\n\n\n\nclass horizon_plugin_pytorch.march.March\n\nBPU platform.","routePath":"/en/guide/plugin/plugin_api_reference/march","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":97,"title":"ONNX","content":"#\n\n\n\nhorizon_plugin_pytorch.utils.onnx_helper.export_to_onnx (model, args, f,\nexport_params=True, verbose=False, training=, input_names=None,\noutput_names=None, operator_export_type=, opset_version=11,\ndo_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None,\ncustom_opsets=None)\n\nExport a (float or qat)model into ONNX format.\n\n * Parameters:\n * model ( torch.nn.Module/torch.jit.ScriptModule/ScriptFunction) – the model to\n   be exported.\n * args ( tuple or torch.Tensor) –\n\n\n\n\n\n * f – a file-like object or a string containing a file name. A binary protocol\n   buffer will be written to this file.\n * export_params ( bool , default True) – if True, all parameters will be\n   exported.\n * verbose ( bool , default False) – if True, prints a description of the model\n   being exported to stdout, doc_string will be added to graph. doc_string may\n   contaion mapping of module scope to node name in future torch onnx.\n * training ( enum , default TrainingMode.EVAL) –\n\n\n\n\n\n * input_names ( list of str , default empty list) – names to assign to the\n   input nodes of the graph, in order.\n * output_names ( list of str , default empty list) – names to assign to the\n   output nodes of the graph, in order.\n * operator_export_type ( enum , default ONNX_FALLTHROUGH) – *\n   OperatorExportTypes.ONNX: Export all ops as regular ONNX ops (in the default\n   opset domain). * OperatorExportTypes.ONNX_FALLTHROUGH: Try to convert all ops\n   to standard ONNX ops in the default opset domain. *\n   OperatorExportTypes.ONNX_ATEN: All ATen ops (in the TorchScript namespace\n   “aten”) are exported as ATen ops. * OperatorExportTypes.ONNX_ATEN_FALLBACK:\n   Try to export each ATen op (in the TorchScript namespace “aten”) as a regular\n   ONNX op. If we are unable to do so,fall back to exporting an ATen op.\n * opset_version ( int , default 11) – by default we export the model to the\n   opset version of the onnx submodule.\n * do_constant_folding ( bool , default False) – Apply the constant-folding\n   optimization. Constant-folding will replace some of the ops that have all\n   constant inputs with pre-computed constant nodes.\n * dynamic_axes ( dict , list ( int ) /dict , str>> , default empty dict) –\n\n\n\n\n\n * keep_initializers_as_inputs ( bool , default None) – If True, all the\n   initializers (typically corresponding to parameters) in the exported graph\n   will also be added as inputs to the graph. If False, then initializers are\n   not added as inputs to the graph, and only the non-parameter inputs are added\n   as inputs. This may allow for better optimizations (e.g. constant folding) by\n   backends/runtimes.\n * custom_opsets ( dict , int> , default empty dict) –\n\n\n\n","routePath":"/en/guide/plugin/plugin_api_reference/onnx","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":98,"title":"fx.fx_helper.wrap","content":"#\n\n\n\nExtended tracer and wrap of torch.fx.\n\nThis file defines a inherit tracer of torch.fx.Tracer and a extended wrap to\nallow wrapping of user-defined Module or method, which help users do some\noptimization of their own module by torch.fx\n\nhorizon_plugin_pytorch.fx.fx_helper.wrap (skip_compile: bool = False)\n\nExtend torch.fx.wrap.\n\nThis function can be: : 1) called or used as a decorator on a string to register\na builtin function as a “leaf function”\n\n 2. called or used as a decorator on a function to register this function as a\n    “leaf function”\n\n 3. called or used as a decorator on subclass of torch.nn.Module to register\n    this module as a “leaf module”, and register all user defined method in this\n    class as “leaf method”\n\n 4. called or used as a decorator on a class method to register it as “leaf\n    method”\n\n * Parameters: skip_compile ( bool , optional) – Whether wrapped obj is skipped\n   in compile, used by\n   horizon_plugin_pytorch.quantization.fx.split_compilable_model\n   .split_compilable_model. Defaults to False.\n * Returns: The actural decorator.\n * Return type: FxWrapManager.wrap","routePath":"/en/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_fx_fx_helper_wrap","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":99,"title":"quantization.fuse_known_modules","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.fuse_known_modules (mod_list, is_qat=False,\nadditional_fuser_method_mapping=None)\n\nFuse modules.\n\nReturn a list of modules that fuses the operations specified in the input module\nlist.\n\nFuses only the following sequence of modules: conv, bn; conv, bn, relu; conv,\nrelu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear,\nbn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu;\nlinear, add; linear, add, relu. For these sequences, the first element in the\noutput module list performs the fused operation. The rest of the elements are\nset to nn.Identity()","routePath":"/en/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_fuse_known_modules","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":100,"title":"quantization.fuse_modules","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.fuse_modules (model, modules_to_fuse,\ninplace=False, fuser_func=, fuse_custom_config_dict=None)\n\nFuses a list of modules into a single module.\n\nFuses only the following sequence of modules: conv, bn; conv, bn, relu; conv,\nrelu; conv, bn, add; conv, bn, add, relu; conv, add; conv, add, relu; linear,\nbn; linear, bn, relu; linear, relu; linear, bn, add; linear, bn, add, relu;\nlinear, add; linear, add, relu. For these sequences, the first element in the\noutput module list performs the fused operation. The rest of the elements are\nset to nn.Identity()\n\n * Parameters:\n * model – Model containing the modules to be fused\n * modules_to_fuse – list of list of module names to fuse. Can also be a list of\n   strings if there is only a single list of modules to fuse.\n * inplace – bool specifying if fusion happens in place on the model, by default\n   a new model is returned\n * fuser_func – Function that takes in a list of modules and outputs a list of\n   fused modules of the same length. For example, fuser_func([convModule,\n   BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to\n   torch.ao.quantization.fuse_known_modules\n * fuse_custom_config_dict – custom configuration for fusion\n\n\n\n * Returns: model with fused modules. A new copy is created if inplace=True.\n\nExamples:\n\n","routePath":"/en/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_fuse_modules","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":101,"title":"quantization.prepare","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.prepare (model: Module, example_inputs: Any\n| None = None, qconfig_setter: Tuple[QconfigSetterBase, ...] | QconfigSetterBase\n| None = None, method: PrepareMethod = PrepareMethod.JIT_STRIP)\n\nPrepare model.\n\nPrepare and check a copy of the model for QAT.\n\n * Parameters:\n * model (Module) – Model to be prepared.\n * example_inputs (Optional[Any]) – Model inputs. Used to trace and check model.\n * qconfig_setter (Union[Tuple[QconfigSetterBase, ...], QconfigSetterBase,\n   None]) – Qconfig setter. Used to set qconfig.\n * method (PrepareMethod) – Method used to trace model, availiable options are:\n   ‘eager’: Don’t trace. ‘symbolic’: Use symbolic trace. ‘jit’: Use jit trace.\n   ‘jit-strip’: Use jit trace and strip the graph outside QuantStub and\n   Dequantstub.\n * Return type: Module","routePath":"/en/guide/plugin/plugin_api_reference/qat_api/horizon_plugin_pytorch_quantization_prepare","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":102,"title":"qconfig","content":"#\n\n\n\nhorizon_plugin_pytorch.quantization.get_qconfig (observer:\n~typing.Type[~horizon_plugin_pytorch.quantization.observer_v2.ObserverBase] = ,\nin_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType | None = None,\nweight_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType | None =\n'qint8', out_dtype: ~torch.dtype | ~horizon_plugin_pytorch.dtype.QuantDType |\nNone = 'qint8', fix_scale: bool = False)\n\nGet qconfig.\n\n * Parameters:\n * observer (Type[ObserverBase]) – observer type for input and output. Support\n   MinMaxObserver and MSEObserver\n * in_dtype (Union[dtype, QuantDType, None]) – input dtype.\n * weight_dtype (Union[dtype, QuantDType, None]) – weight dtype.\n * out_dtype (Union[dtype, QuantDType, None]) – output dtype.\n * fix_scale (bool) – Whether fix input/output scale.","routePath":"/en/guide/plugin/plugin_api_reference/qconfig","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":103,"title":"Common Failure","content":"#\n\n\nimport Error#\n\nError I: Cannot find the extension library(_C.so)\n\nSolution:\n\n * Make sure that the horizon_plugin_pytorch version corresponds to the cuda\n   version.\n * In python3, find the execution path of horizon_plugin_pytorch and check for\n   .so files in that directory. There may be multiple versions of\n   horizon_plugin_pytorch at the same time, so you need to uninstall it and keep\n   only the one you need.\n\n--------------------------------------------------------------------------------\n\nError II: RuntimeError: Cannot load custom ops. Please rebuild the\nhorizon_plugin_pytorch\n\nSolution: check if the local CUDA environment is OK, such as path, version, etc.\n\n\nUnable to prepare_calibration/qat#\n\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support\nthe deepcopy protocol at the moment\n\nSolution: generally it is the inclusion of a non-leaf tensor in the model that\ncauses this error, try the following:\n\n * Set the inplace of prepare_calibration/qat to True.\n * This error does not occur with normal horizon_plugin_pytorch definitions\n   operators, check if there is a non-leaf tensor defined for the customized\n   operator in the model.\n\n\nforward Error after prepare_qat#\n\nTypeError: when calling function\n\nSolution: the customized operator inherits the Module operator of a torch, which\nresults in prepare_qat not being converted to a qat module. it is recommended to\nuse the submodule method to call conv2d.\n\n\nQuantized Accuracy Anomaly#\n\nThe QAT/Quantized accuracy is not as expected, there is a NAN, or the initial\nQAT loss is clearly anomalous with respect to float.\n\nSolution: please refer to the section Accuracy Tuning Tool Guide.\n\n\nCannot find the extension library(_C.so)#\n\nSolution: it mainly happens when horizon_plugin_pytorch installs successfully\nbut import fails, the solution is as follows:\n\n 1. Make sure that the horizon_plugin_pytorch version and the cuda version\n    correspond;\n 2. In python3, find the execution path of horizon-plugin-pytorch and check for\n    .so files in that directory. There may be multiple versions of\n    horizon-plugin-pytorch at the same time, so you need to uninstall it and\n    keep only the one you need.\n\n\nRuntimeError: Cannot load custom ops. Please rebuild the\nhorizon_plugin_pytorch.#\n\nSolution: please make sure that the local CUDA environment is working properly,\ne.g., the path and version are as expected.\n\n\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support\nthe deepcopy protocol at the moment#\n\nSolution: it mainly occurs in the phase of not being able to prepare properly,\nwhich is usually caused by non-leaf tensor in the model, please configure the\ninplace of prepare_calibration/qat to True.\n\n\ntorch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with\nsignal SIGKILL#\n\nSolution: probably caused by a multi-threaded, python program that wasn't\ncompletely killed.\n\n\nAttributeError: 'NoneType' object has no attribute 'numel'#\n\nSolution: this error occurs mainly during the insertion of pseudo-quantized\nnodes and is caused by the input scale of the operator being None. The reason\nmay be that the output layer conv is inserted into dequant and then connected to\nan op, which is similar to the structure of conv+dequant+conv; or the conv\nconfigured with high accuracy output is connected to other operators. In this\ncase, please check whether the dequant operator or the high accuracy output\nconfiguration is used correctly.\n\n\nsymbolically traced variables cannot be used as inputs to control flow#\n\nSolution: this error is caused by using dynamic control flow such as if, loop,\netc. in fx mode. Currently, fx mode only supports static control flow, so you\nneed to avoid using dynamic statements such as if, for, assert, etc. in forward.\n\n\nNotImplementedError: function is not implemented for QTensor.#\n\nSolution: this error may occur in the Calibration phase in fx mode because fx\nmode does not support calculations of the form (-x), please change (-x) to\n(-1)*(x).\n\n\nNotimplementedError: function rsub at 0x7f5a7cdiee50> is not implemented for\nQTensor.#\n\nSolution: this error may occur in the Calibration phase in fx mode because the\nlogic of operator substitution in fx mode is that if the subtracted number in\nthe subtraction is a constant, the operator substitution is not performed\nautomatically, so you need to change the subtraction to addition, e.g., change\n(1-x) to (x+(-1))*(-1).","routePath":"/en/guide/plugin/qat_faq/failure","lang":"en","toc":[{"text":"import Error","id":"import-error","depth":2,"charIndex":3},{"text":"Unable to prepare_calibration/qat","id":"unable-to-prepare_calibrationqat","depth":2,"charIndex":678},{"text":"forward Error after prepare_qat","id":"forward-error-after-prepare_qat","depth":2,"charIndex":1186},{"text":"Quantized Accuracy Anomaly","id":"quantized-accuracy-anomaly","depth":2,"charIndex":1459},{"text":"Cannot find the extension library(_C.so)","id":"cannot-find-the-extension-library_cso","depth":2,"charIndex":1688},{"text":"RuntimeError: Cannot load custom ops. Please rebuild the horizon_plugin_pytorch.","id":"runtimeerror-cannot-load-custom-ops-please-rebuild-the-horizon_plugin_pytorch","depth":2,"charIndex":-1},{"text":"RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment","id":"runtimeerror-only-tensors-created-explicitly-by-the-user-graph-leaves-support-the-deepcopy-protocol-at-the-moment","depth":2,"charIndex":-1},{"text":"torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL","id":"torchmultiprocessingspawnprocessexitedexception-process-0-terminated-with-signal-sigkill","depth":2,"charIndex":-1},{"text":"AttributeError: 'NoneType' object has no attribute 'numel'","id":"attributeerror-nonetype-object-has-no-attribute-numel","depth":2,"charIndex":2926},{"text":"symbolically traced variables cannot be used as inputs to control flow","id":"symbolically-traced-variables-cannot-be-used-as-inputs-to-control-flow","depth":2,"charIndex":3486},{"text":"NotImplementedError: function <method ‘neg’ of ‘torch._C._TensorBase’ objects> is not implemented for QTensor.","id":"notimplementederror-function-method-neg-of-torch_c_tensorbase-objects-is-not-implemented-for-qtensor","depth":2,"charIndex":-1},{"text":"NotimplementedError: function <function Tensor.**rsub** at 0x7f5a7cdiee50> is not implemented for QTensor.","id":"notimplementederror-function-function-tensorrsub-at-0x7f5a7cdiee50-is-not-implemented-for-qtensor","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":104,"title":"FAQ","content":"#\n\n\nTraining Environment#\n\n\nDocker container can't use Nvidia resources?#\n\nSolution: It is recommended to refer to the description in the GPU Docker\nsection of the Environment Deployment section of the documentation.\n\n\nQAT Quantized Training#\n\n\nQuantized Accuracy Anomaly#\n\nSolution: The QAT/Quantized accuracy is not as expected, there is a NAN, or the\ninitial QAT loss is clearly anomalous with respect to float. Please refer to the\nsection Accuracy Tuning Tool Guide.\n\n\nWhy does accuracy performance deteriorate after turning on multi-machine\ntraining?#\n\nSolution: The batchsize increases exponentially when multi-computer training is\nturned on, and the LR and other hyperparameters need to be adjusted\nsynchronously to balance the batchsize.\n\n\nDoes Qconfig require user intervention?#\n\nSolution: Horizon provides Qconfig which defines how activation and weight are\nquantized, and currently supports quantization algorithms such as FakeQuantize,\nLSQ, and PACT.\n\n\nHow to export ONNX model for each phase?#\n\nSolution:\n\nPlease refer to the following code implementation:\n\n\n\n\nWhy does nan exist during QAT training?#\n\nSolution:\n\nThere are more factors affecting the problem and it is recommended to check the\nfollowing aspects:\n\n 1. Checks whether the input data contains nan;\n 2. Check that the floating-point model has converged. Floating-point models\n    that do not converge may cause large fluctuations for certain minimization\n    errors;\n 3. Check whether calib is turned on, it is recommended to turn it on to give\n    better initial coefficients to the model;\n 4. Check whether the training strategy is appropriate. Unsuitable training\n    strategy can also lead to NAN values, such as learning rate lr is too large\n    (either by lowering the learning rate or using gradient truncation). In\n    terms of training strategy, the default QAT is consistent with\n    floating-point, and it is recommended to replace it with SGD if the\n    floating-point training uses optimizers such as OneCycle that affect the LR\n    settings.\n\n\nConfigure the int16 node or high-precision output node to be invalid#\n\nSolution: This may be due to a misconfiguration of module_name, which only\nsupports string and does not support configuration by index.\n\n\nHow to check whether high-accuracy output is turned on for a particular layer?#\n\nSolution: You can print the layer where the qat_model is located to see if it\nhas (activation_post_process): FakeQuantize, and if it doesn't, then it is a\nhigh-accuracy output. For example, the int32 high-accuracy conv prints as\nfollows:\n\n\n\nThe int8 low-accuracy conv prints as follows:\n\n\n\n\nWhether auxiliary branches can insert pseudo-quantized nodes?#\n\nSolution: It is recommended that pseudo-quantization nodes be inserted only for\nthe portion of the model that is deployed on the board. Since QAT training is a\nglobal training, the existence of auxiliary branches will lead to an increase in\nthe difficulty of training, and if the data distribution at the auxiliary\nbranches is different from other branches, it will also increase the risk of\naccuracy, so it is recommended to remove them.\n\n\nHow to rewrite the horizon gridsample operator into a torch public\nimplementation?#\n\nSolution: The non-public implementation of the gridsample operator in\nhorizon_plugin_pytorch has grid inputs (input 2) that are absolute coordinates\nof type int16, while the public version of the torch is normalized coordinates\nof type float32 in the range [-1, 1].\n\nTherefore, after importing the grid_sample operator from the\ntorch.nn.functional.grid_sample path, the grid can be normalized in the\nfollowing way:\n\n\n\n\nWhy are the weight parameters not updated for QAT training after load\ncalibration?#\n\nSolution:\n\nIt can be checked in turn as follows:\n\n 1. Whether prepare is before the optimizer definition. Because prepare_qat\n    performs operator fusion, resulting in a change in model structure;\n 2. Whether fake_quant_enabled and observe_enabled are 1;\n 3. Whether the training variable in the module is True.\n\n\nHow to handle float type constants +-*/ operations not supported by prepare_fx?#\n\nSolution: Since float operations are not recorded by fx when the model is\nconverted to a static graph, errors such as add not supported, float not\nconverted to qtensor, etc. are triggered. In this case, the constant needs to be\nchanged to a tensor, and the input tensor needs to be quantized (inserted into\nquantstub) to convert the symbolic operation to a FloatFunction. A reference\nexample is provided below.\n\nBefore modification:\n\n\n\nAfter modification:\n\n\n\n\nSetting Error#\n\n\nModules that do not need to be quantized set a non-None qconfig, e.g.,\npre/post-processing, loss function, etc.#\n\nSolution: set qconfig only for modules that need to be quantized.\n\n\nThe march is not set correctly, which may result in model compilation failures\nor inconsistent deployment accuracy.#\n\nSolution: select the correct BPU architecture based on the processor to be\ndeployed, e.g. J6 requires Nash:\n\n\n\n\nThe model output node is not set to high accuracy output, resulting in\nquantization accuracy that is not as expected.#\n\nAn error example is shown below:\n\nAssume that the model is defined as follows:\n\n\n\nSolution: in order to improve the model accuracy, set the model output node to\nhigh accuracy, as shown in the example below:\n\n\n\n\nMethod Error#\n\n\nThe Calibration process uses multi-cards.#\n\nSolution: Due to underlying limitations, currently Calibration does not support\nmulti-cards, please use a single card for Calibration\n\n\nThe model input image data is in a non-centered YUV444 format such as RGB, which\nmay result in inconsistent model deployment accuracy.#\n\nSolution: since the image format supported by Horizon hardware is centered\nYUV444, it is recommended that you use the YUV444 format directly as the network\ninput from the beginning of model training.\n\n\nUse the qat model for model accuracy evaluation and monitoring in quantized\nawareness training, which leads to the problem of failing to detect the abnormal\naccuracy at the time of deployment in a timely manner.#\n\nSolution: the reason for the error between QAT and Quantized is that the QAT\nstage cannot fully simulate the pure fixed-point computation logic in Quantized,\nso it is recommended to use the quantized model for model accuracy evaluation\nand monitoring.\n\n\n\n\nNetwork Error#\n\n\nCall the same member defined by FloatFunctional() multiple times.#\n\nThe error example is as follows:\n\n\n\nSolution: prohibit calling the same variable defined by FloatFunctional()\nmultiple times in forward.\n\n\n\n\nOperator Error#\n\n\nSome of the operators in the Quantized model have not gone through the\ncalibration or QAT, for example, a post-processing operator wants to be\naccelerated on the BPU but has not gone through the quantization stage, which\nwill lead to the failure of quantization inference or abnormal accuracy when\ndeployed.#\n\nSolution: the Quantized phase is not completely unable to add operators\ndirectly, such as color space conversion operators, see the document for details\non how to add operators. However, not all operators can be added directly, such\nas cat, this kind of operator must be obtained in the calibration or QAT phase\nof the statistics of the real quantization parameters in order not to affect the\nfinal accuracy, if you have a similar need to adjust the structure of the\nnetwork, you can consult with the framework developers.\n\n\nModel Error#\n\n\nFloating-point model overfitting.#\n\nCommon determination of model overfitting:\n\n * Large changes in the output after a slight transformation of the input data.\n * The model parameters are assigned large values.\n * The model activation is large.\n\nSolution: Solve the floating-point model overfitting problem on your own.","routePath":"/en/guide/plugin/qat_faq/faq","lang":"en","toc":[{"text":"Training Environment","id":"training-environment","depth":2,"charIndex":3},{"text":"Docker container can't use Nvidia resources?","id":"docker-container-cant-use-nvidia-resources","depth":3,"charIndex":27},{"text":"QAT Quantized Training","id":"qat-quantized-training","depth":2,"charIndex":218},{"text":"Quantized Accuracy Anomaly","id":"quantized-accuracy-anomaly","depth":3,"charIndex":244},{"text":"Why does accuracy performance deteriorate after turning on multi-machine training?","id":"why-does-accuracy-performance-deteriorate-after-turning-on-multi-machine-training","depth":3,"charIndex":-1},{"text":"Does Qconfig require user intervention?","id":"does-qconfig-require-user-intervention","depth":3,"charIndex":747},{"text":"How to export ONNX model for each phase?","id":"how-to-export-onnx-model-for-each-phase","depth":3,"charIndex":965},{"text":"Why does nan exist during QAT training?","id":"why-does-nan-exist-during-qat-training","depth":3,"charIndex":1074},{"text":"Configure the int16 node or high-precision output node to be invalid","id":"configure-the-int16-node-or-high-precision-output-node-to-be-invalid","depth":3,"charIndex":2034},{"text":"How to check whether high-accuracy output is turned on for a particular layer?","id":"how-to-check-whether-high-accuracy-output-is-turned-on-for-a-particular-layer","depth":3,"charIndex":2243},{"text":"Whether auxiliary branches can insert pseudo-quantized nodes?","id":"whether-auxiliary-branches-can-insert-pseudo-quantized-nodes","depth":3,"charIndex":2615},{"text":"How to rewrite the horizon gridsample operator into a torch public implementation?","id":"how-to-rewrite-the-horizon-gridsample-operator-into-a-torch-public-implementation","depth":3,"charIndex":-1},{"text":"Why are the weight parameters not updated for QAT training after load calibration?","id":"why-are-the-weight-parameters-not-updated-for-qat-training-after-load-calibration","depth":3,"charIndex":-1},{"text":"How to handle float type constants +-*/ operations not supported by prepare_fx?","id":"how-to-handle-float-type-constants---operations-not-supported-by-prepare_fx","depth":3,"charIndex":4024},{"text":"Setting Error","id":"setting-error","depth":2,"charIndex":4566},{"text":"Modules that do not need to be quantized set a non-None qconfig, e.g., pre/post-processing, loss function, etc.","id":"modules-that-do-not-need-to-be-quantized-set-a-non-none-qconfig-eg-prepost-processing-loss-function-etc","depth":3,"charIndex":-1},{"text":"The march is not set correctly, which may result in model compilation failures or inconsistent deployment accuracy.","id":"the-march-is-not-set-correctly-which-may-result-in-model-compilation-failures-or-inconsistent-deployment-accuracy","depth":3,"charIndex":-1},{"text":"The model output node is not set to high accuracy output, resulting in quantization accuracy that is not as expected.","id":"the-model-output-node-is-not-set-to-high-accuracy-output-resulting-in-quantization-accuracy-that-is-not-as-expected","depth":3,"charIndex":-1},{"text":"Method Error","id":"method-error","depth":2,"charIndex":5326},{"text":"The Calibration process uses multi-cards.","id":"the-calibration-process-uses-multi-cards","depth":3,"charIndex":5342},{"text":"The model input image data is in a non-centered YUV444 format such as RGB, which may result in inconsistent model deployment accuracy.","id":"the-model-input-image-data-is-in-a-non-centered-yuv444-format-such-as-rgb-which-may-result-in-inconsistent-model-deployment-accuracy","depth":3,"charIndex":-1},{"text":"Use the qat model for model accuracy evaluation and monitoring in quantized awareness training, which leads to the problem of failing to detect the abnormal accuracy at the time of deployment in a timely manner.","id":"use-the-qat-model-for-model-accuracy-evaluation-and-monitoring-in-quantized-awareness-training-which-leads-to-the-problem-of-failing-to-detect-the-abnormal-accuracy-at-the-time-of-deployment-in-a-timely-manner","depth":3,"charIndex":-1},{"text":"Network Error","id":"network-error","depth":2,"charIndex":6331},{"text":"Call the same member defined by `FloatFunctional()` multiple times.","id":"call-the-same-member-defined-by-floatfunctional-multiple-times","depth":3,"charIndex":-1},{"text":"Operator Error","id":"operator-error","depth":2,"charIndex":6557},{"text":"Some of the operators in the Quantized model have not gone through the calibration or QAT, for example, a post-processing operator wants to be accelerated on the BPU but has not gone through the quantization stage, which will lead to the failure of quantization inference or abnormal accuracy when deployed.","id":"some-of-the-operators-in-the-quantized-model-have-not-gone-through-the-calibration-or-qat-for-example-a-post-processing-operator-wants-to-be-accelerated-on-the-bpu-but-has-not-gone-through-the-quantization-stage-which-will-lead-to-the-failure-of-quantization-inference-or-abnormal-accuracy-when-deployed","depth":3,"charIndex":-1},{"text":"Model Error","id":"model-error","depth":2,"charIndex":7410},{"text":"Floating-point model overfitting.","id":"floating-point-model-overfitting","depth":3,"charIndex":7425}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":105,"title":"Algorithm Model QAT + Deployment Quick Start","content":"#\n\n\nBasic Process#\n\nThe basic process for using the Quantized Awareness Training Tool is as follows:\n\n\n\nThe following is an example of the MobileNetV2 model from torchvision to\nintroduce you to each stage of the process.\n\nWe used the cifar-10 dataset instead of the ImageNet-1K dataset due to the speed\nof execution of the process display.\n\n\n\n\n\n\nGetting Floating-point Model#\n\nFirst, the floating-point model is modified as necessary to support\nquantization-related operations. The operations necessary for model\ntransformation are:\n\n * Insert QuantStub before the model inputs.\n * Insert DequantStub before the model inputs.\n\nAttention is needed when remodeling the model:\n\n * The inserted QuantStub and DequantStub must be registered as submodules of\n   the model, otherwise their quantized state will not be handled correctly.\n * Multiple inputs can share QuantStub only if the scale is the same, otherwise\n   define a separate QuantStub for each input.\n * If you need to specify the source of the data entered during board up as\n   “pyramid”, please manually set the scale parameter of the corresponding\n   QuantStub to 1/128.\n * It is also possible to use torch.quantization.QuantStub, but only\n   horizon_plugin_pytorch.quantization.QuantStub supports manually fixing the\n   scale with the parameter.\n\nThe modified model can seamlessly load the parameters of the pre-modified model,\nso if there is an existing trained floating-point model, it can be loaded\ndirectly, otherwise you need to do floating-point training normally.\n\nAttention\n\nThe input image data is typically in centered_yuv444 format when the model is on\nboard, so the image needs to be converted to centered_yuv444 format when the\nmodel is trained (note the use of rgb2centered_yuv in the code below).\n\nIf it is not possible to convert to centered_yuv444 format for model training,\nplease insert the appropriate color space conversion node on the input when the\nmodel is deployed. (Note that this method may result in lower model accuracy)\n\nThe example has fewer floating-point and QAT training epochs, just to illustrate\nthe process of using the training tool, and the accuracy does not represent the\nbest level of the model.\n\n\n\n\n\n\nCalibration#\n\nAfter the model is transformed and the floating point training is completed,\nCalibration can be performed. This process is done by inserting Observer in the\nmodel and counting the distribution of data in each place during the forward\nprocess, so as to calculate a reasonable quantization parameter:\n\n * For part of the model, the accuracy can be achieved by Calibration only,\n   without the need for the more time-consuming quantized perception training.\n\n * Even if the model cannot meet the accuracy requirements after quantization\n   calibration, this process can reduce the difficulty of subsequent\n   quantization awareness training, shorten the training time, and improve the\n   final training accuracy.\n\n\n\n\n\nIf the quantization accuracy of the model after Calibration meets the\nrequirements, the model_deploy step can be carried out directly, otherwise the\nquantization_awareness_training needs to be carried out to further improve the\naccuracy.\n\n\nQuantization Awareness Training#\n\nThe quantization awareness training makes the model aware of the impact of\nquantization during the training process by inserting pseudo-quantization nodes\nin the model, in this case fine-tuning the model parameters in order to improve\nthe accuracy after quantization.\n\n\n\n\n\n\nModel Deployment#\n\nOnce the pseudo-quantization accuracy is up to standard, the processes related\nto model deployment can be executed.\n\n\nExport Hbir Model#\n\nModel deployment first requires exporting the pseudo-quantization model as a\nHbir model.\n\nAttention\n * The batch_size of the example_input used in model export determines the\n   batch_size for model simulation and model uploading, if you need to use\n   different batch_size for simulation and uploading, please use different data\n   to export hbir model separately.\n * It is also possible to skip the actual calibration and training process in\n   Calibration and Quantization Awareness Training and go directly to the model\n   deployment process first to ensure that there are no operations in the model\n   that cannot be exported or compiled.\n\n\n\n\n\n\nConvert to Fixed-point Model#\n\nOnce the pseudo-quantization accuracy is up to standard, the model can be\nconverted to a fixed-point model. The results of the fixed-point model are\ngenerally considered to be identical to those of the compiled model.\n\nAttention\n * Hbir models support only a single Tensor or Tuple[Tensor] as input, and only\n   Tuple[Tensor] as output.\n * It is not possible to achieve complete numerical agreement between the\n   fixed-point model and the pseudo-quantization model, so please take the\n   accuracy of the fixed-point model as the standard. If the fixed-point\n   accuracy is not up to standard, you need to continue the quantized awareness\n   training.\n\n\n\n\n\n\nModel Compilation#\n\nAfter testing the accuracy of the fixed-point model and confirming that it meets\nthe requirements, the model can be compiled, performance tested and visualized.\n\nAttention\n\nThe model used for perf should be calibrated at least once (with no limit on the\nnumber of steps) to ensure that the statistics in the model match the actual\nsituation, otherwise it may cause inaccurate perf results.\n\n\n\n\n\n\n\n\n\nBefore the model compilation, we also support modify the board-side deployable\nmodel, the common operations and the calling API interfaces are as follows:\n\n * After the model export and before convert:\n   \n   1. Batch input splitting is performed by calling the insert_split()\n      interface.\n   \n   2. Insert the image preprocessing node to the model:\n      \n      a. The layout needs to be adjusted to NHWC for subsequent operations,\n      which is performed by calling the insert_transpose() interface.\n      \n      b. Image normalization is performed by calling the\n      insert_image_preprocess() interface.\n      \n      c. Color conversion (typically nv12 input for board-side deployment) is\n      performed by calling the insert_image_convert() interface.\n      \n      d. Configuration of the input as a resizer input to support roi-based\n      keying and scaling is performed by calling the insert_roi_resize()\n      interface.\n   \n   3. Adjustment the input and output data layout is performed by calling the\n      insert_transpose() interface.\n\n * After the model convert and before compile, remove operators\n   (Quantize/Dequantize/Cast, etc.) by calling the remove_io_op() interface.\n\nFor details, you can refer to section HBDK Tool API Reference for the above\nmentioned APIs.","routePath":"/en/guide/plugin/qat_quickstart/qat_quickstart","lang":"en","toc":[{"text":"Basic Process","id":"basic-process","depth":2,"charIndex":3},{"text":"Getting Floating-point Model","id":"getting-floating-point-model","depth":2,"charIndex":345},{"text":"Calibration","id":"calibration","depth":2,"charIndex":2203},{"text":"Quantization Awareness Training","id":"quantization_awareness_training","depth":2,"charIndex":3172},{"text":"Model Deployment","id":"model_deploy","depth":2,"charIndex":3480},{"text":"Export Hbir Model","id":"export-hbir-model","depth":3,"charIndex":3617},{"text":"Convert to Fixed-point Model","id":"convert-to-fixed-point-model","depth":3,"charIndex":4287},{"text":"Model Compilation","id":"model-compilation","depth":2,"charIndex":4976}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":106,"title":"Terminology","content":"#\n\n\nFloat model / floating-point model#\n\nThe floating-point models that meet quantized awareness training requirements.\n\n\nCalibration#\n\nThe process of obtaining quantitative parameters using calibration data.\n\n\nCalibration model#\n\nPseudo-quantized model obtained after Calibration.\n\n\nQAT / quantized awareness training#\n\nTraining for quantized awareness.\n\n\nQAT model#\n\nPseudo-quantized models obtained after quantized awareness training.\n\n\nPseudo-quantization#\n\nThe process of first quantizing and then dequantizing floating-point data which\nis generally implemented in network models through pseudo-quantized nodes.\n\n\nPseudo-quantized model#\n\nModels with pseudo-quantized nodes which are typically obtained by Calibration\nor QAT.\n\n\nQuantized model / fixed-point model / quantized model#\n\nConvert the floating-point parameters in a pseudo-quantized model to fixed-point\nparameters through parameter transformations, and convert the floating-point\noperators to fixed-point operators, the transformed model is called a Quantized\nmodel or fixed-point model or quantized model.\n\n\nHbir model#\n\nModels exported for deployment, typically exported from a QAT model, can be used\nfor accuracy simulation and compilation on boards.\n\n\nNash#\n\nName of the BPU architecture.\n\n\nJ6#\n\nName of the processor.\n\n\nCorrespondence between BPU Architecture and Processor#\n\nPROCESSOR   J6E            J6M\nBPU         Nash-e         Nash-m\nenum        March.NASH_E   March.NASH_M","routePath":"/en/guide/plugin/terminology","lang":"en","toc":[{"text":"Float model / floating-point model","id":"float-model--floating-point-model","depth":2,"charIndex":3},{"text":"Calibration","id":"calibration","depth":2,"charIndex":121},{"text":"Calibration model","id":"calibration-model","depth":2,"charIndex":210},{"text":"QAT / quantized awareness training","id":"qat--quantized-awareness-training","depth":2,"charIndex":283},{"text":"QAT model","id":"qat-model","depth":2,"charIndex":356},{"text":"Pseudo-quantization","id":"pseudo-quantization","depth":2,"charIndex":439},{"text":"Pseudo-quantized model","id":"pseudo-quantized-model","depth":2,"charIndex":618},{"text":"Quantized model / fixed-point model / quantized model","id":"quantized-model--fixed-point-model--quantized-model","depth":2,"charIndex":732},{"text":"Hbir model","id":"hbir-model","depth":2,"charIndex":1075},{"text":"Nash","id":"nash","depth":2,"charIndex":1222},{"text":"J6","id":"j6","depth":2,"charIndex":1261},{"text":"Correspondence between BPU Architecture and Processor","id":"correspondence-between-bpu-architecture-and-processor","depth":2,"charIndex":1291}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":107,"title":"Calibration Tutorial","content":"#\n\nIn quantization, an important step is to determine the quantized parameters, a\nreasonable initial quantized parameter can significantly improve the accuracy of\nthe model and speed up the convergence of the model. Calibration is the process\nof inserting an Observer into the floating-point model, using a small amount of\ntraining data, and counting the distribution of the data at various points\nduring the forward process of the model to determine a reasonable quantized\nparameter. Although it is possible to do quantized awareness training without\nCalibration, but in general it is beneficial and not detrimental to quantized\nawareness training, so it is recommended that you make this step a required\noption.\n\n\nProcess and Example#\n\nThe overall flow of Calibration and QAT is shown below:\n\nFollowing is a description of each step:\n\n 1. Build and train the floating-point model. Refer to the section Getting\n    Floating-point Model in the quick start section of horizon_plugin_pytorch.\n\n 2. Insert the Observer node into the floating-point model. Refer to the section\n    Calibration in the quick start section of horizon_plugin_pytorch. Before\n    converting the floating-point model using the prepare method, you need to\n    set qconfig for the model. Refer to the section QConfig in Detail.\n\n 3. Set fake quantize state to CALIBRATION.\n    \n    \n    \n    There are three states of fake quantize, which need to be set to the\n    corresponding state of fake quantize of the model before QAT, calibration,\n    and validation, respectively. In the calibration state, only the statistics\n    of the inputs and outputs of each operator are observed. In the QAT state,\n    pseudo-quantization is performed in addition to observing the statistics. In\n    validation state, no statistics are observed and only pseudo-quantization is\n    performed.\n    \n    \n\n 4. Perform calibration. Feed the prepared calibration data to the model, and\n    the model will be observed by the observer during the forward process to\n    observe the relevant statistics.\n\n 5. Set the model state to eval and set the fake quantize state to VALIDATION.\n    \n    \n\n 6. Verify the effect of calibration. If you are satisfied with the result, you\n    can directly convert the model to fixed-point or perform quantized awareness\n    training based on it, if not, you can adjust the parameters in calibration\n    qconfig to continue calibration.\n\n\nCommon Algorithms Introduction#\n\nNote\n\nReference the API documentation at the end of this section for a description of\nthe parameters of each operator.\n\nALGORITHM    SPEED RANK   ACCURACY RANK   EASY-TO-USE RANK\nmin_max      1            5               1\npercentile   2            4               4\nmse          4            1               2\nkl           5            2               3\nmix          3            2               1\n\nThe performance of several popular calibration methods is shown in the table\nabove, where smaller numbers are better, speed indicates the time taken to\ncalibrate the same data, accuracy indicates how well the method calibrates on\nmost models, and easy-to-use indicates the complexity of the method's tuning\nparameters.\n\nFor a same model, the accuracy/speed of different methods with different\nparameters can be quite different, and some recent research work has shown that\nno one method can achieve the best accuracy on all models, and that its\nparameters need to be adjusted specifically. So it is recommended that you try\nall of these calibration methods.\n\n 1. min_max. This method only counts the sliding average of the maximum and\n    minimum values, and is used for quickly determining general parameters such\n    as batch size, average_constant, and so on.\n\n 2. percentile. This method has the highest accuracy upper limit among all the\n    methods, but it is also the most troublesome to adjust, so if the accuracy\n    requirement can be satisfied by other methods or the default parameters of\n    this method, then it is not recommended to spend too much time on adjusting\n    the parameters. The percentile can be adjusted by two parameters, bins and\n    percentile. The more bins there are, the smaller the interval between the\n    candidates of max, the finer the granularity of tuning, but it also means\n    higher computation time. It is recommended to determine the percentile first\n    and then adjust the bins, alternating between the two iterations to narrow\n    down the tuning range until a satisfactory result is achieved. In most\n    cases, the bins of 2048 provides enough granularity for tuning, so there is\n    no need to adjust this parameter separately. The following is the tuning\n    path of a model:\n    \n    ORDER   PERCENTILE   BINS   ACCURACY\n    1       99.99        2048   53.75\n    2       99.99        4096   54.38\n    3       99.995       4096   16.25\n    4       99.985       4096   32.67\n    5       99.9875      4096   57.06\n    6       99.9875      8192   62.84\n    7       99.98875     8192   57.62\n    8       99.988125    8192   63.15\n    \n    In this example, it can be seen that the accuracy is improved by about 10%\n    after careful adjustment. There is a big difference between the inputs and\n    outputs of different ops in the model, a set of global percentile parameters\n    may be hard to satisfy the needs of all ops, if you need higher accuracy,\n    you can find a better global parameter by the above method, and then use\n    debug tool to find a few ops with bigger error, and then set up the\n    percentile parameters of these ops individually. Refer to the qconfig\n    setting. Below is a list of some common data distributions that are prone to\n    large errors:\n    \n    \n    \n    For ultra-long-tailed distributions, the percentile should be set to a\n    smaller value, 99.9 in the picture is a better value.\n    \n    \n    \n    The value domain is too large, and the distribution is not concentrated in\n    one place, this situation either retain the tail or ignore the tail will\n    bring a large loss of accuracy, should be adjusted in the training of\n    floating-point model by adjusting the weight decay and other parameters to\n    avoid this situation.\n    \n    \n    \n    The layernorm output distribution will show a number of very high\n    concentration of the region, at this time the percentile adjusted in\n    accordance with the normal method will not have any effect on the\n    quantization results, you need to increase the percentile adjustment\n    amplitude.\n\n 3. mse. The only parameter that can be adjusted is stride, the default stride\n    is 1, it will gradually try the 100th quantile of the maximum value and\n    select the value corresponding to the quantile with the smallest error (L2\n    distance) before and after the quantization and dequantization. This method\n    is time-consuming for large models, if you increase stride within a\n    reasonable range, you can reduce the time-consumption under the premise of\n    guaranteeing the accuracy, but if stride is adjusted too large, the accuracy\n    will be affected. Note that adjusting the parameters of this method can only\n    optimize the time consumption, but not improve the accuracy significantly.\n\n 4. kl. There are two adjustable parameters, bin and update_interval, it is not\n    recommended to adjust the default bin because this method is too time\n    consuming. The update_interval is 1 by default, it can be adjusted to reduce\n    the time consuming, but you need to make sure that the update_interval is\n    smaller than the total calibration step, otherwise you can not get a normal\n    quantization parameter.\n\n 5. mix.This method is a hybrid calibration, and for each place where statistics\n    are needed, different parameters of the percentile method are tried, and the\n    method with the smallest error (L2 distance) before and after the\n    quantization and dequantization. There is a high degree of automation and no\n    parameters that need to be adjusted.\n\n\nTuning Technique#\n\n 1. When calibration is performed, the more data the better. However, because of\n    the marginal effect, when the amount of data reaches a certain level, the\n    improvement of accuracy will be very limited. If your training set is small,\n    you can use all of it for calibration. If your training set is large, you\n    can select a subset of the right size in combination with the calibration\n    time consumed, and it is recommended to calibrate at least 10 - 100 steps.\n\n 2. The data can be flipped horizontally augmentation, do not do mosaic\n    augmentation, try to use the pre-processing + training data of the infer\n    stage for calibration.\n\n 3. The Batch size should be as large as possible, but can be reduced if the\n    data are noisy or if there are many outliers in the model. This parameter\n    should be determined when trying the min max method.\n\n 4. The average_constant indicates the effect of each step on the maximum and\n    minimum values, the smaller average_constant is, the smaller the effect of\n    the current step is, and the larger the effect of the historical sliding\n    average is. This parameter needs to be adjusted between 0.01 ~ 0.5 with the\n    amount of data. When there is enough data (step > 100), average_constant can\n    be 0.01, when there is not enough data, average_constant can be increased as\n    appropriate, in extreme case, there are only 2 steps of data,\n    average_constant can be 0.5. This parameter should be determined when trying\n    the min max method, and all other methods will follow this parameter after\n    that.\n\n 5. When the accuracy of calibration model is good, fixing the quantization\n    parameter of the feature map for QAT training can achieve better results,\n    while when the accuracy is poor, the quantization parameter obtained from\n    calibration cannot be fixed. There is no clear standard about whether the\n    accuracy is good or bad, we need to try. For example, if the accuracy of a\n    certain model is 100, and if the accuracy of calibration is 50, then the\n    accuracy is not good, but if the accuracy of calibration is 95, then whether\n    the accuracy can reach the degree of fixing the quantization parameters of\n    feature map needs to be tried, and the usual practice is to do experiments\n    to compare the fixing and not fixing.\n\n 6. Priority to try min max method, the method is the fastest, used to run\n    through the calibration process, adjust and determine the batch size and\n    average_constant two parameters, and then try percentile, kl, mse and mix\n    four methods and select the most effective method.\n\n\nObserver Parameters#\n\nclass horizon_plugin_pytorch.quantization.observer_v2.KLObserver (bins: int =\n512, update_interval: int = 1, averaging_constant: float = 0.01, ch_axis: int =\n-1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nKL observer.\n\nKL observer based on histogram. Histogram is calculated online and won’t be\nsaved.\n\n * Parameters:\n * bins (int) – Number of histograms bins.\n * update_interval (int) – Interval of computing KL entropy and update min/max.\n   KLObserver will constantly collect histograms of activations, but only\n   perform KL calculation when update_interval is satisfied. if it is set to 1,\n   KL entropy will be computed every forward step. Larger interval guarantees\n   less time and does no harm to calibration accuracy. Set it to the total\n   calibration steps can achieve best performance. update_interval must be no\n   greater than total calibration steps, otherwise no min/max will be computed.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MSEObserver (stride: int =\n1, averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype |\nQuantDType = 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min:\nint | None = None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMSE observer.\n\nObserver module for computing the quantization parameters based on the Mean\nSquare Error (MSE) between the original tensor and the quantized one.\n\nThis observer linear searches the quantization scales that minimize MSE.\n\n * Parameters:\n * stride (int) – Searching stride. Larger value gives smaller search space,\n   which means less computing time but possibly poorer accuracy. Default is 1.\n   Suggests no greater than 20.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MinMaxObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMin max observer.\n\nThis observer computes the quantization parameters based on minimums and\nmaximums of the incoming tensors. The module records the moving average minimum\nand maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.MixObserver\n(averaging_constant: float = 0.01, ch_axis: int = -1, dtype: dtype | QuantDType\n= 'qint8', qscheme: qscheme = torch.per_tensor_symmetric, quant_min: int | None\n= None, quant_max: int | None = None, is_sync_quantize: bool = False,\nfactory_kwargs: Dict | None = None)\n\nMix observer.\n\nThis observer computes the quantization parameters based on multiple calibration\nmethods and selects the quantization parameters with the smallest quantization\nerror.\n\n * Parameters:\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.observer_v2.PercentileObserver\n(percentile: float = 99.99, bins: int = 2048, averaging_constant: float = 0.01,\nch_axis: int = -1, dtype: dtype | QuantDType = 'qint8', qscheme: qscheme =\ntorch.per_tensor_symmetric, quant_min: int | None = None, quant_max: int | None\n= None, is_sync_quantize: bool = False, factory_kwargs: Dict | None = None)\n\nPercentile observer.\n\nPercentile observer based on histogram. Histogram is calculated online and won’t\nbe saved. The minimum and maximum are moving averaged to compute the\nquantization parameters.\n\n * Parameters:\n * percentile (float) – Index percentile of histrogram\n * bins (int) – Number of histograms bins.\n * averaging_constant (float) – Averaging constant for min/max.\n * ch_axis (int) – Channel axis.\n * dtype (Union[dtype, QuantDType]) – Quantized data type.\n * qscheme (qscheme) – Quantization scheme to be used.\n * quant_min (Optional[int]) – Min quantization value. Will follow dtype if\n   unspecified.\n * quant_max (Optional[int]) – Max quantization value. Will follow dtype if\n   unspecified.\n * is_sync_quantize (bool) – If sync statistics when training with multiple\n   devices.\n * factory_kwargs (Optional[Dict]) – kwargs which are passed to factory\n   functions for min_val and max_val.\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.\n\nclass horizon_plugin_pytorch.quantization.MovingAverageMinMaxObserver\n(averaging_constant=0.01, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric,\nquant_min=None, quant_max=None, is_sync_quantize=False, factory_kwargs=None)\n\nMovingAverageMinMax Observer.\n\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving averages\nof minimums and maximums of the incoming tensors. The module records the average\nminimum and maximum of incoming tensors, and uses this statistic to compute the\nquantization parameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, only support per_tensor_symmetric\n   scheme\n * reduce_range – Reduces the range of the quantized data type by 1 bit\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – Whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nRecord the running minimum and maximum of x.\n\nclass horizon_plugin_pytorch.quantization.MovingAveragePerChannelMinMaxObserver\n(averaging_constant=0.01, ch_axis=0, dtype=torch.qint8,\nqscheme=torch.per_channel_symmetric, quant_min=None, quant_max=None,\nis_sync_quantize=False, factory_kwargs=None)\n\nMovingAveragePerChannelMinMax Observer.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\n * Parameters:\n * averaging_constant – Averaging constant for min/max.\n * ch_axis – Channel axis\n * dtype – Quantized data type\n * qscheme – Quantization scheme to be used, Only support per_channel_symmetric\n * quant_min – Minimum quantization value.\n * quant_max – Maximum quantization value.\n * is_sync_quantize – whether use sync quantize\n * factory_kwargs – Arguments for register data buffer\n\nforward (x_orig)\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNOTE: Although the recipe for forward pass needs to be defined within this\nfunction, one should call the Module instance afterwards instead of this since\nthe former takes care of running the registered hooks while the latter silently\nignores them.","routePath":"/en/guide/plugin/user_guide/calibration","lang":"en","toc":[{"text":"Process and Example","id":"process-and-example","depth":2,"charIndex":715},{"text":"Common Algorithms Introduction","id":"common-algorithms-introduction","depth":2,"charIndex":2419},{"text":"Tuning Technique","id":"tuning-technique","depth":2,"charIndex":7975},{"text":"Observer Parameters","id":"observer-parameters","depth":2,"charIndex":10608}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":108,"title":"Floating-point Model Requirements","content":"#\n\n\nsymbolic_trace#\n\nSimilar to PyTorch's quantized awareness training, our horizon_plugin_pytorch is\ndesigned and developed based on fx, and as such, requires that the\nfloating-point model be one that can correctly complete the symbolic_trace.\n\n\nOnly Partial Operators are Supported#\n\nSince the BPU only supports partial operators, horizon_plugin_pytorch only\nsupports operators in the operator list and special operators that are\nspecifically defined internally based on BPU limitations.\n\n\nBuild Quantization Friendly Model#\n\nThe process of changing a floating-point model into a fixed-point model has a\ncertain accuracy error. The more quantization-friendly the floating-point model\nis, the easier it is to improve the qat accuracy, and the higher the accuracy\nafter quantization. In general, there are several situations that can cause a\nmodel to become quantization-unfriendly:\n\n 1. Use operators with accuracy risk. For example: softmax, layernorm, etc. (see\n    op document), these operators are usually realized by table lookup or by\n    multiple op splicing at the bottom, and are prone to dropout problems.\n\n 2. Call the same operator multiple times in a forward. If the same operator is\n    called multiple times, the corresponding output distribution is different,\n    but only a set of quantization parameters will be counted, when the output\n    distribution of multiple calls is too different, the quantization error will\n    become larger.\n\n 3. There are too many differences in the inputs of add, cat and other\n    multi-input operators, which may cause large errors.\n\n 4. The data distribution is not reasonable. plugin adopts uniform symmetric\n    quantization, so the uniform distribution of 0 mean is the best, and long\n    tail and outliers should be avoided as much as possible. At the same time,\n    the value range needs to match the quantization bit, if you use int8 to\n    quantize the data distributed uniformly as [-1000, 1000], then the accuracy\n    is obviously not enough. For example, the following three distributions,\n    from left to right, the friendliness of quantization decreases, the\n    distribution of most of the values in the model should be the middle of this\n    distribution. In practice, you can use the debug tool to see if the\n    distributions of the model weight and feature map are quantization-friendly.\n    Because of the redundancy of the model, some ops that seem to have very\n    quantization-unfriendly distributions will not significantly reduce the\n    final accuracy of the model, and need to be considered in conjunction with\n    the actual difficulty of qat training and the final quantization accuracy\n    achieved.\n    \n    \n\nSo how can we make the model more quantization-friendly? Specifically:\n\n 1. Minimize the use of operators with excessive precision risk, see the op\n    documentation for details.\n\n 2. Ensure that the output distribution of multiple calls to the shared operator\n    does not vary too much, or split the shared operator to use separately.\n\n 3. Avoid large differences in the range of values of different inputs of the\n    multi-input operator.\n\n 4. Use int16 to quantize ops with very large value ranges and errors. You can\n    find such ops with the debug tool.\n\n 5. Prevent the model from overfitting by increasing the weight decay and adding\n    data enhancement. Overfitting models are prone to large values and are very\n    sensitive to inputs, so a small error can cause the output to be completely\n    wrong.\n\n 6. Use BN.\n\n 7. Normalize the model inputs with respect to zero symmetry.\n\nNote that qat itself has some adjustment ability, quantization unfriendly\ndoesn't mean it can't be quantized, in many cases, even if the above unsuitable\nquantization phenomenon occurs, it can still be quantized well. Since the above\nsuggestions may also lead to a degradation of the floating-point model accuracy,\nthey should be attempted when the qat accuracy is not achievable, especially\nsuggestions 1 - 5, and in the end you should find a balance between the\nfloating-point model accuracy and the quantized model accuracy.","routePath":"/en/guide/plugin/user_guide/float_model_requirements","lang":"en","toc":[{"text":"symbolic_trace","id":"symbolic_trace","depth":2,"charIndex":3},{"text":"Only Partial Operators are Supported","id":"only-partial-operators-are-supported","depth":2,"charIndex":246},{"text":"Build Quantization Friendly Model","id":"build-quantization-friendly-model","depth":2,"charIndex":491}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":109,"title":"Quantization Accuracy Tuning Guide","content":"#\n\nQuantization accuracy tuning involves two aspects:\n\n 1. Model structure and quantization configuration check. The main purpose is to\n    avoid non-tuning issues affecting quantization accuracy, such as incorrect\n    qconfig settings or using shared modules which are not\n    quantization-friendly.\n\n 2. Mixed precision tuning. Start with a model using high precision operators to\n    quickly achieve the desired accuracy, establishing the upper bound of\n    accuracy and lower bound of performance. Then, use accuracy tuning tools to\n    analyze and adjust quantization configurations to obtain a model that\n    balances accuracy and performance.\n\nIt is important to emphasize that before performing the following precision\ntuning, you need to first verify the correctness of your pipeline.\n\n\nModel Structure and Quantization Configuration Check#\n\nAfter preparing the model, first check for quantization configuration errors and\nmodel structures which are not quantization-friendly. You can use the\ncheck_qat_model interface in the debug tools. Please refer to Accuracy Tuning\nTool Guide for interface usage.\n\nAttention\n\ncheck_qat_model is integrated into the prepare interface, and you can directly\nview the model_check_result.txt in the running directory.\n\n\nOperator Fusion#\n\nCheck if there are modules in the model that can be fused but are not. When\ndeploying the model on BPU, operators like conv, bn, add, and relu will be\nfused. In a QAT model, these operators will be replaced with a single module to\navoid inserting fake quantization nodes. If these operators are not fused,\nadditional quantization nodes will be inserted, potentially causing slight\nimpacts on accuracy and performance. The following example shows that conv and\nrelu are not fused.\n\n\n\nPossible causes and solutions for operator fusion errors in different prepare\nmethods:\n\n 1. PrepareMethod.JIT and PrepareMethod.JIT_STRIP:\n    \n    a. Operator fusion in dynamic code blocks requires marking with\n    dynamic_block.\n    \n    b. Code with varying call counts were only executed once during tracing. Use\n    inputs which make the code execute multiple times as example_inputs.\n\n 2. PrepareMethod.EAGER: Missing or incorrect fuse operations. You need to check\n    and fix the handwritten fuse logic.\n\n 3. PrepareMethod.SYMBOLIC: Modules that can be fused are included in fx.wrap,\n    which needs to be moved out to ensure these modules exist in the graph or\n    use manual fusion like PrepareMethod.EAGER.\n\n\nShared Modules#\n\nSince the horizon_plugin_pytorch inserts quantization nodes by module\nreplacement, only one set of quantization information can be collected for each\nmodule. When a module is called multiple times with significantly different\noutput data distributions, using the same set of quantization parameters will\ncause large errors, and the shared module needs to be copied. If the output data\ndistribution of multiple calls is not significantly different, the shared module\ndoes not need to be copied. Here, we explain the concept of shared modules,\nwhich will help decide whether to copy shared modules during layer-by-layer\ncomparison.\n\n\n\nThe difference of three common-understanding \"shared modules\":\n\nA. A module followed by multiple modules. Module A is considered shared, but\nhere module A is only called once, and the output data distribution is not\ndifferent, so it does not affect quantization accuracy and will not be reflected\nin the call count check.\n\nB. A module is called multiple times, but the output data distribution are\nsimilar. Although this can be seen in the call count check, it has little impact\non quantization accuracy and does not need modification.\n\nC. A module is called multiple times, with significantly different output\ndistributions each time. This will be reflected in the call count check and\nsignificantly impacts quantization accuracy, requiring manual splitting.\n\nIn model_check_result.txt, you can see the call count of each module. Normally,\neach op is called only once. Zero means it is not called, and more than one\nindicates multiple calls. In the following example, conv is a shared module.\n\n\n\n\nQConfig Configuration Errors#\n\nUsing qconfig incorrectly can cause the model to quantize in unexpected ways,\nresulting in low accuracy (e.g. mixing template and qconfig attribute settings).\nHere, we mainly check if the input and output of each operator meet expectations\nby looking at model_check_result.txt:\n\n 1. Whether the dtype matches the settings.\n\n 2. Whether high precision output is enabled.\n\n\n\nAdditionally, model_check_result.txt will have prompts for abnormal qconfig.\nThese are configurations that the tool identifies for you to double-check. They\nneed verification to see if they meet expectations.\n\n 1. Weight int16. J6M does not support both of input and weight being int16. If\n    weight int16 is found, check whether the input is int16.\n\n 2. Fixed scale. Check if the fixed scale settings are as expected.\n\n\n\n\nMixed Precision Tuning#\n\n\nTuning Pipeline#\n\nThe entire pipeline first involves all-int16 accuracy tuning to confirm the\nmodel's upper bound of accuracy and check for tool usage issues or\nquantization-unfriendly modules.\n\n 1. After confirming that all-int16 accuracy meets the requirements, perform\n    all-int8 accuracy tuning. If the accuracy is not good enough, perform\n    int8/int16 mixed precision tuning. Starting with an all-int8 model,\n    gradually increase the proportion of int16 operators, balancing accuracy and\n    performance.\n\n 2. If all-int16 accuracy does not meet the requirements, perform int16/fp16\n    mixed precision tuning. Ideally, int16/fp16 mixed precision tuning can solve\n    all accuracy problems. Based on this, perform int8/int16/fp16 mixed\n    precision tuning, fixing all fp16 operator configurations, and adjust the\n    proportion of int16 operators as described in step 1.\n\n\nBasic Tuning Methods#\n\nThe goal of basic tuning methods is to get a quantization model quickly.\n\nCalibration#\n\n 1. Adjust calibration steps. The more calibration data, the better. However,\n    due to the marginal effect, when the data volume reaches a certain level,\n    the improvement in accuracy will be very limited. If the training set is\n    small, use it all for calibration. If the training set is large, select a\n    subset for calibration to balance time and accuracy. It's recommended to\n    perform at least 10 to 100 steps of calibration.\n\n 2. Adjust batch size. Generally, a larger batch size is preferable. However, if\n    the data is noisy or the model has many outliers, it may be necessary to\n    reduce the batch size.\n\n 3. Use inference stage preprocessing with training data for calibration.\n    Calibration data should reflect the real distribution and can use data\n    augmentation methods like flipping, but avoid using augmentation methods\n    that destory the real distribution, such as rotation and mosaic.\n\nQAT#\n\n 1. Adjust learning rate.\n    \n    a. Initial Learning Rate: Disable warmup and learning rate decay strategies.\n    Use different fixed learning rates (e.g., 1e-3, 1e-4, etc.) to fine-tune for\n    a few steps and select the learning rate with best accuracy. If the\n    floating-point model uses different learning rates for different modules,\n    replicate this in QAT.\n    \n    b. Scheduler: Align the learning rate decay strategy with the floating-point\n    model's but ensure no warmup strategies are used.\n\n 2. Experiment with fixing or updating input/output scales. When the accuracy of\n    calibration model is good, fixing the input/output scale for QAT training\n    can achieve better results. Otherwise, do not fix the scales. Both of the\n    two strategies need to be tested as there are no definitive metrics to guide\n    it.\n\n 3. Training steps. Generally, QAT training steps should not exceed 20% of the\n    floating-point training steps. Adjust training steps based on training loss\n    and evaluation results.\n\nSpecial considerations:\n\n 1. Apart from the specific adjustments mentioned above, ensure all other QAT\n    training configurations align with the floating-point training.\n\n 2. If the floating-point training uses freeze BN trick, set the QAT mode to\n    WithBN.\n\n\n\n 3. If accuracy issues arise despite tuning, such as non-converging loss or\n    NaNs, first fine-tune the floating-point model with the same configuration\n    to rule out issues on misalignment or overfitting.\n\n\nAdvanced Tuning Methods#\n\nAdvanced tuning methods generally require more time and experiments. They are\nused when higher accuracy is required.\n\nSetting Fixed Scale#\n\nSome parts of the model may require fixed scales due to difficulty in\ndetermining the optimal quantization scale through statistics. Common situations\nrequiring fixed scales include operators with known output ranges.\n\nFor example: If input data represents speed in km/h with a range of [0, 200],\nthe quantstub output range is fixed, and the scale should be set to 200 divided\nby the quantization range. Statistical methods might underestimate this range\ndue to outliers being averaged out, causing error for samples exceeding this\nrange.\n\nIn the case below, the value range of input a and b is determined, and the value\nrange of input c is uncertain. Except for quantstub_c and the last add, all\nother operators need to set fixed scale.\n\nCalibration#\n\nExperiment with different calibration methods. The plugin supports various\nmethods, including min max, percentile, kl, mse, and mix. For tuning\nexperiences, refer to the calibration guide.\n\nQAT#\n\n 1. Adjust weight decay. Weight decay impacts the range of weight values.\n    Smaller range means more quantization-friendly. Adjust weight decay during\n    both floating-point and QAT phases if necessary.\n\n 2. Adjust data augmentation. Quantized models have lower learning capabilities\n    compared to floating-point models. Strong data augmentation may hinder QAT\n    model convergence, so it's generally advisable to moderate the augmentation.\n\n\nINT8/INT16 & INT16/FP16 Mixed Precision Tuning#\n\nThe basic idea of mixed precision tuning is to incrementally introduce higher\nprecision operators into a lower precision model until the desired accuracy is\nachieved. For int8/int16 tuning, start with an all-int8 model and incrementally\nincrease the number of int16 operators. For int16/FP16 tuning, start with an\nall-int16 model and incrementally increase the number of FP16 operators.\n\nThe calibration and QAT tuning in the figure above can reference both basic and\nadvanced tuning methods chapters. Increasing the number of high-precision\noperators for int16/fp16 relies on a series of debug results produced by\naccuracy debugging tools.\n\nQAT accuracy debugging is entirely based on the comparison between the\nfloating-point model and the calibration model. Generally, it is not recommended\nto compare the floating-point model with the QAT model directly, as they become\nincomparable after QAT training. Please refer to Accuracy Tuning Tool Guide for\nbackground information. First, provide a dataset to find bad cases with\nsignificant calibration error, then compare layer-by-layer and calculate\nquantization sensitivity based on these bad cases.\n\nFind Badcase#\n\nAll accuracy debugging operations are based on bad cases. You need to provide a\nsample set with low quantization accuracy. This process will traverse the sample\nset and compare the output error of each sample in the floating-point model and\nthe calibration model. Generally, you do not need to provide an error\nmeasurement function.\n\nAttention\n\nStart from finding bad cases, models need to include some post-processing logic\n(removing or replacing parts of the original post-processing logic that make the\nmodel outputs completely incomparable). For example:\n\n 1. Sigmoid should not be removed. Values in the sigmoid saturation domain are\n    not sensitive to errors, but values near 0 are highly sensitive. Removing\n    sigmoid will prevent accurate reflection of quantization errors within\n    different domains.\n\n 2. NMS should be removed. Small errors can lead to completely different NMS\n    results, making the output incomparable.\n\nThe debug tool supports automatic replacement of sort/topk/argmax. Besides these\noperators, you need to check if there are similar operators in the model and\npost-processing, and delete all parts after these operators.\n\nUse the auto_find_bad_case interface in the debugging tool to find bad cases.\n\n\n\nAfter finding the bad cases, check the result file. For each output of the\nmodel, the debug tool will find the worst sample under each error metric. In the\nexample below, the model has three outputs. The first table shows the worst\nsample index for each output under each metric, the second table shows the\ncorresponding errors, and the third table shows the worst bad case index under\nthe current metric for all outputs.\n\n\n\nAttention\n\nWe can only debug the outputs related to accuracy drop. Different model outputs\nrequire different error metrics. Typically, L1 and Cosine are suitable for most\nissues. L1 is suitable for tasks requiring absolute error reflection, such as\nbbox regression, while Cosine is suitable for tasks requiring overall\ndistribution error reflection, such as classification.\n\nLayer-by-Layer Comparison#\n\nLayer-by-layer comparison uses the bad cases to run the floating-point model and\nthe calibration model separately, comparing the output of each layer. This can\nbe done using the compare_per_layer interface in the debug tool. This method is\nsuitable for detailed accuracy analysis. If the accuracy drop is not\nsignificant, you can skip this step and analyze it later with sensitivity\nresults.\n\n\n\nThe results of the layer-by-layer comparison can be viewed in generated text\nfiles. In the text file, you can see where the error starts to magnify from top\nto bottom.\n\n\n\nAttention\n\nAfter finding the operators with significant accuracy drop, first check\nbase_model_min / base_model_max / analy_model_min / analy_model_max to confirm\nif there are substantial errors in the extreme values.\n\n 1. Significant errors in min / max values: This indicates that the output range\n    of the operator in the floating-point model significantly exceeds the range\n    obtained from calibration. Check the scale of the operator and calculate the\n    maximum value obtained from calibration using the dtype and scale. For\n    instance, if the scale is 0.0078 and the dtype is int8, then the maximum\n    value should be 0.0078 × 128 = 0.9984. Compare this with base_model_max and\n    analy_model_max. Reasons for a very small scale may include: unreasonable\n    calibration data (too little calibration data or biased distribution\n    resulting in an excessively small output range), shared module etc.\n\n 2. No significant errors in min / max values: Similarly, calculate the\n    calibration maximum value as described in point 1, compare it with\n    base_model_max and analy_model_max to confirm that the output range of the\n    operator in the floating-point model does not significantly exceed the range\n    obtained from calibration. These issues may be caused by insufficient\n    resolution of the quantization type or an excessively large numerical range.\n    \n    a. Check if the current quantization dtype matches the numerical range.\n    Generally, if the maximum value exceeds 10, it is not recommended to use\n    int8 quantization.\n    \n    b. Identify why the calibration get a large numerical range. The reason may\n    be outliers or unreasonable settings.\n\nCalculating Quantization Sensitivity#\n\nThis step evaluates the impact of quantization nodes on model accuracy. The\nsensitivity interface in the debug tool can be used to get quantization\nsensitivity. The specific evaluation method is to use badcase as the model\ninput, individually enabling each quantization node, and comparing the error\nbetween the calibration model and the floating-point model. The error\nmeasurement metric is the same as the one used when finding badcase.\n\n\n\nIn the results of quantization sensitivity, operators ranked higher have a\ngreater impact on model accuracy and need to be set to a higher precision data\ntype.\n\nThe sensitive_type column has two values: weight and activation, representing\nthe cases of enabling only the weight quantization node or the output\nquantization node of the operator, respectively.\n\n\n\nAttention\n\nEven for very quantization-unfriendly models, some operators should have\nrelatively low quantization sensitivity. Therefore, in a normal sensitivity\ntable, the sensitivity should vary, with the last few operators' sensitivities\nnear to zero. If the errors for the last few operators are large, consider\nwhether there are incorrect post-processing operations, such as NMS, that have\nnot been thoroughly removed.\n\nIf the model has multiple outputs, each output will generate a corresponding\nsensitivity table. You can select the sensitivity tables corresponding to\nseveral outputs with different indicators and set the sensitivity template. The\nfollowing is an example of setting 2 output sensitivity tables int8 sensitivity\ntop 20% int16 in int8 / int16 mixed precision tuning. The total number of int16\nis the union of the top 20% int16 operators in the two tables. Then keep\nadjusting the ratio of int16 until you find the minimum ratio of int16 that\nmeets the accuracy requirements.\n\nYou need to use a sensitivity template to set the qconfig. See the qconfig\nsection for detailed usage. If the model has multiple outputs, each output will\ngenerate a corresponding sensitivity table. You can select the sensitivity\ntables corresponding to several outputs with large difference and set the\nsensitivity template. Below is an example of setting the top 20% most sensitive\noperators to int16 according to 2 sensitive tables in an int8/int16 mixed\nprecision tuning. The total number of int16 is the union of the top 20% int16\noperators in the two tables. Adjust the int16 ratio continuously until you find\nthe minimal int16 ratio that meets the accuracy requirements.\n\n\n\nCurrently, there is no interface to set fp16 based on sensitivity in bulk. You\nneed to set a small number of fp16 based on the int16 sensitivity results using\nModuleNameQconfigSetter. Below is an example of setting the top 1 most sensitive\nint16 operator to fp16 in int16/fp16 mixed precision optimization.\n\n\n\nJ6M has limited floating-point compute power. If fp16 is not absolutely\nnecessary, try to use int8/int16 mixed precision tuning. When the all-int16\nmodel cannot meet the accuracy requirements, introduce a small number of fp16\noperators into the all-int16 model.\n\nTwo scenarios in which the all-int16 model fail to meet accuracy standards:\n\n 1. Dual int16 needed: This manifests as certain operators having high\n    sensitivity under both activation and weight sensitivity types. Setting\n    weight and activation to int16 meets the accuracy requirements. Since J6M\n    does not support using int16 for both activation and weight simultaneously,\n    adjust the floating-point model to make one of them more\n    quantization-friendly. Common methods include increasing weight decay and\n    adding normalization operators.\n\n 2. Dual int16 not needed: This manifests as certain operators having high\n    sensitivity under either activation or weight sensitivity type, generally\n    due to plugin usage issues or certain operators requiring fixed scale\n    settings. Accuracy debugging can find specific problems.\n\n\nINT8 / INT16 / FP16 Mixed Precision Tuning#\n\nBefore performing int8/int16/fp16 mixed precision tuning, you should have\ncompleted int16/fp16 mixed precision tuning. Reuse the fp16 configuration from\nint16/fp16 mixed precision tuning and perform accuracy debugging based on the\nint8/fp16 mixed calibration model. Refer to the accuracy debugging methods in\nthe previous section, continuously adjusting the int16 ratio until you find the\nminimal int16 ratio that meets the accuracy requirements.\n\nBelow is an example of setting the top 1 most sensitive int16 operator to fp16\nand the top 20% most sensitive int8 operators to int16 in int8/int16/fp16 mixed\nprecision optimization.\n\n","routePath":"/en/guide/plugin/user_guide/precision_tuning","lang":"en","toc":[{"text":"Model Structure and Quantization Configuration Check","id":"model-structure-and-quantization-configuration-check","depth":2,"charIndex":795},{"text":"Operator Fusion","id":"operator-fusion","depth":3,"charIndex":1262},{"text":"Shared Modules","id":"shared-modules","depth":3,"charIndex":2483},{"text":"QConfig Configuration Errors","id":"qconfig-configuration-errors","depth":3,"charIndex":4131},{"text":"Mixed Precision Tuning","id":"mixed-precision-tuning","depth":2,"charIndex":4959},{"text":"Tuning Pipeline","id":"tuning-pipeline","depth":3,"charIndex":4985},{"text":"Basic Tuning Methods","id":"basic-tuning-methods","depth":3,"charIndex":5870},{"text":"Calibration","id":"calibration","depth":4,"charIndex":5967},{"text":"QAT","id":"qat","depth":4,"charIndex":6905},{"text":"Advanced Tuning Methods","id":"advanced-tuning-methods","depth":3,"charIndex":8413},{"text":"Setting Fixed Scale","id":"setting-fixed-scale","depth":4,"charIndex":8557},{"text":"Calibration","id":"calibration-1","depth":4,"charIndex":9318},{"text":"QAT","id":"qat-1","depth":4,"charIndex":9522},{"text":"INT8/INT16 & INT16/FP16 Mixed Precision Tuning","id":"int8int16--int16fp16-mixed-precision-tuning","depth":3,"charIndex":9977},{"text":"Find Badcase","id":"find-badcase","depth":4,"charIndex":11177},{"text":"Layer-by-Layer Comparison","id":"layer-by-layer-comparison","depth":4,"charIndex":13232},{"text":"Calculating Quantization Sensitivity","id":"calculating-quantization-sensitivity","depth":4,"charIndex":15509},{"text":"INT8 / INT16 / FP16 Mixed Precision Tuning","id":"int8--int16--fp16-mixed-precision-tuning","depth":3,"charIndex":19450}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":110,"title":"Prepare in Detail","content":"#\n\n\nDefinition of Prepare#\n\nPrepare is the process of converting a floating-point model into a\npseudo-quantized model. This process involves several key steps:\n\n 1. Operator Replacement: Some torch function operators (such as F.interpolate)\n    need to have FakeQuantize nodes inserted during quantization. Therefore,\n    these operators are replaced with corresponding Module type implementations\n    (horizon_plugin_pytorch.nn.Interpolate) to place the FakeQuantize nodes\n    inside this Module. The model before and after replacement is equivalent.\n\n 2. Operator Fusion: BPU supports fusing specific computational patterns, where\n    the intermediate results of fused operators are represented with high\n    precision. Therefore, we replace multiple operators to be fused with a\n    single Module to prevent quantizing the intermediate results. The model\n    before and after fusion is also equivalent.\n\n 3. Operator Conversion: Floating-point operators are replaced with QAT\n    (Quantization-Aware Training) operators. According to the configured\n    qconfig, QAT operators will add FakeQuantize nodes at the\n    input/output/weights.\n\n 4. Model Structure Check: The QAT model is checked, and a check result file is\n    generated.\n\nAttention\n\nDue to historical reasons, there are two early interfaces, prepare_qat and\nprepare_qat_fx, in the Plugin. These will gradually be deprecated. We only\nrecommend using the prepare interface introduced in this document.\n\nThe usage of the prepare interface is as follows:\n\n\n\n\nPrepareMethod#\n\nThere are four prepare methods, compared as follows:\n\nMETHOD                                        PRINCIPLE                                                    ADVANTAGES                                                DISADVANTAGES\nPrepareMethod.JIT & PrepareMethod.JIT_STRIP   Use hooks and subclass tensor to get the graph structure,    Fully automatic, minimal code modification, hides many    Dynamic code blocks need special handling.\n                                              performing operator replacement/operator fusion on the       detail issues, easy to debug.\n                                              original forward.\nPrepareMethod.SYMBOLIC                        Use symbolic trace to get the graph structure, performing    Fully automatic, hides many detail issues.                Does not support dynamic control flow, some data types and\n                                              operator replacement/operator fusion on the recompiled new                                                             Python operations, less convenient for debugging.\n                                              forward.\nPrepareMethod.EAGER                           Does not sense the graph structure. operator                 Flexible usage, controllable process, easy to debug and   Requires more manual operations, more code modifications,\n                                              replacement/operator fusion needs to be done manually.       handle various special needs.                             high learning cost.\n\nCurrently, JIT and JIT_STRIP are our recommended methods. The difference between\nthem is that JIT_STRIP will identify and skip pre-process and post-process based\non the positions of QuantStub and DequantStub in the model. Therefore, if there\nare pre-process and post-process steps in the model that do not need to be\nquantized, use JIT_STRIP. Otherwise, they will be quantized. Apart from this\ndifference, they are completely identical. SYMBOLIC and EAGER are earlier\nsolutions with many usability issues. We do not recommend using these two\nmethods.\n\n\nExample#\n\n\n\nAttention\n 1. When dynamic code blocks involve operator replacement or fusion, they must\n    be marked with Tracer.dynamic_block. Otherwise, it will lead to quantization\n    information confusion or forward errors.\n 2. Parts of the model where the call count changes (sub-modules or dynamic\n    blocks), if only executed once during the trace, may get fused with\n    non-dynamic parts, leading to forward errors.\n\n\nModel Check#\n\nWhen example_inputs is provided, prepare will perform a model structure check by\ndefault. If the check completes, a model_check_result.txt file can be found in\nthe running directory. If the check fails, you need to modify the model based on\nthe warning prompts or call\nhorizon_plugin_pytorch.utils.check_model.check_qat_model separately to check the\nmodel. The check process is the same as check_qat_model in the debug tool, and\nthe analysis of the result file is detailed in the check_qat_model related\ndocumentation.","routePath":"/en/guide/plugin/user_guide/prepare","lang":"en","toc":[{"text":"Definition of Prepare","id":"definition-of-prepare","depth":2,"charIndex":3},{"text":"PrepareMethod","id":"preparemethod","depth":2,"charIndex":1519},{"text":"Example","id":"example","depth":2,"charIndex":3633},{"text":"Model Check","id":"model-check","depth":2,"charIndex":4060}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":111,"title":"Quantized Awareness Training Guide","content":"#\n\nThe quantized awareness training is performed by inserting some pseudo-quantized\nnodes into the model, so as to minimize the loss of accuracy when the model\nobtained through quantized awareness training is converted into a fixed-point\nmodel. The quantized awareness training is no different from traditional model\ntraining in that one can start from scratch, build a pseudo-quantized model, and\nthen train on that pseudo-quantized model. Due to the limitations of the\ndeployed hardware platform, it is challenging to understand these limitations\nand build a pseudo-quantization model based on them. The quantized awareness\ntraining tool reduces the challenges of developing quantized models by\nautomatically inserting pseudo-quantization operators into the provided\nfloating-point model based on the limitations of the deployment platform.\n\nThe quantized awareness training is generally more difficult than the training\nof pure floating-point models due to the various restrictions imposed. The goal\nof the quantized awareness training tool is to reduce the difficulty of\nquantized awareness training and to reduce the engineering difficulty of\nquantized model deployment.\n\n\nProcess and Example#\n\nAlthough it is not mandatory for a quantized awareness training tool to start\nwith a pre-trained floating-point model, experience has shown that typically\nstarting quantized awareness training with a pre-trained high-precision\nfloating-point model can greatly reduce the difficulty of quantized awareness\ntraining.\n\n\n\nAttention\n\nDue to the underlying limitations of the deployment platform, the QAT model\ncannot fully represent the final on-board accuracy, please make sure to monitor\nthe quantized model accuracy to ensure that the quantized model accuracy is\nnormal, otherwise the model on-board dropout problem may occur.\n\nAs can be seen from the above sample code, there are two additional steps in\nquantized awareness training compared to traditional pure floating-point model\ntraining:\n\n 1. prepare.\n 2. Load the Calibration model parameters.\n\n\nprepare#\n\nThe goal of this step is to transform the floating-point network and insert\npseudo-quantized nodes.\n\n\nLoad the Calibration Model Parameters#\n\nA better initialization is obtained by loading the pseudo-quantization\nparameters obtained from Calibration.\n\n\nIterative Training#\n\nAt this point, the construction of the pseudo-quantized model and the\ninitialization of the parameters are completed, and then the regular training\niterations and model parameter updates can be performed, and the quantized model\naccuracy can be monitored.\n\n\nPseudo-quantized Operator#\n\nThe main difference between the quantized awareness training and the traditional\nfloating-point model's training is the insertion of pseudo-quantization\noperators, and, as different quantized awareness training algorithms are also\nrepresented by pseudo-quantization operators, here we introduce the\npseudo-quantization operators.\n\nNote\n\nSince the BPU only supports symmetric quantization, here we take the symmetric\nquantization as an example.\n\n\nPseudo-quantization Process#\n\nTake the int8 quantized awareness training as an example, in general, the\npseudo-quantization operator is computed as follows:\n\nfake_quant_x = clip(round(x / scale)，-128, 127) * scale\n\nSimilar to Conv2d, which optimizes the weight and bias parameters through\ntraining, the pseudo-quantization operator needs to be trained to optimize the\nscale parameter. However, the gradient of round as a step function is 0, which\nmakes it impossible to train the pseudo-quantization operator by backpropagation\nof the gradient directly. To solve this problem, there are usually two\nsolutions: a statistical-based approach and a learning-based approach.\n\n\nStatistical-based Approach#\n\nThe goal of quantization is to uniformly map the floating point numbers in\nTensor to the range [-128, 127] represented by int8 via the scale parameter.\nSince the mapping is uniform, it is easy to see how scale is calculated:\n\n\n\nDue to the uneven distribution of data in Tensor and the outlier problem,\ndifferent methods for calculating xmin and xmax have been developed. See\nMinMaxObserver and so on.\n\nPlease refer to QConfig in Detail for the usage in the tool.\n\n\nLearning-based Approach#\n\nAlthough the gradient of round is 0, the researcher found experimentally that in\nthis scenario, if the gradient is directly set to 1, the model can also be made\nto converge to the expected accuracy.\n\n\n\nPlease refer to Definition of FakeQuantize for the usage in the tool.\n\nIf you are interested in learning more, you can refer to the paper Learned Step\nSize Quantization.","routePath":"/en/guide/plugin/user_guide/qat_guide","lang":"en","toc":[{"text":"Process and Example","id":"process-and-example","depth":2,"charIndex":1177},{"text":"prepare","id":"prepare","depth":3,"charIndex":2050},{"text":"Load the Calibration Model Parameters","id":"load-the-calibration-model-parameters","depth":3,"charIndex":2162},{"text":"Iterative Training","id":"iterative-training","depth":3,"charIndex":2313},{"text":"Pseudo-quantized Operator","id":"pseudo-quantized-operator","depth":2,"charIndex":2592},{"text":"Pseudo-quantization Process","id":"pseudo-quantization-process","depth":3,"charIndex":3066},{"text":"Statistical-based Approach","id":"statistical-based-approach","depth":3,"charIndex":3738},{"text":"Learning-based Approach","id":"learning-based-approach","depth":3,"charIndex":4232}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":112,"title":"QConfig in Detail","content":"#\n\n\nDefinition of QConfig#\n\nThe quantization mode of the model is determined by qconfig, which needs to be\nset for the model before preparing the qat / calibration model.\n\nAttention\n\nDue to historical reasons, there are different definitions and usages of qconfig\nin the Plugin. Earlier versions of qconfig will be deprecated in the near\nfuture, and we only recommend that you use the qconfig usage described in this\ndocument.\n\nA qconfig object can set three keywords: input, weight, and output, representing\nthe quantization configuration of the operator's input, weight, and output\nrespectively. When preparing model, these configurations determine whether to\ninsert FakeQuantize or FakeCast nodes at the corresponding positions. None means\nno nodes will be inserted.\n\n\n\n\nDefinition of FakeQuantize#\n\nFakeQuantize is a fake quantization node that performs quantization and\ndequantization operations on the input. Inserting fake quantization can simulate\nthe errors caused by quantization in the forward pass of a floating-point model.\nThe horizon_plugin_pytorch supports three types of fake quantization:\nFakeQuantize, PACTFakeQuantize, and _LearnableFakeQuantize. We recommend using\nthe statistic-based FakeQuantize. The document won't introduce PACTFakeQuantize\nand _LearnableFakeQuantize. If required, please read the papers before using\nthem.\n\n\n\nYou can call the with_args method of FakeQuantize to get a constructor and use\nit to construct qconfig as shown in the previous section. The parameters of\nwith_args include parameters supported by FakeQuantize and observer,\ntheoretically allowing configuration of all parameters declared in the init\nmethod of the FakeQuantize and observer classes. However, to avoid unnecessary\ndetails, we recommend you to configure the observer-related parameters only.\n\nDifferent observers have different parameters. Below are examples of\nconstructing FakeQuantize with common used observers. For the specific usage of\nother observers, see the calibration chapter.\n\n\n\n\nDefinition of FakeCast#\n\nFakeCast is a fake conversion node that converts the input to float32 data type.\nIf the data type is float16, it also simulates the truncation error caused by\nconverting value to float16. This node is mainly used to mark operators that\nrequire floating-point computation.\n\nThe method of using FakeCast to construct qconfig is similar to FakeQuantize,\nbut it only has one parameter.\n\n\n\n\nConstruct QConfig#\n\n 1. Construct the QConfig object directly as introduced above. This method is\n    flexible, allowing the configuration of any configurable parameter, but\n    requires deep understanding of QConfig.\n\n 2. Use the get_qconfig interface. This interface is simpler and easier to use\n    than directly constructing QConfig objects but less flexible, and cannot be\n    used for advanced requirements.\n\n\n\n\nUse QConfig#\n\n 1. Directly set the qconfig attribute. This method has the highest priority,\n    and other methods will not override the directly set qconfig.\n\n\n\n 2. QConfig template. Specify the qconfig setter and example_inputs on the\n    prepare interface to automatically set qconfig for the model.\n\n\n\n\nQConfig Template#\n\nQConfig templates get model's graph structure based on subclass trace and\nautomatically set qconfig according to the specified rules. This is the most\nrecommended method for setting qconfig. Usage is as follows:\n\n\n\nAttention\n\nThe template priority is lower than directly setting the qconfig attribute of\nthe model. If the model has been configured using model.qconfig = xxx before\nprepare, the template will not take effect. We do not recommend mixing the two\nmethods unless there is a special need, as this can easily cause errors. In most\ncases, using one of the two methods is sufficient.\n\nTemplates can be divided into three categories:\n\n 1. Fixed templates. The difference between calibration / qat /\n    qat_fixed_act_scale in fixed templates is the type of observer used and the\n    scale updating logic, which is used for calibration, qat training, and fixed\n    activation scale qat training, respectively. The default template (\n    default_calibration_qconfig_setter / default_qat_qconfig_setter /\n    default_qat_fixed_act_qconfig_setter ) does three things: First, it will set\n    all the high accuracy outputs that can be set, and will give a hint for\n    outputs that don't support high accuracy; Then, it searches forward from the\n    grid input of the grid sample operator until the first gemm-like operator or\n    QuantStub and sets all the intermediate operators to int16. From experience,\n    the grid here is usually expressed over a wide range, so int8 is more likely\n    to be insufficient to meet the accuracy requirements; Finally, set the\n    remaining operators to int8. int16 template (\n    qat_8bit_weight_16bit_act_qconfig_setter /\n    qat_8bit_weight_16bit_fixed_act_qconfig_setter /\n    calibration_8bit_weight_16bit_act_qconfig_setter ) will do two things:\n    First, set all the high accuracy outputs that can be set, and will give a\n    hint for outputs that don't support high accuracy; Second, set the rest of\n    the operators to int16.\n\n\n\n 2. Sensitivity templates. The sensitivity templates are\n    sensitive_op_calibration_8bit_weight_16bit_act_qconfig_setter,\n    sensitive_op_qat_8bit_weight_16bit_act_qconfig_setter and\n    sensitive_op_qat_8bit_weight_16bit_fixed_act_qconfig_setter. The difference\n    among the three is consistent with the difference among the three in the\n    fixed template, which is also used for calibration, qat training, and fixed\n    activation scale qat training, respectively. The first input of the\n    sensitivity template is the sensitivity result generated by the accuracy\n    debug tool, and the second parameter can be specified as ratio or topk, the\n    sensitivity template will set the topk operators with the highest\n    quantization sensitivity to int16. With the fixed template, it is easy to\n    realize the mixed dtype tuning. If the model has multiple outputs, each\n    output will generate a sensitivity table, and you can set multiple\n    sensitivity templates.\n\n\n\n 3. Customized templates. Customized template only has ModuleNameQconfigSetter,\n    you need to pass in the module name and the corresponding qconfig\n    dictionary. It is generally used for setting fixed scale and other special\n    needs. Also, it can be used with fixed templates and sensitivity templates.\n\n","routePath":"/en/guide/plugin/user_guide/qconfig","lang":"en","toc":[{"text":"Definition of QConfig","id":"definition-of-qconfig","depth":2,"charIndex":3},{"text":"Definition of FakeQuantize","id":"definition-of-fakequantize","depth":2,"charIndex":773},{"text":"Definition of FakeCast","id":"definition-of-fakecast","depth":2,"charIndex":2007},{"text":"Construct QConfig","id":"construct-qconfig","depth":2,"charIndex":2418},{"text":"Use QConfig","id":"use-qconfig","depth":2,"charIndex":2836},{"text":"QConfig Template","id":"qconfig-template","depth":2,"charIndex":3142}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":113,"title":"Accuracy Tuning Tool Guide","content":"#\n\nDue to the error in the floating point to fixed point process, you will\ninevitably encounter the problem of quantization model accuracy dropout when\nusing quantization training tools. Typically, there are several reasons for\nthis:\n\n 1. The original floating-point model is not favorable for quantization, such as\n    the existence of shared ops or shared structures.\n\n 2. The QAT network structure or configuration is abnormal, such as there is a\n    pattern without fuse in the model, and the output is not set with high\n    accuracy.\n\n 3. Some operators are sensitive to quantization, and the quantization error of\n    the operator accumulates layer by layer in the forward propagation process,\n    which ultimately leads to a large error in the model output.\n\nFor the above cases, we provides the accuracy tuning tool to help you quickly\nlocate and solve the accuracy problems, which mainly includes the following\nmodules:\n\n * Model Structure Checking: check if there are shared ops, patterns without\n   fuse, or quantization configurations in the model that do not meet\n   expectations.\n\n * QuantAnalysis Class: automatically compare and analyze the two models to\n   locate anomalous operators or quantization-sensitive ops in the quantization\n   model.\n\n * ModelProfiler Class and HbirModelProfiler Class: get information about the\n   numerical characteristics of each op in the model, such as the maximum and\n   minimum values of the inputs and outputs. The functionality of these two\n   classes is identical, the difference is that HbirModelProfiler accepts as\n   input only the qat hbir model. Usually you don't need to call this module\n   manually, you can get the numerical information of both models directly from\n   QuantAnalysis.run.\n\n\nQuickstart#\n\nWhen encountering quantization model accuracy dropout problems, we recommend\nusing the accuracy tuning tool according to the following process.\n\n 1. Check if there are any unfavorable structures or abnormal configurations in\n    the model.\n\n 2. Use QuantAnalysis module to analyze the model as follows:\n    \n    1). Find a bad case as an input to the model. The bad case is the input that\n    has the largest difference between the outputs of the baseline model and the\n    model to be analyzed.\n    \n    2). Perform quantization sensitivity analysis, the current experience is\n    that the first n L1 sensitivities are usually the quantitative sensitivity\n    ops (the value of n varies from model to model, and there is no automatic\n    method to determine it, so we need to try it manually, e.g., the first 10,\n    20...). Set the quantization sensitive op to high accuracy quantization\n    (e.g., int16 quantization), and redo the quantization process.\n    \n    3). Or compare the inputs and outputs of the two models layer by layer to\n    check whether there is an abnormal quantization op such as the data range is\n    too large or the scale is unreasonable, e.g., certain ops with physical\n    meanings should be set to a fixed scale.\n\nThe overall flow chart is as follows:\n\nA whole example is as follows:\n\n\n\n\nAPI Reference#\n\n\nModel Structure Checking #\n\n\n\nCheck if there are structures in the calibration/qat model that are not\nfavorable for quantization and if the quantization qconfig configuration is as\nexpected.\n\nParameters\n\n * model: model to be checked.\n\n * example_inputs: model inputs.\n\n * save_results: whether to save the check results to a txt file. Default is\n   False.\n\n * out_dir: save path of the result file 'model_check_result.txt'. Default is\n   empty, save to current path.\n\nOutput\n\n * screen output: abnormal layers that check out.\n\n * model_check_result.txt: generated when save_results = True. It consists of 5\n   main parts:\n   \n   1). Unfused pattern.\n   \n   2). The calling times of each module. Normally each op is called only 1 time,\n   0 means it is not called, more than 1 time means it is shared many times. If\n   it is not called or shared multiple times, an exception prompt will be\n   displayed.\n   \n   3). The qconfig configuration for each op output.\n   \n   4). The qconfig configuration for each op weight (if any).\n   \n   5). Exception qconfig hints (if any).\n\n\n\nNote\n\nThe prepare interface has integrated this check. Please pay attention to the\ninspection results output by this interface and make targeted adjustments to the\nmodel based on the inspection results.\n\n\nQuantAnalysis Class #\n\nQuantAnalysis class can automatically find the bad case with the largest output\nof two models, and use it as input to compare the output of two models layer by\nlayer. In addition, QuantAnalysis class also provides the function of\ncalculating the sensitivity, you can try to set the node with the topk\nsensitivity ranking with high accuracy, such as int16 quantization, to improve\nthe accuracy of quanitized model.\n\n\n\nParameters\n\n * baseline_model: baseline model (high accuracy).\n\n * analysis_model: model to be analyzed ( accuracy dropping points).\n\n * analysis_model_type: model to be analyzed. Support input:\n   \n   * fake_quant: the model to be analyzed can be a calibration model with\n     dropped accuracy, in which case the baseline model can be either the\n     original floating-point model or a accuracy-compliant calibration model in\n     a mixed int8/int16 configuration.\n\n * device_ids: GPU device ids to run analysis. Default None.\n\n * post_process: post process function which performs on model output.\n\n * out_dir: specify the output directory for the comparison results.\n\nAttention\n\nBecause QAT training changes the model weight distribution, we generally do not\nrecommend comparing floating point or calibration models with qat models.\n\nThe methods in this class are as follows.\n\nauto_find_bad_case#\n\n\n\nAutomatically find the badcase that causes the worst output for the two models.\n\nParameters\n\n * data_generator: dataloader or a custom iterator that produces one piece of\n   data per iteration.\n\n * num_steps: number of iteration steps.\n\n * metric: Specify which metric to use as the metric for the badcase. default is\n   to use the worst result of L1. Support Cosine/MSE/L1/KL/SQNR/custom. The\n   custom means use custom metric calculation method, in which case\n   custom_metric_func and custom_metric_order_seq must not be None.\n\n * device: specify the model run device.\n\n * custom_metric_func: customize the model output comparison function.\n\n * custom_metric_order_seq: customize the sorting rule of the model output\n   comparison function, only \"ascending\"/\"descending\" is supported, which means\n   ascending/descending.\n\n * cached_attrs: cached attrs to use as input. Usually used in sequence model.\n   For instance, some results of the first frame must be treated as input when\n   running the second frame, Default None.\n\nNote\n\nFunction auto_find_bad_case goes through data_generator, runs baseline model and\nanalysis model, computes each output results on Cosine/MSE/L1/KL/SQNR metrics\nand finds the baddest input case on each metric.\n\nOutput\n\n * badcase.txt: It has three parts.\n   \n   * The baddest input index in data_generator of each output under different\n     metrics.\n   \n   * The baddest metric value computed by the baddest input of each output.\n   \n   * The baddest input index of each metric.\n     \n     \n   \n   * badcase.pt: The baddest input data under the metric pass by parameter\n     metric. It is used as default input of function run.\n\nset_bad_case#\n\n\n\nSet the badcase manually.\n\nAttention\n\nUsually, we suggest that you find badcase by function auto_find_bad_case. If the\nmanual set badcase is not the actual badcase, it is difficult for quant analysis\ntool to find quantization sensitive layers.\n\nParameters\n\n * data: badcase input.\n\n * baseline_model_cached_attr: baseline model cached attr.\n\n * analysis_model_cached_attr: analysis model cached attr.\n\nload_bad_case#\n\n\n\nLoad badcase from the specified file.\n\nParameters\n\n * filename: specified file path. Defaultly, it loads all badcase related files\n   saved by function auto_find_bad_case from directory specified by out_dir.\n\nsave_bad_case#\n\n\n\nSave badcase to the file.\n\nAttention\n\nIt is used with set_bad_case. Usually, you do not need to invoke this function.\n\nset_model_profiler_dir#\n\n\n\nSpecify the path to save the output of model_profiler manually.\n\nIn some cases, the ModelProfiler is defined and run before QuantAnalysis is\ninitialized, in which case you can directly specify the path to the existing\nModelProfiler, skipping the run step of QuantAnalysis and comparing the output\nof the two models directly.\n\nParameters\n\n * baseline_model_profiler_path: profiler path for the baseline model.\n\n * analysis_model_profiler_path: profiler path for the model to be analyzed.\n\nrun#\n\n\n\nRun the two models and save the results for each layer in the model separately.\n\nParameters\n\n * device: model running device.\n\n * index: use which index input as example input.\n\nAttention\n\nOnly index found by auto_find_bad_case and shown in badcase.txt is allowed to be\nparameter.\n\ncompare_per_layer#\n\n\n\nCompare the results of each layer in the two models.\n\nParameters\n\n * prefixes: the prefix of ops.\n\n * types: the types of ops.\n\nNote\n\nUsually you do not need to specify the prefixes and types parameters. If you\nwant to skip the comparison of certain ops with less quantitative impact based\non some prior experience, or want to save time, you can use these two parameters\nto specify the comparison of certain ops or a certain type of ops.\n\nOutput\n\n * abnormal_layer_advisor.txt: all anomaly layers, including cases of data range\n   too large/outputs with inf or NaN/outputs without high accuracy.\n\n * profiler.html: show visually all metrics and the data range diff for each\n   layer in the model.\n\n\n\n * compare_per_layer_out.txt: show the specific information of each layer in the\n   model in the form of a table, including various metrics, data ranges,\n   quantized dtype, etc.. Each column from left to right represents:\n   \n   * Index: op index.\n   \n   * mod_name: name of the op, if the op is of module type, the prefix name of\n     the module in the model will be shown, if it is of function type, it will\n     not be shown.\n   \n   * base_op_type: type of the op in the base model, may be module type or\n     function name.\n   \n   * analy_op_type: type of the op in the model to be analyzed, could be module\n     type or function name.\n   \n   * Shape: shape of the op.\n   \n   * quant_dtype: quantized type output of the op.\n   \n   * Qscale: quantized scale output of the op.\n   \n   * Cosine: cosine similarity of the op output in the two models.\n   \n   * L1: L1 distance of the op output in the two models.\n   \n   * Atol: absolute error of the op output in the two models.\n   \n   * max_qscale_diff: the max N scale diff of the op output in the two models.\n   \n   * base_model_min: minimum value of the op output in the baseline model.\n   \n   * analy_model_min: minimum value of the op output in the model to be\n     analyzed.\n   \n   * base_model_max: maximum value of the op output in the baseline model.\n   \n   * analy_model_max: maximum value of the op output in the model to be\n     analyzed.\n   \n   * base_model_mean: average of the op output in the baseline model.\n   \n   * analy_model_mean: average of the op output in the model to be analyzed.\n     \n     \n\n * compare_per_layer_out.csv: show the specific information of each layer in csv\n   format. The content is exactly the same as compare_per_layer_out.txt, and the\n   csv file format is convenient for you to open and analyze by excel and other\n   software.\n\nsensitivity#\n\n\n\nSensitivity ordering of individual nodes in the model. Applies to the float\nconversion to calibration accuracy dropout problem.\n\nAttention\n\nThe sensitivity function is not supported for calculating the sensitivity of\nhbir models.\n\nParameters\n\n * device: specify the model running device.\n\n * metric: metric for similarity ordering, default is L1, support\n   Cosine/MSE/L1/KL/SQNR.\n\n * reserve: whether to print sensitivity nodes in reverse order to support\n   returning some int16 operators to int8 to improve on-board performance.\n\nOutput\n\n * sensitive_ops.txt. The file is organized in order of quantization sensitivity\n   from highest to lowest op. Each column from left to right represents:\n   \n   * op_name: op name.\n   \n   * sensitive_type: Type of calculating quantization sensitivities, including:\n     \n     * activation: quantization sensitivity to quantize only the output of this\n       op.\n     \n     * weight: quantization sensitivity of the weight of the op only.\n   \n   * op_type: op type.\n   \n   * metric: the metric for calculating sensitivity. Sort the metrics in\n     descending order of sensitivity. Support Cosine/L1/MSE/KL/SQNR. L1 is used\n     by default.\n     \n     * L1:value range [0, $+\\infty$], the higher the value, the more sensitive\n       the op is to quantization (in descending order).\n     \n     * Cosine:value range [0,1], the closer to 0, the more sensitive the op is\n       to quantization (in descending order).\n     \n     * MSE:value range [0, $+\\infty$], the larger the value, the more sensitive\n       the op is to quantization (in descending order).\n     \n     * KL:Range [0, $+\\infty$], the larger the value, the more sensitive the op\n       is to quantization (in descending order).\n     \n     * SQNR: range [0, $+\\infty$], the smaller the value, the more sensitive the\n       op is to quantization (in descending order).\n   \n   * quant_dtype: the quant dtype of this op output. Usually qint8/qint16.\n   \n   * flops: the flops of this op and its proportion in the whole model.\n\n * sensitive_ops.pt. A sensitivity-ordered list saved using torch.save for your\n   subsequent loading use. The format of the list is described in the Return\n   Values section.\n\nReturn Value\n\nSensitivity List, each element in the List is a sub-list recording an op's\nsensitivity information. each item in the sub-list from left to right is\n[op_name, sensitive_type, op_type, metric, quant_dtype, flops] .\n\nAn example of the whole List is as follows.\n\n\n\nYou can configure the ops with the top n quantization sensitivities with high\naccuracy (e.g., int16) to try to improve the quantized model accuracy.\n\n\n\nclean#\n\n\n\nClears intermediate results. Only files such as comparison results are retained.\n\n\nModelProfiler Class #\n\nStatistic about the inputs and outputs of each layer of operators in the forward\nprocess of the model.\n\n\n\nParameters\n\n * model: model that requires statistics.\n\n * out_dir: path where the associated file is saved.\n\nNote\n\nThis class only supports use by means of a with statement.\n\n\n\nThe methods in this class are as follows.\n\nget_info_manager#\n\n\n\nGet the structure that manages the information for each op.\n\nReturn Value\n\nThe structure OpRunningInfoManager manages the information stored for each op.\nTwo of the important interfaces are as follows.\n\ntable#\n\n\n\nShow individual model statistics in a table. Store to the statistic.txt file.\n\nParameters\n\n * out_dir: storage path of statistic.txt file, default None, store to\n   self.out_dir.\n\n * prefixes: prefixes of ops in the model to be counted. Default is all ops.\n\n * types: types of the ops in the model to be counted, defaults to all ops.\n\n * with_stack: if or not show the position of each op in the code.\n\nOutput\n\nstatistic.txt file, each column from left to right reads.\n\n * Index: op index.\n\n * Op Name: op type, module class name or function name.\n\n * Mod Name: if it is module class, the prefix name of the module in the model;\n   if it is function type, the prefix name of the module where the function is\n   located.\n\n * Attr: input/output/weight/bias.\n\n * Dtype: data type of the tensor.\n\n * Scale: scale of the tensor.\n\n * Min: minimum value of the current tensor.\n\n * Max: maximum value of the current tensor.\n\n * Mean: average value of the current tensor.\n\n * Var: variance of the values in the current tensor.\n\n * Shape: tensor shape.\n\n\n\ntensorboard#\n\n\n\nShow the input and output histograms for each layer in the tensorboard.\n\nParameters\n\n * out_dir: directory where tensorboard related files are kept. Default is\n   self.out_dir/tensorboard.\n\n * prefixes: prefixes of the ops in the model to be counted, default is all.\n\n * types: types of the ops in the model to be counted, default is all.\n\n * force_per_channel: if or not to display the histogram in per_channel\n   quantization.\n\nOutput\n\nThe tensorboard file, opened with the following screenshot.\n\n\n\n\nHbirModelProfiler Class #\n\nThe functionality and usage of this class is identical to the ModelProfiler\nclass. Please refer to ModelProfiler Class for usage.\n\nAttention\n\nDue to the special format of the hbir model, the qat hbir model needs to add\nindex 0 at forward process.\n\n","routePath":"/en/guide/plugin/user_guide/quant_analysis","lang":"en","toc":[{"text":"Quickstart","id":"quickstart","depth":2,"charIndex":1751},{"text":"API Reference","id":"api-reference","depth":2,"charIndex":3081},{"text":"Model Structure Checking","id":"model-structure-checking","depth":3,"charIndex":-1},{"text":"QuantAnalysis Class","id":"quantanalysis-class","depth":3,"charIndex":-1},{"text":"auto_find_bad_case","id":"auto_find_bad_case","depth":4,"charIndex":5698},{"text":"set_bad_case","id":"set_bad_case","depth":4,"charIndex":7383},{"text":"load_bad_case","id":"load_bad_case","depth":4,"charIndex":7802},{"text":"save_bad_case","id":"save_bad_case","depth":4,"charIndex":8029},{"text":"set_model_profiler_dir","id":"set_model_profiler_dir","depth":4,"charIndex":8166},{"text":"run","id":"run","depth":4,"charIndex":8681},{"text":"compare_per_layer","id":"compare_per_layer","depth":4,"charIndex":8971},{"text":"sensitivity","id":"sensitivity","depth":4,"charIndex":11518},{"text":"clean","id":"clean","depth":4,"charIndex":14162},{"text":"ModelProfiler Class","id":"modelprofiler-class","depth":3,"charIndex":-1},{"text":"get_info_manager","id":"get_info_manager","depth":4,"charIndex":14604},{"text":"HbirModelProfiler Class","id":"hbirmodelprofiler-class","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":114,"title":"OpenExplorer Introduction","content":"#\n\n\nWhat is OE#\n\nOE is the abbreviation of OpenExplorer, which is an full lifecycle development\nplatform based on Horizon's self-developed computing platform.\n\nIt mainly includes three functional modules: model compilation optimization\ntoolset, algorithm warehouse and application development SDK. The application\nreference solutions developed based on these three functional modules provide\ncase supports for smart driving, smart IoT and other industry solutions.\n\n\n\n 1. Model compilation optimization toolset: It focuses on intelligent business\n    scenarios, including a series of software toolsets involved in completing\n    the algorithmic model transformation and compilation process, providing\n    model quantization, optimization, compilation, debugging and other\n    functions.\n 2. Algorithm warehouse: It fits the basic algorithmic structure of the current\n    smart driving field, and the key business algorithmic technology points,\n    Horizon provides a wealth of open source best practices, empowering Horizon\n    partners to develop their own smart products faster and more economically.\n 3. Application Development SDK: It provides rich basic interfaces and tools to\n    support the deployment of business algorithms on the Journey platform, which\n    can completely support the whole process of customer deployment in\n    simulation and physical environment.\n\nOE provides Horizon partners with rich and diverse algorithm resources, flexible\nand efficient development tools, and easy-to-use development frameworks. The\nfeatures and advantages of OE are as follows:\n\n\n\nTo help you deploy various solutions to a series of Horizon dev boards, we\nprovide a full-volume development package, called OE package, which integrates\nthe development environment deployment, application reference solution sample\ncode, user manuals, etc. After you get the OE package, you can first follow the\nsteps below to understand the OE:\n\n 1. First refer to Release Content for the directory structure of the release\n    package.\n 2. Then refer to Environment Deployment for the development environment and\n    runtime environment deployment.\n 3. Next, refer to Post-training Quantization (PTQ), Quantized Awareness\n    Training (QAT) and Embedded Application Development to complete the entire\n    process of model conversion and deployment.\n\nFor more tutorials on using the OE package, please refer to the instruction\nmanual below. We believe Horizon's OE package can make your development more\nefficient and easier!\n\n\nRelease Content #\n\n\npackage#\n\nThe package directory contains some base libraries and components for the\ndistribution to run.\n\n * package.board\n   \n   package.board contains the board-side executables.\n   \n   * hrt_model_exec is a model execution tool that can be used directly on the\n     dev board to evaluate the inference performance of the model and obtain\n     model information, which provides funcitons including model inference\n     infer, model performance analysis perf and viewing model information\n     model_info.\n   * install.sh is an installation script that installs the hrt tool to the\n     specified dev board with one click.\n\n * package.host\n   \n   The package.host folder contains the environment dependencies and tool\n   dependencies for the distribution in the x86 development environment. By\n   executing the script install.sh in this directory, you can install all\n   environment and tool dependencies on the development machine. By executing\n   the script resolve.sh in this directory, you can download the\n   cross-compilation tools and torch etc. dependencies.\n\n\nsamples#\n\nThe samples contains ai_toolchain, model_zoo and ucp_tutorial.\n\n * ai_toolchain provides a series of samples of some model algorithms\n   (horizon_model_train_samples: samples of floating-point model training\n   frameworks; horizon_model_convert_sample: a conversion sample for\n   floating-point to fixed-point models; model_zoo: a model library that is used\n   to place the original model and runtime model for the toolchain sample model\n   compilation).\n * model_zoo, soft link points to the model_zoo path in the ai_toolchain folder.\n * ucp_tutorial, the UCP samples package, which provides the necessary\n   dependencies for UCP and related samples.\n\n\nresolve_all.sh#\n\nScript for automatically downloading all downloadable dependencies within the OE\npackage.\n\nRunning this script will sequentially download the following:\n\n 1. Execute resolve.sh under the package/host path to download the\n    cross-compilation tools and torch etc. dependencies.\n\n 2. Execute resolve_ai_benchmark_ptq.sh under the\n    samples/ai_toolchain/model_zoo/runtime/ai_benchmark path to download the hbm\n    model used on the board.\n\n 3. Execute resolve_ai_benchmark_qat.sh under the\n    samples/ai_toolchain/model_zoo/runtime/ai_benchmark path to download the hbm\n    model used on the board.\n\n 4. Execute resolve_runtime_sample.sh under the\n    samples/ai_toolchain/model_zoo/runtime/basic_samples path to download the\n    hbm model used on the board for the corresponding sample.\n\n 5. Execute resolve.sh under the samples/ucp_tutorial/dnn/basic_samples/code/\n    path to download the dataset and other files required for the corresponding\n    sample.\n\n 6. Execute all the 00_init.sh scripts in the\n    samples/ai_toolchain/horizon_model_convert_sample folder to download the\n    calibration dataset and the original model for the sample.\n\n\nrun_docker.sh#\n\nIn case the evaluation dataset and the required docker download for the OE\npackage are completed, you can use the command sh run_docker.sh {dataset path}\nto automatically mount the OE package and start the docker.","routePath":"/en/guide/preface/learn_oe","lang":"en","toc":[{"text":"What is OE","id":"what-is-oe","depth":2,"charIndex":3},{"text":"Release Content","id":"release-content","depth":2,"charIndex":-1},{"text":"package","id":"package","depth":3,"charIndex":2532},{"text":"samples","id":"samples","depth":3,"charIndex":3602},{"text":"resolve_all.sh","id":"resolve_allsh","depth":3,"charIndex":4266},{"text":"run_docker.sh","id":"run_dockersh","depth":3,"charIndex":5432}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":115,"title":"Toolchain Overview","content":"#\n\nHorizon J6 algorithm toolchain (hereinafter referred to as the toolchain) is a\ncomplete set of artificial intelligence edge algorithm solution, which can help\nyou quantify floating-point models into fixed-point models and rapidly deploy\nself-developed algorithm models on Horizon computing platforms.\n\nCurrently, most of the models trained on GPUs are floating-point models, that\nis, the parameters are stored in the float data type.\n\nThe computing platforms with Horizon's BPU architecture use the int8 calculation\nprecision (common precision for computing platforms in the industry) and is\ncapable of running fixed-point quantization models.\n\nThe quantization is the process of converting a trained floating-point model to\na fixed-point model.\n\nIn addition, model quantization can effectively reduce the model size,\naccelerate the speed of deep-learning inference, therefore, it is also widely\nstudied and applied in academia and industry.\n\nDepending on whether to adjust the parameters after quantization, we can\nclassify the quantization methods into post-training quantization (PTQ) and\nquantization-aware training (QAT).\n\nThe difference in operation between these two methods is shown in the following\ndiagram (Left: PTQ; Right: QAT).\n\nThe PTQ uses a batch of calibration data to calibrate the trained models, which\nconverts the trained FP32 model directly into a fixed-point computational model\nwithout any training of the original model. As the quantization process is\nsimple and fast, requiring a few adjustments to the hyperparameters and no\ntraining, this method has been widely used in a large number of end-side and\ncloud-side deployment scenarios. We recommend that you to try the PTQ method\nfirst to see if it meets your requirements on the deployment accuracy and\nperformance. For more information about the PTQ scheme, please read\nPost-training Quantization (PTQ).\n\nThe QAT is to quantize the trained model before training it again. Since the\nfixed-point values cannot be used for backward gradient calculation, the actual\nprocedure is to insert fake quantization nodes in front of some operators to\nobtain the truncated values of the data flowing through the op during the\ntraining, so that they can be easily used when quantizing the nodes during the\ndeployment of the quantization models. We need to continuously optimize the\naccuracy during training to obtain the best quantization parameters. Since the\nmodel training is involved, it requires the developers to have higher levels of\ntechnical skills. For more information about the QAT scheme, please read\nQuantized Awareness Training (QAT).\n\nThe toolchain consists of PTQ, QAT, embedded compilation, etc. The schematic\ndiagram of the toolchain composition is as follows:\n\nRuntime SDK provides runtime library support. The runtime library contains two\nparts, ARM and x86, which are used to execute models on Horizon computing\nplatform and x86 simulation platform respectively.\n\nFor more information about embedded application development, please read\nEmbedded Application Development.\n\nIn addition, the toolchain provides rich development tools, samples, and model\nreleases with a large number of built-in algorithmic models to help you get\nstarted and to improve your development efficiency.\n\nThe overall workflow of using the toolchain is shown in the figure below. We\nrecommend that you to try the PTQ method first to see if it meets your\nrequirements on the deployment accuracy and performance.","routePath":"/en/guide/preface/toolchain_overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":116,"title":"Data Normalization Parameter Configuration","content":"#\n\n\nParameters#\n\nParameter: mean_value\n\nThis parameter means the average value subtracted from the image of specified\npreprocessing method.\n\n * Parameter descriptions:\n   \n   * When there is only one input node, only one value needs to be configured,\n     which indicates that all channels need to subtract this mean value.\n   \n   * When there are multiple nodes, provide values consistent with the number of\n     channels (these values are separated by spaces), indicating that each\n     channel is subtracted by a different mean value.\n\nAttention\n\nIf there exists a node that does not require mean processing, configure 'None'\nfor that node.\n\nParameter: scale_value\n\nThis parameter indicates the numerical scale coefficient for the specified\npreprocessing method.\n\n * Parameter descriptions:\n   \n   * When there is only one input node, only one value needs to be configured,\n     indicating that all channels are multiplied by this coefficient.\n   \n   * When there are multiple nodes, provide values consistent with the number of\n     channels (these values are separated by spaces), indicating that each\n     channel is multiplied by a different coefficient.\n\nAttention\n\nIf there exists a node that does not require scale processing, configure 'None'\nfor that node.\n\nParameter: std_value\n\nThis parameter indicates the numerical std coefficient for the specified\npreprocessing method.\n\n * Parameter descriptions:\n   \n   * When there is only one input node, only one value needs to be configured,\n     indicating that all channels are divided by this coefficient.\n   \n   * When there are multiple nodes, provide values consistent with the number of\n     channels (these values are separated by spaces), indicating that each\n     channel is divided by a different coefficient.\n\nAttention\n\nIf there exists a node that does not require std processing, configure 'None'\nfor that node.\n\n\nFormulas and Samples#\n\nThe following combines the data normalization formulas for model training to\nprovide you with an understanding of the process:\n\nThe mean and scale parameters in the YAML file need to be converted with the\nmean and std during training.\n\nThe calculation of the data normalization operation in the preprocess node is:\n$norm_data = ( data - mean ) * scale$.\n\nTaking YOLOv3 as an example, its preprocessing code during the training is as\nfollows:\n\n\n\nThen the calculation formula is $norm_data= (\\frac-mean) * \\frac$.\n\nRewrite it to the computation for preprocessing nodes is: $norm_data=\n(\\frac-mean) * \\frac =(data-255*mean) * \\frac$.\n\nThen $mean_yaml = 255*mean, scale_yaml= \\frac$.","routePath":"/en/guide/ptq/ptq_appendix/normalize_introduction","lang":"en","toc":[{"text":"Parameters","id":"parameters","depth":2,"charIndex":3},{"text":"Formulas and Samples","id":"formulas-and-samples","depth":2,"charIndex":1883}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":117,"title":"Image Processing Transformer","content":"#\n\nThis section will explain the concepts and parameters of each transformer you\nuse when scaling and cropping images, and provide you with reference to the use\nof samples to make it easier for you to perform transformer operations.\n\nBefore reading the contents of the document, please note the following:\n\nAttention\n\nThe image data is three-dimensional data, but the default first dimension of the\ntransformer provided by Horizon is N-dimensional, and it will process the data\nwith the first dimension split cycle, so if you need to do processing on the\nimage, please provide four-dimensional data.\n\n\nAddTransformer#\n\nDescription:\n\nAdds values to all the pixel values in the input image. The transformer converts\nthe data format to float32 in the output.\n\nParameters:\n\n * value: Value to be added to each pixel. Note that value can be negative,\n   e.g., -128.\n\nExamples of use:\n\n\n\n\nMeanTransformer#\n\nDescription:\n\nsubtracts the mean_value from all pixel values in the input image.\n\nParameters:\n\n * means: Added value of each pixel. Note that the value can be a negative\n   number, e.g., -128.\n * data_format: Layout type of the input. Value range: [\"CHW\", \"HWC\"], defaults\n   to \"CHW\".\n\nExamples of use:\n\n\n\n\nScaleTransformer#\n\nDescription:\n\nMultiplies all pixel values in the input image by the data_scale factor.\n\nParameters:\n\n * scale_value: Factor to be multiplied, such as 0.0078125 or 1/128.\n\nExamples of use:\n\n\n\n\nNormalizeTransformer#\n\nDescription:\n\nNormalizes the input image and converts the data format to float32 in the\noutput.\n\nParameters:\n\n * std: Value by which the first input image needs to be divided.\n\nExamples of use:\n\n\n\n\nTransposeTransformer#\n\nDescription:\n\nOperation used to perform the layout conversion.\n\nParameters:\n\n * order: Order of the input image after the layout conversion (the order is\n   related to the original layout order), e.g., suppose HWC order is (0,1,2),\n   when converted to CHW, the order is (2,0,1).\n\nExamples of use:\n\n\n\n\nHWC2CHWTransformer#\n\nDescription:\n\nConverts NHWC to NCHW.\n\nParameters: None.\n\nExamples of use:\n\n\n\n\nCHW2HWCTransformer#\n\nDescription:\n\nConverts NCHW to NHWC.\n\nParameters: None.\n\nExamples of use:\n\n\n\n\nCenterCropTransformer#\n\nDescription:\n\nCuts out a square image from the center of the image by directly truncating the\nvalue. This transformer will convert the data format to float32 in the output.\nWhen the value of data_type is uint8, the output is uint8.\n\nParameters:\n\n * crop_size: Size of the sides of the square cropped from the center.\n * data_type: Type of the output result. Value range: [\"float\", \"uint8\"].\n\nExamples of use:\n\n\n\n\nPILCenterCropTransformer#\n\nDescription:\n\nCrops a square image from the center of the image by using the PIL method. This\ntransformer will convert the data format to float32 in the output.\n\nParameters:\n\n * size: Size of the sides of the square cropped from the center.\n\nExamples of use:\n\n\n\n\nLongSideCropTransformer#\n\nDescriptions:\n\nCrops the longer side. This transformer will convert the data format to float32\nin the output.\n\nWhen width > height, crops a square based on the height, e.g., suppose width=100\nand height=70, then the size after cropping is 70*70.\n\nWhen height > width, crops a rectangle whose width remains the same and height\nis (height-weight)/2 + width, e.g., suppose width=70 and height=100, the size\nafter cropping is 70*(100-70) /2+70 , which is a rectangle of size 70*85.\n\nParameters: None.\n\nExamples of use:\n\n\n\n\nPadResizeTransformer#\n\nDescription:\n\nEnlarges the image by using the padding method. The transformer converts the\ndata format to float32 in the output.\n\nParameters:\n\n * target_size: Target size. The value is a tuple, e.g., (240,240).\n * pad_value: Value to be padded into the array, defaults to 127.\n * pad_position: Padding position. Values range: [\"boundary\",\"bottom_right\"],\n   defaults to \"boundary\".\n\nExamples of use:\n\n\n\n\nResizeTransformer#\n\nDescription:\n\nResizes the image.\n\nParameters:\n\n * target_size: Target size. The value is a tuple, e.g., (240,240): width=240,\n   height=240.\n\n * mode: Image processing mode, takes a range of values (\"skimage\", \"opencv\"),\n   defaults to \"skimage\".\n\n * method: Interpolation method, this parameters only works in the skimage mode.\n   Value range: [0,5], defaults to 1, of which :\n   \n   * 0: Nearest-neighbor;\n   * 1: Bi-linear(default);\n   * 2: Bi-quadratic;\n   * 3: Bi-cubic;\n   * 4: Bi-quartic;\n   * 5: Bi-quintic.\n\n * data_type: Output type. Value range: (uint8, float), defaults to float. When\n   set to uint8, the output type is uint8, otherwise it is float32.\n\n * interpolation: Interpolation method. This parameter only takes effect when\n   mode is opencv. Value range: (opencv's interpolation method), defaults to\n   null. Currently, interpolation only supports empty, or two interpolation\n   methods of INTER_CUBIC in OpenCV, when it is empty, INTER_LINEAR method is\n   used by default.\n   \n   The following are the interpolation methods supported in OpenCV and their\n   descriptions (interpolation methods not supported will be gradually added in\n   subsequent iterations):\n   \n   * INTER_NEAREST: Nearest Neighbor Interpolation.\n   * INTER_LINEAR: Bi-linear interpolation, used by default when the\n     interpolation is empty.\n   * INTER_CUBIC: Bi-cubic interpolation within a 4x4 pixel neighborhood.\n   * INTER_AREA: Resampling using pixel area relation. It may be the preferred\n     method for image decimation as it can provide moiré-free results. But when\n     the image is scaled, it is similar to the INTER_NEAREST method.\n   * INTER_LANCZOS4: Lanczos interpolation of 8x8 neighborhood.\n   * INTER_LINEAR_EXACT: Bit-accurate bilinear interpolation.\n   * INTER_NEAREST_EXACT: Bit-exact nearest neighbor interpolation. This will\n     produce the same results as the nearest neighbor method in PIL,\n     scikit-image, or Matlab.\n   * INTER_MAX: Mask for interpolation code.\n   * WARP_FILL_OUTLIERS: Flag, padding all target image pixels. If some of them\n     correspond to outliers in the source image, set them to zero.\n   * WARP_INVERSE_MAP: Flag, inverter.\n\nExamples of use:\n\n\n\n\nPILResizeTransformer#\n\nDescription:\n\nResizes the image by using the PIL library.\n\nParameters:\n\n * size: Target size. The value is a tuple, e.g., (240,240).\n * interpolation: Specifies the interpolation method. Value range:\n   (Image.NEAREST, Image.BILINEAR, Image.BICUBIC, Image.LANCZOS), default to\n   Image.BILINEAR.\n   * Image.NEAREST: Nearest neighbor sampling;\n   * Image.BILINEAR: Linear interpolation;\n   * Image.BICUBIC: Cubic spline interpolation;\n   * Image.LANCZOS: High quality downsampling filter.\n\nExamples of use:\n\n\n\n\nShortLongResizeTransformer#\n\nDescription:\n\nScales the input image according to the original scale, and the size of the new\nimage is related to the parameters set.\n\nPerform the operation as follows:\n\n 1. First, divide the size of short_size by the smaller value of the width and\n    height of the original image, and use this value as the scaling factor.\n 2. When the scaling factor is multiplied by the larger value of the width and\n    height of the original image and the result is greater than the value of\n    long_size, the scaling factor will be changed to long_size divided by the\n    larger value of the width and height of the original image.\n 3. Use the resize method in OpenCV to re-crop the image according to the\n    scaling factor obtained above.\n\nParameters:\n\n * short_size: Expected length of the short edge after cutting.\n * long_size: Expected length of the short edge after cutting.\n * include_im: Defaults to True. When set to True, it will return the original\n   image in addition to the processed image.\n\nExamples of use:\n\n\n\n\nPadTransformer#\n\nDescription:\n\nResizes the image by dividing the target size by the larger value of the width\nor height of the input image, and then multiplying this factor by the original\nwidth and height.\n\nThen according to the size of the new image, divide it by size_divisor and round\nit up, then multiply it by size_divisor to generate a new image with the new\nwidth and height.\n\nParameters:\n\n * size_divisor: Size divisor, defaults to 128.\n * target_size: Target size, defaults to 512.\n\nExamples of use:\n\n\n\n\nShortSideResizeTransformer#\n\nDescription:\n\nAccording to the desired length of the short side, this transformer use the\ncurrent ratio of the long and short sides, and the crop out the operation of the\nnew image size from the image center.\n\nParameters:\n\n * short_size: Expected length of short side.\n\n * data_type: Type of the output result. Value range: (\"float\", \"uint8\"),\n   defaults to \"float32\", output in float32 type, and when set to uint8, the\n   output type will be uint8.\n\n * interpolation: Interpolation method. Value range: (interpolation method used\n   in OpenCV), defaults to empty. Currently, interpolation only supports empty,\n   or two interpolation methods of INTER_CUBIC in OpenCV, when it is empty,\n   INTER_LINEAR method is used by default.\n   \n   The following are the interpolation methods supported in OpenCV and their\n   descriptions (interpolation methods not supported will be gradually added in\n   subsequent iterations).\n   \n   * INTER_NEAREST: Nearest Neighbor Interpolation.\n   * INTER_LINEAR: Bi-linear interpolation, which is used by default when the\n     interpolation is empty.\n   * INTER_CUBIC: Bi-cubic interpolation within a 4x4 pixel neighborhood.\n   * INTER_AREA: Resampling using pixel area relation. It may be the preferred\n     method for image decimation as it can provide moiré-free results. But when\n     the image is scaled, it is similar to INTER_NEAREST method.\n   * INTER_LANCZOS4: Lanczos interpolation of 8x8 neighborhood.\n   * INTER_LINEAR_EXACT: Bit-accurate bilinear interpolation.\n   * INTER_NEAREST_EXACT: Bit-exact nearest neighbor interpolation. This will\n     produce the same results as the nearest neighbor method in PIL,\n     scikit-image, or Matlab.\n   * INTER_MAX: Mask for interpolation code.\n   * WARP_FILL_OUTLIERS: Flag, padding all target image pixels. If some of them\n     correspond to outliers in the source image, set them to zero.\n   * WARP_INVERSE_MAP； Flag, inverter.\n\nExamples of use:\n\n\n\n\nPaddedCenterCropTransformer#\n\nDescription:\n\nCrops the center of the image with padding.\n\nAttention\n\nApplicable only to EfficientNet-lite related instance models.\n\nCalculation method:\n\n 1. Calculates the factor, int((float( image_size ) / ( image_size + crop_pad\n    )).\n 2. Calculates the size of the center, coefficient * np.minimum(height of\n    original image, width of original image)).\n 3. Crops the image from its center according to the calculated size.\n\nParameters:\n\n * image_size: Size of the image, defaults to 224.\n * crop_pad: Size of the center padding, defaults to 32.\n\nExamples of use:\n\n\n\n\nBGR2RGBTransformer#\n\nDescription:\n\nConverts the input format from BGR to RGB.\n\nParameters:\n\n * data_format: Data format. Value range: (CHW,HWC), defaults to CHW.\n\nExamples of use:\n\n\n\n\nRGB2BGRTransformer#\n\nDescription:\n\nConverts the input format from RGB to BGR.\n\nParameters:\n\n * data_format: Data format.Value range: (CHW,HWC), defaults to CHW.\n\nExamples of use:\n\n\n\n\nRGB2GRAYTransformer#\n\nDescription:\n\nConverts the input format from RGB to GRAY.\n\nParameters:\n\n * data_format: Input layout type. Value range: (\"CHW\", \"HWC\"), defaults to\n   \"CHW\".\n\nExamples of use:\n\n\n\n\nBGR2GRAYTransformer#\n\nDescription:\n\nConverts the input format from BGR to GRAY.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], defaults to\n   \"CHW\".\n\nExamples of use:\n\n\n\n\nRGB2GRAY_128Transformer#\n\nDescription:\n\nConverts the input format from RGB to GRAY_128. Value range of GRAY_128:\n(-128,127).\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nRGB2YUV444Transformer#\n\nDescription:\n\nConverts the input format from RGB to YUV444.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nBGR2YUV444Transformer#\n\nDescription:\n\nConvers the input format from BGR to YUV444.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nBGR2YUV444_128Transformer#\n\nDescription:\n\nConverts the input format from BGR to YUV444_128. Values range of YUV444_128:\n(-128,127).\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nRGB2YUV444_128Transformer#\n\nDescription:\n\nConverts the input format from RGB to YUV444_128. Values range of YUV444_128:\n(-128,127).\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nBGR2YUVBT601VIDEOTransformer#\n\nDescription:\n\nConverts the input format from BGR to YUV_BT601_Video_Range.\n\nYUV_BT601_Video_Range, some camera input data are YUV BT601 (Video Range)\nformat. Value range: 16~235, this transformer is adapted to this format of data\ngenerated.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\",\"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nRGB2YUVBT601VIDEOTransformer#\n\nDescription:\n\nConverts the input format from RGB to YUV_BT601_Video_Range.\n\nYUV_BT601_Video_Range, some camera input data are YUV BT601 (Video Range)\nformat. Value range: 16~235, this transformer is adapted to this format of data\ngenerated.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\",\"HWC\"], required field,\n   defaults to \"CHW\".\n\nExamples of use:\n\n\n\n\nYUVTransformer#\n\nDescription:\n\nConverts the input format to YUV444.\n\nParameters:\n\n * color_sequence: Color sequence, required field.\n\nExamples of use:\n\n\n\n\nReduceChannelTransformer#\n\nDescription:\n\nReduces the C channel to a single channel. The transformer is mainly for C\nchannel, such as shape 1*3*224*224 to 1*1*224*224. In practice, the layout must\nbe aligned with data_format value to avoid causing the wrong channel deletion.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], defaults to\n   \"CHW\".\n\nExamples of use:\n\n\n\n\nBGR2NV12Transformer#\n\nDescription:\n\nConverts the input format from BGR to NV12.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\",\"HWC\"], defaults to\n   \"CHW\".\n * cvt_mode: cvt mode. Value range: (rgb_calc, OpenCV), defaults to rgb_calc.\n   * rgb_calc: Image processing using mergeUV.\n   * opencv: Image processing using OpenCV.\n\nExamples of use:\n\n\n\n\nRGB2NV12Transformer#\n\nDescription:\n\nConverts the input format from RGB to NV12.\n\nParameters:\n\n * data_format: Input layout type. Value range: [\"CHW\", \"HWC\"], defaults to\n   \"CHW\".\n * cvt_mode: cvt mode. Value range: (rgb_calc,opencv), defaults to rgb_calc.\n   * rgb_calc: Image processing using mergeUV.\n   * opencv: Image processing using opencv.\n\nExamples of use:\n\n\n\n\nNV12ToYUV444Transformer#\n\nDescription:\n\nConverts the input format from NV12 to YUV444.\n\nParameters:\n\n * target_size: Target size. Value is a tuple, e.g., (240,240).\n * yuv444_output_layout: YUV444 output layout. Value range: (HWC,CHW), defaults\n   to \"HWC\".\n\nExamples of use:\n\n\n\n\nWarpAffineTransformer#\n\nDescription:\n\nPerforms image affine transformations.\n\nParameters:\n\n * input_shape: Input shape value.\n * scale: Factor to be multiplied.\n\nExamples of use:\n\n\n\n\nF32ToS8Transformer#\n\nDescription:\n\nConvers the input format from float32 to int8.\n\nParameters: None.\n\nExamples of use:\n\n\n\n\nF32ToU8Transformer#\n\nDescription:\n\nConverts the input format from float32 to uint8.\n\nParameters: None.\n\nExamples of use:\n\n","routePath":"/en/guide/ptq/ptq_appendix/transformer","lang":"en","toc":[{"text":"AddTransformer","id":"addtransformer","depth":2,"charIndex":601},{"text":"MeanTransformer","id":"meantransformer","depth":2,"charIndex":882},{"text":"ScaleTransformer","id":"scaletransformer","depth":2,"charIndex":1208},{"text":"NormalizeTransformer","id":"normalizetransformer","depth":2,"charIndex":1419},{"text":"TransposeTransformer","id":"transposetransformer","depth":2,"charIndex":1640},{"text":"HWC2CHWTransformer","id":"hwc2chwtransformer","depth":2,"charIndex":1965},{"text":"CHW2HWCTransformer","id":"chw2hwctransformer","depth":2,"charIndex":2064},{"text":"CenterCropTransformer","id":"centercroptransformer","depth":2,"charIndex":2163},{"text":"PILCenterCropTransformer","id":"pilcentercroptransformer","depth":2,"charIndex":2600},{"text":"LongSideCropTransformer","id":"longsidecroptransformer","depth":2,"charIndex":2890},{"text":"PadResizeTransformer","id":"padresizetransformer","depth":2,"charIndex":3435},{"text":"ResizeTransformer","id":"resizetransformer","depth":2,"charIndex":3862},{"text":"PILResizeTransformer","id":"pilresizetransformer","depth":2,"charIndex":6077},{"text":"ShortLongResizeTransformer","id":"shortlongresizetransformer","depth":2,"charIndex":6610},{"text":"PadTransformer","id":"padtransformer","depth":2,"charIndex":7658},{"text":"ShortSideResizeTransformer","id":"shortsideresizetransformer","depth":2,"charIndex":8172},{"text":"PaddedCenterCropTransformer","id":"paddedcentercroptransformer","depth":2,"charIndex":10137},{"text":"BGR2RGBTransformer","id":"bgr2rgbtransformer","depth":2,"charIndex":10742},{"text":"RGB2BGRTransformer","id":"rgb2bgrtransformer","depth":2,"charIndex":10926},{"text":"RGB2GRAYTransformer","id":"rgb2graytransformer","depth":2,"charIndex":11109},{"text":"BGR2GRAYTransformer","id":"bgr2graytransformer","depth":2,"charIndex":11311},{"text":"RGB2GRAY_128Transformer","id":"rgb2gray_128transformer","depth":2,"charIndex":11513},{"text":"RGB2YUV444Transformer","id":"rgb2yuv444transformer","depth":2,"charIndex":11776},{"text":"BGR2YUV444Transformer","id":"bgr2yuv444transformer","depth":2,"charIndex":11998},{"text":"BGR2YUV444_128Transformer","id":"bgr2yuv444_128transformer","depth":2,"charIndex":12219},{"text":"RGB2YUV444_128Transformer","id":"rgb2yuv444_128transformer","depth":2,"charIndex":12489},{"text":"BGR2YUVBT601VIDEOTransformer","id":"bgr2yuvbt601videotransformer","depth":2,"charIndex":12759},{"text":"RGB2YUVBT601VIDEOTransformer","id":"rgb2yuvbt601videotransformer","depth":2,"charIndex":13168},{"text":"YUVTransformer","id":"yuvtransformer","depth":2,"charIndex":13577},{"text":"ReduceChannelTransformer","id":"reducechanneltransformer","depth":2,"charIndex":13732},{"text":"BGR2NV12Transformer","id":"bgr2nv12transformer","depth":2,"charIndex":14129},{"text":"RGB2NV12Transformer","id":"rgb2nv12transformer","depth":2,"charIndex":14499},{"text":"NV12ToYUV444Transformer","id":"nv12toyuv444transformer","depth":2,"charIndex":14869},{"text":"WarpAffineTransformer","id":"warpaffinetransformer","depth":2,"charIndex":15149},{"text":"F32ToS8Transformer","id":"f32tos8transformer","depth":2,"charIndex":15332},{"text":"F32ToU8Transformer","id":"f32tou8transformer","depth":2,"charIndex":15455}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":118,"title":"Post-training Quantization (PTQ) FAQ","content":"#\n\n\nHow to understand the two forms of BPU acceleration and CPU computation?#\n\n * BPU acceleration: It means that the operator can be quantized and accelerated\n   by BPU hardware when the model is inferred at the board side, where most of\n   the operators ( such as conv, and so on) are directly supported by the\n   hardware. Some will be replaced with other operators to achieve acceleration\n   (e.g., gemm will be replaced with conv); and others depend on specific\n   contexts (e.g., Reshape, Transpose need to be BPU operators before and after)\n   to be quantized passively.\n * CPU computation: For operators in the model that cannot be directly or\n   indirectly accelerated by the BPU hardware, our toolchain will put them on\n   the CPU for computation, and the runtime prediction library will also\n   automatically complete the heterogeneous scheduling of the execution hardware\n   during model inference.\n\n\nHow does model segmentation affect the performance?#\n\nWhen the model has CPU operators that cannot be accelerated in the middle of BPU\noperators, some performance loss will be introduced when switching computation\nbetween BPU and CPU operators, thus introducing a certain performance loss in\ntwo ways.\n\n * CPU operator performance is much lower than that of the BPU operator.\n * The heterogeneous scheduling between CPU and BPU also introduces quantization\n   and dequantization operators (running on CPU), and because the internal\n   computation needs to traverse the data, its time consumption will be\n   proportional to the shape size.\n\nThe above CPU operator and quantization and dequantization operators can be\nmeasured by passing profile_path parameter through the board-side tool\nhrt_model_exec. Horizon Robotics recommends that you try to build the model with\nBPU operator to get better performance.\n\n\nWhy some OPs supported by BPU at the tail part of the model running on CPU?#\n\nFirst, we need to understand the following two concepts:\n\n * Currently supports only the Conv operator at the tail of the model to be\n   output with int32 high precision, while all other operators can only be\n   output with int8 low accuracy.\n * Normally, the model conversion will fuse Conv with its subsequent BN and\n   ReLU/ReLU6 in the optimization stage for calculation. However, due to the\n   limitation of the BPU hardware itself, Conv, which is output with int32 high\n   precision at the end of the model, does not support operator fusion.\n\nTherefore, if the model is ended with Conv+ReLU/ReLU6, then to ensure the\noverall accuracy of the quantized model, the Conv will by default use int32\noutput, while the ReLU/ReLU6 will run on the CPU. Similarly, the other tail-part\nOPs who run on the CPU are also because that the Conv OP needs higher-accuracy\noutput. However, Horizon supports running these operators on the BPU by\nconfiguring quant_config in the yaml file to get better performance, but\nintroduces some loss of accuracy.\n\n\nHow to understand the compiler optimization level parameters in yaml files?#\n\nIn the yaml configuration file of the model conversion, the compilation\nparameter group provides the optimize_level parameter to select the optimization\nlevel of the model compilation, the available range is O0~O2. Among which:\n\n * O0 without any optimization, the fastest compilation speed, suitable for use\n   in model conversion function verification, debugging different calibration\n   methods.\n * O1 - O2 as the optimization level gets higher, the search space during\n   compilation and optimization gets larger.\n * The compiler's optimization strategy is not at the operator granularity\n   level, but is a global optimization for the whole model.\n\n\nHow to compile to get a multi-batch model?#\n\nAccording to the original model type, we will discuss this issue by dividing it\ninto dynamic input models and non-dynamic input models.\n\nNote\n * The input_batch parameter can only be used only for the model which first\n   dimension of the input_shape is 1 (if the model has multiple inputs, the\n   first dimension of the input_shape needs to be 1 for all inputs), and only\n   effective when the original onnx model itself supports multi-batch inference.\n   This parameter only supports specifying a single value, which will act on all\n   inputs of the model when the model has multiple inputs.\n * The size of each calibration data shape should be the same as the size of\n   input_shape.\n\nDynamic input model: If the original model is a dynamic input model, for\nexample, ? x3x224x224 (dynamic input models must use the input_shape parameter\nto specify the model input information).\n\n 1. When input_shape is configured as 1x3x224x224, if you want to compile the\n    model to get a multi-batch model, you can use input_batch parameter, then\n    each calibration data shape size is 1x3x224x224.\n 2. When the first dimension of input_shape is an integer greater than 1, the\n    original model itself will be recognized as a multi-batch model, and the\n    input_batch parameter can not be used, and you need to pay attention to the\n    size of each calibration data shape. For example, if the input_shape is\n    4x3x224x224, the size of each calibration data shape need to be 4x3x224x224.\n\nNon-dynamic input model.\n\n 1. When the input shape[0] of the input is 1, you can use the input_batch\n    parameter. Each calibration data shape size is the same as the original\n    model shape.\n\n 2. When the input shape[0] is not 1, the input_batch parameter is not\n    supported.\n\n\nIs it normal for the order of model inputs to change during the conversion of a\nmulti-input model?#\n\nThis is normal. It is possible for the model input order to change during the\nconversion of a multi-input model. The possible cases are shown in the following\nexample.\n\n * original floating-point model input order: input1, input2, input3.\n * original.onnx model input order: input1, input2, input3.\n * quanti.bc model input order: input2, input1, input3.\n * hbm model input order: input3, input2, input1.\n\nAttention\n * When you do accuracy consistency alignment, please make sure the input order\n   is correct, otherwise it may affect the accuracy result.\n * If you want to check the hbm model input order, you can use the hb_model_info\n   command to check it. The input order listed in the input_parameters info\n   group is the hbm model input order.\n\n\nHow to prepare a floating-point model for Horizon support conversion?#\n\nThe Horizon PTQ floating-point conversion toolchain supports both Caffe1.0 and\nONNX (10 ≤ opset_version ≤ 19 and ir_version ≤ 9) model formats, and the ONNX\nexport method is as follows:\n\nTRAINING FRAMEWORK   ONNX EXPORT METHODS\nCaffe1.0             Horizon native support, no need to export ONNX\nOthere frameworks    ONNX Tutorials\n\n\nHow to verify the correctness of the original floating-point ONNX model?#\n\nThe toolchain uses an interface based on the public ONNXRuntime wrapper to\nimplement model parsing and forwarding when converting models. So we should\ncheck the legitimacy of the original floating-point ONNX model itself (i.e.,\nwhether it is capable of inference properly) and whether accuracy bias was\nintroduced during the export of ONNX from the training framework before using\nthe toolchain. Specific tests can be found in the The HBRuntime Inference\nLibrary section.\n\n\nHow to convert the model of fp16?#\n\nThe toolchain does not currently support direct conversion of fp16 models, but\nyou can convert them to fp32 models first, and then use the Horizon toolchain\nfor quantization conversion.\n\n\nHow to modify the input layout of the model?#\n\nJ6 PTQ tool no longer supports to modify the input layout format of the model\nduring the conversion process, PTQ will keep your model layout unchanged during\nthe conversion process. If you need to modify the layout, you can directly\nmodify the model before exporting onnx to finish the layout change.\n\nIf modifications are required at the model level, it is recommended that they be\nimplemented in the following way:\n\n 1. Add transpose to the DL framework so that its input layout is NHWC and\n    re-export the ONNX model;\n 2. Use the onnx library to modify the model directly, as referenced below:\n\n\n\n\nHow to understand shape_inference_fail.onnx?#\n\nThis model will only be output when the PTQ model conversion fails, it has no\nspecial meaning, you can provide this model and the log file of the model\nconversion to Horizon for debug analysis.\n\n\nHow to understand that the model input and output shape in log is 0?#\n\nThe model transformation log prints the name and shape of the input and output\nnodes of the model, but sometimes the shape is 0, as shown in the following\nfigure. This occurs mainly in the batch dimension, because the output shape of\nsome models are dynamic (stored using dim_param), and converting the tool\nforward inference model (shape_inference) uses a '?' to take up space, and the\nlog print will show 0.\n\nThis is as expected and does not affect the model transformation and does not\nneed to be of undue concern.\n\n\nHow to understand that the operator changes in the optimized model?#\n\nIn order to improve the performance of the model on the board, the model\ntransformation has an Optimizer module which performs some graphs optimization\non the model to improve the performance of the model on the board. The Optimizer\nmodule mainly includes functions such as Constant Folding, Operator Replacement,\nOperator Property/Input Update, Operator Fusion, and Operator Move.\n\nWarning\n\nThe Optimizer module causes a change in the operator of the your model, and this\nmodule performs some equivalent graph optimization of the model.\n\n\nIn the conversion process, what are the several scenarios in which the\nCalibrated Cosine column appears as nan in the print log of the hb_compile tool\nand what causes such problems?#\n\n 1. Case 1: The second input to the where operator in the model is -Inf\n    \n    During the optimization phase of the transformation process, in order to be\n    able to ensure that the where operator can be quantized, this operator will\n    be split into a combination of several quantizable operators during the\n    optimization phase. When the second input of where is -Inf, there will be a\n    case of nan in the transformation process. The reason is as follows:\n\n 2. Case 2: All 0 tensor in the model\n    \n    1). concat result in all 0 tensor.\n    \n    \n    \n    Taking the above structure as an example, the two inputs to concat are all 0\n    constants, so it is possible that the subsequent output after slice may have\n    a branch in which the data flowing are all 0, and lead to illegal thresholds\n    (i.e., thresholds of 0) in HzCalibration.\n    \n    2). equal result in all 0 tensor.\n    \n    Taking the above structure as an example, the input onnx::Not_3 to the model\n    is a bool type tensor, which after cast+equal results in all false data in\n    the tensor, and after cast(bool -> float32) results in an all 0 tensor.\n\nWarning\n\nThis case requires the customer to try to test with multiple sets of data, thus\nruling out the possibility that the model has all 0 caused by the calibration\ndata.\n\n\nIs there hardware support for model input color space conversion?#\n\nBPU supports conversion between common model input formats (e.g. nv12 ->\nrgb/bgr) and data normalization, which can be configured via yaml file. See the\ndescription of input_type_train, input_type_rt, mean_value, scale_value and\nstd_value parameters.\n\n\nHow to understand compiler optimization levels?#\n\nThe yaml file optimize_level parameter configures the compiler optimization\nlevel from O0 to O2, and the higher the optimization level, the larger the\nsearch space is available and usually more time consuming. The optimization\nlevel does not perform some defined optimization strategy at the operator\ngranularity level, and most of the operator optimizations are independent of the\noptimization level (they are not time-consuming). The optimization level is\nmainly for global optimization, which is the analysis and optimization of the\nwhole model.\n\n\nHow to handle quantization/dequantization operators in the first and last parts\nof the model?#\n\nBy default, PTQ toolchain inserts quantization operator at the beginning of\nfeaturemap input model to implement the mapping of input data from float32 to\nint8, and inserts dequantization operator at the end of all models to implement\nthe mapping of output data from int8 (or int32 by default if BPU ends with conv)\nto float32. The quantization/dequantization operators are not efficient on the\nCPU, especially when the data shape is large.\n\nTherefore, we prefer to integrate quantization/dequantization operations into\npre- and post-processing, which is the most efficient way.\n\n\nHow to use unitconv operator to optimize model performance?#\n\nFor an operator in the model to run on BPU, besides satisfying the BPU support\nconditions, it also needs to be able to find its quantization threshold on\ncalibration. The quantization thresholds of some non-computation intensive\noperators (such as concat, reshape, etc.) depend on the featuremap Tensor of the\nupstream and downstream operators. Therefore, if these operators are at the\nbeginning and end of the model they will run on the CPU by default.\n\nAt this point, for more efficient performance, unitconv can be inserted\nbefore/after this operator to introduce a new quantization threshold statistic,\nwhich in turn can be quantized on the BPU.\n\nHowever, it should be noted that this approach may introduce some quantization\nloss. Take the model output by conv+reshape+concat structure as an example, by\ndefault, the toolchain will output conv with high accuracy of int32, and then\nsend it to reshape and concat on CPU after dequantization to float32. If\nunitconv is inserted after concat, the whole structure will run on BPU with low\naccuracy of int8. At this point, although the final unitconv can still be output\nwith high accuracy of int32, the accuracy compression of the previous conv\noutput has already introduced a certain amount of quantization loss. Therefore,\nplease consider whether to insert unitconv to optimize the performance.\n\n\nWhether int16/int32 calculations are supported?#\n\nMost of the operators use int8 calculation by default, and some of them support\nint16 and fp16 calculation, and the range of operator support continues to\nexpand, see Toolchain Operator Support Constraints List - ONNX Operator Support\nList, in addition:\n\n 1. If the BPU part of the model ends in Conv, the operator defaults to int32\n    high accuracy output;\n 2. The DSP hardware is also int8/int16/float32 capable.\n\n\nHow to handle the calibration data of the model correctly?#\n\nFor the preparation of PTQ model calibration data, refer to the Prepare\nCalibration Data section. Also, for featuremap input models, please do your own\npreprocessing of the data and save it as an npy file of the desired input type\nvia the numpy.save interface.\n\n\nHow to dump model middle tier output?#\n\nDuring the model conversion phase, if debug_mode parameter in the yaml file is\nconfigured with dump_all_layers_output, adds a dequantization output node for\neach convolution and matmul operator, which significantly degrades the\nperformance of the model once it is on the board.\n\nAmong them, the output_nodes parameter can specify any node in the model as an\noutput node, which is more convenient for us to debug and tune. In addition, the\nhb_verifier tool can be used to compare the consistency of the fixed-point model\nquantized_model.bc with the hbm model on the upper board.\n\nDuring the board deployment phase, the hrt_model_exec tool also supports saving\nnode outputs (including nodes specified with the output_nodes parameter) in bin\nor txt format, as described in The hrt_model_exec Tool Introduction 。","routePath":"/en/guide/ptq/ptq_faq_troubleshooting/ptq_faq","lang":"en","toc":[{"text":"How to understand the two forms of BPU acceleration and CPU computation?","id":"how-to-understand-the-two-forms-of-bpu-acceleration-and-cpu-computation","depth":2,"charIndex":3},{"text":"How does model segmentation affect the performance?","id":"how-does-model-segmentation-affect-the-performance","depth":2,"charIndex":912},{"text":"Why some OPs supported by BPU at the tail part of the model running on CPU?","id":"why-some-ops-supported-by-bpu-at-the-tail-part-of-the-model-running-on-cpu","depth":2,"charIndex":1822},{"text":"How to understand the compiler optimization level parameters in yaml files?","id":"how-to-understand-the-compiler-optimization-level-parameters-in-yaml-files","depth":2,"charIndex":2940},{"text":"How to compile to get a multi-batch model?","id":"how-to-compile-to-get-a-multi-batch-model","depth":2,"charIndex":3673},{"text":"Is it normal for the order of model inputs to change during the conversion of a multi-input model?","id":"is-it-normal-for-the-order-of-model-inputs-to-change-during-the-conversion-of-a-multi-input-model","depth":2,"charIndex":-1},{"text":"How to prepare a floating-point model for Horizon support conversion?","id":"how-to-prepare-a-floating-point-model-for-horizon-support-conversion","depth":2,"charIndex":6340},{"text":"How to verify the correctness of the original floating-point ONNX model?","id":"how-to-verify-the-correctness-of-the-original-floating-point-onnx-model","depth":2,"charIndex":6746},{"text":"How to convert the model of fp16?","id":"how-to-convert-the-model-of-fp16","depth":2,"charIndex":7295},{"text":"How to modify the input layout of the model?","id":"how-to-modify-the-input-layout-of-the-model","depth":2,"charIndex":7519},{"text":"How to understand shape_inference_fail.onnx?","id":"how-to-understand-shape_inference_failonnx","depth":2,"charIndex":8169},{"text":"How to understand that the model input and output shape in log is 0?","id":"how-to-understand-that-the-model-input-and-output-shape-in-log-is-0","depth":2,"charIndex":8412},{"text":"How to understand that the operator changes in the optimized model?","id":"how-to-understand-that-the-operator-changes-in-the-optimized-model","depth":2,"charIndex":9003},{"text":"In the conversion process, what are the several scenarios in which the Calibrated Cosine column appears as nan in the print log of the hb_compile tool and what causes such problems?","id":"in-the-conversion-process-what-are-the-several-scenarios-in-which-the-calibrated-cosine-column-appears-as-nan-in-the-print-log-of-the-hb_compile-tool-and-what-causes-such-problems","depth":2,"charIndex":-1},{"text":"Is there hardware support for model input color space conversion?","id":"is-there-hardware-support-for-model-input-color-space-conversion","depth":2,"charIndex":11110},{"text":"How to understand compiler optimization levels?","id":"how-to-understand-compiler-optimization-levels","depth":2,"charIndex":11431},{"text":"How to handle quantization/dequantization operators in the first and last parts of the model?","id":"how-to-handle-quantizationdequantization-operators-in-the-first-and-last-parts-of-the-model","depth":2,"charIndex":-1},{"text":"How to use unitconv operator to optimize model performance?","id":"how-to-use-unitconv-operator-to-optimize-model-performance","depth":2,"charIndex":12708},{"text":"Whether int16/int32 calculations are supported?","id":"whether-int16int32-calculations-are-supported","depth":2,"charIndex":14120},{"text":"How to handle the calibration data of the model correctly?","id":"how-to-handle-the-calibration-data-of-the-model-correctly","depth":2,"charIndex":14588},{"text":"How to dump model middle tier output?","id":"how-to-dump-model-middle-tier-output","depth":2,"charIndex":14912}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":119,"title":"Common Failure Resolutions","content":"#\n\n\nSegmentation fault：core dumped#\n\nThis error may be caused by insufficient memory on the physical machine, an old\ntool version, an illegal model, or a tool bug. Please troubleshoot it in the\nfollowing ways:\n\n 1. Verify that there are no other tasks on the current physical machine that\n    are taking up memory (or switch to a development machine with more memory);\n 2. Upgrade the toolchain version to the latest and reconvert the model;\n 3. Verify that the original model itself is legal, e.g. that the onnx model\n    runs correctly in the onnxruntime;\n 4. If the above checks do not resolve the issue, please contact the Horizon\n    team.\n\n\nERROR Wrong mean_value num received#\n\n\n\nThis error is caused by the number of mean_value parameter in the yaml file does\nnot match the number of model input nodes, please check accordingly. The\nmean_value of different input nodes need to be separated by \";\" signs; If the\nmodel is a hybrid multi-input model (i.e. contains both image and featuremap\ninput nodes), the mean_value parameter of the featuremap node needs to be given\na placeholder with \"NA\" corresponding to input_name.\n\n\nERROR The input model is invalid#\n\nThis error is caused by the model itself being illegal, i.e. it does not satisfy\nthe checking logic of the public onnxruntime, so please check the model by\nyourself.","routePath":"/en/guide/ptq/ptq_faq_troubleshooting/troubleshooting","lang":"en","toc":[{"text":"Segmentation fault：core dumped","id":"segmentation-faultcore-dumped","depth":2,"charIndex":3},{"text":"ERROR Wrong mean_value num received","id":"error-wrong-mean_value-num-received","depth":2,"charIndex":646},{"text":"ERROR The input model is invalid","id":"error-the-input-model-is-invalid","depth":2,"charIndex":1130}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":120,"title":"Common Algorithm Model Samples","content":"#\n\n\nSample Location#\n\nThe common algorithmic model samples are located at 02_preq_examples/,\n03_classification/, 04_detection/ and 07_segmentation/ folders in the\nhorizon_model_convert_sample path.\n\n\nPrepare Datasets #\n\nDataset Download Address#\n\nThe dataset can be downloaded from the following address.\n\nDATASET      ADDRESS\nImageNet     https://www.image-net.org/download.php\nCOCO         https://cocodataset.org/\nVOC          http://host.robots.ox.ac.uk/pascal/VOC/ (need to download\n             both versions 2007 and 2012)\nCityscapes   https://github.com/mcordts/cityscapesScripts\n\nDataset Reference Structure #\n\nTo facilitate your subsequent steps, after the dataset has been downloaded, you\nneed to process the evaluation dataset according to the structure suggested by\nHorizon below.\n\n\nPrepare Models #\n\nWhen using the model conversion sample package, please prepare the corresponding\nfloating-point model first.\n\nNote\n\nIf you need to do this process in the sample folder, you need to execute the\n00_init.sh script in the folder first to get the corresponding original model\nand dataset.\n\nSources and modifications (if any) of the original models, please refer to below\nsubsections.\n\n\nMobileNetv1#\n\n 1. model source: https://github.com/shicai/MobileNet-Caffe\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    3fd6889ec48bda46451d67274144e2a8   mobilenet.caffemodel\n    8922f90f629d428fecf866e798ac7c08   mobilenet_deploy.prototxt\n\n\nMobileNetv2#\n\n 1. model source: https://github.com/shicai/MobileNet-Caffe\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    54aab8425ea068d472e8e4015f22360c   mobilenet_v2.caffemodel\n    13101ee86ab6d217d5fd6ed46f7a4faa   mobilenet_v2_deploy.prototxt\n\n\nResNet50#\n\n 1. model source:\n    https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50\n    .html\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    ebaeb70c972d1a3e6eb06c0e1541eeb4   resnet50.onnx\n\n\nGoogleNet#\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/GoogleNet\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    f107ae6806ea1016afbc718210b7a617   googlenet.onnx\n\n\nEfficientNet_Lite0/1/2/3/4#\n\nAttention\n\nTo quickly start running samples and avoid the risks caused by third party\ntools, we strongly recommend using the off-the-shelf ONNX model in the\nmodel_zoo/mapper/ directory in Horizon model release package. If you are\ninterested in reproducing the model conversion process of tflite2onnx, you can\nstill try using the below 3rd-party tool, however, we do not guarantee the\nquality and success of the conversion.\n\n 1. model source: obtain the TAR package from\n    https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/l\n    ite\n\n 2. md5sum of the converted ONNX models in Horizon model_zoo:\n    \n    MD5SUM                             FILE\n    001a329bd367fbec22b415c7a33d7bdb   efficientnet_lite0_fp32.onnx\n    1205e95aea66650c71292bde236d55a9   efficientnet_lite1_fp32.onnx\n    474741c15494b79a89fe51d89e0c43c7   efficientnet_lite2_fp32.onnx\n    550455b41848d333f8359279c89a6bae   efficientnet_lite3_fp32.onnx\n    bde7fe57eadb4a30ef76f68da622dcd5   efficientnet_lite4_fp32.onnx\n\n 3. Download and get .tflite from the TAR package, and then convert it to an\n    ONNX model using the tflite2onnx tool\n    (https://pypi.org/project/tflite2onnx/).\n\nNote that model layouts may vary by tflite2onnx version. If the input layout of\nthe converted ONNX model is NHWC, when building, the configure\ninput_layout_train of the EfficientNet_Lite0/1/2/3/4 should be NHWC.\n\n\nVargconvnet#\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/VargConvNet\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    e21b8db17916f9046253bbe0bb8de3ef   vargconvnet.onnx\n\n\nEfficientnasnet_m#\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientna\n    sNet\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    fc36c052c6f034c0b64a6197b91b0c62   efficientnasnet-m.onnx\n\n\nEfficientnasnet_s#\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Efficientna\n    sNet\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    e2744bd748f4265f4488676835a6ca24   efficientnasnet-s.onnx\n\n\nResNet18#\n\n 1. model source:\n    https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18\n    .html\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    62de4ff68317c65ab4bb6a451e719e6d   resnet18.onnx\n\n\nYOLOv2_Darknet19#\n\nAttention\n * To quickly start running samples and avoid the risks caused by third party\n   tools, we strongly recommend using the off-the-shelf Caffe model in the\n   model_zoo/mapper/ directory in Horizon model release package. If you are\n   interested in reproducing the model conversion process of darknet2caffe, you\n   can still try using the below 3rd-party tool, however, we do not guarantee\n   the quality and success of the conversion.\n * To achieve optimal on-board performance, we modified the remove_node_type\n   parameter in the YAML file used for hbm model compilation and removed the\n   Dequantize node from the hbm model.\n\n 1. To get YOLOv2_Darknet19, first download the .cfg and .weight files of YOLOv2\n    608x608 from YOLO's official website (https://pjreddie.com/darknet/yolo/)\n    and convert it to a Caffe model using the darknet2caffe conversion tool\n    (https://github.com/xingyanan/darknet2caffe).\n\nNote\n\nAs this conversion tool is a simplified version, you should modify the .prototxt\nfile generated by this tool before the conversion, change the 'Reshape' layer to\n'Passthrough' layer, and then add an NCHW2NHWC Permute operation to the output\nnode.\n\nFor details about the parameters of the modified Passthrough layer, refer to the\nyolov2.prototxt sample.\n\n2.md5sum code:\n\nMD5SUM                             FILE\n7aa7a6764401cebf58e73e72fcbd2a45   yolov2.caffemodel\n72e9a51c1e284e4b66e69f72ca9214c8   yolov2_transposed.prototxt\n\n\nYOLOv3_Darknet53#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. YOLOv3_Darknet53 model:\n\nURL: https://github.com/ChenYingpeng/caffe-yolov3/ .\n\nThe caffemodel file can be downloaded from the Baidu Cloud URL provided in the\nREADME.md file in GitHub, in which you should add an NCHW2NHWC Permute operation\nto the output node.\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    935af6e1530af5c0017b3674adce95e9   yolov3_transposed.prototxt\n    9a0f09c850656913ec27a6da06d9f9cc   yolov3.caffemodel\n\n\nYOLOv5x#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. YOLOv5x model: Download the corresponding pt file from\n    https://github.com/ultralytics/yolov5/releases/tag/v2.0\n\nImportant\n\nWhen cloning the source code, be sure to use tags, otherwise the conversion may\nfail.\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    2e296b5e31bf1e1b6b8ea4bf36153ea5   yolov5l.pt\n    16150e35f707a2f07e7528b89c032308   yolov5m.pt\n    42c681cf466c549ff5ecfe86bcc491a0   yolov5s.pt\n    069a6baa2a741dec8a2d44a9083b6d6e   yolov5x.pt\n\n * To better adapt to post-processing code, before exporting the ONNX model, we\n   modified the code at Github as follows (for more code details see:\n   https://github.com/ultralytics/yolov5/blob/v2.0/models/yolo.py):\n\n\n\nNote\n\nRemove the reshape from 4D to 5D at the end of each output branch (i.e., not\nsplitting the channel from 255 to 3x85), then convert the layout from NHWC to\nNCHW before dumping.\n\nThe bottom left image displays the visualization of a certain output node before\nmodifying the model, while the bottom right image displays the visualization of\nthe corresponding output node after the modification.\n\n * After download, perform the pt to ONNX file conversion using the script\n   https://github.com/ultralytics/yolov5/blob/v2.0/models/export.py .\n\nAttention\n\nWhen using the export.py script:\n\n 1. Because Horizon algorithm toolchain only supports ONNX opset ~ , please\n    modify the opset_version parameter in the torch.onnx.export as per the\n    version you use.\n 2. Change the default input name parameter in the torch.onnx.export from\n    'image' into 'data' to keep it consistent with that of in the YOLOv5s sample\n    in the model conversion sample package.\n 3. Change the default data input size in the parser.add_argument from 640x640\n    to 672x672 as in the YOLOv5x sample in the model conversion sample package.\n\n\nSSD_MobileNetv1#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. SSD_MobileNetv1 model: Obtain the Caffe model from URL:\n    https://github.com/chuanqi305/MobileNet-SSD\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    bbcb3b6a0afe1ec89e1288096b5b8c66   mobilenet_iter_73000.caffemodel\n    3c230e4415195a50c6248be80c49882d   MobileNetSSD_deploy.prototxt\n\n\nEfficientdetd0#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/EfficientDe\n    t\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    ec4129c4b300cd04f1e8f71e0fe54ca5   efficientdet_nhwc.onnx\n\n\nCenterNet_Resnet101#\n\nAttention\n\nTo achieve optimal on-board performance:\n\n * We put the maxpool and sigmoid nodes into the model and specify them to be\n   compiled as BPU nodes for reducing the amount of computation during\n   post-processing.\n\n * We modified the remove_node_type parameter in the YAML file used for hbm\n   model compilation and removed the Dequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Centernet\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    db195ff784792f475e573c5126401d2a   centernet_resnet101_coco_modify.onnx\n\n\nFcos_efficientnetb0#\n\nAttention\n * This model is a trained model using the PTQ method.\n * To achieve optimal on-board performance, we modified the remove_node_type\n   parameter in the YAML file used for hbm model compilation and removed the\n   Dequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/blob/master/Fcos_Effici\n    entnetb0\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    9f9a1fe8508e2bd068e70146eb559b4f   fcos_efficientnetb0.onnx\n\n\nYolov4#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/YoloV4\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    aaa3c3e5e4c4c1d4830b6501b1720e4d   yolov4_efficientnetb0.onnx\n\n\nYOLOv3_VargDarknet#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. YOLOv3_VargDarknet model\n\nURL:\nhttps://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Yolov3_VargDark\nnet . The model file can be downloaded from the Baidu Cloud URL provided in the\nREADME.md file in GitHub.\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    fd4e46bc7c9798b51778d3aa09c5053a   yolov3_vargdarknet53.onnx\n\n\nFcos_resnet50#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Fcos_Resnet\n    50\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    0218942777615fac2f54cefdac4fbfa7   fcos_resnet50.onnx\n\n\nFcos_resnext101#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/Fcos_Resnex\n    t101\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    4b80efd22448021721ac5a860909c59f   fcos_resnext101.onnx\n\n\nUnet_mobilenet#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nDequantize node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/MobilenetUn\n    et\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    21c6c645ebca92befbebc8c39d385c1e   tf_unet_trained.onnx\n\n\nDeeplabV3plus_efficientnetb0#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nReshape and Cast node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Pl\n    us\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    cf3a683f31b4b0ebe090647729f869d9   deeplabv3plus_efficientnetb0.onnx\n\n\nFastscnn_efficientnetb0#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nReshape and Cast node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/FastSCNN\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    c1ace8f08a9c7b9c91509fa68327d0c8   fastscnn_efficientnetb0.onnx\n\n\nDeeplabv3plus_dilation1248#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nTranspose node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Pl\n    us\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    ad002e572cbb49e1e99d893aac69f3e3   deeplabv3_cityscapes_dila1248_permute.onnx\n\n\nDeeplabv3plus_efficientnetm1#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nReshape, Cast, and Transpose node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Pl\n    us\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    0a1dfd01e173c68630d9e8dc1a6036fe   deeplabv3plus_efficientnetm1.onnx\n\n\nDeeplabv3plus_efficientnetm2#\n\nAttention\n\nTo achieve optimal on-board performance, we modified the remove_node_type\nparameter in the YAML file used for hbm model compilation and removed the\nReshape, Cast, and Transpose node from the hbm model.\n\n 1. model source:\n    https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master/DeeplabV3Pl\n    us\n\n 2. md5sum code:\n    \n    MD5SUM                             FILE\n    c11a2673c4b3cf6e5d7bf1a051925d38   deeplabv3plus_efficientnetm2.onnx\n\n\nDemo Of Algorithm Model Samples#\n\nTaking the Resnet50 model as an example, This section illustrates the steps of\nfloating-point to fixed-point model conversion by using the scripts in\n03_classification/03_resnet50/.\n\n\nDocker Container Prep#\n\nFirst complete the docker installation and configuration and enter the docker\ncontainer accoridng to Docker Container Deployment .\n\n\nObtain The Original Model And Calibration Dataset#\n\nExecute 00_init.sh in the 03_classification/03_resnet50/ folder to obtain the\nmodel and calibration dataset needed for the current sample.\n\n\n\n\nCheck If the Model is Executable#\n\n1.Run the script as follows:\n\n\n\n2.Output the model inspection result:\n\nUse the hb_compile tool to check whether the model can be supported by Horizon\nASIC.\n\n\nPrepare Calibration Dataset#\n\nRun the 02_preprocess.sh script in the same directory, as follows:\n\n\n\nNote\n * We extracted 100 images from the ImageNet dataset and use them as a\n   calibration dataset. Before calibration, we pre-processed the data and\n   convert short size resize/crop size/NHWC to NCHW/to rgb.\n * The hb_compile tool will load data from the converted binary data, the format\n   of the pre-processed binary data file is npy, the dtype is unit8.\n\n\nBuilding Models#\n\nRun the 03_build.sh script in the same directory, as follows:\n\n\n\nNote\n\nThe above script uses the hb_compile tool to convert the model. The most\nimportant thing to focus on is the conversion configuration file, refer to Model\nQuantized Compilation section.\n\nThe output of the above script is as follows:\n\n\n\nNote\n\nFor now you ONLY need to focus on the resnet50_224x224_nv12.hbm file.\n\n\nSingle Image Inference#\n\nRun the 04_inference.sh script to inference a single image, as follows:\n\n\n\nNote\n * As image pre-processing, model data post-processing are required in the image\n   inference, we offered a Python sample script, please refer to sh\n   04_inference.sh.\n * This script is to perform the inference against a single image and verify\n   whether the inference result meets the expectations. If you want to perform\n   accuracy evaluation, refer to the script 05_evaluate.sh.\n\n\nAccuracy Evaluation#\n\nRun the 05_evaluate.sh script to evaluate the accuracy, as follows:\n\n\n\nNote\n * As image pre-processing, model data post-processing are required in accuracy\n   evaluation, awe offered a Python sample script, please refer to sh\n   05_evaluate.sh.\n * To accelerate the evaluation, you can adjust the number of concurrent\n   processes by using the -p option; meanwhile, pay attention to the memory\n   usage. When the value of the -p option is set to 0 or left unfilled, the\n   fixed-point models in the CPU environment will be processed by 10 processes,\n   while other scenarios using 1 process.\n\n\nFAQ#\n\n\nWhy is the reproduced accuracy slightly different from the indicators in the\ndocumentation?#\n\nThere are two possible reasons:\n\n 1. There may be minor differences in calculation methods when in different\n    server environments, which can cause small data fluctuations in the accuracy\n    of the compiled fixed-point ONNX models in different server environments\n    when compared with the documentation.\n 2. The different versions of third-party libraries such as OpenCV and NumPy,\n    which may produce different results after pre-processing, and this may also\n    lead to slight data fluctuations in reproduced accuracy when compared with\n    the documentation.\n\nNo need to worry much about this situation, the records provided in the\ndocumentation is only for reference, and it is ok that your reproduced accuracy\nis slightly different from those in documents.\n\n\nWhy is the fixed-point model accuracy not aligned with the on-board accuracy of\nthe hbm file in the ai_benchmark example?#\n\nIn the standard delivery, when adding the example, we have already aligned the\nfixed-point model accuracy with the hbm on-board accuracy in the ai_benchmark\nexample.\n\nIf you find any unaligned accuracy, we recommend you first checking the model\ninput for consistency.\n\nWhen executing the fixed-point model evaluation script, you use the dataset of\nimage type, while for the hbm model used on board, you use the binary dataset\nconverted by the hb_eval_preprocess tool.\n\nBased on this, if the dataset you used on-board is not generated by using the\nabove methods, we recommend that you first use our data preprocessing tool\n(i.e., hb_eval_preprocess) to regenerate the dataset needed for on-board running\non the same server that you run the fixed-point model accuracy and rerun the\non-board accuracy to ensure the model inputs are consistent.\n\nAttention\n\nMake sure to use the same environment to generate the dataset by using the\nhb_eval_preprocess tool and to run the fixed-point model accuracy.","routePath":"/en/guide/ptq/ptq_sample/algorithm_sample","lang":"en","toc":[{"text":"Sample Location","id":"sample-location","depth":2,"charIndex":3},{"text":"Prepare Datasets","id":"prepare-datasets","depth":3,"charIndex":-1},{"text":"Dataset Download Address","id":"dataset-download-address","depth":4,"charIndex":219},{"text":"Dataset Reference Structure","id":"dataset-reference-structure","depth":4,"charIndex":-1},{"text":"Prepare Models","id":"prepare-models","depth":2,"charIndex":-1},{"text":"MobileNetv1","id":"mobilenetv1","depth":3,"charIndex":1194},{"text":"MobileNetv2","id":"mobilenetv2","depth":3,"charIndex":1462},{"text":"ResNet50","id":"resnet50","depth":3,"charIndex":1736},{"text":"GoogleNet","id":"googlenet","depth":3,"charIndex":1978},{"text":"EfficientNet_Lite0/1/2/3/4","id":"efficientnet_lite01234","depth":3,"charIndex":2210},{"text":"Vargconvnet","id":"vargconvnet","depth":3,"charIndex":3630},{"text":"Efficientnasnet_m","id":"efficientnasnet_m","depth":3,"charIndex":3868},{"text":"Efficientnasnet_s","id":"efficientnasnet_s","depth":3,"charIndex":4127},{"text":"ResNet18","id":"resnet18","depth":3,"charIndex":4386},{"text":"YOLOv2_Darknet19","id":"yolov2_darknet19","depth":3,"charIndex":4628},{"text":"YOLOv3_Darknet53","id":"yolov3_darknet53","depth":3,"charIndex":6103},{"text":"YOLOv5x","id":"yolov5x","depth":3,"charIndex":6773},{"text":"SSD_MobileNetv1","id":"ssd_mobilenetv1","depth":3,"charIndex":8807},{"text":"Efficientdetd0","id":"efficientdetd0","depth":3,"charIndex":9337},{"text":"CenterNet_Resnet101","id":"centernet_resnet101","depth":3,"charIndex":9786},{"text":"Fcos_efficientnetb0","id":"fcos_efficientnetb0","depth":3,"charIndex":10423},{"text":"Yolov4","id":"yolov4","depth":3,"charIndex":10949},{"text":"YOLOv3_VargDarknet","id":"yolov3_vargdarknet","depth":3,"charIndex":11383},{"text":"Fcos_resnet50","id":"fcos_resnet50","depth":3,"charIndex":11956},{"text":"Fcos_resnext101","id":"fcos_resnext101","depth":3,"charIndex":12401},{"text":"Unet_mobilenet","id":"unet_mobilenet","depth":3,"charIndex":12852},{"text":"DeeplabV3plus_efficientnetb0","id":"deeplabv3plus_efficientnetb0","depth":3,"charIndex":13300},{"text":"Fastscnn_efficientnetb0","id":"fastscnn_efficientnetb0","depth":3,"charIndex":13781},{"text":"Deeplabv3plus_dilation1248","id":"deeplabv3plus_dilation1248","depth":3,"charIndex":14242},{"text":"Deeplabv3plus_efficientnetm1","id":"deeplabv3plus_efficientnetm1","depth":3,"charIndex":14723},{"text":"Deeplabv3plus_efficientnetm2","id":"deeplabv3plus_efficientnetm2","depth":3,"charIndex":15216},{"text":"Demo Of Algorithm Model Samples","id":"demo-of-algorithm-model-samples","depth":2,"charIndex":15709},{"text":"Docker Container Prep","id":"docker-container-prep","depth":3,"charIndex":15927},{"text":"Obtain The Original Model And Calibration Dataset","id":"obtain-the-original-model-and-calibration-dataset","depth":3,"charIndex":16084},{"text":"Check If the Model is Executable","id":"check-if-the-model-is-executable","depth":3,"charIndex":16279},{"text":"Prepare Calibration Dataset","id":"prepare-calibration-dataset","depth":3,"charIndex":16472},{"text":"Building Models","id":"building-models","depth":3,"charIndex":16934},{"text":"Single Image Inference","id":"single-image-inference","depth":3,"charIndex":17336},{"text":"Accuracy Evaluation","id":"accuracy-evaluation","depth":3,"charIndex":17828},{"text":"FAQ","id":"faq","depth":2,"charIndex":18444},{"text":"Why is the reproduced accuracy slightly different from the indicators in the documentation?","id":"why-is-the-reproduced-accuracy-slightly-different-from-the-indicators-in-the-documentation","depth":3,"charIndex":-1},{"text":"Why is the fixed-point model accuracy not aligned with the on-board accuracy of the hbm file in the ai_benchmark example?","id":"why-is-the-fixed-point-model-accuracy-not-aligned-with-the-on-board-accuracy-of-the-hbm-file-in-the-ai_benchmark-example","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":121,"title":"Release Introduction","content":"#\n\nThe PTQ conversion sample release includes the model quantization sample\npackage(horizon_model_convert_sample) and the original models used for\ncompilation under the model_zoo/mapper.\n\n\nModel Quantization Sample Package: horizon_model_convert_sample#\n\nNote\n\nThe OE package does not contain the calibration dataset and the original model\nfor the sample by default, you need to execute 00_init.sh in the corresponding\nsample folder to get the model and calibration dataset for the current sample.\n\nAfter the original model and dataset for all samples are acquired, the contents\nof the model conversion sample package are shown below:\n\n\n\nMain contents in the sample package:\n\nCommon Algorithm Model Samples are those in the 03_classification/,\n04_detection/, and 07_segmentation/ folders.\n\nThe main goal of these algorithm model samples is to help you:\n\n * experience the model conversion process.\n * experience the accuracy evaluation of model conversion.\n * experience the model conversion results.\n\nNote\n\nSamples in this section will be updated from time to time to provide sample\nanswers to frequently asked questions.\n\n\nModel Release: model_zoo#\n\nThe model_zoo contains two paths: mapper and runtime.\n\n * The mapper path contains the models (floating point models in ONNX or Caffe\n   format) to be used by the PTQ and QAT schemes for model conversion.\n * The runtime path contains the hbm models that you will use for embedded\n   runtime development.\n\nThe next section will introduce you the algorithm model samples.","routePath":"/en/guide/ptq/ptq_sample/general_description","lang":"en","toc":[{"text":"Model Quantization Sample Package: horizon_model_convert_sample","id":"model-quantization-sample-package-horizon_model_convert_sample","depth":2,"charIndex":188},{"text":"Model Release: `model_zoo`","id":"model-release-model_zoo","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":122,"title":"Accuracy Debug Tool","content":"#\n\nThe model conversion toolchain will quantize the calibration of the model based\non the calibration samples you provide and guarantee efficient deployment of the\nmodel on the Horizon computing platform. In the process of model conversion,\naccuracy loss is inevitably introduced due to the quantization process from\nfloating point to fixed point. Usually, the main reasons for accuracy loss may\nbe the following:\n\n 1. A part of the nodes in the model is more sensitive to quantizationwill\n    introduce larger errors, i.e., sensitive node quantization problem.\n\n 2. The error cumulation of each node in the model leads to a large calibration\n    error in the model as a whole, which mainly contains: error cumulation\n    caused by weight quantization, error cumulation caused by activation\n    quantization and error cumulation caused by full quantization.\n\nIn this case, Horizon provides the accuracy debug tool to help you locate\naccuracy problems in the model quantization process on your own. The tool can\nhelp you to analyze the node-granularity quantization error of the calibration\nmodel, and finally help you to quickly locate the nodes with accuracy anomalies.\n\nThe accuracy debug tool provides a variety of analysis functions for you to use,\nsuch as:\n\n * Get the node quantization sensitivity.\n * Get the cumulative error curve of the model.\n * Get the data distribution of the specified node.\n * Get the box plot of data distribution between input data channels of a\n   specified node, etc.\n\n\nQuickstart#\n\nThere are several steps to use the accuracy debug tool as follows:\n\n 1. Configure the parameter debug_mode: \"dump_calibration_data\" in the\n    model_parameters group in yaml to save the calibration data.\n\n 2. Import the debug module and load the calibration model and data.\n\n 3. Analyze the models with significant accuracy loss through the API provided\n    by the accuracy debug tool.\n\nThe overall process is shown as the follow:\n\n\n\n * Confirm cumulative error of separate quantized activation/weight, see:\n   plot_acc_error section for details.\n * Activation sensitivity sort, see: get_sensitivity_of_nodes section for\n   details.\n * Weight sensitivity sort, see:get_sensitivity_of_nodes section for details.\n * Node sensitivity sort, see: get_sensitivity_of_nodes section for details.\n * Check the distribution of sensitive activation layers, see: plot_distribution\n   and get_channelwise_data_distribution for details.\n * Check the distribution of sensitive weights, see: plot_distribution and\n   get_channelwise_data_distribution for details.\n * Individual quantization and partial quantization testing of sensitive nodes,\n   see: sensitivity_analysis section for details.\n\n\nCalibration Models and Data Storage#\n\nFirst you need to configure debug_mode: \"dump_calibration_data\" in the yaml file\nto enable the accuracy debug function. Save the calibration data\n(calibration_data), and the related calibration model(calibrated_model.onnx) is\nsaved in a constant state. In which:\n\n * Calibration data (calibration_data): In the calibration phase, the model\n   obtains the quantization parameters of each quantized node by forward\n   inference on these data, including: scale and threshold.\n * Calibration model (calibrated_model.onnx): the quantization parameters of\n   each quantized node calculated in the calibration phase are saved in the\n   calibration node to obtain the calibration model.\n\nNote\n\nWhat is the difference between the calibration data saved here and the\ncalibration data generated by 02_preprocess.sh?\n\nIn J6 toolchain, 02_preprocess.sh gets the same calibration data as the one\nsaved here, both in .npy format, which can be used directly in the debug tool to\ntest the model accuracy. Note that you need to make sure that the folder\nstructure of the calibration data from 02_preprocess.sh is the same as the\nfolder structure of the calibration data saved here before feeding it into the\ndebug tool.\n\nNote\n\nCalibration model (calibrated_model.onnx) interpretation\n\nThe calibration model is an intermediate product obtained by the model\ntransformation tool chain by taking the floating point model after structural\noptimization, calculating the quantization parameters corresponding to each node\nfrom the calibration data and saving them in the calibration node. The main\nfeature of the calibration model is that the model contains calibration nodes\nwith the node type HzCalibration. These calibration nodes are divided into two\nmain categories: activation calibration nodes and weight calibration nodes .\n\nThe input of activation calibration node is the output of the previous node of\nthe current node, and the input data is quantized and inverse quantized based on\nthe quantization parameters (scales and thresholds) saved in the current\nactivation calibration node and then output.\n\nThe input of weight calibration node is the original floating point weights of\nthe model, and the input original floating point weights are quantized and\ninverse quantized based on the quantization parameters (scales and thresholds)\nsaved in the current weight calibration node and then output.\n\nIn addition to the above calibration nodes, other nodes in the calibration model\nare called general nodes by the accuracy debug tool. The types of general nodes\nare: Conv, Mul, Add, etc.\n\n\n\nThe folder structure of calibration_data is as follows:\n\n\n\n\nAccuracy Debug Module Import and Usage#\n\nNext, you need to import the debug module into your code and get the\nquantization sensitivity of nodes (by default, the cosine similarity of the\nmodel output is used) via the get_sensitivity_of_nodes interface. A detailed\ndescription of the parameters of get_sensitivity_of_nodes can be found in\nget_sensitivity_of_nodes section.\n\n\n\n\nAnalysis Result Display#\n\nThe following is the result when verbose=True is printed:\n\n\n\nIn addition, the API will return the node quantization sensitivity information\nto you in the form of a dictionary (Dict) for subsequent analysis.\n\n\n\nFor more functions, see Function Description section.\n\nFor customer convenience, the precision debug tool can also be used via the\ncommand line, which can be viewed via hmct-debugger -h/--help the subcommands\ncorresponding to each function. The detailed arguments and usage of each\nsubcommand can be found in the Function Description section.\n\n\nFunction Description #\n\n\nget_sensitivity_of_nodes #\n\nFunction: Get the node quantization sensitivity.\n\nCommand line format:\n\n\n\nThe parameters can be viewed via hmct-debugger get-sensitivity-of-nodes\n-h/--help.\n\nParameters:\n\nPARAMETER          ABBR FOR COMMAND LINE PARAMTERS   DESCRIPTION                                                    REQUIRED/OPTIONAL\nmodel_or_file      Fixed parameter                   PURPOSE: Specify the calibration model. RANGE: None. DEFAULT   Required\n                                                     VALUE: None. DESCRIPTIONS: Required, specify the calibration\n                                                     model to be analyzed.\nmetrics            -m                                PURPOSE: The measure of node quantification sensitivity.       Optional\n                                                     RANGE: 'cosine-similarity' , 'mse' , 'mre' , 'sqnr' ,\n                                                     'chebyshev' . DEFAULT VALUE: 'cosine-similarity'.\n                                                     DESCRIPTIONS: Specify how the node quantization sensitivity\n                                                     is calculated, and this parameter can be a List, i.e.,\n                                                     calculating the quantized sensitivities in a variety of\n                                                     ways. However, the output is sorted by the calculation of\n                                                     the first position in the list only, and the higher the\n                                                     ranking indicates that the error introduced by quantifying\n                                                     that node is greater.\ncalibrated_data    Fixed parameter                   PURPOSE: Specify the calibration data. RANGE: None. DEFAULT    Required\n                                                     VALUE: None. DESCRIPTIONS: Required, specify the calibration\n                                                     data needed for the analysis.\noutput_node        -o                                PURPOSE: Specify the output node. RANGE: General nodes with    Optional\n                                                     corresponding calibration nodes in the calibration model.\n                                                     DEFAULT VALUE: None. DESCRIPTIONS: This parameter allows you\n                                                     to specify intermediate nodes as output and calculate the\n                                                     node quantization sensitivity. If the default parameter None\n                                                     is kept, the accuracy debug tool will obtain the final\n                                                     output of the model and calculate the quantization\n                                                     sensitivity of the nodes on this basis.\nnode_type          -n                                PURPOSE: The node type. RANGE: 'node' , 'weight' ,             Optional\n                                                     'activation'. DEFAULT VALUE: 'node'. DESCRIPTIONS: The types\n                                                     of nodes that need to calculate the quantization\n                                                     sensitivity, including: node (general node), weight (weight\n                                                     calibration node), activation (activation calibration node).\ndata_num           -d                                PURPOSE: The amount of data needed to calculate the            Optional\n                                                     quantized sensitivities. RANGE: Greater than 0, less than or\n                                                     equal to the total number of data in calibration_data.\n                                                     DEFAULT VALUE: 1. DESCRIPTIONS: Set the amount of data\n                                                     needed to calculate the node quantization sensitivity. The\n                                                     default is 1, in which case the tool will use the first\n                                                     single data in the calibration_data set for the calculation.\n                                                     The minimum setting is 1 and the maximum is the amount of\n                                                     data in calibration_data.\nverbose            -v                                PURPOSE: Select whether to print the information to the        Optional\n                                                     terminal. RANGE: True , False. DEFAULT VALUE: False.\n                                                     DESCRIPTIONS: If set to True, the quantization sensitivity\n                                                     information will be printed to the terminal. If the metrics\n                                                     contain multiple metrics, they will be sorted by first.\ninterested_nodes   -i                                PURPOSE: Set the node of interest. RANGE: All nodes in the     Optional\n                                                     calibration model. DEFAULT VALUE: None. DESCRIPTIONS: If\n                                                     specified, only the quantization sensitivity of the node\n                                                     will be obtained, and the rest of the nodes will not be\n                                                     obtained. Also, if this parameter is specified, the node\n                                                     type specified by node_type will be ignored. That is, this\n                                                     parameter has a higher priority than node_type. If the\n                                                     default parameter None is kept, the quantization sensitivity\n                                                     of all quantizable nodes in the model is calculated.\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nAnalysis results presentation:\n\nDescription: First you set the node types that need to calculate the sensitivity\nby node_type, then the tool obtains all the nodes in the calibration model that\nmatch node_type and gets the quantization sensitivity of these nodes. When\nverbose is set to True, the tool sorts the node quantization sensitivity and\nprints it in the terminal. The higher the sort, the greater the quantization\nerror introduced by the node quantization. Also for different node_types, the\ntool will display different node quantization sensitivity information.\n\nWhen verbose=True and node_type='node', the following results are printed:\n\n\n\nWhere,\n\n * node: The node name.\n * cosine-similarity, mse: Quantized sensitivity values of each node.\n\nWhen verbose=True and node_type='weight', the following results are printed:\n\n\n\nWhere,\n\n * weight: The weight calibration node name.\n * node: The name of the common node corresponding to the weight calibration\n   node, i.e., the output of the weight calibration node is its input.\n * cosine-similarity, mse: The quantization sensitivity values of each node.\n\nWhen verbose=True and node_type='activation', the following results are printed:\n\n\n\nWhere,\n\n * activation: The activation calibration node name.\n * node: The common node after the activation calibration node in the model\n   structure, i.e., the output of the activation calibration node is its input.\n * threshold: The calibration threshold, and the maximum value is taken if there\n   are multiple thresholds.\n * bit: The quantization bit.\n * cosine-similarity, mse: The quantization sensitivity values of each node.\n\nAPI return value:\n\nThe API returns the value of the quantization sensitivity saved in dictionary\nformat (Key is the node name, Value is the quantization sensitivity information\nof the node), in the following format:\n\n\n\n\nplot_acc_error #\n\nFunction: Only one node in the floating-point model will be quantized, and the\nerror of that model and the output of the node in the floating-point model will\nbe calculated sequentially to obtain the cumulative error curve.\n\nCommand line format:\n\n\n\nThe parameters can be viewed via hmct-debugger plot-acc-error -h/--help.\n\nAPI parameters:\n\n\n\nAnalysis results presentation:\n\n1. Specify the node to quantify the cumulative error test:\n\n * Specify single-node quantization\n\nConfiguration method: quantize_node=['Conv_2', 'Conv_90'], quantize_node is\nsingle list.\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nDescription: When quantize_node is a single list, for the quantize_node you set,\nquantize the nodes in the quantize_node separately and keep the other nodes in\nthe model unquantized to get the corresponding model, and then calculate the\nerror between the output of each node in the model and the output of the\ncorresponding node in the floating-point model, and get the corresponding\ncumulative error curve.\n\nWhen average_mode = False:\n\n\n\nWhen average_mode = True:\n\n\n\nNote\n\naverage_mode\n\naverage_mode defaults to False. For some models, it is not possible to determine\nwhich quantization strategy is more effective by the cumulative error curve.\nTherefore, we need to set average_mode to True, which takes the average value of\nthe cumulative error of the first n nodes as the cumulative error of the nth\nnode.\n\nThis is calculated as follows, for example:\n\nWhen average_mode=False, accumulate_error=[1.0, 0.9, 0.9, 0.8].\n\nBut when average_mode=True, accumulate_error=[1.0, 0.95, 0.933, 0.9].\n\n * Specify multiple nodes for quantization\n\nConfiguration method: quantize_node=[['Conv_2'], ['Conv_2',\n'Conv_90']]，quantize_node is nested list.\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nDescription: When quantize_node is a nested list, for the quantize_node you set,\nquantize the nodes specified for each single list in the quantize_node\nseparately and keep the other nodes in the model unquantized to get the\ncorresponding model, then calculate the error between the output of each node in\nthe model and the output of the corresponding node in the floating-point model,\nand get the corresponding cumulative error curve.\n\n * partial_qmodel_0: Only quantize the Conv_2 node, the rest of the nodes are\n   not quantized;\n * partial_qmodel_1: Only the Conv_2 and Conv_90 nodes are quantified, and the\n   rest of the nodes are not quantified.\n\nWhen average_mode = False:\n\n\n\nWhen average_mode = True:\n\n\n\n2.Cumulative error test after unquantizing some nodes of the model\n\n * Specify single node not quantified\n\nConfiguration method: non_quantize_node=['Conv_2', 'Conv_90']，non_quantize_node\nis single list.\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nDescription: When the non_quantize_node is a single list, for the\nnon_quantize_node you set, unquantize each node in the non_quantize_node\nseparately while keeping all other nodes quantized, and then get the\ncorresponding model, calculate the error between the output of each node in the\nmodel and the output of the corresponding node in the floating-point model, and\nget the corresponding cumulative error curve.\n\nWhen average_mode = False:\n\n\n\nWhen average_mode = True:\n\n\n\n * Specify multiple nodes not to quantize\n\nConfiguration method: non_quantize_node=[['Conv_2'], ['Conv_2',\n'Conv_90']]，non_quantize_node is nested list.\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nDescription: When non_quantize_node is a nested list, for the non_quantize_node\nyou set, do not quantize each node specified in the non_quantize_node and keep\nall the other nodes in the model quantized, and get the corresponding model,\nthen calculate the error between the output of each node in the model and the\noutput of the corresponding node in the floating-point model, and get the\ncorresponding cumulative error curve.\n\n * partial_qmodel_0: No quantization of Conv_2 nodes and quantization of the\n   remaining nodes;\n * partial_qmodel_1: No quantization of Conv_2 and Conv_90 nodes, and\n   quantization of the rest of the nodes.\n\nWhen average_mode = False:\n\n\n\nWhen average_mode = True:\n\n\n\nTest skills:\n\nWhen testing the accuracy of partial quantification, you may compare the\naccuracy of multiple sets of quantified strategies in order of quantified\nsensitivity, in which case you can refer to the following usage:\n\n\n\n3. Activation weights are quantified separately\n\nConfiguration method: quantize_node=['weight','activation'].\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nDescription: The quantize_node can also be specified directly as 'weight' or\n'activation'.\n\n * quantize_node = ['weight']: Quantify weights, not activation.\n * quantize_node = ['activation']: Quantify activation, not weights.\n * quantize_node = ['weight', 'activation']: Weights and activations are\n   quantified separately.\n\n\n\nNote\n\nIn general, it is recommended that you pay more attention to the portion of the\ncumulative error curve near the model output location in the cumulative error\ncurve graph. When the cumulative error curve obtained from the test after using\na certain quantization method has a smaller cumulative error close to the model\noutput position, i.e., a higher degree of similarity, then we recommend that you\nprioritize testing this quantitative method.\n\n\nplot_distribution #\n\nFunction：Select the node and obtain the output of that node in the floating\npoint model and the calibration model respectively to get the output data\ndistribution. In addition, the two outputs are subtracted to obtain the error\ndistribution between the two outputs.\n\nCommand line format:\n\n\n\nThe parameters can be viewed via hmct-debugger plot-distribution -h/--help.\n\nParameters:\n\n\n\nAnalysis results presentation:\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nWhen node_type = 'node_output':\n\n\n\nWhen node_type = 'weight':\n\n\n\nWhen node_type = 'activation':\n\n\n\nNote\n\nIn the above three pictures, the blue triangle indicates that the maximum value\nof the absolute data. The red dashed line indicates that the maximum calibration\nthreshold.\n\n\nget_channelwise_data_distribution #\n\nFunction: Draw the box line plot of the data distribution between the input data\nchannels of the specified calibration node.\n\nCommand line format:\n\n\n\nThe parameters can be viewed via hmct-debugger get-channelwise-data-distribution\n-h/--help.\n\nParameters:\n\n\n\nAnalysis results presentation:\n\nDescription: For your set calibration node list(node_list), get the dimension\nwhere the channel is located from the parameter axis, and get the data\ndistribution among the node input data channels. Where axis is None by default,\nif the node is a weight calibration node, then the dimension where the channel\nis located is 0 by default; if the node is an activation calibration node, then\nthe dimension where the channel is located is 1 by default.\n\nWeight calibration node:\n\n\n\nActivation calibration node:\n\n\n\nThe output is shown as follows:\n\n\n\nIn the figure:\n\n * The horizontal coordinate indicates the number of channels of input data of\n   the node, and there are 96 channels of input data in the figure example.\n * The vertical coordinate indicates the data distribution range of each\n   channel, where the red solid line indicates the median of the data in that\n   channel and the blue dashed line indicates the mean. The upper and lower\n   limits of each box indicate the upper quartile and the lower quartile,\n   respectively, and the discrete points outside the upper and lower limits\n   indicate the outliers, and the maximum of the absolute value of these\n   outliers is observed to determine whether the current node input data is\n   experiencing large fluctuations.\n\n\nsensitivity_analysis #\n\nFunction: For quantization-sensitive nodes, the model accuracy after quantizing\nthese nodes individually as well as partially is analyzed and tested separately.\n\nCommand line usage:\n\n\n\nThe parameters can be viewed via hmct-debugger sensitivity-analysis -h/--help.\n\nParameters:\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nAnalysis results presentation:\n\n\n\nIn the figure:\n\n * Blue dashed line: baseline, i.e., the cosine similarity of the floating point\n   model output to itself, is 1.\n * Green cross: Quantize only the current node to get a partially quantized\n   model, and compute the similarity between the partially quantized model and\n   the final output of the floating-point model.\n * Red solid line: without quantizing the current node and all nodes before the\n   current node, the similarity between the partially quantized model and the\n   floating-point model is calculated. For example, the similarity value of\n   Conv_92 in the above figure is around 0.995, indicating that the cosine\n   similarity between the final output of the partially quantized model and the\n   floating-point model is around 0.995 when we unquantize the nodes of Conv_2,\n   Conv_7, and Conv_92, and keep all the rest of the nodes quantized to get the\n   partially quantized model. The first none of the horizontal coordinates, in\n   the red solid line, means calibrated_model.\n\n\nrunall#\n\nFunction: Run all the functions in the original debug tool with one click.\n\nCommand line usage:\n\n\n\nThe parameters can be viewed via hmct-debugger runall -h/--help.\n\nParameters:\n\nAPI Usage:\n\n\n\nCommand Line Usage:\n\n\n\nrunall Process:\n\n\n\nWhen all parameters are left as default, the tool performs the following\nfunctions in sequence:\n\n * STEP1 and STEP2: Obtain the quantization sensitivity of the weight\n   calibration node and activation calibration node, respectively.\n * STEP3: Based on the results of STEP1 and STEP2, take the TOP5 of the weight\n   calibration nodes and the TOP5 of the activation calibration nodes to plot\n   their data distribution, respectively.\n * STEP4: For the nodes obtained in STEP3, draw box-and-line plots of their\n   inter-channel data distribution, respectively.\n * STEP5: Plot the cumulative error curves for quantizing only the weights and\n   only the activations, respectively.\n * STEP6: Partial quantization and single-node quantization accuracy analysis\n   for sensitive nodes. Since the example in the figure does not specify\n   sensitive_nodes, the debug tool needs to calculate the quantization\n   sensitivity of common nodes by itself and select nodes with sensitivity less\n   than the specified pick_threshold for testing and analysis.\n\nWhen node_type='node' is specified, the tool will get the top5 nodes and find\nthe calibration nodes corresponding to each node separately, and get the data\ndistribution and box line diagram of the calibration nodes.","routePath":"/en/guide/ptq/ptq_tool/accuracy_debug","lang":"en","toc":[{"text":"Quickstart","id":"quickstart","depth":2,"charIndex":1504},{"text":"Calibration Models and Data Storage","id":"calibration-models-and-data-storage","depth":3,"charIndex":2697},{"text":"Accuracy Debug Module Import and Usage","id":"accuracy-debug-module-import-and-usage","depth":3,"charIndex":5368},{"text":"Analysis Result Display","id":"analysis-result-display","depth":3,"charIndex":5743},{"text":"Function Description","id":"function-description","depth":2,"charIndex":-1},{"text":"get_sensitivity_of_nodes","id":"get_sensitivity_of_nodes","depth":3,"charIndex":-1},{"text":"plot_acc_error","id":"plot_acc_error","depth":3,"charIndex":-1},{"text":"plot_distribution","id":"plot_distribution","depth":3,"charIndex":-1},{"text":"get_channelwise_data_distribution","id":"get_channelwise_data_distribution","depth":3,"charIndex":-1},{"text":"sensitivity_analysis","id":"sensitivity_analysis","depth":3,"charIndex":-1},{"text":"runall","id":"runall","depth":3,"charIndex":23280}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":123,"title":"Model Checking","content":"#\n\nIn engineering practice, since not all floating-point models can be converted to\nquantized models, checking is required before conversion. This checking process\nwill go through the process of a model conversion.\n\nHowever, for those more time-consuming procedures, we simplified the checking\nprocess. This command will output the check results and the OP deployment on the\ndevice when it finishes the checking of the model.\n\n\nUsage#\n\nAttention\n\nPlease use the HBRuntime to inference your floating-point onnx model to make\nsure the model is legal, and then refer to the method below to use hb_compile to\ncheck if the model can be converted to a quantized model.\n\nHow To Use:\n\n\n\nIf your model is a multi-input model, you can refer to the following command:\n\n\n\n\nParameters Introduction#\n\nNote\n\nIf you find that the The converted model node information results during model\nchecking does not match the The converted model node information results during\nmodel quantized compilation, it may be because there is actually a default yaml\nconfiguration during the model checking process, and if you configured the yaml\nbefore the conversion, some parameter differences may cause this if you\nconfigure yaml before model quantized compilation, some parameters may cause\nthis situation. The yaml configuration parameters that may cause this to happen\ninclude: mean_value , scale_value std_value and quant_config.","routePath":"/en/guide/ptq/ptq_tool/hb_compile/check","lang":"en","toc":[{"text":"Usage","id":"usage","depth":2,"charIndex":427},{"text":"Parameters Introduction","id":"parameters-introduction","depth":2,"charIndex":760}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":124,"title":"Model Quantized Compilation","content":"#\n\nDuring the model quantized compilation, the tool will generate the intermediate\nstage onnx model as well as the runtime model for simulating the on-board\nsituations according to the configuration file.\n\n\nUsage#\n\nThe hb_compile provides two modes when doing the model quantized compilation,\nfast performance evaluation mode (with fast-perf turned on) and traditional\nmodel conversion compilation mode (without fast-perf turned on).\n\nThe fast performance evaluation mode, when turned on, will generate the hbm\nmodel that can have the highest performance when running on the board side\nduring the conversion process, and the tool internally performs the following\noperations:\n\n * Run BPU executable operators on the BPU whenever possible.\n * Remove CPU operators that are removable at the beginning and end of the\n   model, including: Quantize/Dequantize, Transpose, Cast, Reshape, etc.\n\nIf you want to use the fast performance evaluation mode (i.e., turn on\nfast-perf), the reference command is as follows:\n\n\n\nAttention\n * Please note that if you need to enable fast performance evaluation mode, do\n   not configure the -config parameter as the tool uses the built-in\n   high-performance configuration in this mode.\n * When using hb_compile for model quantized compilation, the --input-shape\n   parameter configuration only works in fast performance evaluation mode (i.e.\n   fast-perf is turned on).\n\nIf you want to use the traditional model conversion compilation mode (without\nfast-perf enabled), the reference command is as follows:\n\n\n\n\nParameters Introduction#\n\nThe log file generated by the compilation will be stored in the directory where\nthe command is executed under the default name hb_compile.log.\n\n\nConfiguration File Template #\n\nA complete configuration file template is shown as below:\n\nNote\n\nBelow configuration file is only for display, in an actual configuration file of\na model, the caffe_model and onnx_model parameters are not coexisting. The model\nshould be either a Caffe or an ONNX model. That is, caffe_model + prototxt or\nonnx_model , you need to choose one of the two when configuring.\n\n\n\nThe configuration file mainly contains model parameters, input information\nparameters, calibration parameters, compilation parameters. All parameter groups\nmust exist in your configuration file. Parameters can be divided into the\noptional and the mandatory, while the optional parameters can be left\nunconfigured.\n\nThe following is the specific parameter information, the parameters will be\nmore, we follow the above parameter group order to introduce. Required/Optional\nindicates whether this parameter must be specified in the Yaml file.\n\n\nSpecific Parameter Information #\n\n\nModel Parameters #\n\n\nInput Information Parameters#\n\ninput_type_rt/input_type_train additional description\n\nTo boost the ASIC performance, 2 assumptions have been made in the design of\nASIC micro architecture:\n\n 1. All inputs are quantized int8 data.\n 2. All camera captured data are NV12.\n\nTherefore, if you use the RGB (NCHW) format in the model training and expect the\nmodel to process NV12 data efficiently, then you will need to configure as\nfollow:\n\n\n\nIn addition to converting the input data to NV12, you can also use different\nRGB-orders in the training and runtime infer. The tool will automatically add\ndata conversion nodes according to the data formats specified by the\ninput_type_rt and input_type_train. Not any type combination is needed, in order\nto avoid your misuse, we only open some fixed type combinations in the following\ntable (Y for supported image types, while N for unsupported image types. The\nfirst row of the table is the data types supported in input_type_rt and the\nfirst column is the data types supported in input_type_train.):\n\nINPUT_TYPE_TRAIN \\ INPUT_TYPE_RT   NV12   YUV444   RGB   BGR   GRAY   FEATUREMAP\nyuv444                             Y      Y        N     N     N      N\nrgb                                Y      Y        Y     Y     N      N\nbgr                                Y      Y        Y     Y     N      N\ngray                               N      N        N     N     Y      N\nfeaturemap                         N      N        N     N     N      Y\n\nNote\n\nTo meet the requirements of Horizon ASICs on input data type (int8) and reduce\nthe inference costs, when input_type_rt is of the RGB(NHWC/NCHW)/BGR(NHWC/NCHW)\ntype, the input data type of the models converted by using the conversion tool\nwill all be int8. That is, for regular image formats, pixel values should be\nsubtracted by 128, which has already been done by the API and you do not need to\ndo it again.\n\nIn the final hbm model obtained from the conversion, the conversion from\ninput_type_rt to input_type_train is an internal process. You only need to focus\non the data format of input_type_rt. It is of vital importance to understand the\nrequirement of the input_type_rt when preparing the inference data for embedded\napplications, please refer to the following explanations to each format of the\ninput_type_rt.\n\n * rgb, bgr, and gray are commonly used image format. Note that each value is\n   represented using UINT8.\n * yuv444 is a popular image format. Note that each value is represented using\n   UINT8.\n * NV12 is a popular YUV420 image format. note that each value is represented\n   using UINT8.\n * One special case of NV12 is to specify the bt601_video of the\n   input_space_and_range. Compared with typical NV12 format, its value range has\n   changed from [0,255] to [16,235]. Each value is still represented as UINT8.\n   Note that bt601_video is supported configuring via input_space_and_range only\n   when input_type_train is bgr or rgb.\n * Featuremap is suitable for cases where the above listed formats failed to\n   meet your needs, and this type uses float32 for each value. For example, this\n   format is commonly used for model processing such as radar and speech.\n\nTip\n\nThe above input_type_rt and input_type_train are integrated into the toolchain\nprocessing procedure. If you are very sure that no format conversion is\nrequired, then set the two input_type to be the same, so that the same\ninput_type will perform the processing in a straight-through way and will not\naffect the actual execution performance of the model.\n\nSimilarly, data pre-processing also is also integrated into the process. If you\ndon't need to do any pre-processing, just do not specify mean_value, scale_value\nand std_value, which will not affect the actual execution performance of the\nmodel.\n\n\nCalibration Parameters#\n\n\nCompilation Parameters #\n\n\nparam_value Configuration #\n\nYou can specify the parameters like this: param_name: 'param_value', while\nmultiple values can be separated by ';': param_name:\n'param_value1;param_value2;param_value3'.\n\nTip\n\nTo avoid parameter sequence problems, You are strongly suggested to specify the\nparameters(such as input_shape etc.) explicitly when there are multi-input\nmodels.\n\nAttention\n\nPlease note that, if set input_type_rt to nv12 , an odd number cannot appear in\nthe input size of model.","routePath":"/en/guide/ptq/ptq_tool/hb_compile/convert","lang":"en","toc":[{"text":"Usage","id":"usage","depth":2,"charIndex":206},{"text":"Parameters Introduction","id":"parameters-introduction","depth":2,"charIndex":1540},{"text":"Configuration File Template","id":"configuration-file-template","depth":2,"charIndex":-1},{"text":"Specific Parameter Information","id":"specific-parameter-information","depth":2,"charIndex":-1},{"text":"Model Parameters","id":"model-parameters","depth":3,"charIndex":-1},{"text":"Input Information Parameters","id":"input-information-parameters","depth":3,"charIndex":2713},{"text":"Calibration Parameters","id":"calibration-parameters","depth":3,"charIndex":6496},{"text":"Compilation Parameters","id":"compilation-parameters","depth":3,"charIndex":-1},{"text":"param_value Configuration","id":"param_value-configuration","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":125,"title":"Model Modification and HBIR Model Compilation","content":"#\n\nTo pursue ultimate performance in those scenarios with enormous input size, the\nquantization and conversion of some inputs and outputs can be fused into data\npre-processing. At this point you can choose to remove these nodes by using the\nhb_compile tool, the tool supports also the compilation of HBIR models at the\nsame time.\n\n\nUsage#\n\nHow To Use:\n\n\n\n\nParameters Introduction#","routePath":"/en/guide/ptq/ptq_tool/hb_compile/modify","lang":"en","toc":[{"text":"Usage","id":"usage","depth":2,"charIndex":331},{"text":"Parameters Introduction","id":"parameters-introduction","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":126,"title":"The quant_config Introduction","content":"#\n\nWhen converting a model, you can use quant_config to configure the model's\nquantization parameters from multiple levels: model_config, op_config, and\nnode_config:\n\n * model_config: Configure the overall quantization parameters for the model,\n   key is a custom name.\n\n * op_config: Configure the quantization parameters for nodes with a certain\n   type, key is the operator type.\n\n * node_config: Configure the quantization parameters for a specific node, key\n   is the name of the node.\n\nThere is a priority relationship between the three levels, the smaller the\nconfiguration granularity, the higher the priority, i.e., priority model_config\n< op_config < node_config. When a node is configured by more than one level at\nthe same time, the level with the highest priority takes effect in the end.\n\n\nConfigure computation accuracy#\n\nThe quant_config supports the configuration of int8/int16/float16 three kinds of\ncomputation accuracy data types, about these three kinds of data types are\ndescribed as follows:\n\n * int8: The default quantization type for most operators, which generally does\n   not need to be actively configured by you.\n\n * int16: You can refer to the section int16 Configuration.\n\n * float16: When configured as the float16 type, the tool will internally only\n   configure this operator as the float16 computation accuracy type (there will\n   be no computation broadcast updates to the float16 computation operator\n   context operators).\n\n\nConfigure the computation accuracy for the entire model#\n\n\nConfigure the computation accuracy for nodes with certain type#\n\n\nConfigure the computation accuracy for a specific node#\n\n\nConfiguring calibration parameters#\n\nThe quant_config supports configuring multiple calibration algorithms such as\nkl, max, etc. For each calibration algorithm, you can also flexibly control the\nspecific hyperparameter configuration(if not configured, default value will be\nused). In addition, some independent calibration functions such as per_channel,\nasymmetric, bias_correction can also be configured.\n\nAttention\n\nIf quant_config is not configured, multiple pre-set calibration algorithms will\nbe tried by default, and the calibration algorithm with minimum quantization\nloss will be selected.\n\n\nConfiguring calibration parameters for activation#\n\n\nConfiguring calibration parameters for weight#\n\n\nConfiguring search methods for calibration parameters#\n\nThe quant_config supports two search methods with different granularities:\n\n * modelwise_search: Search for quantization parameters at model level. This\n   method allows multiple calibration algorithms to be configured at one time.\n   By comparing the quantization loss with metric configured by you based on the\n   model output before and after quantization, a calibration algorithm with the\n   minimum quantization loss is selected.\n\n * layerwise_search: Search for quantization parameters at node level. This\n   method calculates the quantization loss with metric configured by you based\n   on the model output before and after the quantization of each node, and\n   assigns the calibration algorithm with the minimum quantization loss to the\n   node.\n\nAttention\n\nIf multiple calibration algorithms are configured, modelwise search will be\nenabled by default to find the optimal algorithm for current model; if the\nlayerwise search parameters are configured, a layer by layer search for the\noptimal algorithm will be initiated.\n\n\nConfiguring the modelwise search method#\n\n\nConfiguring the layerwise search method#\n\n\nConfiguration example of the json template#\n\nThe following is an example of a json template configuration of the quant_config\nwith all the configurable options, you can refer to this template for\nconfiguration.\n\n\n\n\nint16 Configuration #\n\nIn the process of model conversion, most of the operators in the model are\nquantized to int8 for computation, and by configuring the quant_config\nparameter. You can specify in detail the input or the output of an op as int16\ncalculation (The range of operators supporting the configuration of int16 you\ncan refer to Toolchain Operator Support Constraint List-ONNX Operator Support\nList) The basic principle is as follows:basically as follows.\n\nAfter you configure an op input/output data type to int16, the model\ntransformation automatically performs an update and check of the op input/output\ncontext int16 configuration internally. For example, when configuring op_1\ninput/output data type as int16, it actually potentially specifies that the\nprevious/next op of op_1 needs to support computation in int16 at the same time.\nFor unsupported scenarios, the model conversion tool will print a log indicating\nthat the int16 configuration combination is temporarily unsupported and fall\nback to int8 computation.","routePath":"/en/guide/ptq/ptq_tool/hb_compile/quant_config","lang":"en","toc":[{"text":"Configure computation accuracy","id":"configure-computation-accuracy","depth":2,"charIndex":803},{"text":"Configure the computation accuracy for the entire model","id":"configure-the-computation-accuracy-for-the-entire-model","depth":3,"charIndex":1462},{"text":"Configure the computation accuracy for nodes with certain type","id":"configure-the-computation-accuracy-for-nodes-with-certain-type","depth":3,"charIndex":1521},{"text":"Configure the computation accuracy for a specific node","id":"configure-the-computation-accuracy-for-a-specific-node","depth":3,"charIndex":1587},{"text":"Configuring calibration parameters","id":"configuring-calibration-parameters","depth":2,"charIndex":1645},{"text":"Configuring calibration parameters for activation","id":"configuring-calibration-parameters-for-activation","depth":3,"charIndex":2245},{"text":"Configuring calibration parameters for weight","id":"configuring-calibration-parameters-for-weight","depth":3,"charIndex":2298},{"text":"Configuring search methods for calibration parameters","id":"configuring-search-methods-for-calibration-parameters","depth":2,"charIndex":2347},{"text":"Configuring the modelwise search method","id":"configuring-the-modelwise-search-method","depth":3,"charIndex":3435},{"text":"Configuring the layerwise search method","id":"configuring-the-layerwise-search-method","depth":3,"charIndex":3478},{"text":"Configuration example of the json template","id":"configuration-example-of-the-json-template","depth":2,"charIndex":3521},{"text":"int16 Configuration","id":"int16-configuration","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":127,"title":"The hb_config_generator Tool","content":"#\n\nThe hb_config_generator is used to support you in obtaining the simplest yaml\nconfiguration file and yaml configuration file with all parameter defaults\nduring model compiling. Among them:\n\n * The simplest yaml configuration file contains the minimal parameter\n   configuration which you can use to quickly verify that the model conversion\n   process performs properly.\n * The yaml configuration files with all parameter defaults contains all the\n   parameters that have default values when the model is compiled.\n\n\nHow to Use#\n\n 1. Generate a simplest yaml configuration file:\n\n\n\n 2. Generate a simplest yaml configuration file based on model information:\n\n\n\n 3. Generate a yaml configuration file containing all the default values of the\n    parameters:\n\n\n\n 4. Generate a yaml configuration file containing all the default values of the\n    parameters based on model information:\n\n\n\n\nParameters#\n\n\nOutput Contents#\n\nThe hb_config_generator command will generate the simplest yaml configuration\nfile or the yaml configuration file with default values for all parameters,\ndepending on your configuration.\n\nAttention\n\nPlease note that you need to modify the default values in the yaml file that\ncontained the full parameter defaults to your current reality before you can use\nit.","routePath":"/en/guide/ptq/ptq_tool/hb_config_generator","lang":"en","toc":[{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":518},{"text":"Parameters","id":"parameters","depth":2,"charIndex":888},{"text":"Output Contents","id":"output-contents","depth":2,"charIndex":902}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":128,"title":"The hb_eval_preprocess Tool","content":"#\n\nThe hb_eval_preprocess tool is used for preprocessing image data on x86 system.\n\nPreprocessing refers to certain image data operations before being fed into the\nmodel, e.g., image resizing, image cropping, image padding, etc.\n\n\nHow to Use#\n\n\n\n\nParameters#\n\n\nOutput Contents#\n\nThe hb_eval_preprocess tool will generate binary image files in the directory\nspecified by the --output_dir parameter.\n\nTip\n\nMore application samples of the hb_eval_preprocess tool in the accuracy\nevaluations of on-board models can be found in Data Pre-process.","routePath":"/en/guide/ptq/ptq_tool/hb_eval_preprocess","lang":"en","toc":[{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":230},{"text":"Parameters","id":"parameters","depth":2,"charIndex":246},{"text":"Output Contents","id":"output-contents","depth":2,"charIndex":260}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":129,"title":"The hb_model_info Tool","content":"#\n\nThe hb_model_info tool is used to parse the dependencies and parameters of the\n*.hbm, the *.bc in compilation and the basic information of the *.onnx model, at\nthe same time support the checking of *.bc deletable nodes.\n\n\nHow to Use#\n\n\n\n\nParameters#\n\n\nOutput Contents#\n\nThe outputs will be part of the inputs when the model is compiled, as follows:\n\nNote\n\nThe version number information and other content in the following code block\nwill change with the release package version, and here is only as an example.\n\n","routePath":"/en/guide/ptq/ptq_tool/hb_model_info","lang":"en","toc":[{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":224},{"text":"Parameters","id":"parameters","depth":2,"charIndex":240},{"text":"Output Contents","id":"output-contents","depth":2,"charIndex":254}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":130,"title":"The hb_verifier Tool","content":"#\n\nhb_verifier is a consistency verification tool that supports performing cosine\nsimilarity comparisons between onnx models, between onnx model and hbir model,\nbetween hbir model and hbir model, and output consistency comparisons between bc\nmodel and Hbm model.\n\nThe cosine similarity indicates the consistency between quantized models at\ndifferent stages. As the cosine similarity gets closer to 1, it indicates that\nthe outputs of the two quantized models being compared are closer to each other.\n\nConsistency comparison prints output consistency information for the comparison\nmodels, including output name, consistency, number of inconsistent elements,\nmaximum absolute error, and maximum relative error.\n\n\nRange of Support#\n\n * Fbc represents the floating-point hbir model.\n\n * Qbc represents the fixed-point hbir model.\n\n\nHow to Use#\n\n\n\n\nParameters#\n\n\nExamples of Reference Usage#\n\n 1. Cosine similarity comparisons were performed between the ONNX model and the\n    ONNX model.\n\nTake the model optimization stage model optimized_float_model.onnx and the model\ncalibration stage model calibrated_model.onnx as an example:\n\n\n\n 2. Cosine similarity comparisons were performed between the ONNX model and the\n    HBIR model.\n\nTake the model optimization stage model optimized_float_model.onnx and the model\nquantization stage fixed-point model quantized_model.bc as an example:\n\n\n\n 3. Output consistency comparisons were performed between the HBIR model and the\n    HBM model.\n\nTake the model quantization stage fixed-point model quantized_model.bc and the\nmodel compilation stage model googlenet.hbm as an example:\n\n\n\n\nOutput Contents#\n\n\nCosine Similarity Comparison#\n\nThe cosine similarity information for the compared models will be printed within\nthe terminal, as shown in the example below:\n\n\n\nAmong them:\n\n * NodeName represents the name of the operator.\n * TensorName represents the Tensor name of the first output of the operator.\n * ConsineSimilarity represents the calculated cosine similarity.\n\n\nOutput Consistency Comparison#\n\n\n\nAmong them:\n\n * OutputName represents the output name.\n\n * Consistency represents whether the output is consistent or not.\n\n * Mismatched Elements represents the number and percentage of inconsistent\n   elements.\n\n * Max Abs Diff represents the maximum absolute error.\n\n * Max Rel Diff represents the maximum relative error.","routePath":"/en/guide/ptq/ptq_tool/hb_verifier","lang":"en","toc":[{"text":"Range of Support","id":"range-of-support","depth":2,"charIndex":711},{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":828},{"text":"Parameters","id":"parameters","depth":2,"charIndex":844},{"text":"Examples of Reference Usage","id":"examples-of-reference-usage","depth":2,"charIndex":858},{"text":"Output Contents","id":"output-contents","depth":2,"charIndex":1621},{"text":"Cosine Similarity Comparison","id":"cosine-similarity-comparison","depth":3,"charIndex":1640},{"text":"Output Consistency Comparison","id":"output-consistency-comparison","depth":3,"charIndex":2008}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":131,"title":"The HBRuntime Inference Library","content":"#\n\nThe HBRuntime is a x86-side model inference library provided by Horizon, which\nsupports inference on the original ONNX models directly exported by commonly\nused training frameworks, the ONNX models at various stages generated during the\nPTQ conversion process of the Horizon toolchain, and the HBIR(*.bc) models and\nHBM(*.hbm) models generated during the Horizon toolchain conversion process. The\nusage flow is shown as follows:\n\n\nUsage#\n\nReference usage when using HBRuntime for model inference is as follows:\n\n\n\nNote\n * output_names: used to specify the output name, support to specify as None or\n   custom configuration. If there are no special requirements, here we recommend\n   you to set it to None.\n   * Specify as None, the tool will internally read the information of the\n     output nodes in the model and give the inference result in the order of\n     parsing.\n   * If you customize the configuration, you can specify full or partial\n     output_name and support modifying the order of outputs. Then, when the\n     inference is complete, the output will be returned according to the output\n     name and order that you specified.\n * input_feed: Used to configure the inputs for the model running, which need to\n   be prepared according to the input type and layout. The format needs to be\n   the dict, where the input name and input data form a key-value pair\n\nIn addition, HBRuntime supports you to view model attribute information during\nusage, the following model attribute information is supported to be viewed. For\nexample, if you want to print to see the model input number, you can use\nprint(f\"input_num: {sess.input_num}\").\n\nMODEL_ATTRIBUTE   DESCRIPTION\ninput_num         Number of model input\noutput_num        Number of model output\ninput_names       Names of model input\noutput_names      Names of model output\ninput_types       Types of model input\noutput_types      Types of model output\ninput_shapes      Shapes of model input\nouput_shapes      Shapes of model output\n\n\nUsage Example#\n\nIn the following, we provide you with the usage examples of HBRuntime for two\nscenarios, ONNX model inference and HBIR model inference, respectively.\n\n\nONNX Model Inference#\n\nThe basic flow for loading ONNX model inference using HBRuntime is shown below,\nand this sample code applies to inference for all ONNX models. Prepare the data\naccording to the input type and layout requirements of different models:\n\n\n\n\nHBIR Model Inference#\n\nThe basic flow for loading HBIR model inference using HBRuntime is shown below,\nand this sample code applies to inference for all HBIR models. Prepare the data\naccording to the input type and layout requirements of models:\n\n\n\n\nHBM Model Inference#\n\nThe basic flow for loading HBM model inference using HBRuntime is shown below,\nand this sample code applies to inference for all HBM model. Prepare the data\naccording to the input type and layout requirements of different models:\n\n","routePath":"/en/guide/ptq/ptq_tool/hbruntime","lang":"en","toc":[{"text":"Usage","id":"usage","depth":2,"charIndex":433},{"text":"Usage Example","id":"usage-example","depth":2,"charIndex":1998},{"text":"ONNX Model Inference","id":"onnx-model-inference","depth":3,"charIndex":2166},{"text":"HBIR Model Inference","id":"hbir-model-inference","depth":3,"charIndex":2426},{"text":"HBM Model Inference","id":"hbm-model-inference","depth":3,"charIndex":2676}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":132,"title":"PTQ Conversion Tool Overview","content":"#","routePath":"/en/guide/ptq/ptq_tool/tool_overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":133,"title":"Model Accuracy Analysis","content":"#\n\nThere are inevitable accuracy loss with the post-training model quantization\nthat converting the floating-point models into the fixed-point models based on\ndozens or hundreds of calibration data. But it has been proofed by a large\nnumber of production experience that as long as the most optimized parameter\ncombination can be found out, in most cases, Horizon's conversion tools can keep\nthe accuracy loss within 1%.\n\nThis section explains how to correctly analyze model accuracy. In case the\nevaluation results fail your expectations, please refer to the Model Accuracy\nOptimization section and try to optimize the accuracy. If you still can't solve\nit, please don't hesitate to contact Horizon and seek for technical support.\n\nYou are expected to understand how to evaluate model accuracy when reading this\nsection. This section explains how to run the model inference using the outputs\nof model conversion. As previously described, successful model conversion\nconsists of the following model outputs:\n\n * *_original_float_model.onnx\n * *_optimized_float_model.onnx\n * *_calibrated_model.onnx\n * *_ptq_model.onnx\n * *_quantized_model.bc\n * *.hbm\n\nAlthough the final hbm model is the one that will be deployed to the computing\nplatform, in order to facilitate the accuracy evaluation on Ubuntu development\nmachines. We provide *_quantized_model.bc to complete this accuracy evaluation\nprocess. The model has already been quantized and has the same accuracy results\nas the final hbm model. The basic process for loading the model inference model\nusing the Horizon development library is shown below, and this following\nillustrative code is not only applicable to quantized model, but also applicable\nto original and optimized onnx models (just replace the model file). You only\nneed to prepare corresponding data in line according to the input types and\nlayouts of the models.\n\n\n\n","routePath":"/en/guide/ptq/ptq_usage/accuracy_evaluation","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":134,"title":"Model Accuracy Tuning","content":"#\n\nBased on the accuracy evaluation in the previous section, you may find that the\naccuracy is less than expected. This section introduces you how to perform\naccuracy tuning with the accuracy tuning tools and functions to reduce\nquantization accuracy loss or to assist you in locating the cause of\nquantization accuracy loss when you experience a loss of quantization accuracy\nduring the PTQ model conversion.\n\nAttention\n\nAll the quantization accuracy tuning below refers to the calibrated_model.onnx\nquantization accuracy tuning generated during the quantization process described\nin the previous section.\n\n\nAccuracy Tuning Advice#\n\nYou can tune the model accuracy by adjusting the quantization method or the\ncomputation accuracy as follows:\n\n\nQuantization Method#\n\nYou can try to adjust the model quantization method by configuring different\nquantization methods, quantization parameter search methods, or by trying to\nconfigure the independent calibration functions:\n\n 1. Configure the calibration method\n    \n    You can try to adjust the model calibration method, such as kl, max, and\n    other calibration methods, the configuration method can be found in section\n    The quant_config Introduction.\n\n 2. Configure the quantization parameter search methods\n    \n    Two different granularity calibration parameter search methods are\n    supported:\n    \n    * modelwise_search: search for quantization parameters at the model level,\n      this method allows multiple calibration methods to be configured at once,\n      which will find a minimally quantization-loss calibration method by\n      comparing the quantization loss metric (configurable) of the model output\n      before and after quantization.\n    \n    * layerwise_search: search for quantization parameters at the node level,\n      this method calculates the quantization loss metric (configurable) based\n      on the model output before and after quantization for each node and\n      assigns the calibration method with the the minimally quantization loss to\n      that node.\n    \n    The configuration method can be found in section The quant_config\n    Introduction.\n\n 3. Independent Quantization Function Configuration\n    \n    Enable the independent quantization mode can reduce the computational\n    resource consumption, you can try to configure the parameters per_channel,\n    asymmetric, bias_correction, the configuration method can be found in\n    section The quant_config Introduction.\n\n\nComputation Accuracy#\n\nIn addition to the configuration of the quantization method, you can try to\nconfigure the computation accuracy (dtype) of the model operator to try to\naccuracy tuning, currently we support configuring the computation accuracy of\nthe operator at three levels: model, op_type, and op_name, and the supported\nconfiguration types include int8, int16, float16, and float32. The configuration\nmethod can be found in section The quant_config Introduction.\n\n\nAccuracy Debug Tool#\n\nIf you want to locate the exact operators that caused the loss of quantization\naccuracy, we also provide you with the accuracy debug tool to assist you in\nlocating them, which can be found in section Accuracy Debug Tool.\n\n\nQuantization Accuracy Tuning Flow#\n\nBased on our previous experience with typical model accuracy tuning processes,\nbelow we provide you with an accuracy tuning process that balances ease of use\nand practicality:\n\nThe tuning flowchart is described in detail below:","routePath":"/en/guide/ptq/ptq_usage/accuracy_tune","lang":"en","toc":[{"text":"Accuracy Tuning Advice","id":"accuracy-tuning-advice","depth":2,"charIndex":608},{"text":"Quantization Method","id":"quantization-method","depth":3,"charIndex":744},{"text":"Computation Accuracy","id":"computation-accuracy","depth":3,"charIndex":2464},{"text":"Accuracy Debug Tool","id":"accuracy-debug-tool","depth":2,"charIndex":2938},{"text":"Quantization Accuracy Tuning Flow","id":"quantization-accuracy-tuning-flow","depth":2,"charIndex":3183}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":135,"title":"Model Checking","content":"#\n\nTo ensure that the model can run smoothly and efficiently on the Horizon\nplatform, the operators used in the model need to conform to the operator\nconstraints of the platform. The OP Restriction section gives a list of the\nspecific operators we support, each with specific parameter constraints. For\nmore details, please refer to the Toolchain Operator Support Constraint\nList-ONNX Operator Support List section.\n\n\nUse the hb_compile Command to Check Your Model#\n\nConsidering the large number of operators supported by Horizon, we support you\nto check the details of each OP by using the hb_compile in order to save your\ntrouble of verifying each OP manually.\n\nPlease refer to section Model Checking for tool usage.\n\n\nException Handling#\n\nWhen the floating-point model check fails, the hb_compile tool will report an\nError message. A file named hb_compile.log will be generated in current\ndirectory to provide error details.","routePath":"/en/guide/ptq/ptq_usage/check_model","lang":"en","toc":[{"text":"Use the hb_compile Command to Check Your Model","id":"use-the-hb_compile-command-to-check-your-model","depth":2,"charIndex":417},{"text":"Exception Handling","id":"exception-handling","depth":2,"charIndex":720}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":136,"title":"Prepare Floating-point Model","content":"#\n\nBefore you read this section, we recommend that you read Toolchain Operator\nSupport Constraint List-ONNX Operator Support List section for an understanding\nof the operators and constraints supported by Horizon Robotics. Or, after you\nexport your ONNX model, you can first refer to the Check the Model section to\ncheck that the model can be deployed with Horizon support for normal conversion.\n\nA floating-point model trained on an open source DL framework is the input to\nthe conversion tool, which currently supports the following DL frameworks:\n\nFRAMEWORK         HORIZON'S TOOLCHAIN\nCaffe             Supported\nPyTorch           Supported (convert into ONNX)\nTensorFlow        Supported (convert into ONNX)\nTensorFlow Lite   Supported (convert into ONNX)\nPaddlePaddle      Supported (convert into ONNX)\nOthers            Contact Horizon\n\nAs shown above, the caffemodel exported from Caffe framework can be supported\ndirectly, While the models trained from the rest frameworks must be first\nconverted into ONNX before using the conversion tool. Presently Horizon supports\nONNX opset10-19.\n\n * For the Caffe model, you can first evaluate the floating-point model\n   accuracy, confirm the correct model weights and structure.\n\n * For the ONNX model, you need to preform the model inference using HBRuntime\n   to verify the inference results are consistent between ONNX and the original\n   DL framework model(i.e., make sure the model is legal).\n\nThere are standard solutions to convert models of different frameworks into\nONNX, refer to the following:\n\n * Pytorch2Onnx: PyTorch's official API can support exporting models into ONNX\n   models.\n   \n   Click here to see more:\n   https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n   .\n\n * Tensorflow2Onnx: Conversion based on the onnx/tensorflow-onnx of the ONNX\n   community.\n   \n   Click here to see more: https://github.com/onnx/tensorflow-onnx .\n\n * TensorflowLite2Onnx: Conversion based on the open-source tool tflite2onnx.\n   \n   Click here to see more: https://github.com/zhenhuaw-me/tflite2onnx .\n\n * PaddlePaddle2Onnx: PaddlePaddle's official API can support exporting models\n   into ONNX models.\n   \n   Click here to see more:\n   https://www.paddlepaddle.org.cn/documentation/docs/en/api/paddle/onnx/export_\n   en.html .\n\n * More solutions to convert models of other frameworks into ONNX, you can click\n   https://github.com/onnx/tutorials#converting-to-onnx-format .\n\nAttention\n\nOriginal model limitations: ir_version≤9, 10≤opset=≤19, for the correspondence\nbetween ir_version and onnx version, please refer to onnx official\ndocumentation.","routePath":"/en/guide/ptq/ptq_usage/model_prepare","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":137,"title":"Model Performance Analysis","content":"#\n\nThis section describes how to use the hb_compile tool provided by Horizon to\nevaluate the model performance.\n\n\nUse the hb_compile tool#\n\nThe hb_compile tool provided by Horizon supports the model conversion, for the\nusage of the tool and the related specific configurations and parameters, please\nrefer to the Model Quantized Compilation.\n\nAfter the model conversion completed, the model static performance evaluation\nfiles(the model.html with the better readability and the model.json) of the BPU\npart of the model predicted by the compiler will be generated under the\nworking_dir path specifed by the yaml file.\n\n\nCall the API Interface#\n\nYou can also call the API interface for model analysis. The reference commands\nare listed below:\n\n\n\nAttention\n\nPlease note that this model.hbm is for example only, for actual use, please\nreplace it with the correct path of the model you are using.\n\nUpon successful execution, basic information such as model FPS will be printed\nwithin the terminal, and at the same time, a static performance evaluation file\nfor the model will be generated in the directory where the API interface is\ncurrently called:\n\n\n\nYou can select either the model.html or the model.json to view the static\nperformance data for the BPU part.","routePath":"/en/guide/ptq/ptq_usage/performance_evaluation","lang":"en","toc":[{"text":"Use the `hb_compile` tool","id":"use-the-hb_compile-tool","depth":2,"charIndex":-1},{"text":"Call the API Interface","id":"call-the-api-interface","depth":2,"charIndex":618}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":138,"title":"Prepare Calibration Data","content":"#\n\nNote\n\nIf you need to do this process in the sample folder, you need to execute the\n00_init.sh script in the folder first to get the corresponding original model\nand dataset.\n\nWhen performing model calibration, 20~100 samples are required at the\ncalibration stage, each is an independent data file. To ensure the accuracy of\nthe calibrated models, these calibration samples better come from the training\nor validation dataset when training the models. In addition, please try NOT to\nuse rare samples, e.g. single colored images or those images who don't contain\nany detection or classification targets in them.\n\nYou need to preprocess the samples from the training/verification sets (the\npreprocessing process is the same as the original floating-point model data\npreprocessing process), and the calibrated samples after processing will have\nthe same data type (input_type_train), size (input_shape) and layout\n(input_layout _train) with the original floating-point model. You can save the\ndata as an npy file with the numpy.save command, and the toolchain will read it\nbased on the numpy.load command when it is calibrated. For example, there is an\nImageNet trained original classification floating-point model with only one\ninput node, it should be described as below:\n\n * Input type: BGR.\n * Input layout: NCHW.\n * Input size: 1x3x224x224.\n\nThe steps for data preprocessing of the original floating point model are as\nfollows:\n\n 1. Uniformly scale the image and resize the shorter side to 256.\n\n 2. Get 224x224 image using the center_crop method.\n\n 3. Align the input layout to the NCHW required by the model.\n\n 4. Convert the color space to the BGR required by the model.\n\n 5. Adjust the range of image values to [0, 255] as required by the model.\n\n 6. Subtract mean value by the channel.\n\n 7. Data multiple by the scale factor.\n\nThe sample processing code for the above example model is as follows (to avoid\nexcessive code length, some simple transformer implementation codes are ignored,\nthe usage of transformer can be found in Image Processing).\n\n\n\nAttention\n\nNote that the input_shape parameter in the yaml file serves to specify the input\ndata size of the original floating-point model. If it is a dynamic input model,\nyou can use this parameter to set the converted input size, and the shape size\nof the calibration data should be consistent with input_shape.\n\nFor example, if the original floating-point model input node shape is\n?x3x224x224 (\"?\" sign represents the placeholder, i.e., the first dimension of\nthe model is dynamic input), and the input_shape: 8x3x224x224 is set in the\nconversion profile, then the size of each calibration data that you need to\nprepare is 8x3x224x224 (Please be aware that the input_batch parameter does not\nsupport modifying the model batch information for models with the first\ndimension of the input shape not equal to 1).","routePath":"/en/guide/ptq/ptq_usage/prepare_calibration_data","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":139,"title":"Interpret Conversion Results","content":"Model Quantization and Compilation#\n\nThe conversion of the floating-point model to the Horizonboard-side deployable\nmodel will be completed in the Convert Model phase, after which you will get a\nmodel that can run on the Horizon computing platform. Before performing the\nconversion, make sure you have successfully passed the model check as described\nin the Check the Model section.\n\nDuring the conversion, some important procedures such as model optimization and\ncalibration quantization must prepare the data in line with model pre-processing\nrequirements. You can refer to Prepare Calibration Data section to prepare the\ncalibration data in advance.\n\n\nConvert the Model Using the hb_compile Tool#\n\nThe model conversion process is performed using the hb_compile tool, please\nrefer to section Model Quantized Compilation for the usage of the tool and the\nrelated specific configuration and parameters.\n\n\nModel Conversion Interpretation #\n\nModel conversion is completed from a floating-point model to a board-side\ndeployable model supported by Horizon's computing platform. To make this model\nrun quickly and efficiently on the embedded end, model conversion focuses mainly\non two phases: input data processing and model optimization compilation, and\nthis section will focus on these two problems in turn.\n\nIn terms of Input data processing, Horizon's edge computing platform can provide\nhardware-level solutions for specific types of input channels, but the output of\nthese solutions may not comply with the input requirements of your models. For\nexample, the video processing sub-systems for video channels have the abilities\nto crop and scale images or optimize the image quality. The output of these\nsub-systems are mostly in the YUV420 format, however, the algorithm models are\noften trained based on commonly-used image formats such as bgr/rgb. To solve\nthis problem, Horizon provides 2 kinds of input descriptions for each converted\nmodel: The one is used for the original floating-point model input\n(input_type_train and input_layout_train); while the other one is used for the\ninput data ( input_type_rt ) of the edge platform that you are going to use.\n\nFor the frequently-used image data pre-processing, such as mean and scale, the\nedge platform data formats such as yuv420 are no longer suitable for such\noperations, therefore, we integrate these common image pre-processing into the\nmodel. After the above two processes, the input part of the converted model will\nbe shown as follows\n\nThere are only 2 types of data layouts in the above diagram: NCHW and NHWC.\nWherein, N denotes quantity, C denotes channel, H denotes height and W denotes\nwidth. The two different layouts reflect different memory access\ncharacteristics. The NHWC layout are more often used by the TensorFlow models;\nwhile the NCHW layout is used by the Caffe models. Although Horizon's edge\nplatform doesn't restrict the data layout, there is still a requirement: the\ninput_layout_train must be consistent with the data layout of the original\nfloating-point model, as specifying correct data layout is the basis for smooth\ndata parsing.\n\nModel Optimization and Compilation: It includes several important steps,\nincluding model parsing, model optimization, model calibration and\nquantification, and model compilation, and its internal working process is shown\nin the figure below.\n\n * Model Parse Stage: It completes the conversion from Caffe floating-point\n   model to ONNX floating-point model. This stage will name the operator (with a\n   unique name) for the unnamed node/tensor, producing an\n   original_float_model.onnx, and the computing accuracy of this ONNX model is\n   float32.\n\n * Model Optimization Stage: It implements some operator optimization strategies\n   for the model that are applicable to the Horizon platform, such as BN fusion\n   to Conv, etc. The output of this phase is an optimized_float_model.onnx. The\n   computational accuracy of this ONNX model is still float32, which will not\n   affect the computational results of the model after optimization.\n\n * Model Calibration Stage: It uses the calibration data you provide to\n   calculate the necessary quantization parameters, and the quantization\n   parameters corresponding to each node calculated from the calibration data\n   will be saved in the calibration node. The output of this phase is a\n   calibrated_model.onnx. After model calibration, some processing will be\n   performed on the model as well, the output of this process is a\n   ptq_model.onnx.\n\n * Model Quantization Stage: It uses Horizon's model compiler, which uses the\n   generated model during the model calibration stage(ptq_model.onnx), to\n   perform model quantization according to your pre-processing configuration\n   (including the color conversion between input_type_rt to input_type_train,\n   the handling of mean/scale, etc). The output of this phase is a\n   quantized_model.bc. The loss of accuracy due to model quantization can be\n   evaluated using this model. If during model quantization there exists a\n   situation of removing nodes at the input/output, a quantized_removed_model.bc\n   will also be saved. In this scenario, we recommend you to use this HBIR model\n   to compare with the final generated hbm model if you need to do consistency\n   comparison later.\n\nAttention\n\nPlease note that if input_type_rt is nv12, the input layout of quantized.bc is\nNHWC.\n\n * Model Compilation Stage: It uses Horizon's model compiler to convert the\n   quantized model computational instructions and data supported by the Horizon\n   platform. The output of this stage is a *.hbm model, this hbm model is the\n   model that will be subsequently run on the Horizon Edge embedded platform,\n   which is the final output result of the model conversion.\n\n\nInterpret Conversion Results#\n\nThis section will introduce the interpretation of successful model conversion\nstatus and the analysis of unsuccessful conversions in turn. To confirm the\nsuccess of the model conversion, you need to check the compile status\ninformation, the similarity information and the working_dir output. For the\ncompile status information, after a successful conversion, the model's\ndependencies and parameters will be output on the console.\n\nSimilarity information will be printed in the console output after compile,\nwhich takes the following form:\n\n\n\nAs shown above:\n\n * The Node and NodeType represents node name and type.\n\n * The ON represents the node executed device, include BPU, CPU and JIT(first\n   generated BPU instructions by the CPU then BPU performs the computation).\n\n * The Threshold represents to the calibration threshold at each layer, which is\n   used to provide feedback to Horizon technical support in abnormal states and\n   is not of concern in normal conditions.\n\n * The Calibrated Cosine represents the cosine similarity between the outputs of\n   the optimized model (optimized_float_model.onnx) and the calibrated model\n   (calibrated_model.onnx) in the nodes indicated by the Node.\n\n * The Quantized Cosine represents the cosine similarity between the outputs of\n   the optimized model (optimized_float_model.onnx) and the quantized model\n   (quantized_model.bc) generated after model quantization in the nodes\n   indicated by the Node.\n\n * The Output Data type represents the node output data type, range with ['si8',\n   'si16', 'si32', 'si64', 'ui8', 'ui16', 'ui32', 'ui64', 'f32'].\n\nAttention\n\nNote that the cosine similarity field only serves as a reference to indicate the\nstability of the quantized data. It cannot directly tell the model accuracy\nloss. In general, there is a significant loss of accuracy if the similarity of\nthe output nodes is below 0.8. Of course, since there is no absolute direct\ncorrelation with accuracy, a fully accurate accuracy situation should be\ndescribed in Model Accuracy Analysis section.\n\nThe conversion output is stored in the path specified by the conversion\nconfiguration parameter working_dir. You can get the following files in this\ndirectory (* part is what you specify by the conversion configuration parameter\noutput_model_file_prefix).\n\n * *_original_float_model.onnx\n\n * *_optimized_float_model.onnx\n\n * *_calibrated_model.onnx\n\n * *_ptq_model.onnx\n\n * *_quantized_model.bc\n\n * *_quantized_removed_model.bc(exist a situation of removing nodes at the\n   input/output)\n\n * *.hbm\n\n * *_advice.json\n\n * *_quant_info.json\n\n * *_node_info.csv\n\n * *.html\n\n * *.json\n\nNote\n\nThe * indicates the model file prefix you specify via the\noutput_model_file_prefix parameter.\n\nThe Interpret Conversion Output section explains the function of each model\noutput. However, before running on the board, we strongly recommend you to\nproceed the procedures as described in the sections Model Performance Analysis\nand Model Accuracy Analysis , to avoid extending the model conversion problem to\nthe subsequent embedded terminal.\n\nIf any of the above-mentioned 3 outputs of verifying the success of the model\nconversion is missing, there must be something wrong with the conversion. In\nsuch cases, the compile tool will output error messages to your console in case\nof errors. For example, if we do not configure the prototxt and caffe_model\nparameters during the Caffe model conversion, the tool gives the following\nmessage:\n\n\n\n\nInterpret Conversion Output #\n\nThe outputs of the successful conversion of the model mentioned above. This\nsection explains the use of each output.\n\n * The output process of *_original_float_model.onnx can be found in Model\n   Conversion Interpretation. The computing accuracy of this model is the same\n   as the original floating-point model. In general, you don't need to use this\n   model. In case of errors in the conversion results, it would be helpful to\n   provide this model to Horizon's technical support to help you solve the\n   problem quickly.\n\n * The output process of *_optimized_float_model.onnx can be found in Model\n   Conversion Interpretation. This model undergoes some operator-level\n   optimization operations, commonly known as operator fusion. You can visually\n   compare it with the original_float model, and clearly find out some operator\n   structural changes, which will not affect the computational accuracy of the\n   model. In general, you do not need to use this model. In case of errors in\n   the conversion results, it would be helpful to provide this model to\n   Horizon's technical support to help you solve the problem quickly.\n\n * The output process of *_calibrated_model.onnx can be found in Model\n   Conversion Interpretation. This model is an intermediate product obtained by\n   the model transformation tool chain by taking the floating-point model after\n   structural optimization, calculating the quantization parameters\n   corresponding to each node from the calibration data and saving them in the\n   calibration node.\n\n * The output process of *_ptq_model.onnx can be found in Model Conversion\n   Interpretation. This model is the product of pre-quantization of the model\n   obtained from calibration by the model conversion toolchain.\n\n * The output process of the *_quantized_model.bc can be found in Model\n   Conversion Interpretation. This model has completed the calibration and\n   quantization process, and the quantized accuracy loss can be viewed here.\n   This model is a mandatory model in the accuracy verification process, please\n   refer to the introduction of Model Accuracy Analysis.\n\n * The output process of the *_quantized_removed_model.bc can be found in Model\n   Conversion Interpretation. If during model quantization there exists a\n   situation of removing nodes at the input/output, this removed node's HBIR\n   model will be automatically saved. In this scenario, we recommend you to use\n   this HBIR model to compare with the final generated hbm model if you need to\n   do consistency comparison later.\n\n * The *.hbm is the model that can be used to load and run on the Horizon\n   computing platform. After reading Embedded Application Development. You can\n   then deploy the model to run on the computing platform quickly. However, to\n   ensure that the performance and accuracy of the model is as good as you\n   expect, we strongly recommend completing the Model Performance Analysis and\n   Model Accuracy Analysis , before moving on to application development and\n   development.\n\n * The *_advice.json file contains the results printed by the Horizon Model\n   Compiler op checker.\n\n * The *_quant_info.json file contains the calibrated quantization information\n   of the operators.\n\n * The *_node_info.csv file contains the result of the cosine similarity and\n   other information of the operator after successful conversion, which is the\n   same as the similarity information output in the console after successful\n   execution of hb_compile.\n\n * The *.json is the model static performance evaluation file.\n\n * The *.html is the model static performance evaluation file (better\n   readability).","routePath":"/en/guide/ptq/ptq_usage/quantize_compile","lang":"en","toc":[{"text":"Convert the Model Using the hb_compile Tool","id":"convert-the-model-using-the-hb_compile-tool","depth":2,"charIndex":654},{"text":"Model Conversion Interpretation","id":"model-conversion-interpretation","depth":2,"charIndex":-1},{"text":"Interpret Conversion Output","id":"interpret-conversion-output","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":140,"title":"PTQ Conversion Principle and Process","content":"#\n\n\nOverview#\n\nThe process of the model from training to conversion to running on the\ndevelopment board is shown below:\n\n\n\nModel Training: It is the process of getting a usable model by using public deep\nlearning frameworks such as TensorFlow, PyTorch, Caffe, etc. The trained model\nwill serve as the input for the model conversion stage. The toolchain does not\nprovide training-related libraries or tools. For the detailed supported public\nlearning frameworks, please refer to the instructions in the Floating-point\nModel Preparation.\n\nModel Conversion: Taking the floating-point model obtained from model training\nas input, this stage transforms floating-point models into models that can run\nefficiently on the Horizon computing platform through important steps such as\nmodel structure optimization, model calibration quantization and model\ncompilation. To verify the usability of the heterogeneous model, the toolchain\nalso provides you with performance analysis, accuracy analysis, and a rich set\nof exception debugging tools and recommendations. For more information, please\nrefer to Model Quantization and Compilation section.\n\nEmbedded Application development: The toolchain supports application development\nin both X86 emulation environment and real embedded environment. In case you are\nnot convenient to use the development board, you can debug the program and\nverify the calculation results in the emulation environment. In order to reduce\nthe cost of simulation verification, the toolchain provides the exact same\nsimulation library interface as the embedded interface, only with different\ncompilation configurations. For more information, please refer to Embedded\nApplication Development section.\n\n\nPTQ Conversion Process#\n\nThe complete the model development process with the Horizon toolchain involves\nfive important stages: Floating-point Model Preparation, Model Checking, Model\nConversion, Performance Evaluation, and Accuracy Evaluation, as shown in the\nfigure below.\n\nThe Floating-point model, as the output of the Floating-point Model Preparation\nstage, will serve as the input of the model conversion tool. The floating-point\nmodel is usually trained on basis of some open source deep learning frameworks.\nNote that the model must be exported to a format supported by Horizon Robotics.\nFor more information, please refer to the Floating-point Model Preparation.\n\nThe Model Checking stage is used to ensure that the model is computing platform\ncompliant. Horizon Robotics provides specified tools to complete model\nvalidation, and for non-compliance, such tools will explicitly give you the\nspecific operator information for the non-compliance, so that you can easily\nadjust the model with the description of the operator constraints. For more\ninformation, please refer to the Model Checking.\n\nThe Model Conversion stage converts the floating-point model to the board-side\ndeployable model supported by Horizon Robotics. To run models efficiently on the\nHorizon computing platform, critical steps such as model optimization,\nquantization, and compilation are completed by Horizon's model conversion tools.\nHorizon's model quantization method has undergone long-term technological and\nproduction validation, and can guarantee an accuracy loss of less than 1% on\nmost typical deep learning models. For more details about model conversion\nplease refer to the Prepare Calibration Data and Model Quantization and\nCompilation.\n\nThe Performance Evaluation stage contains a series of tools to evaluate the\nmodel performance. Before deploying your application, you can use these tools to\nverify that the model performance meets application requirements. For some cases\nwhere the performance is not as good as expected, you can optimize the models\nbased on Horizon's model optimization advices. For more information, please\nrefer to the Model Performance Analysis.\n\nThe Accuracy Evaluation stage contains a series of tools to evaluate the\naccuracy of the model. In most cases, Horizon's converted-models can maintain\nalmost the same accuracy as the original floating-point model. Before\napplication deployment, you can use these tools to verify that the accuracy of\nthe model meets the expectations. For some cases where the accuracy is not as\ngood as expected, you can optimize the models based on Horizon's model\noptimization advices. For more information about evaluation, please refer to the\nModel Accuracy Analysis.\n\nAttention\n * In general, you can get a qualified runtime model after model conversion.\n   However, make sure that the performance and accuracy of the model are in line\n   with the application requirements. Horizon Robotics strongly suggest you that\n   each conversion shall be followed by the evaluation steps of performance and\n   accuracy.\n * The model conversion process will generate onnx models and bc model, which\n   are intermediate products and only facilitate you to verify the accuracy of\n   the model. Therefore, the compatibility between versions is not guaranteed.\n   When using the evaluation script in the example to evaluate the onnx model in\n   a single image or on a test set, please use the onnx model generated by the\n   current version of the tool.\n\n\nModel Conversion Process Detailed Explanation#\n\nSo how to convert the floating-point models trained by using the opensource ML\nframeworks (such as Caffe, TensorFlow, PyTorch, etc.) to the Horizon hardware\nsupported fixed-point models?\n\nIn most cases, the threshold values and weights of the floating-point models\nobtained from either the opensource ML frameworks or trained by yourself are\nfloating-point numbers (float32) and each number occupies 4 bytes.\n\nHowever, by converting the floating-point numbers to fixed-point numbers (int8),\neach number occupies only 1 byte, thus the computation operations in the\nembedded runtime can be dramatically reduced.\n\nTherefore, it brings significant performance boost by converting the\nfloating-point models to fixed-point models with no loss or very small loss.\n\nTypically, model conversion can be divided into the following steps:\n\n 1. Check if there are unsupported OPs in the models to be converted.\n\n 2. Prepare 20~100 images for calibration use at the conversion stage.\n\n 3. Convert the floating-point models to fixed-point models using the\n    floating-point conversion tools.\n\n 4. Evaluate the performance and accuracy of the converted models to ensure that\n    there isn't huge difference in model accuracy before and after the\n    conversion.\n\n 5. Run models in simulator/dev board to validate model performance and\n    accuracy.\n\n\nModel Checking#\n\nNote\n\nIf you need to do this process in the sample folder, you need to execute the\n00_init.sh script in the folder first to get the corresponding original model\nand dataset.\n\nBefore converting the floating-point models into the fixed-point models, we\nshould check if there are Horizon hardwares unsupported OPs in the\nfloating-point models using the hb_compile tool. If yes, the tool will report\nthe unsupported OP(s). The usage of the hb_compile tool for checking the model\ncan be found in Model Checking section.\n\nTip\n\nMore information about Horizon hardware supported OPs, refer to Toolchain\nOperator Support Constraint List-ONNX Operator Support List.\n\n\nCalibration Image Preparation#\n\nNote\n\nIf you need to do this process in the sample folder, you need to execute the\n00_init.sh script in the folder first to get the corresponding original model\nand dataset.\n\nWhen converting the floating-point models, you need to prepare 20~100 images for\ncalibration use at the calibration stage.\n\nInput image formats may vary by input type and layout. In this stage, because\nboth original (JPG, etc.) and the processed images are valid, you can either\nfeed the calibration images used in the model training or feed your own\nprocessed images.\n\nNote\n\nThe format of the calibration dataset should be npy .\n\nWe recommend you preprocessing the calibration images on your own, you need to\ncomplete the operations such as image channel (BGR/RGB), data layout\n(NHWC/NCHW), and image resizing/padding (Resize&Padding). The tool will then\nfeed the images to the calibration stage after loading them as npy format.\n\nTaking Resnet50 as an example, the required transformer operations are as\nfollows:\n\n\n\n\nModel Conversion#\n\nWhen you confirm that the floating-point model can be successfully converted,\nyou can then convert the floating-point model to a Horizon hardware supported\nfixed-point model by using the hb_compile tool.\n\nThis process requires you to pass a configuration file(*.yaml) containing\nconversion requirements. For specific configuration file settings and the\ninsturctions of each parameter, refer to the descriptions in sections Specific\nParameter Information and Configuration File Template .\n\nWhen the model conversion process ends, it also prints the level of similarity\nbetween the floating-point model and fixed-point model to the log, you can\ntherefore judge the similarity before and after conversion according to the\nQuantized Cosine field.\n\nIf the value of Quantized Cosine is very close to 1, so the performance of the\nfixed-point model should be very close to that of the floating-point model\nbefore the conversion.\n\nNote\n\nThe CosineSimilarity in the log refers to the very first image in the\ncalibration images, it cannot fully represent the model accuracy before and\nafter the conversion.\n\nAfter a successful model conversion, the model outputs and information files for\neach phase will be generated in the generated folder (model_output by default),\nwhere the model output files will be used in subsequent phases.\n\nNote\n * You can use the 03_classification/03_resnet50/03_build.sh script to\n   experience the hb_compile tool doing the model quantized compilation.\n * If you want to learn more about the model conversion workflow, please read\n   Model Quantification and Compilation .\n\n\nSingle Image Inference#\n\nThe accuracy of the fixed-point model generated by the floating-point conversion\nmust be evaluated.\n\nYou should have good understanding of the input/output structures of the model.\nYou should also be able to accurately preprocess the input images of the model,\npostprocess the model outputs, and write the model execution scripts on your\nown.\n\nYou can refer to the sample code in 04_inference.sh of the sample in Horizon\nmodel convert sample package.\n\nThe code logic is as follows:\n\n\n\nOnce the script used, you can verify its own accuracy by inputting a single\nimage.\n\nFor example, the input to this script is a picture of a zebra, preprocessing the\nimage data from rgb to the data type configured by input_type_rt(for informaiton\nabout intermediate types, refer to the Model Conversion Interpretation ). Then,\ninfer the model by passing the above image data by using the HB_HBIRRuntime\ncommand, post-processing after inference, and finally print out 5 of its most\nlikely types.\n\nThe output of the script is shown as follows with the most possible class being\nlabel: 340:\n\n\n\nlabel uses the ImageNet label classes, where the corresponding class of 340 is\nzebra, so the inference result is correct.\n\n\nModel Accuracy Evaluations#\n\nIt's insufficient to determine the model accuracy by single image inference, so\nyou still need to use scripts to evalute the model accuracy after the\nconversion.\n\nTo do so, you need some coding work to enable the model to loop the image\ninference and compare the inference results with standard results to get model\naccuracy results.\n\nIn model accuracy evaluations, images must be pre-processed and the model output\nmust be post-processed, so here we provide a Python script as a sample.\n\nThe logic of this script is the same as that of single image inference, yet it\nmust run on the entire dataset.\n\nThe script can evaluate the model output results and generate evaluation\nresults.\n\nBecause it takes a long time to run the script, you can set the number of\nthreads to run the evaluation by specifying the PARALLEL_PROCESS_NUM environment\nvariable.\n\nAfter the execution of this script, you can get the accuracy of the converted\nfixed-point model from the output of the script.\n\nNote\n * Model accuracy may vary slightly due to the differences of operating systems\n   and dependencies.\n * Model accuracy may vary slightly by iteration.\n\n\n[Reference] Supported Calibration Methods#\n\nWe currently support the following calibration methods:\n\n 1. Default\n\nDefault is a strategy that automatically searches the calibrated quantization\nparameters to obtain a relatively good combination.\n\n 2. Mix\n\nMix is a search strategy that integrates multiple calibration methods, which\nautomatically identifies quantization-sensitive nodes, selects the best from a\ngroup of calibration methods at node granularity, and finally build a hybrid\ncalibration method absorbing the advantages of multiple calibration methods.\n\n 3. KL\n\nKL learns from the Solution proposed by TensorRT, uses the KL entropy value to\ntraverse the data distribution of each quantized layer, and determines threshold\nvalue by searching for the lowest KL entropy value.\n\nAs this method can cause more data saturation and smaller data quantization\ngranularity, it more suitable than max for those neural network models with more\nconcentrated data distribution.\n\n 4. Max\n\nMax refers to a calibration method that automatically selects the max value in\nquantized layer as the threshold.\n\nThis method can cause oversized quantization granularity; however, it also\ncauses less saturated points than the KL method, which makes it suitable for\nthose neural network models with more discrete data distribution.\n\n\n[Reference] OP List#\n\nFor more information about the operators and corresponding constraints currently\nsupported by Horizon Algorithm Toolchain, please refer to Toolchain Operator\nSupport Constraint List.","routePath":"/en/guide/ptq/ptq_workflow","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"PTQ Conversion Process","id":"ptq-conversion-process","depth":2,"charIndex":1712},{"text":"Model Conversion Process Detailed Explanation","id":"model-conversion-process-detailed-explanation","depth":2,"charIndex":5204},{"text":"Model Checking","id":"model-checking","depth":3,"charIndex":6588},{"text":"Calibration Image Preparation","id":"calibration-image-preparation","depth":3,"charIndex":7263},{"text":"Model Conversion","id":"model-conversion","depth":3,"charIndex":8289},{"text":"Single Image Inference","id":"single-image-inference","depth":3,"charIndex":9902},{"text":"Model Accuracy Evaluations","id":"model-accuracy-evaluations","depth":3,"charIndex":11126},{"text":"[Reference] Supported Calibration Methods","id":"reference-supported-calibration-methods","depth":3,"charIndex":12291},{"text":"[Reference] OP List","id":"reference-op-list","depth":3,"charIndex":13610}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":141,"title":"Object Detection Full Process Sample","content":"#\n\nThe OE package provides a full process object detection sample in the\nsamples/ucp_tutorial/all-round path. This sample shows how to run a detection\nmodel on J6 and display the results.\n\n\nProgram Principle#\n\nThis sample includes CameraInputModule, InferenceModule, PostProcessModule,\nCodecModule and WebDisplayModule, it uses the producer + consumer model to\nconnect the modules in series. The relationship is shown in the following\nfigure:\n\n * CameraInputModule: Get the pyramid image passed in from the sensor.\n * InferenceModule: Select a layer of the pyramid image and call the UCP model\n   inference interface to perform the detection task.\n * PostProcessModule: Post-process the predicted results to obtain detection\n   results.\n * CodecModule: Get the pyramid image and encode it as jpg format data output.\n * WebDisplayModule: Get the intelligent results and encoded jpg data, and send\n   them to the web for display via websocket.\n\n\nCompile and Deploy#\n\n\n\nAfter the above command is executed, a deploy directory will be generated and\ncopied to the J6 board.\n\n\nRun#\n\nEnter the deploy folder and execute the following command:\n\n\n\nx indicates the debug level of the log, which supports i/d/w/e.\n\n * i: Print info information to show the time and FPS of each module. The\n   default setting is i.\n * d: Print debug information.\n * w: Print warning information.\n * e: Print error information.\n\nThe default mode is camera mode. To re-run the system, modify the configuration\nfile.\n\n\nCamera Supports#\n\nCurrently supports OVX8b (Fov120) camera.\n\n\nHow to Use#\n\nEnter the deploy folder and modify the post-processing configuration file to\nadjust the detection effect:\n\n\n\nRun the full process sample:\n\n\n\nOpen the browser and visit http://IP. IP is the address of J6 board. Click the\nweb display link to view the detection effect.","routePath":"/en/guide/ucp/full_process_sample","lang":"en","toc":[{"text":"Program Principle","id":"program-principle","depth":2,"charIndex":189},{"text":"Compile and Deploy","id":"compile-and-deploy","depth":2,"charIndex":943},{"text":"Run","id":"run","depth":2,"charIndex":1070},{"text":"Camera Supports","id":"camera-supports","depth":2,"charIndex":1486},{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":1548}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":142,"title":"HPL Introduction","content":"#\n\nThe HPL module mainly works in the pre- and post-processing stages of the model\nin the NN-centered computing scheme.\n\nIn the horizon UCP, various hardware have been equipped with image processing\noperators, and the HPL module encapsulates the hardware calling related to image\nprocessing, and selects different hardware solutions by setting the backend (if\nyou don't specify the backend, the UCP will automatically adapt the processing\nunit with a lower load), which balances the load of the development board, fully\nexploits the board's computing power, and avoids the trouble of the difference\nbetween the different hardware calling, so that you can pay more attention to\nthe software functions.\n\nAs shown in the above picture, through the task constructor fuctions of the\noperators provided by the HPL module, such as hbFFT1D, hbIFFT1D, etc., the\napplication generates the task handle of the corresponding operator. UCP\nprovides the Service that contains modules such as task scheduling, session\nmanagement, engine management, etc. After the task handles of the corresponding\noperators are generated, the operator tasks are committed to the task queue\nthrough the UCP task scheduling interface and allocated to different underlying\nhardware to realize the functional logic of the operator. The bottom layer is\nthe function of the operators implemented encapsulated in the different\nprocessing units.\n\nBefore reading this section, please clarify the following basic concepts, which\nmay be mentioned several times in the following sections:\n\n * High Performance Library, refers to the library of encapsulated\n   high-performance operator library.\n\n * Operator, refers to the high-performance functions contained in the operator\n   library.\n\n * DSP, the whole name Digital Signal Processing, refers to the Digital Signal\n   Processing.\n\n * Backends, refers to the allocatable processing units in the UCP framework.\n\n * Kernel, refers to the convolutional kernel parameter in the operator.","routePath":"/en/guide/ucp/hpl/hpl1_introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":143,"title":"Quickstart","content":"#\n\nThis section introduces how to use the sample of HPL module in UCP sample\npackage ucp_tutorial, and how to configure the development environment, compile\nand run the sample application code, which helps to get started with the HPL\nfunction module in UCP quickly. The main architecture is as follows:\n\n\nSample Package Usage#\n\nThe structure of the sample package is shown as below:\n\n\n\nThe HPL samples located under the ucp_tutorial/hpl folder, including Fast\nFourier Transform sample, which can be compiled by both board-side running and\nx86 emulation. For details, please refer to the section HPL Sample.\n\n\nCompile Sample Operator#\n\nBefore compiling and running the sample application code, you need to ensure\nthat the environment meets the requirements, according to the guidelines in\nsection Environment Deployment, your development machine should already have the\nrelevant environment installed, the requirements are as follows:\n\n * cmake >= 3.0.\n\n * For board-side compilation, you need to specify the cross-compile toolchain,\n   for x86 emulation docker, you can use the compiler that comes with it.\n\nIn the ucp_tutorial/hpl/code directory of the sample, there is a pre-configured\nbuild script build.sh, with the options -a x86 and -a aarch64 to support two\ntypes of builds respectively, and executing the build.sh script directly will\ncomplete the one-click build, and the generated files will be saved to the\nucp_tutorial/hpl/hpl_samples directory. Moreover, the directory encompasses two\ncompilation scripts, namely build_aarch64.sh and build_x86.sh, tailored to\ndistinct compilation configurations. Employing these scripts mirrors the\nfunctionality of the build.sh script. The command you need to execute to compile\nthe HPL module running on x86 simulation is as follows:\n\n\n\nAfter executing the compilation script, the executable programs and dependency\nfiles required to run the samples will be generated and saved in the\nucp_tutorial/hpl/hpl_samples directory.\n\nTaking the HPL as a sample, its generated objects are shown below, containing\nimage data, sample running scripts, running dependency libraries, executable\nfiles, and script directories for running the samples, which together form a\ncomplete running environment and running dependencies.\n\n\n\n\nRun Sample#\n\nAfter all the steps of compiling are completed correctly, the executable samples\nwill be configured and saved in the hpl_samples folder. Depending on the\nexecution environment, the two ways of commands for executing the samples on the\nboard and simulation are introduced as follows.\n\nRun on Board#\n\nCopy the entire hpl_samples folder to the development board, and go to the\nhpl_samples/script folder and execute the provided running script in sample\nfolder directly to see the results of the sample run. The reference command for\nexecuting the script as follows:\n\n\n\nRun on x86 Emulation#\n\nGo to the hpl_samples/script_x86 folder and execute the provided run script in\nsample folder directly to see the results of the sample run. The reference\ncommand for executing the script as follows:\n\n\n\nNote\n\nThe Horizon J6 SOC uses the Tensilica Vision Q8 DSP from Cadence, so the dsp\noperators running in the x86 simulation sample relies on a set of toolchain\nprovided by Cadence. The environment configuration can be found in the\nguidelines in section Install DSP Toolchain And Configure Core. The correct\nconfiguration of License and environment variable XTENSA_ROOT is required.\n\n\nOutput Description#\n\nTake the sample performing on x86 emulator as an example, when the sample is\nrunning, the process log will be printed on the console and the corresponding\noutput file will be generated. The log will contain the flow of all the operator\ncallings, and the output will be saved in the data folder. The output of the\nsample section as follows:\n\n\n\nThe generation will be saved to the hpl_samples/data directory with the\nfollowing contents:\n\n\n\n\nUsage of HPL Operator#\n\nThis section shows how to implement the Fast Fourier Transform using\nHPL-encapsulated operators with a simple operator calling. The main steps\ninclude data loading, task creation, task commit, task completion, task\ndestruction, save output and so on. You can read the corresponding source code\nand comments to learn.\n\nThe role of the sample is to perform an FFT transform of the input data using\nthe hbFFT1D operator, which is implemented as follows:\n\n","routePath":"/en/guide/ucp/hpl/hpl2_quick_start","lang":"en","toc":[{"text":"Sample Package Usage","id":"sample-package-usage","depth":2,"charIndex":304},{"text":"Compile Sample Operator","id":"compile-sample-operator","depth":3,"charIndex":608},{"text":"Run Sample","id":"run-sample","depth":3,"charIndex":2265},{"text":"Run on Board","id":"run-on-board","depth":4,"charIndex":2562},{"text":"Run on x86 Emulation","id":"run-on-x86-emulation","depth":4,"charIndex":2844},{"text":"Output Description","id":"output-description","depth":3,"charIndex":3452},{"text":"Usage of HPL Operator","id":"usage-of-hpl-operator","depth":2,"charIndex":3912}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":144,"title":"Module Architecture","content":"#\n\n\nHPL Module Architecture#\n\nThe task handles for the operators are created through the API in the HPL\nmodule, and each operator interface has at least one hardware implementation,\nsuch that the fft operator can be executed on DSP hardware and the corresponding\nbackend includes the DSP in the UCP. After the task handle has been created, you\ncan be deploy the corresponding processing unit by specifying the backend,\nexecution mode, and other parameters through the scheduling parameters. In the\nentire process from task creation to task committing and releasing, the UCP\nservice layer provides interfaces and functional support for each step.\n\n\nOperator Execution Process#\n\nThe FFT_1D operator is used here as an example to show the actual calling method\nand flow of the operator, and the usage of other operators is basically the same\nas its flow.\n\n\n\nNote\n\nThe dashed box indicates that the step is non-required, which can be omitted by\nreusing parameters, using default parameters, etc.\n\n 1. Prepare the input and output data: that is applying memory for the data and\n    constructing the associated description.\n\n 2. Create an operator task: this step calls the operator task interface\n    directly, while pass the parameters required for the execution of the\n    operator, the UCP task handle will be output after the execution is\n    completed.\n\n 3. Commit task: commit operator tasks to different processing cores by passing\n    in scheduling parameters, task committing supports specifying the backend,\n    if you do not specify the backend, the system will automatically adapt.\n\n 4. Specify the interface to wait for the end of the task: at the end of the\n    task, the system will return different return values according to the\n    execution status. At this point, you can view the results of the task\n    execution based on the return value.\n\n 5. Destroy the task: after the successful execution of the task, you need to\n    destroy the task and free the applied memory.","routePath":"/en/guide/ucp/hpl/hpl3_architecture","lang":"en","toc":[{"text":"HPL Module Architecture","id":"hpl-module-architecture","depth":2,"charIndex":3},{"text":"Operator Execution Process","id":"operator-execution-process","depth":2,"charIndex":647}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":145,"title":"Performance Indicator","content":"#\n\nThis section provides you with HPL performance test instructions and shows you\nthe performance data result statistics for reference.\n\n\nTest Platform#\n\nTest conditions (hardware and software platform requirements):\n\n * Test development board: J6 development board.\n * OpenCV version: 3.4.5.\n * OpenCV runtime environment: A78 single-threaded.\n\n\nTest Method#\n\n * Performance test method: Since the time consumed to run the operator will\n   fluctuate, here we take the average performance data of multi runs of the\n   operator.\n * Test time with steady_clock.\n\n\n\n * Data description:\n   \n   * Task time: Operator task execution time.\n   * Baseline Time: OpenCV execution time.\n   * Ratio: Baseline Avg / Task Avg.\n\n\nPerformance Comparison#\n\nNote\n\nThe number of FFT points used in the test is 1024 points, and the amount of data\nused in the FFT 1D test is 20,480,000 floating point numbers.\n\nALGORITHM   HIGH PERFORMANCE LIBRARY(US)   OPENCV 3.4.5 A78(US)   RATIO(DEFAULT DSP)\nfft1d       88584                          1108364                12.5\nifft1d      68325                          1154695                16.9","routePath":"/en/guide/ucp/hpl/hpl4_performance","lang":"en","toc":[{"text":"Test Platform","id":"test-platform","depth":2,"charIndex":137},{"text":"Test Method","id":"test-method","depth":2,"charIndex":346},{"text":"Performance Comparison","id":"performance-comparison","depth":2,"charIndex":715}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":146,"title":"Fast Fourier Transform","content":"#\n\nFast Fourier Transform (hereafter referred to FFT) is a fast algorithm for\nDiscrete Fourier Transform (hereinafter referred to DFT), which is one of the\nmost basic methods in the analysis of time-domain a frequency-domain transform.\nThe algorithm enables interconversion of data from the time domain to the\nfrequency domain, which provides another dimension of data processing method\nsupport. In engineering practices, DFT algorithms cannot be widely implemented\nin reality due to the computationally excessive. The FFT algorithm optimizes the\ncomputation to a practicable order of magnitude by improving the computing\nmethod of the DFT, and the discrete Fourier transform is really widely used in\nthe engineering field.\n\n\nOperator Effect#\n\nTIME DOMAIN INPUT DATA   PARAMETER                                               FREQUENCY DOMAIN OUTPUT DATA\n                         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n                         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n                         numDimensionSize = 1\n\n\nPrinciple #\n\nFFT is an improved method based on the DFT algorithm, which is equivalent to DFT\nin terms of computational results and optimizes the computational process. If\nF(n) is the discrete Fourier transform of f(n), the DFT formula is expressed as\nfollows:\n\n\n\nThe rotation factor is $W_N^=e^$ , and N is the input sequence length.\n\nIDFT is the inverse of DFT which is expressed as follows:\n\n\n\nAnother fundamental formula is Euler's formula:\n\n\n\nFrom Euler's formula is deduced that the rotation factor has the following\nproperties:\n\n\n\n\n\n\n\nFor the N-point DFT formula, the formula can be decomposed in parity order as\nfollows:\n\n\n\n\n\n\n\nThe decomposition effect is shown in the figure, bisecting the overall\ncomputational task layer by layer.\n\nAs shown above, the FFT tasks can all be decomposed into single-point\ncomputational tasks when N is a power of two.\n\nFrom Euler's formula, $W_N^$ can be written in the following form, which shows\nthat $W_N^$ has a periodicity and symmetry associated with N.\n\n\n\nAccording to this property one obtains $W_N^=W_N^$ , where $n$ takes the range\n$[0, N/2)$ .\n\nTaking the FFT operation with N of 8 as an example, the specific decomposition\nprocess as follows:\n\n 1. Split 8 points into 4 points\n\n 2. Split 4 points into 2 points\n\n 3. Calculate the single-point FFT\n\nIn which, the computational process of interleaving the nodes two by two is\nknown as butterfly operation as follows:\n\nThe specific formula as follows:\n\n\n\n\n\nIn the above figure, it can be seen that there is a bit-reversal relationship\nbetween F(n) and f(n) for n. That is, in FFT, the indices of the input and\noutput parameters are bit-reversals of each other,providing better support for\noptimized computational efficiency.\n\nThe calculation process of IFFT is similar to that of FFT, which can be derived\nbased on the IDFT formula.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbFFT1D.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/hpl/hpl5_op/op_fft","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":725},{"text":"Principle","id":"principle","depth":2,"charIndex":-1},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":2897},{"text":"Usage","id":"usage","depth":2,"charIndex":2978}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":147,"title":"Fast Fourier Transform 2D","content":"#\n\nFast Fourier Transform 2D (hereafter referred to FFT2D) is a fast algorithm\nbased on fast Fourier transform (hereafter referred to FFT), which converts\ntwo-dimensional data from time domain to frequency domain. This algorithm can\nconvert 2D data from time domain to frequency domain, and provide basic support\nfor signal processing in frequency domain. It is usually used as the pre-step\nfor frequency domain analysis of 2D data such as image.\n\n\nOperator Effect#\n\nTIME DOMAIN INPUT DATA   PARAMETER                                               FREQUENCY DOMAIN OUTPUT DATA\n                         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n                         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n                         numDimensionSize = 2\n\n\nPrinciple #\n\nFFT2D is an extension method developed based on the FFT algorithm. It conducts\nFFT processing separately on two-dimensional data along both the x and y axes,\nthereby mapping time-domain data to frequency-domain data. The specific\nprocedure is as follows:\n\n 1. Perform FFT calculation on each row of the two-dimensional data, where nx\n    represents the number of FFT points set in the interface parameters for the\n    x-axis.\n\n 2. Building upon the processing in step 1, perform FFT calculation on each\n    column of the two-dimensional data, where ny represents the number of FFT\n    points set in the interface parameters for the y-axis.\n\n 3. Return the results of the calculations from step 2.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbFFT2D。\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/hpl/hpl5_op/op_fft_2d","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":448},{"text":"Principle","id":"principle","depth":2,"charIndex":-1},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1497},{"text":"Usage","id":"usage","depth":2,"charIndex":1578}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":148,"title":"Inverse Fast Fourier Transform","content":"#\n\nInverse Fast Fourier Transform (hereinafter referred to as IFFT) is a fast\nalgorithm of Inverse Discrete Fourier Transform (hereinafter referred to as\nIDFT). It is one of the most basic methods in time domain and frequency domain\ntransform analysis. The algorithm enables interconversion of data from frequency\ndomain to time domain.\n\n\nOperator Effect#\n\nTIME DOMAIN INPUT DATA   PARAMETER                                               FREQUENCY DOMAIN OUTPUT DATA\n                         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n                         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n                         numDimensionSize = 1\n\n\nPrinciple#\n\nPlease refer to the section FFT section principle.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbIFFT1D.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/hpl/hpl5_op/op_ifft","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":338},{"text":"Principle","id":"principle","depth":2,"charIndex":675},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":740},{"text":"Usage","id":"usage","depth":2,"charIndex":822}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":149,"title":"Inverse Fast Fourier Transform 2D","content":"#\n\nThe Inverse Fast Fourier Transform 2D (hereinafter referred to as IFFT2D) is a\nfast algorithm derived from the Inverse Fast Fourier Transform (hereinafter\nreferred to as IFFT). It efficiently converts two-dimensional data from the\nfrequency domain to the time domain, commonly employed for the restoration of\nimages and other two-dimensional frequency domain data to their original time\ndomain representation.\n\n\nOperator Effect#\n\nTIME DOMAIN INPUT DATA   PARAMETER                                               FREQUENCY DOMAIN OUTPUT DATA\n                         p_size = HB_HPL_FFT16 normalize = 0 dataType =          \n                         HB_HPL_DATA_TYPE_I16 imFormat = HB_IM_FORMAT_SEPARATE\n                         numDimensionSize = 2\n\n\nPrinciple #\n\nIFFT2D is an extension of the IFFT algorithm, designed to map frequency domain\ndata to time domain data. It operates on two-dimensional data by performing IFFT\noperations separately along the y and x directions. The specific procedure is\noutlined as follows:\n\n 1. Perform IFFT calculation on each column of the two-dimensional data, where\n    ny represents the number of FFT points set in the interface parameters for\n    the y-axis.\n\n 2. Building upon the processing in step 1, perform IFFT calculation on each row\n    of the two-dimensional data, where nx represents the number of FFT points\n    set in the interface parameters for the x-axis.\n\n 3. Return the calculation results from step 2.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbIFFT2D。\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/hpl/hpl5_op/op_ifft_2d","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":414},{"text":"Principle","id":"principle","depth":2,"charIndex":-1},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1461},{"text":"Usage","id":"usage","depth":2,"charIndex":1543}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":150,"title":"Overview","content":"#\n\nOPERATOR NAME                       DESCRIPTION                         DSP\nFast Fourier Transform              Fast Fourier Transform              Y\nInverse Fast Fourier Transform      Fast Fourier inverse Transform      Y\nFast Fourier Transform 2D           Fast Fourier Transform 2D           Y\nInverse Fast Fourier Transform 2D   Fast Fourier inverse Transform 2D   Y","routePath":"/en/guide/ucp/hpl/hpl5_op/overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":151,"title":"Sample","content":"#\n\n\nOverview#\n\nThis section introduces the function, realization process and details of each\nexample of HPL module to help you quickly understand how to use the HPL module,\nand the introduction of the following samples are based on the board side\nrunning example. For sample to compile and run, please refer to the section\nQuick Start.\n\nSample list, dsp backend is used by default.\n\nSAMPLE NAME          DESCRIPTION\nfft_ifft_transform   FFT IFFT Transform Sample\n\n\nFourier Transform Sample #\n\nThe Fast Fourier Transform sample execution script is located in the directory\nucp_tutorial/hpl/hpl_samples/script/01_fft_ifft_transform. This sample\ndemonstrates how to filter images using Fast Fourier Transform and Inverse Fast\nFourier Transform interfaces. Information may be interfered by various noises\nduring the processes of collection,transmission, or processing. For some types\nof noise, dealing with them in the frequency domain is a more effective and\nconvenient method. At this point, the relevant interfaces of Fourier transform\ncan be used to remove these noises in the frequency domain, and then the\nfiltered data can be restored to the time domain to achieve the purpose of\ndenoising. For detailed implementation methods, please refer to the sample\nsource code for practice comparison.\n\nIn this sample, directly executing the sample script can start the sample\nprocessing flow: reading noisy sine waves, fast fourier transformation,\nfrequency domain filtering, inverse fast fourier transformation, and saving\ndenoising data. The execution method is as follows:\n\n\n\nIn addition, you can obtain sample usage instructions by appending the -help\ncommand.\n\nAfter the operator is executed, the data processing results will be saved in the\nhpl_samples/data directory. The output content of this sample is as follows:\n\n\n\nThe execution flow of the sample is shown below for 1D FFT:\n\nAs shown in the execution flow chart of the sample, the data processing process\nof this sample can be divided into the following four steps:\n\n 1. Generate sine wave data and add noise. Save the generated data in a file as\n    a floating point type and read the data directly in subsequent runs.\n\n 2. Execute Fast Fourier Transform on the generated data.\n\n 3. Construct a frequency domain filter to filter the transformed data.\n\n 4. Execute Inverse Fast Fourier Transform on the filtering result to restore\n    the sine wave data.\n\n\nConstructing Noisy Data#\n\nThe sine wave is used as the original data, and after adding noise, it becomes\nthe input data, and the noise is removed in subsequent processing. The noise\ndata is a sine wave with a frequency of 5HZ superimposed with Gaussian noise.\nThe noise construction code is as follows:\n\n\n\nAmong them, the original sine wave image is:\n\nThe image after adding noise is:\n\n\nFourier Transform#\n\nThe fourier transform interface converts data from the time domain to the\nfrequency domain, allowing us to use some frequency domain methods to process\nthe data. The HPL module provides a 1-dimensional fast Fourier transform\ninterface. For details, please refer to Fast Fourier Transform. The specific\ncode is as follows:\n\n\n\nAmong them, dst is the output data in the frequency domain, and src is the input\ndata in the time domain, that is, noise data.\n\n\nFrequency Domain Filtering#\n\nThis step is to process the data in the frequency domain to achieve the effect\nof noise filtering. Here a filter is constructed and convolved with the\nfrequency domain data. The specific implementation is as follows:\n\n\n\nIn the code, the complex number structure data is converted into the data type\nof cv::Mat, and the frequency domain filter is created and applied through the\nrelevant interface provided by OpenCV. Finally, the filtering result is\nconverted into an complex number structure.\n\n\nInverse Fourier Transform#\n\nThe Inverse Fourier Transform transforms data from the frequency domain to the\ntime domain, making the display and storage of data more consistent with daily\ncognition. The HPL module provides a 1-dimensional inverse fast Fourier\ntransform interface. For details, please refer to Inverse Fast Fourier\nTransform. The specific code is as follows:\n\n\n\nAmong them, dst is the output data in the time domain, and src is the input data\nin the frequency domain.\n\nThe image after removing noise is:","routePath":"/en/guide/ucp/hpl/hpl6_sample","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Fourier Transform Sample","id":"fourier-transform-sample","depth":2,"charIndex":-1},{"text":"Constructing Noisy Data","id":"constructing-noisy-data","depth":3,"charIndex":2413},{"text":"Fourier Transform","id":"fourier-transform","depth":3,"charIndex":2800},{"text":"Frequency Domain Filtering","id":"frequency-domain-filtering","depth":3,"charIndex":3274},{"text":"Inverse Fourier Transform","id":"inverse-fourier-transform","depth":3,"charIndex":3799}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":152,"title":"HB_HPL_MAX_DIMS","content":"#\n\n\n\nDefine the dimension of the maximum support for imaginary numbers to be 3.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hb_hpl_max_dims","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":153,"title":"hbFFTPointSize","content":"#\n\n\n\nEnumeration of FFT operation points.\n\n * Member\n   \n   MEMBER NAME              DESCRIPTION\n   HB_FFT_POINT_SIZE_16     16 points of FFT.\n   HB_FFT_POINT_SIZE_32     32 points of FFT.\n   HB_FFT_POINT_SIZE_64     64 points of FFT.\n   HB_FFT_POINT_SIZE_128    128 points of FFT.\n   HB_FFT_POINT_SIZE_256    256 points of FFT.\n   HB_FFT_POINT_SIZE_512    512 points of FFT.\n   HB_FFT_POINT_SIZE_1024   1024 points of FFT.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbfftpointsize","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":154,"title":"hbHPLDataType","content":"#\n\n\n\nData types supported by the HPL module.\n\n * Member\n   \n   MEMBER NAME            DESCRIPTION\n   HB_HPL_DATA_TYPE_I16   Data Type: int16_t\n   HB_HPL_DATA_TYPE_I32   Data Type: int32_t\n   HB_HPL_DATA_TYPE_F32   Data Type: float32_t","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhpldatatype","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":155,"title":"hbHPLImaginaryData","content":"#\n\n\n\nStructures for imaginary numbers.\n\n * Member\n   \n   MEMBER NAME        DESCRIPTION\n   realDataVirAddr    The virtual address of the real section.\n   realDataPhyAddr    The physical address of the real section.\n   imDataVirAddr      The virtual address of the virtual section.\n   imDataPhyAddr      The physical address of the virtual section.\n   dataType           Data type that supports the type in hbHPLDataType .\n   imFormat           Data format that supports the types in hbHPLImFormat.\n   dimensionSize      The size of the dimensions of the data.\n   numDimensionSize   The largest dimension supported, with a maximum value of\n                      HB_HPL_MAX_DIMS .","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhplimaginarydata","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":156,"title":"hbHPLImFormat","content":"#\n\n\n\nEnumeration of data layout formats for imaginary numbers.\n\n * Member\n   \n   MEMBER NAME                DESCRIPTION\n   HB_IM_FORMAT_INTERLEAVED   The real and imaginary data are interleaved, as follows, R I\n                              R I..., which stored in the address pointed to by the real\n                              part pointer.\n   HB_IM_FORMAT_SEPARATE      The real and imaginary data are arranged independently, as\n                              follows, R R... I I..., which stored in the addresses\n                              pointed to by the real and imaginary pointers, respectively.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_data_structure/hbhplimformat","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":157,"title":"hbFFT1D","content":"#\n\n\n\nFFT 1D operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pSize         FFT points support the types in hbFFTPointSize.\n   normalize     Reserved parameter, defaults 0. Whether specification is\n                 performed is identified by 0 for non-specification and other\n                 values for specification.\n\nNote\n\nSpecification refers to some form of scaling of the FFT results so that the\nresults meet specific criteria or are easily interpreted.\n\n\n\nThe API for calling the FFT 1D.\n\nWhen the imaginary data type is HB_HPL_DATA_TYPE_I16 or HB_HPL_DATA_TYPE_I32,\nits data layout format is only supported as HB_IM_FORMAT_SEPARATE. When the\nimaginary data type is HB_HPL_DATA_TYPE_F32, its data layout format is only\nsupported as HB_IM_FORMAT_INTERLEAVED.\n\n * Parameter\n   \n   * [out] taskHandle Task handle is responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dst Output dummy data including type, format and dimension\n     information is consistent with the input.\n   * [in] src Enter the dummy data.\n   * [in] param Operator parameter.\n\n * Return Value\n   \n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbfft1d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":158,"title":"hbFFT2D","content":"#\n\n\n\nFFT 2D operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   nx            FFT points along the x-axis support the types in\n                 hbFFTPointSize.\n   ny            FFT points along the x-axis support the types in\n                 hbFFTPointSize.\n   normalize     Reserved parameter, defaults 0. Whether specification is\n                 performed is identified by 0 for non-specification and other\n                 values for specification.\n\nNote\n\nSpecification refers to some form of scaling of the FFT results so that the\nresults meet specific criteria or are easily interpreted.\n\n\n\nThe API for calling the FFT 2D.\n\nWhen the imaginary data type is HB_HPL_DATA_TYPE_I16 or HB_HPL_DATA_TYPE_I32,\nits data layout format is only supported as HB_IM_FORMAT_SEPARATE. When the\nimaginary data type is HB_HPL_DATA_TYPE_F32, its data layout format is only\nsupported as HB_IM_FORMAT_INTERLEAVED.\n\n * Parameter\n   \n   * [out] taskHandle Task handle is responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dst Output dummy data including type, format and dimension\n     information is consistent with the input.\n   * [in] src Enter the dummy data.\n   * [in] param Operator parameter.\n\n * Return Value\n   \n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbfft2d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":159,"title":"hbIFFT1D","content":"#\n\n\n\nIFFT 1D operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pSize         IFFT points support the types inhbFFTPointSize.\n   normalize     Reserved parameter, defaults 0. Whether specification is\n                 performed is identified by 0 for non-specification and other\n                 values for specification.\n\n\n\nThe API for calling the IFFT 1D.\n\nWhen the imaginary data type is HB_HPL_DATA_TYPE_I16 or HB_HPL_DATA_TYPE_I32,\nits data layout format is only supported as HB_IM_FORMAT_SEPARATE. When the\nimaginary data type is HB_HPL_DATA_TYPE_F32, its data layout format is only\nsupported as HB_IM_FORMAT_INTERLEAVED.\n\n * Parameter\n   \n   * [out] taskHandle Task handle is responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dst Output dummy data including type, format and dimension\n     information is consistent with the input.\n   * [in] src Enter the dummy data.\n   * [in] param Operator parameter.\n\n * Return Value\n   \n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbifft1d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":160,"title":"hbIFFT2D","content":"#\n\n\n\nIFFT 2D operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   nx            IFFT points along the x-axis support the types in\n                 hbFFTPointSize.\n   ny            IFFT points along the x-axis support the types in\n                 hbFFTPointSize.\n   normalize     Reserved parameter, defaults 0. Whether specification is\n                 performed is identified by 0 for non-specification and other\n                 values for specification.\n\n\n\nThe API for calling the IFFT 2D.\n\nWhen the imaginary data type is HB_HPL_DATA_TYPE_I16 or HB_HPL_DATA_TYPE_I32,\nits data layout format is only supported as HB_IM_FORMAT_SEPARATE. When the\nimaginary data type is HB_HPL_DATA_TYPE_F32, its data layout format is only\nsupported as HB_IM_FORMAT_INTERLEAVED.\n\n * Parameter\n   \n   * [out] taskHandle Task handle is responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dst Output dummy data including type, format and dimension\n     information is consistent with the input.\n   * [in] src Enter the dummy data.\n   * [in] param Operator parameter.\n\n * Return Value\n   \n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_function_interface/hbifft2d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":161,"title":"HPL API Introduction","content":"#\n\n\nData structure#\n\n\nFunctional Interface#","routePath":"/en/guide/ucp/hpl/hpl7_api/hpl_api_overview","lang":"en","toc":[{"text":"Data structure","id":"data-structure","depth":2,"charIndex":3},{"text":"Functional Interface","id":"functional-interface","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":162,"title":"DSP Development Process","content":"#\n\n\nOverall Framework#\n\nIn the Softmax operator example provided by Horizon, it is demonstrated how to\nimplement the functional encapsulation of DSP custom operators by scheduling the\nUCP framework. You can obtain the sample source code at\nsamples/ucp_tutorial/custom_operator/dsp_sample in the OE package for\nsynchronous reading and understanding.\n\n\nSoftmax Operator Development#\n\nThis chapter mainly introduces the process of DSP operator development by taking\nDSP Softmax operator development as an example, which corresponds to the\nsamples/ucp_tutorial/custom_operator/dsp_sample/dsp_code/softmax/softmax_ivp.cc\npart in Horizon OE.\n\nThe complete operator development is divided into the following three steps:\n\n 1. Calling UCP API, which is mainly responsible for task initiation and\n    computing resource allocation\n\n 2. DSP operator development\n\n 3. Operator registration and operation.\n\n\nSoftmax Analysis#\n\nThe Softmax operator can be divided into the following four basic calculations:\n\n 1. Calculate the maximum value max in the input element.\n\n 2. Calculate and update each element of the input: input = exp(input - max).\n\n 3. Calculate the sum of the updated input.\n\n 4. Calculate output = input / sum.\n\n\nUCP API#\n\nIn this section, we will show you how to use the UCP interface to apply for\nmemory resources and initiate tasks. Taking the Softmax example, since the UCP\ninterface only supports the two parameters of input and output, if you need to\npass other parameters to the DSP, you need to perform a layer of encapsulation,\npass the encapsulated \"input\" to the interface, and then the DSP obtains the\nparameters from the encapsulated \"input\".\n\nIn the following example, the encapsulated \"input\" includes three fields,\ndata_size is the data size, 100000 in the example, the input field is the data\naddress sent to the dsp, and the run_tcm_opt field indicates whether to use tcm\noptimization implementation.\n\n\n\n\nDSP Operator #\n\nThis section will introduce how to implement the four basic operations mentioned\nin the previous section to realize the DSP Softmax operator.\n\nCadence implements some basic mathematical operations to facilitate your\ndevelopment. You can get the source code from the basic examples of Cadence, or\ndirectly get the compiled dependency library dsp_math from Horizon.\n\nIn order to fully utilize the hardware performance, you need to understand the\nDSP features and use these features (VLIW, SIMD). When developing, you can refer\nto the basic operations that Cadence has already implemented.\n\nvecmaxf SIMD is implemented as follows:\n\n\n\nvecexpf_max SIMD is implemented as follows:\n\n\n\nvecsum SIMD is implemented as follows:\n\n\n\nThe division operation can be transformed into a multiplication operation, and\nthe multiplication operation is easier to implement and has better performance.\n\n\n\nThe complete Softmax implementation is as follows:\n\n\n\nThe above Softmax implementation directly accesses DDR. Since J6 is not\nconfigured with vector cache, direct access to DDR memory is very\ntime-consuming. A common optimization implementation is to divide the data into\ntiles, move them to TCM (TCM has the same latency as cache) through idma, and\ndirectly access TCM for calculation. In addition, in order to parallelize the\ncalculation with the idma transfer operation, the DSP is equipped with two TCMs\nto implement ping pong DMA operations, as shown in the figure below.\n\nThe reference code for the implementation of ping pong DMA is as follows:\n\n\n\nThe complete DSP softmax implementation is as follows:\n\n\n\n\nDSP Operator Register#\n\nAfter the operator development is completed, register the hb_dsp_softmax\noperator by calling the hb_dsp_register_fn interface. After registration is\ncompleted, compile the DSP image.\n\n\n\nNote\n\nIf it is x86 simulation running, there is no need to start the image. If it is\nrunning on the previous version, you need to execute the script dsp_deploy.sh to\nstart the image.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_develop","lang":"en","toc":[{"text":"Overall Framework","id":"overall-framework","depth":2,"charIndex":3},{"text":"Softmax Operator Development","id":"softmax-operator-development","depth":2,"charIndex":350},{"text":"Softmax Analysis","id":"softmax-analysis","depth":3,"charIndex":895},{"text":"UCP API","id":"ucp-api","depth":3,"charIndex":1216},{"text":"DSP Operator","id":"dsp-operator","depth":3,"charIndex":-1},{"text":"DSP Operator Register","id":"dsp-operator-register","depth":3,"charIndex":3538}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":163,"title":"DSP Development Documentation","content":"#\n\n\nDSP Introduction#\n\nThe Horizon J6 computing platform contains one DSP cores clocked at 1GHz and\nuses the Cadence's Tensilica Vision Q8 DSP IP.\n\nThe Vision Q8 DSP is dedicated to supporting algorithms such as computer vision\nor image processing, the Very Long Instruction Word (VLIW) and Single\nInstruction Multiple Data Stream (SIMD) can greatly increase the speed of\ncomputation. Vision Q8 DSP adopts 5-way VLIW architecture. Each instruction can\ncontain two 64-byte loads or one 64-byte load and one 64-byte store at most.\nSIMD supports 1024bit operations, such as 128-way 8-bit integer, 64-way 16-bit\ninteger, etc. For more information about the Vision Q8 DSP, see Cadence's Vision\nQ8 User Guide.\n\nDSP boasts powerful computing power, and when used properly, deploying some\ncomputations that cannot be accelerated with BPU and are inefficient with ARM to\nDSP can greatly improve the inference performance of the model.\n\n\nLinux Development Environment Installation#\n\n\nDevelopment Tool Introduction#\n\nXtensa Xplorer is an integrated development environment provided by Cadence for\ncustomers to develop software for their DSPs, which provides such functions as\nsoftware development, compilation, debugging, simulation, profiling, hardware\ntrace, etc. This section introduces only the installation of the Linux\ndevelopment environment. The installation and use of the Windows development\nenvironment can be found in the official documentation provided by Cadence.\n\n\nInstall DSP Toolchain and Configure Core #\n\nYou can get the DSP development package from Horizon Robotics, which contains\nthe Xplorer-10.1.11-linux-x64-installer.bin and the Vision_Q8_linux.tgz\ninstaller.\n\n 1. Install Xtensa Develop Tools\n\nBy default, install Xtensa Develop Tools in the /opt/xtensa directory, but you\ncan specify another directory as required. If you install it in the /opt/xtensa\ndirectory, a root permission is required. Execute the following command:\n\n\n\n 2. Install Vision Q8 Core Configuration\n\nUnzip and extract the Vision_Q8_linux.tgz installation package, put it in the\nspecified location under the Xtensa Develop Tools installation directory (e.g.,\n/opt/xtensa/XtDevTools/install/builds/RI-2023.11-linux/ ), and then install it.\nYou need to execute the following command:\n\n\n\n 3. Configure environment variables\n\nIn order to ensure that Xtensa Develop Tools can be used properly, you need to\nset the following environment variables:\n\n\n\nNote\n\nThe license should be set correctly before you use Xtensa Develop Tools, please\ncontact your Horizon project contact person for application and configuration.\n\n 4. Xtensa Develop Tools testing\n\nExecute the following two commands. If these two commands can be executed\nnormally, the linux development environment is successfully installed.\n\n\n\n\nDSP Development Reference Materials#\n\nFor better DSP development, we recommend that you refer to the following\ndocuments. After the compiler has been installed successfully, the following\ndocuments can be found in the paths XtDevTools/downloads/RI-2023.11/docs and\nXtDevTools/install/builds/RI- Found under 2023.11-linux/Vision_Q8/html/ISA:\n\nNO.   DOCUMENT NAME AND DESCRIPTION                               DOCUMENT CATALOG\n1     Vision Q8 DSP introduction document                         visionq8_ug.pdf\n2     Dev Toolkit introduction document                           sw_dev_toolkit_ug.pdf\n3     Compiler introduction document                              xtensa_xt_clang_compiler_ug.pdf\n4     Xi Library document, located in the XI_Library_7.14.2.xws   XI_Library_UserGuide.pdf\n      project Doc directory\n5     Profiler document                                           gun_profiler_ug.pdf\n6     Vision Q8 DSP instruction                                   NewISAhtml/index.html","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_intro","lang":"en","toc":[{"text":"DSP Introduction","id":"dsp-introduction","depth":2,"charIndex":3},{"text":"Linux Development Environment Installation","id":"linux-development-environment-installation","depth":2,"charIndex":927},{"text":"Development Tool Introduction","id":"development-tool-introduction","depth":3,"charIndex":973},{"text":"Install DSP Toolchain and Configure Core","id":"install-dsp-toolchain-and-configure-core","depth":3,"charIndex":-1},{"text":"DSP Development Reference Materials","id":"dsp-development-reference-materials","depth":3,"charIndex":2778}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":164,"title":"DSP API Overview","content":"#\n\n\nData Structure#\n\n\nUCP Call DSP Data Structure#\n\n\nDSP Backend Data Structure#\n\n\nFunctional Interface#\n\n\nUCP Call DSP Interface#\n\n\nDSP Backend Interface#","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/api_introduction","lang":"en","toc":[{"text":"Data Structure","id":"data-structure","depth":2,"charIndex":3},{"text":"UCP Call DSP Data Structure","id":"ucp-call-dsp-data-structure","depth":3,"charIndex":21},{"text":"DSP Backend Data Structure","id":"dsp-backend-data-structure","depth":3,"charIndex":52},{"text":"Functional Interface","id":"functional-interface","depth":2,"charIndex":82},{"text":"UCP Call DSP Interface","id":"ucp-call-dsp-interface","depth":3,"charIndex":106},{"text":"DSP Backend Interface","id":"dsp-backend-interface","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":165,"title":"handle_fn","content":"#\n\n\n\nDSP operator function pointer.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_data_structure/handle_fn","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":166,"title":"hbDSPRpcCmd","content":"#\n\n\n\nDSP operator command enumeration.\n\n * Member\n   \n   MEMBER NAME             DESCRIPTION\n   HB_DSP_RPC_CMD_R_B      Inside command\n   HB_DSP_RPC_CMD_CONFIG   Inside command\n   HB_DSP_RPC_CMD_R_E      Inside command\n   HB_DSP_RPC_CMD_NN_B     NN operator start command\n   HB_DSP_RPC_CMD_NN_E     NN operator termination command\n   HB_DSP_RPC_CMD_CV_B     CV operator start command\n   HB_DSP_RPC_CMD_CV_E     CV operator termination command\n   HB_DSP_RPC_CMD_HPL_B    HPL operator start command\n   HB_DSP_RPC_CMD_HPL_E    HPL operator termination command\n   HB_DSP_RPC_CMD_BUTT     Operator termination command","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_data_structure/hbDSPRpcCmd","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":167,"title":"Environment Variables","content":"#\n\n\n\n\nLog Level Setting Instructions#\n\n * Log level:\n   \n   The logs on the ARM side in the DSP module are mainly divided into 7 levels:\n   \n   The log level can be set to 0, 1, 2, 3, 4, 5, 6, corresponding to Verbose,\n   Debug, Info, Warning, Error, Critical, Never, and the default is Warning.\n   \n   The logs on the DSP side in the DSP module are mainly divided into 5 levels:\n   \n   The log level can be set to 1, 2, 3, 4, 5, corresponding to Debug, Info,\n   Warning, Error, Always, and the default is Warning.\n\n * Log level setting rules:\n   \n   * If the log level >= the set level, the log can be printed, otherwise it\n     will be blocked.\n   * The lower the set log level, the more information will be printed. For\n     example: if the log level is set to 3, which is the Warning level, logs of\n     levels 3, 4, and 5 can all be printed. The default log level of the VP\n     module is the Warning level, which means that the following log levels can\n     be printed: Warning, Error, and Critical.\n\nNote\n\nDSP side logs can be obtained by following the steps as belows:\n\n * Configure environment variables to enable DSP logging outputs.\n   \n   \n\n * Start the log listening service.\n   \n   ","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_environment_variable","lang":"en","toc":[{"text":"Log Level Setting Instructions","id":"log-level-setting-instructions","depth":2,"charIndex":5}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":168,"title":"hbDSPAddrMap","content":"#\n\n\n\nMap hbUCPSysMem to a memory address accessible to the DSP.\n\n * Parameters\n   * [out] out hbUCPSysMem accessible only to DSP\n   * [in] in hbUCPSysMem input address.\n * Return value\n   * Return 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPAddrMap","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":169,"title":"hbDSPAddrUnmap","content":"#\n\n\n\nUnmap the hbDSPAddrMap output.\n\n * Parameters\n   * [out] out hbUCPSysMem accessible only to DSP.\n   * [in] in hbUCPSysMem input address.\n * Return value\n   * Return 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPAddrUnmap","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":170,"title":"hbDSPRpcV2","content":"#\n\n\n\nDSP task creation interface, J6 interface, task waiting and release use the ucp\ngeneral interface.\n\n * Parameters\n   * [out] task DSP task handle\n   * [in] input DSP task input address.\n   * [in] output DSP task output address.\n   * [in] rpcCmd DSP task command.\n * Return value\n   * Return 0 means the API is executed successfully, otherwise it returns an\n     error code.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hbDSPRpcV2","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":171,"title":"hb_dsp_env_init","content":"#\n\n\n\nInitialize the system environment.\n\n * Return value\n   * Returning 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_env_init","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":172,"title":"hb_dsp_env_init_ex","content":"#\n\n\n\nInitialize the system environment and support command line parameters.\n\n * Parameters\n   * [in] argc The number of parameters\n   * [in] argv The specific parameters\n * Return value\n   * Return 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_env_init_ex","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":173,"title":"hb_dsp_mem_map","content":"#\n\n\n\nDSP-side address mapping, mapping the address passed from the ARM side.\n\n * Parameters\n   * [in] ddr_address The memory address sent by the arm side.\n   * [in] size The memory size\n * Return value\n   * Returns the address accessible to the DSP after mapping.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_mem_map","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":174,"title":"hb_dsp_mem_unmap","content":"#\n\n\n\nThe DSP end address is demapped, and the address passed from the ARM end is\ndemapped.\n\n * Parameters\n   * [in] ddr_address The return value of hb_dsp_mem_map.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_mem_unmap","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":175,"title":"hb_dsp_register_fn","content":"#\n\n\n\nDSP-side custom operator registration.\n\n * Parameters\n   * [in] cmd custom operator command number\n   * [in] handle custom operator function pointer\n   * [in] latency function calculation delay, unit ms","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_register_fn","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":176,"title":"hb_dsp_start","content":"#\n\n\n\nStart the framework multithreading.\n\n * Return value\n   * Returning 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_start","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":177,"title":"hb_dsp_unregister_fn","content":"#\n\n\n\nUnregistering a custom operator on the DSP side.\n\n * Parameters\n   * [in] cmd Custom operator function command number","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_reference/dsp_function_interface/hb_dsp_unregister_fn","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":178,"title":"Introduction to Point Cloud Pre-processing","content":"#\n\nIn order to help you better understand the CenterPoint five-dimensional point\ncloud pre-processing provided by the DSP sample package, this chapter analyzes\nthe CPU and DSP reference implementations based on the algorithm principle, and\nperforms consistency verification and performance evaluation on the CPU and DSP\nreference implementations. The sample package also provides PointPillars\nfour-dimensional point cloud pre-processing, which is similar to CenterPoint.\nThe main feature of the pre-processing of both is to generate pillars pillars\nthrough voxelization. This section mainly introduces the implementation of\nCenterPoint pre-processing, and a separate section introduces the differences in\nthe implementation of PointPillars pre-processing.\n\n\nPrinciple Introduction#\n\nThis chapter uses the five-dimensional CenterPoint model as an example. Its\ncorresponding principles and pre-processing algorithm implementation can also be\napplied to the four-dimensional PointPillars model. The input of the model is\nfive-dimensional point cloud data with a maximum number of 300,000 points. The\npre-processing mainly consists of voxelization, feature encoding, quantization\nand transposition. The pre-processing outputs a feature map of\n1x5x20x40064(nchw) and coordinate information of 1x1x40000x4(nhwc).\n\n\nVoxelization#\n\nThe coordinates of the cylinder are calculated based on the x and y coordinates\nof the input five-dimensional point cloud data. The maximum number of cylinders\nis 40000 and the maximum number of points supported by each cylinder is 20. The\ncalculation principle of the cylinder coordinates is as follows:\n\n 1. Determine whether the x, y, and z of the point cloud are within the valid\n    range and skip invalid point cloud data.\n\n 2. The x and y information of the point cloud can be regarded as a pseudo\n    image, and each pixel in the pseudo image corresponds to a cylinder\n    coordinate.\n\n 3. The coordinates of the input point cloud are unordered, so the cylinder\n    coordinates need to be determined according to the input order of the\n    coordinates. When the number of cylinders exceeds the maximum value limited\n    by the model, no new cylinder will be generated, but the point will be\n    written into the last cylinder.\n\n 4. Points with the same coordinates will be placed in the same cylinder. The\n    model limits the number of points in each cylinder. When the maximum limit\n    is exceeded, the point will be skipped directly.\n\n\n\n\nFeature Encoding#\n\nThe feature encoding of point cloud data uses the following formula:\n\n$$P_0' = \\frac$$\n\n$$P_1' = \\frac$$\n\n$$P_2' = \\frac$$\n\n$$P_3' = \\frac$$\n\n$$P_4' = P_4$$\n\n\nQuantification#\n\nThe quantized scale can be obtained through the model, and the quantization is\ncompleted using the following formula:\n\n$$P' = \\frac$$\n\n\nTranspose#\n\nThe layout mode of the feature map input to the model is NCHW, and the\ntransposition transforms the shape of the feature map from 1x40000x20x5 to\n1x5x20x40064.\n\n\nCPU Reference Implementation#\n\nThe source code of the CPU reference implementation is located in AI Benchmark.\nThe basic pre-processing process is consistent with the principle introduction,\nin which quantization is incorporated into the feature encoding step.\n\n\nVoxelization#\n\n\n\n\nFeature Encoding & Quantize#\n\n\n\n\nTranspose#\n\n\n\n\nDSP Optimization Acceleration#\n\nThe acceleration implementation of operations on DSP is mainly considered in two\naspects. Firstly, the calculation part of the operator can be vectorized as much\nas possible to fully utilize the DSP’s computing power, as the DSP on J5\nsupports 512-bit SIMD . Secondly, the memory access part of the operator can be\noptimized by moving data to the two 128kB TCM on J5’s DSP, which have access\nperformance similar to the cache on the CPU, effectively saving memory access\noverhead. At the same time, the DSP also provides vectorized memory access\ninstructions to improve access efficiency.\n\n\nDSP Acceleration Ideas#\n\nVectorized#\n\nFor the five-dimensional model, the calculation process can be divided into 5\ncycles, with each cycle calculating one dimension using IVP vector instructions\nto complete the operation.\n\nSpecifically regarding the CenterPoint point cloud pre-processing algorithm, the\ncalculation processes such as calculating the coordinates of pseudo-images,\nfeature encoding, and quantization can all be independently calculated for each\ndimension. Therefore, it is easy to convert them into vectorized calculations.\n\nMemory Access Optimization#\n\nThe point cloud input data is unordered from the x and y dimensions, which leads\nto the calculated cylinder coordinates and the index of the cylinder midpoint\nare also unordered, resulting in random memory access instead of more efficient\nsequential operations. Since there is no data cache on the DSP, random memory\naccess is very unfriendly, so it is necessary to make full use of the TCM on the\nDSP. There are only two 256kB TCMs available on the DSP, and the address space\nof the feature data output is 40000x20=781.25kB, which is much larger than the\nmemory size of the TCM. At the same time, when calculating the voxel\ncoordinates, a pseudo-image coordinate and cylinder coordinate lookup table is\nalso required. The memory required for this table is 512x512x4=1MB (512 is the\nwidth and height of the pseudo image, and 4 is the number of bytes required for\nthe cylinder coordinates), which is also far beyond the memory size of the TCM.\nConsidering that it is too costly to move random memory access to the DSP, the\ncalculation and memory access in the pre-processing algorithm are separated, and\nthe calculation part is placed on the DSP, while part of the random access is\nstored on the CPU for execution. At the same time, in order to reduce the amount\nof data accessed, the DSP acceleration implementation adjusts the steps of the\nCPU implementation, bringing feature encoding and quantization forward to after\ncalculating the pseudo-image coordinates. Through this processing, the amount of\ndata required for memory access in the subsequent voxelization and transposition\nsteps will be greatly reduced. Although this will increase the amount of\ncalculation, it can be seen from actual evaluation that reducing the amount of\nmemory access data will bring greater benefits, especially when the number of\nvalid point clouds is large.\n\nBlock strategy#\n\nAlthough there are two 256kB TCMs, only one of them is used in each calculation\ncycle. The main reasons are as follows:\n\n 1. Pingpong buffer is needed to reduce the time waiting for data copying to\n    complete;\n\n 2. Pingpong buffer needs to be located in different TCMs to reduce data bank\n    conflicts.\n\nThe calculation on the DSP is divided into two parts, one is to calculate the\npseudo image coordinates, and the other is feature encoding, quantization and\ntransposition.\n\n 1. Calculation of pseudo image coordinates, in each calculation cycle, the\n    input is five-dimensional point cloud data, and the output is\n    single-dimensional data. The input-to-output ratio is 5:1, so the\n    input-to-output TCM memory size ratio is also 5:1;\n\n 2. Feature encoding, quantization and transposition, in each calculation cycle,\n    the input is five-dimensional point cloud data, and the output is quantized\n    single-dimensional data. The input-to-output ratio is 20:1.\n\n\nDSP Reference Implementation#\n\nFor the IVP instructions of DSP, please refer to the document\nISA/NewISAhtml/index.html provided by Cadence. For a detailed introduction to\nthe TCM memory management interface, please refer to Xtensa Vision Tile Manager\nLibrary User's Guide. For a detailed introduction to the IDMA related\ninterfaces, please refer to Xtensa System Software Reference Manual.\n\nCalculate Pseudo-image Coordinates, Feature Encoding and Quantize(DSP\nCalculation Part)#\n\nThe method of calculating pseudo image coordinates is basically the same as that\nin the CPU reference implementation. The main differences are as follows:\n\n 1. Remove the calculation of one-dimensional coordinates, and store the pseudo\n    image x and y coordinates through the high and low 16 bits of 32 bits (the\n    width and height of the pseudo image of the example model are 512, and 16\n    bits are enough to store), and use the two-dimensional coordinates as an\n    index to find the corresponding cylinder coordinates.\n\n 2. When filtering invalid point cloud data, use mask move instead of\n    conditional judgment statements. Because using conditional judgment in the\n    inner loop will significantly reduce the computing performance.\n\n\n\nVoxelization(CPU Memory Access Part)#\n\nThe process logic and CPU reference implementation are basically consistent,\nwith the main differences being as follows:\n\n 1. Using a two-dimensional array lookup table, and directly using the\n    pseudo-image’s x and y as index.\n\n 2. The index calculation method for outputting voxel data is different.\n    Considering the limit of DSP gather/scatter which can not exceed the 64kB\n    address space, the output index is calculated in advance according to the\n    shape of the final feature data.\n\n\n\nTranspose#\n\nConvert point cloud in AoS format to SoA format.\n\n\n\n\nConsistency Check#\n\nExport the output feature data and coordinate information of the point cloud\npre-processing from the CPU and DSP reference implementation to files\nrespectively. Compare the files content and pass the check if they are\nconsistent.\n\n\n\nIn AI Benchmark, a complete end-to-end accuracy evaluation can be performed, and\nthe current DSP reference implementation is completely consistent with the CPU\nreference implementation in terms of accuracy.\n\nHint\n\nIf you dont’t require the DSP to achieve exactly the same precision as the CPU,\nyou can further improve performance by optimizing floating-point operations, for\nexample, by replacing time-consuming division operations with multiplication\noperations.\n\n\nPerformance Evaluation#\n\nIn AI Benchmark, perform a latency test of the CenterPoint model to compare the\nsingle-frame latency of the CPU and DSP reference implementations. Through\nactual testing, it is found that voxel_num has a greater impact on the time\nconsumption of point cloud pre-processing, so here we select several sets of\ndata with different voxel_num to compare the performance of the CPU reference\nimplementation and the DSP reference implementation.\n\nSingle-thread single-frame latency test results as below:\n\nVOXEL_NUM   CPU REFERENCE   DSP REFERENCE\n6452        7.978 ms        4.809 ms\n11894       11.779 ms       5.037 ms\n24579       20.428 ms       5.320 ms\n40000       30.222 ms       5.677 ms\n\n\nIntroduction to PointPillars Preprocessing Implementation#\n\nThe basic processing logic of PointPillars four-dimensional point cloud\npre-processing is the same as that of CenterPoint five-dimensional point cloud\npre-processing, with the following two main differences:\n\n 1. Different point cloud input dimensions.\n\n 2. Different output layout after point cloud pre-processing.\n\nFor the above two differences, the following will explain the differences in DSP\nacceleration implementation:\n\n 1. Different point cloud input dimensions\n\n * After quantization, the data size of the PointPillars four-dimensional point\n   cloud input is just suitable for the int32_t type, and there is no need to\n   perform padding operation on the fifth-dimensional quantized data to adapt to\n   the int32_t type like CenterPoint.\n\n * The first step is to calculate the pseudo-image coordinates, and the input\n   and output TCM size ratios of the feature encoding and quantization steps are\n   different. CenterPoint is 5:3, PointPillar is 2:1, the input is 4D floating\n   point cloud data (float32_t), and the output is pseudo image coordinates\n   (int32_t) and quantized xyzr data (int32_t).\n\n * Due to the difference in dimensions, the offset calculation required for the\n   gather operation on the input point cloud is different.\n\n 2. The output layout after point cloud pre-processing is different\n\n * In the voxelization (CPU memory access) step, the offset calculation method\n   of voxel_data is different.\n\n * Due to the different output layout, the input and output TCM size ratios of\n   the transposed part are different. The ratio of CenterPoint is 5:1, while the\n   ratio of PointPillars is 4:1.\n\n * The processing logic of transposition is different, see the figure below for\n   the specific differences.\n\nNote\n\nAccording to the diagram, CenterPoint and PointPillars have different orders of\nmax_point_in_voxel and max_voxel_num, which leads to different invalid data.\n\nSpecifically, the invalid data of PointPillars shows a more concentrated\ndistribution, which is conducive to continuous processing. On the contrary, the\ninvalid data of CenterPoint is scattered among each voxel showing a\ncross-distribution characteristic. The parameters of the pre-processing will\nhave a certain impact on the implementation efficiency of the operator. We can\nfurther consider implementing specific logic according to different parameters.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_sample/centerpoint_pointpillar","lang":"en","toc":[{"text":"Principle Introduction","id":"principle-introduction","depth":2,"charIndex":757},{"text":"Voxelization","id":"voxelization","depth":3,"charIndex":1308},{"text":"Feature Encoding","id":"feature-encoding","depth":3,"charIndex":2473},{"text":"Quantification","id":"quantification","depth":3,"charIndex":2651},{"text":"Transpose","id":"transpose","depth":3,"charIndex":2804},{"text":"CPU Reference Implementation","id":"cpu-reference-implementation","depth":2,"charIndex":2978},{"text":"Voxelization","id":"voxelization-1","depth":3,"charIndex":3241},{"text":"Feature Encoding & Quantize","id":"feature-encoding--quantize","depth":3,"charIndex":3259},{"text":"Transpose","id":"transpose-1","depth":3,"charIndex":3292},{"text":"DSP Optimization Acceleration","id":"dsp-optimization-acceleration","depth":2,"charIndex":3307},{"text":"DSP Acceleration Ideas","id":"dsp-acceleration-ideas","depth":3,"charIndex":3929},{"text":"Vectorized","id":"vectorized","depth":4,"charIndex":3954},{"text":"Memory Access Optimization","id":"memory-access-optimization","depth":4,"charIndex":4470},{"text":"Block strategy","id":"block-strategy","depth":4,"charIndex":6342},{"text":"DSP Reference Implementation","id":"dsp-reference-implementation","depth":3,"charIndex":7333},{"text":"Calculate Pseudo-image Coordinates, Feature Encoding and Quantize(DSP Calculation Part)","id":"calculate-pseudo-image-coordinates-feature-encoding-and-quantizedsp-calculation-part","depth":4,"charIndex":-1},{"text":"Voxelization(CPU Memory Access Part)","id":"voxelizationcpu-memory-access-part","depth":4,"charIndex":8563},{"text":"Transpose","id":"transpose-2","depth":4,"charIndex":9102},{"text":"Consistency Check","id":"consistency-check","depth":2,"charIndex":9167},{"text":"Performance Evaluation","id":"performance-evaluation","depth":2,"charIndex":9886},{"text":"Introduction to PointPillars Preprocessing Implementation","id":"introduction-to-pointpillars-preprocessing-implementation","depth":2,"charIndex":10602}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":179,"title":"Instruction","content":"#\n\nThe DSP sample package shows how to use dsp on J6 for task processing. The DSP\nsample package contains softamx, quantize, dequantize, centerpoint, pointpillar,\nand slam:\n\nEach example is mainly divided into two parts: the ARM side and the DSP side.\nThe ARM side is responsible for preparing data and then initiating calls, and\nthe DSP side is responsible for receiving tasks sent by the ARM side, completing\ntask calculations, and sending results to the ARM.\n\nYou can experience and develop applications based on this example to lower the\ndevelopment threshold.\n\nThe dsp sample is located in samples/ucp_tutorial/custom_operator/dsp_sample,\nand the code structure is as follows:\n\n\n\n\nEnvironment Construction#\n\n\nDevelopment Board Preparation#\n\nAfter getting the development board, make sure to use the recommended system\nimage version and that the local development machine and the development board\ncan be remotely connected.\n\n\nCompile#\n\ndsp sample compilation requires compiling the ARM side and DSP side separately.\n\n 1. The ARM side needs to have the cross-compilation tool (X86 GCC12.2, ARM\n    GCC12.2) installed in the current environment. Set the environment variable\n    LINARO_GCC_ROOT to the actual installation location of the cross-compilation\n    tool;\n\n 2. The DSP side needs to have the Cadence cross-compilation tool installed in\n    the current environment. For specific installation and configuration\n    methods, refer to the introduction in the DSP Development Documentation\n    chapter;\n\n 3. After installing the tool and confirming that the tool has been successfully\n    installed, execute the board-side aarch64 or x86 compilation script in the\n    directory. The compiled products are in script and script_x86 respectively.\n    Taking the compilation of the board-side products as an example, the\n    execution script is as follows:\n\n\n\n\nExample Use#\n\n 1. Run on the board, copy the script directory to the board, and execute the\n    following command:\n\n\n\n 2. To run the x86 simulation, execute the following command:\n\n","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_sample/dsp_sample_intro","lang":"en","toc":[{"text":"Environment Construction","id":"environment-construction","depth":2,"charIndex":685},{"text":"Development Board Preparation","id":"development-board-preparation","depth":3,"charIndex":713},{"text":"Compile","id":"compile","depth":3,"charIndex":930},{"text":"Example Use","id":"example-use","depth":2,"charIndex":1864}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":180,"title":"Introduction to Quantization and Dequantization","content":"#\n\nIn order to help you better understand the quantize and dequantize\npre-processing provided by the DSP sample package, this article analyzes the CPU\nand DSP reference implementations based on the algorithm principles, and\nperforms consistency checks and performance evaluations on the CPU and DSP\nreference implementations.\n\n\nPrinciple Introduction#\n\n\nQuantization and Dequantization#\n\nFor the introduction, formulas and reference implementation of quantization and\ndequantization, please refer to the introduction of model quantization in the\nKey Concepts chapter.\n\n\nCPU Reference Implementation#\n\n\nQuantification#\n\n\n\n\nDequantization#\n\n\n\n\nDSP Optimization Acceleration#\n\n\nDSP Acceleration Ideas#\n\nVectorized#\n\nQuantize/dequantize both calculate data continuously, which is very suitable for\nSIMD vector optimization.\n\nBlock strategy#\n\n 1. J6 DSP has two 256kB TCMs, and the actual available space is about 210KB.\n    Each tile is as large as possible without exceeding the total available\n    space. Quantize mainly considers input and output, and the ratio is 4:1.\n    Dequantize also mainly considers input and output, and the ratio is 1:4;\n\n 2. At the same time, because quantize is a linear calculation, and the output\n    is smaller than the output data size, you can consider using inplace (using\n    the same tile for input and output to further increase the available tile).\n\n\nDSP Optimization Implementation#\n\n 1. quantize SIMD computing.\n\n\n\n 2. dequantize SIMD Computing.\n\n\n\n 3. For the framework part, please refer to the introduction of ping pong IDMA\n    implementation in the DSP Operator section.\n\n\nConsistency Check#\n\nCompare the calculated results with the CPU reference implementation, and pass\nthe verification if they are consistent","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_sample/quantize_dequantize","lang":"en","toc":[{"text":"Principle Introduction","id":"principle-introduction","depth":2,"charIndex":327},{"text":"Quantization and Dequantization","id":"quantization-and-dequantization","depth":3,"charIndex":353},{"text":"CPU Reference Implementation","id":"cpu-reference-implementation","depth":2,"charIndex":569},{"text":"Quantification","id":"quantification","depth":3,"charIndex":601},{"text":"Dequantization","id":"dequantization","depth":3,"charIndex":621},{"text":"DSP Optimization Acceleration","id":"dsp-optimization-acceleration","depth":2,"charIndex":641},{"text":"DSP Acceleration Ideas","id":"dsp-acceleration-ideas","depth":3,"charIndex":674},{"text":"Vectorized","id":"vectorized","depth":4,"charIndex":699},{"text":"Block strategy","id":"block-strategy","depth":4,"charIndex":820},{"text":"DSP Optimization Implementation","id":"dsp-optimization-implementation","depth":3,"charIndex":1387},{"text":"Consistency Check","id":"consistency-check","depth":2,"charIndex":1616}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":181,"title":"SLAM Example Introduction","content":"#\n\nThe DSP example package is developed based on the SLAM example package provided\nby Cadence and can be run on a board or in simulation. This chapter will briefly\nintroduce the SLAM DSP example process.\n\n\nPrinciple Introduction#\n\nSLAM(simultaneous localization and mapping), also known as CML(Concurrent\nMapping and Localization), is a concept used to locate in an environment and\ndescribe the current environment for route planning; it records information\nobtained by some form of perception and compares it with the current perception\nresults to support the evaluation of actual positioning. The problem can be\ndescribed as: if a robot is placed in an unknown position in an unknown\nenvironment, is there a way for the robot to gradually draw a complete map of\nthe environment while deciding in which direction the robot should go.\n\n\nSLAM Implementation Process#\n\nThe visual SLAM application example is based on the open source ORB_SLAM2\nimplementation. However, the application only extracts the most basic functions\nand simplifies and modifies them in various functions. These modifications are\ndesigned to adapt to the SIMD implementation on the Vision DSP to improve loop\nperformance and reduce frequent random accesses to system memory. In addition,\nadditional functions for removing outliers have been added in various modules.\n\nThe SLAM application also supports \"binocular stereo\" and \"RGBD\" inputs.\nHowever, its approach is significantly different from that of ORB_SLAM2.\nORB_SLAM2 uses additional information (right image in binocular stereo vision\nand depth information in RGBD) to reduce reprojection error or cost function.\nInstead, the application uses a proprietary method that leverages this\nadditional information to perform outlier rejection.\n\nThe figure below shows the module diagram of the SLAM application. The SLAM\napplication can accept monocular, binocular stereo or RGBD input. In case of\nRGBD input, grayscale monocular image and depth image are provided as input.\nKeypoints or corner points are then detected from the input image and\ndescriptors are created for these keypoints, which are used for matching. In the\ncase of binocular stereo input, depth information is generated from binocular\nstereo images. For monocular input, in the initial state, the SLAM application\ntries to find a set of two keyframes to obtain the initial 3D point set and\npose. For RGBD and stereo input, 3D points are generated on the first frame and\nthe application is initialized. Once the application is initialized, it enters\nthe tracking state, estimating the camera pose through pose optimization in each\nframe using known 3D points and their observed projections. Depending on the\nconditions, keyframes (KFs) are inserted to generate new 3D point sets, and\nlocal bundle adjustments further optimize the estimated 3D points and poses.","routePath":"/en/guide/ucp/plugin/dsp_develop/dsp_sample/slam","lang":"en","toc":[{"text":"Principle Introduction","id":"principle-introduction","depth":2,"charIndex":205},{"text":"SLAM Implementation Process","id":"slam-implementation-process","depth":2,"charIndex":836}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":182,"title":"GPU Development Documentation","content":"#\n\nFor the specific GPU hardware introduction, development environment, and\ndevelopment software interface, please refer to the introduction in the Horizon\nJourney 6 System Software - J6X GPU Module Development and Debugging Guide.\n\n\nSample#\n\nThis sample shows you how to call the OpenCL API to implement a simple threshold\nfunction: The gpu sample is located in samples/ucp_tutorial/gpu_sample, and the\ncode structure is as follows:\n\n\n\n\nPrinciple#\n\n$$\\begindst_x=\\beginmaxval, src_x > threshold\\0, src_x < = threshold\\end\\end$$\n\nINPUT IMAGE   PARAMETERS                 OUTPUT IMAGE\n              threshold=100 maxval=200   \n\n\nThread Configuration#\n\n 1. The total task calculation amount is WIDTHxHEIGHT. Each work item calculates\n    the pixel block size of 32x16. From this, the total number of work items\n    under this division can be obtained as follows:\n\n\n\n 2. Each thread group is configured with 4x4 work items.\n\n\nEnvironment Construction#\n\nDevelopment Board Preparation#\n\nAfter getting the development board, make sure to use the recommended system\nimage version and that the local development machine and the development board\ncan be remotely connected.\n\nCompile#\n\nThe GPU sample only supports compiling and generating products that are executed\non the board, the execution script is as follows:\n\n\n\n\nExample Use#\n\nRun on the board, copy the script directory to the board, and execute the\nfollowing command:\n\n","routePath":"/en/guide/ucp/plugin/gpu_develop","lang":"en","toc":[{"text":"Sample","id":"sample","depth":2,"charIndex":233},{"text":"Principle","id":"principle","depth":3,"charIndex":437},{"text":"Thread Configuration","id":"thread-configuration","depth":3,"charIndex":627},{"text":"Environment Construction","id":"environment-construction","depth":3,"charIndex":922},{"text":"Development Board Preparation","id":"development-board-preparation","depth":4,"charIndex":949},{"text":"Compile","id":"compile","depth":4,"charIndex":1165},{"text":"Example Use","id":"example-use","depth":3,"charIndex":1310}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":183,"title":"Introduction","content":"#\n\nThe custom operator development module provides an interface for you to use the\nprogrammable hardware resources provided by the development board to further\nmeet your diverse operator needs. The current programmable backend includes\nCadence Vision Q8 DSP, Mali A78 GPU。\n\nThe development of DSP is mainly divided into two steps:\n\n 1. Use the tools and materials provided by Cadence to complete the operator\n    development;\n\n 2. Register the operator through the API provided by UCP and complete the\n    deployment on the development version.\n\nIts functional architecture is shown in the figure below:\n\nIn the above architecture diagram, the DSP custom operator application generates\nthe task handle of the corresponding operator through the interface provided by\nthe custom module, registers the DSP operator to the DSP image to perform the\ncompilation and deployment. UCP provides a service including modules such as\ntask scheduling, session management, and engine management. After the task\nhandle of the corresponding operator is generated, the operator task is\nsubmitted to the task queue through the UCP task scheduling interface and\nassigned to the DSP underlying hardware to implement the functional logic of the\noperator.\n\nGPU development uses the native OpenCL interface, and its functional\narchitecture is shown in the following figure:","routePath":"/en/guide/ucp/plugin/introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":184,"title":"AI Benchmark User Guide","content":"#\n\nHorizon's J6 AI Benchmark Sample Package contains the most frequently used\nperformance and accuracy evaluation samples of classification, detection,\nsegmentation, optical-flow, tracking estimation, lidar multitask, bev, depth\nestimation and online map construction models. In model performance evaluation\nsamples, you are able, not only to evaluate the single frame latency, but also\nto evaluate the multi-core latency using multi-thread scheduling. The pre-build\nsource code, executable programs and evaluation scripts in the AI Benchmark\nSample Package allow you to experience the samples, and develop their own\napplications, which makes development easier.\n\n\nDeliverables Description#\n\nThe AI Benchmark sample package is located in the\nsamples/ucp_tutorial/dnn/ai_benchmark/ path of the horizon_j6_open_explorer\nrelease package and consists the following main contents:\n\nNO.   NAME   DESCRIPTION\n1     code   This folder contains sample source code and compilation\n             scripts.\n2     j6     Dev board operating environment of the AI Benchmark Sample\n             Package.\n\n\nSample Code Package#\n\nNote\n\nThe on-board model needs to be obtained first by executing the\nresolve_ai_benchmark_ptq.sh and the resolve_ai_benchmark_qat.sh in the\nsamples/ai_toolchain/model_zoo/runtime/ai_benchmark/ directory of the OE package\nrespectively.\n\nDirectory of the sample code package is shown as below:\n\n\n\n * The code directory contains the source code of the evaluation program, used\n   to evaluate model performance and accuracy.\n * The j6 directory contains various pre-compiled application programs and\n   evaluation scripts, used to evaluate the accuracy and performance of\n   different models in Horizon's BPU (Brain Processing Unit).\n * The build_ptq_j6.sh script is the PTQ real program one-click compilation\n   script.\n * The build_qat_j6.sh script is the QAT real program one-click compilation\n   script.\n\n\nSample Models#\n\nModel releases for the AI Benchmark sample package include PTQ model and QAT\nmodel releases:\n\n * You can get the model_zoo of the PTQ model by executing the script\n   resolve_ai_benchmark_ptq.sh of the\n   samples/ai_toolchain/model_zoo/runtime/ai_benchmark/ path.\n * You can get the model_zoo of the QAT model by executing the script\n   resolve_ai_benchmark_qat.sh of the\n   samples/ai_toolchain/model_zoo/runtime/ai_benchmark/ path.\n\nAmong them, which contain the commonly used classification, detection,\nsegmentation and optical flow prediction models, and the naming rules of the\nmodels is ___.\n\nNote\n\nBoth the PTQ and QAT models in model_zoo are compiled by the original model.\n\nFor the PTQ original model details, you can refer to the section PTQ Conversion\nSamples Guide.\n\nFor the QAT original model details, you can refer to the section EXAMPLES.\n\nThe performance data of the model in the AI Benchmark sample package can be\nfound in the section Benchmark of Model Performance.\n\n\nPublic Datasets#\n\nThe dataset will be used in the sample, you can download the corresponding\ndataset in section Dataset Download. If you have any questions during the data\npreparation process, please contact Horizon.\n\n\nDevelopment Environment#\n\nBefore using the AI Benchmark sample package, you need to ensure that the\ndevelopment board environment and compilation environment are available:\n\n * Prepare the Development Board\n   \n   1. After getting the development board, upgrade the system image file to the\n      version recommended by the sample package.\n   \n   2. Make sure the remote connection between local dev machine and the dev\n      board.\n\n * Prepare the compilation environment\n   \n   Install the aarch64-none-linux-gnu-gcc and aarch64-none-linux-gnu-g++\n   cross-compilation tool in current environment, and then run the\n   build_ptq_j6.sh script and the build_qat_j6 in the code folder to compile\n   executable programs in real machine. The executable programs and\n   corresponding dependencies will be copied into the aarch64 sub-folders of the\n   j6/ptq/script and j6/qat/script folders automatically.\n\nNote\n\nThe cross-compilation tool specified by the build_ptq_j6.sh and the build_qat_j6\nscript is located in the /opt folder. If you want to install it into some other\nlocations, please modify the script.\n\n\n\n\nHow to Use#\n\n\nEvaluation Scripts#\n\nEvaluation sample scripts are in the script and tools folders.\n\nThe script folder contains the scripts used for evaluating frequently used\nclassification, detection, segmentation, optical-flow, tracking estimation,\nlidar multitask, bev and depth estimation models in dev board. There are three\nscripts under each model:\n\nSCRIPT        DESCRIPTION\nfps.sh        The script implements FPS statistics (multi-threading\n              scheduling. You can freely specify number of threads as\n              needed) .\nlatency.sh    The script implements statistics of single-frame latency\n              (one thread, single-frame).\naccuracy.sh   The script is used for evaluating model accuracy.\n\n\n\nThe (PTQ)tools folder contains the precision calculation scripts under\npython_tools, which used for accuracy evaluation.\n\n\n\nThe (QAT)tools folder contains pre-processing scripts and precision calculation\nscripts, which used for QAT model accuracy evaluation.\n\n\n\nAttention\n\nRun the following commands before the evaluation and copy the ptq (or the qat)\ndirectory to the dev board.\n\n\n\n\nParameters of json Configuration File#\n\nThis section we provide you the brief introduction of the configuration for the\nworkflow_fps.json, workflow_latency.json and workflow_accuracy.json. The\nconfiguration can be simply divided into input_config, output_config and the\nworkflow configuration.\n\nNote\n\nThe configuration parameters given below are the general configuration, some\nsample models will have additional configuration due to the model specificity,\nplease refer to the sample model json file for details.\n\ninput_config#\n\noutput_config#\n\nPARAMETER NAME       DESCRIPTION                                                 INVOLVED JSON FILES\noutput_type          Specify the output data format.                             fps.json, latency.json and accuracy.json\nin_order             Specify whether to output in order.                         fps.json, latency.json and accuracy.json\nenable_view_output   Specify whether to visualize the output.                    fps.json and latency.json\nimage_list_enable    When visualizing, set to true to save the output as the     fps.json and latency.json\n                     image type.\nview_output_dir      Specify the path of the visualization result output file.   fps.json and latency.json\neval_enable          Specify whether to evaluate the accuracy.                   accuracy.json\noutput_file          Specify the model output result file.                       accuracy.json\n\nWorkflow Configuration#\n\nModel inference configurations:\n\nPost-processing configurations:\n\nPARAMETER NAME   DESCRIPTION                                               INVOLVED JSON FILES\nthread_count     Specify the post-processing thread count, in range 1-8.   fps.json, latency.json and accuracy.json\nmethod_type      Specify the post-processing method.                       fps.json, latency.json and accuracy.json\nmethod_config    Specify the post-processing parameters.                   fps.json, latency.json and accuracy.json\n\n\nPerformance Evaluation#\n\nPerformance evaluation is divided into latency and fps.\n\nHow to Use Performance Evaluation Scripts#\n\nlatency:\n\nIn the directory of the to-be-evaluated model, run sh latency.sh to evaluate\nsingle frame latency, as shown below:\n\n\n\nNote\n * infer denotes the time consumption of model inference.\n * Post process denotes the time consumption of post-processing.\n\nfps:\n\nThis function uses multi-threaded concurrency and is designed to allow the model\nto reach the ultimate performance on BPU. Due to the multi-thread concurrency\nand data sampling, the frame rate value will be low during the start-up phase,\nthen the frame rate will increase and gradually stabilize, with the frame rate\nfluctuating within 0.5%. To test the frame rate, go to the model directory and\nrun sh fps.sh, as shown below.\n\n\n\nAbout Command-line Parameters#\n\nThe fps.sh script is shown as below:\n\n\n\nThe latency.sh script is shown as below:\n\n\n\nResult Visualization#\n\nIf you want to see the effect of a single inference of the model, you can modify\nworkflow_latency.json and re-run the latency.sh script to generate the display\neffect in the output_dir directory.\n\nAttention\n\nWhen the display effect is generated, the script will run slowly due to the dump\neffect. Only the latency.sh script dump is supported.\n\nThe Visual operation steps are as follows:\n\n 1. Modify the latency configuration file\n    \n    \n\n 2. Execute the latency.sh script\n    \n    \n\nAttention\n\nThe visualization of the bev model needs to specify the scene information and\nthe path of the homography matrix. The homography matrix is used for the\nconversion of the camera perspective and the bird's-eye view. Different scenes\nhave their own homography matrices. The visualization of the online map\nconstruction model needs to specify the perception range of local map.\n\nThe workflow_latency.json configuration file of the bev model is recommended to\nbe modified as follows:\n\n\n\nThe workflow_latency.json configuration file of the online map construction\nmodel is recommended to be modified as follows:\n\n\n\nThe visualization results that can be achieved by different types of models are\ndifferent, refer to the table below:\n\nAttention\n\nIf you need to visualize images other than minidata during trajectory prediction\nvisualization, you need to configure additional road information and trajectory\ninformation files in minidata/argoverse1/visualization. You can use the\ndensent_process.py preprocessing script to generate configuration files, and set\n--is-gen-visual-config parameter to true.\n\n\nModel Accuracy Evaluation#\n\nTake the following 5 steps to perform the model evaluation:\n\n 1. Data pre-process.\n\n 2. Data mounting.\n\n 3. The lst file generation.\n\n 4. Model inference.\n\n 5. Model accuracy computing.\n\nData Pre-processing #\n\nThe following section will provide the description of the PTQ and QAT model data\npreprocessing methods.\n\nPTQ Model Data Pre-processing:\n\nTo the PTQ model data pre-processing, run the hb_eval_preprocess tool in x86 to\npre-process data. The so-called pre-processing refers to the special processing\noperations before images are fed into the model. For example: resize, crop and\npadding, etc. The tool is integrated into the horizon_tc_ui tool and it will be\navailable after the tool is installed using the install script. After the raw\ndataset is pre-processed by the tool, the corresponding pre-processed binary\nfile .bin file set of the model will be generated. You can directly run\nhb_eval_preprocess --help for help.\n\nTip\n\nAbout the hb_eval_preprocess tool command line parameters and usage, you can\ntype hb_eval_preprocess -h, or see the hb_eval_preprocess Tool in the PTQ tools\nguide.\n\nThe datasets corresponding to each model in the sample package are described in\ndetail below, as well as the pre-processing operations for the corresponding\ndatasets.\n\nThe datasets used for PTQ models include the following:\n\nQAT Model Data Pre-processing:\n\nThe QAT model data pre-process needs to execute the preprocess scripts in\nai_benchmark_j6/j6/qat/tools/eval_preprocess in the x86 environment. The\ndatasets corresponding to each model in the sample package are described in\ndetail as below, as well as the pre-processing operations for the corresponding\ndatasets.\n\nTip\n\nBefore use, please modify the dataset path and save path in the script to make\nthe script run properly.\n\nModel Mounting#\n\nBecause datasets are huge, it is recommended to mount them for dev board to\nload, rather than to copy them into the dev board, you need to do the following\non the server PC terminal and the board terminal:\n\nServer PC terminal:\n\nAttention\n\nNote that the root permission is required on the server pc terminal to perform\nthe following actions.\n\n 1. Edit one line into /etc/exports: /nfs\n    *(insecure,rw,sync,all_squash,anonuid=1000,anongid=1000,no_subtree_check).\n    Wherein, /nfs denotes mounting path of local machine, it can be replaced by\n    the directory you specify.\n\n 2. Run exportfs -a -r to bring /etc/exports into effect.\n\nBoard terminal:\n\n 1. Create the directory to be mounted: mkdir -p /mnt.\n\n 2. Mount: mount -t nfs {PC terminal IP}:/nfs /mnt -o nolock.\n\nMount the /nfs folder at PC terminal to the /mnt folder in dev board. In this\nway, mount the folder in which contains preprocessed folder to dev board and\ncreate a soft link of /data folder in the /ptq or /qat folder (at the same\ndirectory level as /script) in dev board.\n\nGenerate lst Files#\n\nThe running process of precision calculation script in the sample is:\n\n 1. According to the value of image_list_file in workflow_accuracy.json, find\n    the lst file of the corresponding dataset.\n\n 2. Load each preprocessing file according to the path information of\n    preprocessing file stored in lst file, and then perform the inference.\n\nTherefore, after generating the preprocessing file, you need to generate the\ncorresponding LST file, and write the path of each preprocessing file into the\nlst file, which is related to the storage location of the dataset at the board\nend. Here, we recommend that its storage location shall be the same level as the\n./data/dataset_name/pre_model_name folder.\n\nThe structure of the PTQ pre-processed dataset is as follows:\n\n\n\nThe structure of the QAT pre-processed data set is as follows:\n\n\n\nThe corresponding lst file is generated by reference as follows:\n\nExcept for the densetnt_argoverse1, bev, motr_efficientnetb3_mot17,\nstereonetplus_mixvargenet_sceneflow and maptroe_henet_tinym_bevformer_nuscenes\nmodels, the reference generation method of the lst files for the other models:\n\n\n\nNote\n\nThe parameters after -name need to be adjusted according to the format of the\npreprocessed dataset, such as bin, png.\n\nThe path stored in the generated lst file is a relative path:\n../../../data/coco/pre_centernet_resnet101/ , which can match the\nworkflow_accuracy.json default configuration path.\n\nIf you need to change the storage location of the preprocessing dataset, you\nneed to ensure that the corresponding lst file can be used by\nworkflow_accuracy.json. Secondly, it is necessary to ensure that the program can\nread the corresponding preprocessing file according to the path information in\nlst file.\n\nFor the densetnt_argoverse1, bev, motr_efficientnetb3_mot17,\nstereonetplus_mixvargenet_sceneflow and maptroe_henet_tinym_bevformer_nuscenes\nmodels, the reference generation method of the lst files:\n\nModel Inference#\n\nThe accuracy.sh script is shown as below:\n\n\n\nAfter the data has been mounted, log in dev board and run the accuracy.sh script\nin the centernet_resnet101 directory, as shown below:\n\n\n\nInference results will be saved into the eval.log file dumped by dev board\nprogram.\n\nModel Accuracy Computing#\n\nAttention\n\nPlease perform the accuracy calculation in docker environment or Linux\nenvironment.\n\nAccuracy computing is presented in two cases: PTQ model accuracy computing and\nQAT model accuracy computing.\n\nPTQ Model Accuracy Computing:\n\nFor the PTQ model, the model accuracy computing scripts are placed under the\nptq/tools/python_tools/accuracy_tools folder, in which:\n\nSCRIPT                   DESCRIPTION\ncls_imagenet_eval.py     The script is used for computing accuracy of classification\n                         models evaluated using the ImageNet dataset.\ndet_coco_eval.py         The script is used for computing the accuracy of models\n                         evaluated using the COCO dataset.\nseg_cityscapes_eval.py   The script is used for computing the accuracy of\n                         segmentation models evaluated using the Cityscapes dataset.\ndet_voc_eval.py          The script is used for computing the accuracy of detection\n                         models using the VOC dataset.\n\nBelow we provide you with the description of the different types of PTQ model\naccuracy computing:\n\nQAT Model Accuracy Computing:\n\nFor the QAT model, the model accuracy computing scripts are placed under the\nqat/tools/python_tools/accuracy_tools folder, in which:\n\nSCRIPT                    DESCRIPTION\nbev_eval.py               The script is used to calculate the accuracy of bev model.\ncenterpoint_eval.py       The script is used to calculate the accuracy of\n                          centerpoint_pointpillar_nuscenes lidar 3D model.\ncls_eval.py               The script is used to calculate the accuracy of the\n                          classification model.\ndensetnt_eval.py          The script is used to calculate the accuracy of\n                          densetnt_argoverse1 model.\ndetr_eval.py              The script is used to calculate the accuracy of detr and\n                          deform_detr_resnet50_mscoco model.\nfcos3d_eval.py            The script is used to calculate the accuracy of\n                          fcos3d_efficientnetb0_nuscenes model.\nfcos_eval.py              The script is used to calculate the accuracy of fcos model.\nganet_eval.py             The script is used to calculate the accuracy of\n                          ganet_mixvargenet_culane model.\nkeypoints_eval.py         The script is used to calculate the accuracy of keypoints\n                          model.\nlidar_multitask_eval.py   The script is used to calculate the accuracy of lidar\n                          centerpoint_mixvargnet_multitask_nuscenes model.\nmotr_eval.py              The script is used to calculate the accuracy of\n                          motr_efficientnetb3_mot17 model.\nparsing_eval.py           The script is used to calculate the accuracy of segmentation\n                          model.\npointpillars_eval.py      The script is used to calculate the accuracy of pointpillars\n                          model.\nstereonet_eval.py         The script is used to calculate the accuracy of\n                          stereonet_plus model.\nmaptroe_eval.py           The script is used to calculate the accuracy of maptr model.\nflashocc_eval.py          The script is used to calculate the accuracy of flashocc\n                          model.\n\nBelow we provide you with the description of the different types of QAT model\naccuracy computing:\n\n\nModel Integration#\n\n\nPre-processing#\n\nYou can add model pre-processing as needed and deploy it to CPU or DSP, taking\ncenterpoint_pointpillar_nuscenes as an example:\n\n 1. Add the preprocessing file qat_centerpoint_preprocess_method.cc and the\n    header file qat_centerpoint_preprocess_method.h.\n\n 2. Add model preprocessing configuration file.\n\nAdd Pre-processing File#\n\nThe pre-processing qat_centerpoint_preprocess_method.cc files are placed under\nthe ai_benchmark/code/src/method/ folder. While the header file\nqat_centerpoint_preprocess_method.h files are placed under the\nai_benchmark/code/include/method/ floder.\n\n\n\nAdd Preprocessing Configuration File#\n\n\n\nPreprocess of centerpoint_pointpillar_nuscenes can deploy to CPU or DSP depends\non whether you config the run_on_dsp parameter in the\ncenterpoint_pointpillar_5dim.json. If run_on_dsp in config file is set to true\nthen preprocess will be running on DSP otherwise it running on CPU.\n\nTo Evalute the Latency of Preprocessing#\n\nRun sh latency.sh to evaluate single frame latency of preprocess, as shown\nbelow:\n\n\n\nIn which:\n\n * Pre process denotes the time consumption of pre-processing.\n * Infer denotes the time consumption of model inference.\n * Post process denotes the time consumption of post-processing.\n\n\nPostprocessing#\n\nPost-processing consists of 2 steps. Let's take integration of CenterNet model\nas an example:\n\n 1. Add the post-processing file ptq_centernet_post_process_method.cc and the\n    header file ptq_centernet_post_process_method.h.\n\n 2. Add a model execution script and a configuration file.\n\nAdd Post-processing File#\n\nPost-processing code file can reuse any post-processing files in the src/method\ndirectory. You only need to modify the InitFromJsonString function and the\nPostProcess function.\n\nThe InitFromJsonString function is used for loading the post-processing related\nparameters in the workflow.json. You can customize the corresponding input\nparameters. The PostProcess function is used for implementing post-processing\nlogic.\n\nThe post-processing ptq_centernet_post_process_method.cc files are placed under\nthe ai_benchmark/code/src/method/ folder. While the header files\nptq_centernet_post_process_method.h are placed under the\nai_benchmark/code/include/method/ folder.\n\n\n\nAdd Model Execution and Configuration Files#\n\nDirectory structure of scripts is shown as below:\n\n * The centerpoint_pointpillar_nuscenes model:\n   \n   \n   \n   To process on DSP, you need to execute dsp_deploy.sh to deploy the DSP\n   environment. For a detailed introduction to dsp deployment, please refer to\n   README.md.\n\n * The motr_efficientnetb3_mot17 model:\n   \n   \n\n * The models except for the centerpoint_pointpillar_nuscenes and\n   motr_efficientnetb3_mot17:\n   \n   \n\n\nHelper Tools#\n\n\nLog#\n\nThere are 2 types of logs: sample Log and DNN Log. Wherein, sample log refers to\nthe log in the AI Benchmark Sample Package deliverables, while DNN log refers to\nthe log in the embedded runtime library. You can specify logs as needed.\n\nSample Log#\n\n 1. Log level.\n\nBoth glog and vlog are used in sample log and there are 4 customized log levels:\n\n * 0: SYSTEM level, this log level is used for generating error information in\n   sample code.\n * 1: REPORT level, this log level is used for generating performance data in\n   sample code.\n * 2: DETAIL level, this log level is used for generating current system status\n   in sample code.\n * 3: DEBUG level, this log level is used for generating debugging information\n   in sample code.\n\n 2. Set log levels.\n\nRules to set log levels: The default ranks of log level:\nDEBUG>DETAIL>REPORT>SYSTEM, the higher the level, the more logs will be output.\nThat is, if you set a high level, the logs corresponding to your own level and\nthe level below it will be output.\n\nWhen running samples, specify the log_level parameter to set log levels. For\nexample, if log_level=0, then SYSTEM log should be dumped; else if log_level=3,\nthen DEBUG, DETAIL, REPORT and SYSTEM logs should be dumped.\n\ndnn Log#\n\nFor the configuration of dnn logs, please read the Configuration Info section in\nthe Model Inference API Instruction.\n\n\nOP Time Consumption#\n\nOverview#\n\nUse the HB_DNN_PROFILER_LOG_PATH environment variable to specify statistics of\nOP performance. Types and values of this environment variable are described as\nbelow:\n\nHB_DNN_PROFILER_LOG_PATH=${path}: denotes the output path of OP node. After the\nprogram is executed, a profiler.log file should be generated.\n\nSample#\n\nTaking the mobilenetv1 as an example, as shown in the following code block:\nStart 1 threads to run the model at the same time, set export\nHB_DNN_PROFILER_LOG_PATH=. /, then the profiler.log file will output the\nperformance data of the OPs.\n\nThe output information contains model_latency and task_latency. Wherein,\nmodel_latency contains the time consumption required to run each operator of the\nmodel; while task_latency contains the time consumption of each task of the\nmodel.\n\n\nDump Tool#\n\nEnable the HB_DNN_DUMP_PATH environment variable to dump the input and output of\neach node in inference process. The dump tool can check if there are consistency\nproblems between simulator and real machine, i.e. Whether the output of the real\nmachine and the simulator are exactly the same, given the same model and the\nsame inputs.","routePath":"/en/guide/ucp/runtime/ai_benchmark","lang":"en","toc":[{"text":"Deliverables Description","id":"deliverables-description","depth":2,"charIndex":664},{"text":"Sample Code Package","id":"sample-code-package","depth":3,"charIndex":1088},{"text":"Sample Models","id":"sample-models","depth":3,"charIndex":1916},{"text":"Public Datasets","id":"public-datasets","depth":3,"charIndex":2918},{"text":"Development Environment","id":"development-environment","depth":2,"charIndex":3137},{"text":"How to Use","id":"how-to-use","depth":2,"charIndex":4247},{"text":"Evaluation Scripts","id":"evaluation-scripts","depth":3,"charIndex":4261},{"text":"Parameters of json Configuration File","id":"parameters-of-json-configuration-file","depth":3,"charIndex":5355},{"text":"input_config","id":"input_config","depth":4,"charIndex":5869},{"text":"output_config","id":"output_config","depth":4,"charIndex":5884},{"text":"Workflow Configuration","id":"workflow-configuration","depth":4,"charIndex":6790},{"text":"Performance Evaluation","id":"performance-evaluation","depth":3,"charIndex":7326},{"text":"How to Use Performance Evaluation Scripts","id":"how-to-use-performance-evaluation-scripts","depth":4,"charIndex":7408},{"text":"About Command-line Parameters","id":"about-command-line-parameters","depth":4,"charIndex":8145},{"text":"Result Visualization","id":"result-visualization","depth":4,"charIndex":8261},{"text":"Model Accuracy Evaluation","id":"model-accuracy-evaluation","depth":3,"charIndex":9876},{"text":"Data Pre-processing","id":"data-pre-processing","depth":4,"charIndex":-1},{"text":"Model Mounting","id":"model-mounting","depth":4,"charIndex":11685},{"text":"Generate lst Files","id":"generate-lst-files","depth":4,"charIndex":12745},{"text":"Model Inference","id":"model-inference","depth":4,"charIndex":14709},{"text":"Model Accuracy Computing","id":"model-accuracy-computing","depth":4,"charIndex":14995},{"text":"Model Integration","id":"model-integration","depth":2,"charIndex":18384},{"text":"Pre-processing","id":"pre-processing","depth":3,"charIndex":18405},{"text":"Add Pre-processing File","id":"add-pre-processing-file","depth":4,"charIndex":18729},{"text":"Add Preprocessing Configuration File","id":"add-preprocessing-configuration-file","depth":4,"charIndex":19006},{"text":"To Evalute the Latency of Preprocessing","id":"to-evalute-the-latency-of-preprocessing","depth":4,"charIndex":19329},{"text":"Postprocessing","id":"postprocessing","depth":3,"charIndex":19655},{"text":"Add Post-processing File","id":"add-post-processing-file","depth":4,"charIndex":19959},{"text":"Add Model Execution and Configuration Files","id":"add-model-execution-and-configuration-files","depth":4,"charIndex":20652},{"text":"Helper Tools","id":"helper-tools","depth":2,"charIndex":21131},{"text":"Log","id":"log","depth":3,"charIndex":21147},{"text":"Sample Log","id":"sample-log","depth":4,"charIndex":21389},{"text":"dnn Log","id":"dnn-log","depth":4,"charIndex":22379},{"text":"OP Time Consumption","id":"op-time-consumption","depth":3,"charIndex":22509},{"text":"Overview","id":"overview","depth":4,"charIndex":22531},{"text":"Sample","id":"sample","depth":4,"charIndex":22851},{"text":"Dump Tool","id":"dump-tool","depth":3,"charIndex":23340}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":185,"title":"Basic Sample User Guide","content":"#\n\nThe Basic Sample Package is designed to help you familiarize yourself with and\nlearn the interfaces and various advanced functions related to model inference.\nThe Basic Sample Package can also be used as a starting point for new projects\nto help you quickly build a basic framework and conduct further development and\ncustomization on this basis.\n\nThe Basic Sample Package provides samples in three aspects:\n\n * Provides a quick start sample based on the resnet50 model. You can experience\n   and develop applications based on this sample to lower the development\n   threshold.\n\n * Provides a sample of using the model inference interface. You can use this\n   sample to better familiarize yourself with the use of various basic\n   functions.\n\n * Provides advanced sample of various special functions. You can choose the\n   appropriate function for model inference according to the actual usage\n   scenario.\n\n\nDeliverables Description#\n\nThe Basic Sample package is located in the\nsamples/ucp_tutorial/dnn/basic_samples/ path of the horizon_j6_open_explorer\nrelease package and consists the following main contents:\n\nRELEASE PACKAGE   DESCRIPTIONS\nbasic_samples     including sample codes and scripts.\n\nNote\n\nThe on-board model needs to be obtained first by executing the\nresolve_runtime_sample.sh script in the\nsamples/ai_toolchain/model_zoo/runtime/basic_samples directory of OE package.\n\nDirectory of the Basic Sample Package is shown as below:\n\n\n\n * The code directory contains source code.\n\n * code/00_quick_start: A quick start sample, based on the model inference\n   interface, using resnet50 to perform single image model inference and result\n   analysis.\n\n * code/01_api_tutorial: Samples of using the model inference interface,\n   including adding/removing padding sample padding, quantization and\n   dequantization samples quanti, memory usage samples mem, model loading and\n   information acquisition samples model, and model inference samples with roi\n   input roi_infer.\n\n * code/02_advanced_samples: Advanced samples of model inference, including\n   cropping an image as model input sample crop, the inference sample roi_infer\n   of the model with roi input and the multi-model batch inference sample\n   multi_model_batch.\n\n * The code/build.sh is a application compilation script, and you need to\n   specify the compilation parameters, called by build_aarch64.sh and\n   build_x86.sh.\n\n * The code/build_aarch64.sh is a quick application compilation script, and the\n   compiled application runs in the aarch64 environment.\n\n * The code/build_x86.sh is a quick application compilation script, and the\n   compiled application runs in the x86 environment.\n\n * The code/resolve.sh is the program running dependency acquisition script,\n   which needs to be executed before the user compiles the model.\n\n * The runtime folder contains scripts to run samples, preset data and related\n   models.\n\n\nPrerequisites#\n\n\nPrepare the Dev Board#\n\n 1. After getting the development board, upgrade the system image file to the\n    version recommended by the sample package.\n\n 2. Make sure the local development machine and development board can be\n    connected remotely.\n\n\nCompilation#\n\nTake the following steps to perform the compilation:\n\n 1. Install the aarch64-none-linux-gnu-gcc and aarch64-none-linux-gnu-g++\n    cross-compilation tool.\n\n 2. In the ucp_tutorial/tools/basic_samples directory, there is a pre-configured\n    build script build.sh, with the options -a x86 and -a aarch64 to support two\n    types of builds respectively, and executing the build.sh script directly\n    will complete the one-click build, and the generated files will be saved to\n    the ucp_tutorial/vp/vp_samples directory. Moreover, the directory\n    encompasses two compilation scripts, namely build_aarch64.sh and\n    build_x86.sh, tailored to distinct compilation configurations. Employing\n    these scripts mirrors the functionality of the build.sh script.\n\n 3. After executing the compilation script, the executable programs and\n    dependency files required to run the samples will be generated and copied to\n    the runtime/script or runtime/script_x86 directory.\n\nNote\n\nBy default, the cross-compilation tool specified in the build.sh script is\nlocated in the /usr/bin directory. You can manually modify the build.sh script\nif you want to change the location.\n\n\nHow to Use (Basic Samples)#\n\nNote\n\nThe version in the log and command line below is just a sample, and the version\nin the actual printed log is subject to the version in the OE package.\n\nIf you want to run the sample in an x86 environment, please use the script under\nruntime/script_x86.\n\nSample scripts are in the runtime/script folder, and the directory structure\nafter compilation is shown as follows:\n\n\n\nBefore running, deploy the runtime directory to the board end and execute the\ncorresponding script.\n\nNote\n\n * To obtain the model publisher for the basic_samples sample package, you can\n   execute the resolve_runtime_sample.sh in the\n   samples/ai_toolchain/model_zoo/runtime/basic_samples dictionary of the OE\n   package.\n\n * To obtain other dependencies of the basic_samples sample package, you can\n   execute the resolve.sh script in the\n   samples/ucp_tutorial/dnn/basic_samples/code directory of the OE package.\n\n * The model folder contains the path of the model, the runtime folder is a soft\n   link and the link path is ../../../model_zoo/runtime/basic_samples.\n\n\nquick_start#\n\nUnder 00_quick_start directory, we provide a quick-start sample to introduce the\nuse process of model inference. Taking the resnet50 as a sample, it shows the\nprocess of model inference for nv12 input and rgb input respectively, including\nthe complete process code from data preparation to model inference, to execution\nof post-processing, and finally to generating classification results.\n\nThe code is mainly divided into six parts:\n\n * Load the model and get the model handle: the main interfaces involved are\n   hbDNNInitializeFromFiles, hbDNNGetModelNameList and hbDNNGetModelHandle.\n\n * Prepare input and output tensors, and complete the dynamic information of the\n   input: the main interfaces involved are hbDNNGetInputCount,\n   hbDNNGetOutputCount, hbDNNGetInputTensorProperties,\n   hbDNNGetOutputTensorProperties, hbDNNGetInputName, hbDNNGetOutputName and\n   hbUCPMallocCached.\n\n * Prepare input data: For nv12 input, the image needs to be converted into\n   color space, and for rgb input, pre-processing from U8 to S8 is required. The\n   interface involved is hbUCPMemFlush.\n\n * Model inference: The main interfaces involved are hbDNNInferV2,\n   hbUCPSubmitTask and hbUCPWaitTaskDone.\n\n * Output data post-processing: The resnet50 model is a classification model,\n   and you can obtain classification results with high confidence through\n   post-processing. Before processing the output, you need to call the\n   hbUCPMemFlush interface for data synchronization.\n\n * Release tasks, memory and models: The main interfaces involved are\n   hbUCPReleaseTask, hbUCPFree and hbDNNRelease.\n\nThe directory structure of the script is as follows:\n\n\n\nWhen using it, enter the 00_quick_start directory, and then directly execute sh\nrun_resnet_rgb.sh and sh run_resnet_nv12.sh. After the script is successfully\nexecuted, it will print the names of the model input and output and the 5\nclassification results with the highest confidence. The classification result\nwith the highest confidence number 340 indicates that it is a zebra.\n\n\napi_tutorial#\n\nThe API tutorial refers to the samples in the 01_api_tutorial folder. These\nsamples help developers understand how to use the model inference APIs. This\nfolder contains the following scripts:\n\n\n\npadding#\n\nThis sample helps you become familiar with how to add padding to model input and\nhow to remove padding from model output. The main data types involved are\nvalidShape and stride in hbDNNTensorProperties.\n\nIn order to speed up calculations, the model has alignment requirements for\ninput data, see alignment rules for details. This sample is divided into two\nparts:\n\n * For model input, when additional alignment operations are required for data,\n   use the add_padding method provided in the sample to add padding.\n\n * For model output, when the output contains padding, use the remove_padding\n   method provided in the sample to remove padding to obtain valid data.\n\nWhen using it, directly enter the 01_api_tutorial directory, and then directly\nexecute sh padding.sh. After the script is successfully executed, it will print\nthe original input and the input data after adding padding, the original output\nand the output data after removing padding.\n\nquanti#\n\nThis sample mainly helps you get familiar with the quantization and\ndequantization of the model input and output. The data types involved are\nquantiType, scale, and quantizeAxis in hbDNNTensorProperties.\n\nWhen the model input contains quantization information, the input floating point\nnumber needs to be quantized according to the specified rules; when the model\noutput contains quantization information, the output needs to be dequantized\nbefore post-processing the data. For the processing method, please refer to\nhbDNNQuantiScale.\n\nThis sample is mainly divided into four parts:\n\n * Quantization of input per tensor\n\n * Quantization of input per axis\n\n * Dequantization of output per tensor\n\n * Dequantization of output per axis\n\nWhen using it, just go to the 01_api_tutorial directory and execute sh quanti.sh\ndirectly. After the script is successfully executed, each part will print the\noriginal input/output and the quantized/dequantized results.\n\nmodel#\n\nThis sample mainly helps you get familiar with the use of interfaces related to\nmodel loading and model information acquisition.\n\nThe interfaces involved mainly include hbDNNInitializeFromFiles,\nhbDNNGetModelNameList, hbDNNGetModelHandle, hbDNNGetInputCount,\nhbDNNGetOutputCount, hbDNNGetInputTensorProperties,\nhbDNNGetOutputTensorProperties and hbDNNRelease。\n\nWhen using it, directly enter the 01_api_tutorial directory, and then execute sh\nmodel.sh directly. After the script is executed successfully, the basic\ninformation of the model will be printed out.\n\nsys_mem#\n\nThis sample mainly helps you get familiar with the use of memory interfaces. The\nmain interfaces involved are hbUCPMalloc、 hbUCPMallocCached, hbUCPMemFlush and\nhbUCPFree.\n\nFor cacheable memory, the usage steps are as follows:\n\n * Apply for cacheable memory.\n\n * Write data to memory.\n\n * Call the hbUCPMemFlush interface and use the HB_SYS_MEM_CACHE_CLEAN parameter\n   to synchronize data to DDR.\n\n * After model inference or operator calculation, call the hbUCPMemFlush\n   interface and use the HB_SYS_MEM_CACHE_INVALIDATE parameter to synchronize\n   data to the cache.\n\n * Read data for processing.\n\n * Release memory.\n\nWhen using it, directly enter the 01_api_tutorial directory and execute sh\nsys_mem.sh.\n\n\nadvanced_samples#\n\nThis sample refers to the samples in the 02_advanced_samples directory, which is\nintended to introduce the use of advanced samples. Its directory contains the\nfollowing scripts:\n\n\n\ncrop#\n\nThis sample mainly helps you get familiar with how to crop images and use them\nas model input for inference. The overall process of the sample code is the same\nas 00_quick_start, and the difference lies mainly in the preparation of input\ndata.\n\nThe principle of cropping in this sample is to offset the memory address of the\nexisting image to the upper left corner of the ROI, and mask the redundant part\nof the image by controlling the size of the stride to prepare the input of the\nmodel.\n\nLimitations of sample use:\n\n * Image: The image resolution is required to be large, at least larger than the\n   input of the model, and the image is read into the BPU memory in advance.\n\n * Model: The input validShape of the model is required to be fixed and the\n   stride is dynamic, so that the image can be cropped by controlling the size\n   of the stride.\n\n * Cropping position: Since cropping is to offset the image memory, and the\n   first address of the input memory requires 32 alignment, there is a limit on\n   the size of the offset.\n\nPreparation of input data:\n\n * Image: The size of the zebra image is 376x376. We read the image and convert\n   it into Y and UV input and store them in two BPU memories after 32 alignment.\n   The width of the zebra image after 32 alignment is image_width_stride = 384.\n\n * Model: The model has two inputs\n   \n   * y: validShape = (1,224,224,1), stride = (-1,-1, 1,1)\n   \n   * uv: validShape = (1,112,112,2), stride = (-1,-1,2,1)\n\n * Cropping: We need to crop a 224x224 area in the image as the input of the\n   model. Selecting (64,50) as the upper left corner point can just get most of\n   the zebra image, and the offset memory address is exactly 32 aligned, which\n   meets the requirements. As shown in the figure below.\n\n * Input tensor preparation:\n   \n   * The stride of the Y input is [224 * image_width_stride, image_width_stride,\n     1, 1], and the coordinates of the upper left corner of the cropped image\n     are [0, 50, 64, 0], then the address offset is 50 * image_width_stride + 64\n     * 1;\n   \n   * The stride of the UV input is [112 * image_width_stride,\n     image_width_stride, 2, 1], and the coordinates of the upper left corner of\n     the cropped image are [0, 25, 32, 0], then the address offset is 25 *\n     image_width_stride + 32 * 2;\n\nWhen using, enter directly 02_advanced_samples directory, and then directly\nexecute sh run_crop.sh. After the script is successfully executed, the five\nclassification results with the highest confidence will be printed. The\nclassification result with the highest confidence number 340 indicates that it\nis a zebra.\n\nmulti_model_batch#\n\nThis sample mainly helps you get familiar with the function of batch processing\nof small models.\n\nWhen there are multiple small models running, if each model task runs\nseparately, the scheduling time of the entire framework will be relatively\nlarge. In order to avoid performance loss, you can combine multiple small model\ntasks into one task for inference to reduce the proportion of framework\nscheduling time.\n\nThis sample takes two small models googlenet and resnet50 as samples, and\ncreates/adds tasks by calling the hbDNNInferV2 interface multiple times.\n\nWhen using it, directly enter the 02_advanced_samples directory and execute sh\nrun_multi_model_batch.sh. After the script is successfully executed, the\nclassification results with the highest confidence of the two models will be\nprinted. The classification number 340 indicates that it is a zebra.\n\nroi_infer#\n\nThis sample mainly helps you understand how to prepare data for model inference\nwhen there is a roi input.\n\nThe overall process of the sample code is the same as 00_quick_start, and the\ndifference lies mainly in the preparation of input data. The sample model is\nmobilenetv1, and the input is 3 in total, as shown below:\n\n * name: data_y；validShape: (1,-1,-1,1)；stride: (-1,-1,1,1)；\n\n * name: data_uv；validShape: (1,-1,-1,2)；stride: (-1,-1,2,1)；\n\n * name: data_roi；validShape: (1,4)；stride: (16,4)；\n\ndata_y and data_uv are dynamic inputs. They are read from the image and\nconverted into Y and UV data for alignment as input, and validShape and stride\nare completed according to the size of the image. For dynamic input, please\nrefer to Introduction to Dynamic Input.\n\ndata_roi is the coordinates of the upper left and lower right corners of the ROI\narea in the image, in the order of left, upper, right, and lower.\n\nWhen using it, directly enter the 01_api_tutorial directory, and then execute sh\nroi_infer.sh directly. After the script is successfully executed, it will print\nthe five classification results with the highest confidence for the two sets of\ninputs of the model. The classification result with the highest confidence\nnumber 340 indicates that it is a zebra.\n\n\nHelper Tools (Log)#\n\nThere are 2 types of logs: sample log and dnn log. Wherein, sample log refers to\nthe log in the Basic Sample Package release package; while dnn log refers to the\nlog in the embedded dnn library.Developers can specify logs according to their\nown needs.\n\n\nSample Log#\n\nThe sample log mainly uses hlog, which is mainly divided into 7 levels:\n\nThe log level can be set to 0, 1, 2, 3, 4, 5 and 6, corresponding to trace,\ndebug, info, warn, error, critical and never, with the default being info.\n\n\ndnn Log#\n\nFor the configuration of dnn logs, please read the Configuration Info section in\nthe Model Inference API Introduction.","routePath":"/en/guide/ucp/runtime/basic_sample","lang":"en","toc":[{"text":"Deliverables Description","id":"deliverables-description","depth":2,"charIndex":911},{"text":"Prerequisites","id":"prerequisites","depth":2,"charIndex":2905},{"text":"Prepare the Dev Board","id":"prepare-the-dev-board","depth":3,"charIndex":2922},{"text":"Compilation","id":"compilation","depth":3,"charIndex":3171},{"text":"How to Use (Basic Samples)","id":"how-to-use-basic-samples","depth":2,"charIndex":4354},{"text":"quick_start","id":"quick_start","depth":3,"charIndex":5434},{"text":"api_tutorial","id":"api_tutorial","depth":3,"charIndex":7478},{"text":"padding","id":"padding","depth":4,"charIndex":7688},{"text":"quanti","id":"quanti","depth":4,"charIndex":8649},{"text":"model","id":"model","depth":4,"charIndex":9613},{"text":"sys_mem","id":"sys_mem","depth":4,"charIndex":10182},{"text":"advanced_samples","id":"advanced_samples","depth":3,"charIndex":10903},{"text":"crop","id":"crop","depth":4,"charIndex":11103},{"text":"multi_model_batch","id":"multi_model_batch","depth":4,"charIndex":13726},{"text":"roi_infer","id":"roi_infer","depth":4,"charIndex":14606},{"text":"Helper Tools (Log)","id":"helper-tools-log","depth":2,"charIndex":15893},{"text":"Sample Log","id":"sample-log","depth":3,"charIndex":16168},{"text":"dnn Log","id":"dnn-log","depth":3,"charIndex":16407}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":186,"title":"Alignment Rule","content":"#\n\nThis section describes the alignment restriction rules when using BPU.\n\n\nModel Input Requirement#\n\nBPU does not restrict the model input size or parity. Both 416x416 inputs (e.g.,\nYOLO) and 227x227 inputs (e.g., SqueezeNet) can be supported.\n\nFor NV12, Both H & W of the input are required to be even to meet the\nrequirement that the UV is half of the Y.\n\n\nAlignment and Efficient Data#\n\nThe BPU has alignment requirements for data. Valid data arrangement and aligned\ndata arrangement are represented by validShapeand stride in\nhbDNNTensorProperties.\n\n * validShape is the shape of the valid data.\n * stride represents the stride of each dimension of validShape, describing the\n   number of bytes required to cross each dimension of the tensor. Note that\n   models with hbDNNDataType as NV12 or Y type inputs are special cases where\n   the obtained stride is always 0, because these types of inputs only require\n   alignment in the width (W) direction by 32.\n\nThe model input and output tensors can get the correct data layout through\nvalidShape and stride. For example, if the getted model input attribute\nhbDNNDataType = HB_DNN_TENSOR_TYPE_U16, validShape = [1, 3, 212, 212] and stride\n= [301056, 100352, 448, 2], this indicates that the valid input size of the\nmodel is 1x3x212x212 .\n\n * stride[3] = 2 indicates that each element is 16 bits in size.\n * stride[2] = 448 = 2 * 224 indicates that the dimension at index=3 is aligned\n   according to 224, therefore the stride for the dimension at index=3 is 448.\n * stride[1] = 100352 = 448 * 224 indicates that the dimension at index=2 is\n   also aligned according to 224, therefore the stride for the dimension at\n   index=2 is 100352.\n * stride[0] = 301056 = 100352 * 3 indicates that ithe dimension at index=1 is\n   aligned according to 3 , consistent with the valid size, therefore the stride\n   for the dimension at index=1 is 301056.\n\nIn subsequent usage scenarios, considering the alignment requirements, when\nalignedByteSize > 0, it is recommended to apply for memory space according to\nthe size of alignedByteSize.\n\nYou can use the following method to determine whether the model input needs to\nbe aligned. If the formula does not hold, you need to perform additional\nalignment operations on the input data.\n\n$\\prod_^validShape.dimensionSize[i] == stride[0]$, where,\nn=validShape.numDimensions.\n\n\nModel input and output memory requirements#\n\nBPU has an alignment restriction on the first address of the model input and\noutput memory, requiring the first address of the input and output memory to be\n32 aligned.\n\nNote\n\n * The first address of the memory requested by hbUCPMalloc and\n   hbUCPMallocCached interfaces is aligned to 32 by default.\n\n * When you request the memory and use the offset address as the input of the\n   model, please check whether the first address after the offset is 32 aligned.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/alignment_rule","lang":"en","toc":[{"text":"Model Input Requirement","id":"model-input-requirement","depth":2,"charIndex":75},{"text":"Alignment and Efficient Data","id":"alignment-and-efficient-data","depth":2,"charIndex":359},{"text":"Model input and output memory requirements","id":"model-input-and-output-memory-requirements","depth":3,"charIndex":2358}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":187,"title":"Model Inference API Overview","content":"#\n\nThis section mainly introduces the API, data, structures, layout, and alignment\nrules of model inference. By reading this section, you can use APIs to complete\noperations such as model loading and releasing, model information obtaining, and\nmodel inference on dev boards provided by Horizon Robotics.\n\n\nData Structure#\n\n\nFunction Interface#","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/bpu_sdk_api_overview","lang":"en","toc":[{"text":"Data Structure","id":"data-structure","depth":2,"charIndex":305},{"text":"Function Interface","id":"function-interface","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":188,"title":"Configuration Info","content":"#\n\n\nEnvironment Variables#\n\n\n\n\nLog Level Setting Instruction#\n\n * Log level:\n\nThe logs in NN module are mainly divided into 7 levels:\n\nThe log level can be set to 0, 1, 2, 3, 4, 5 and 6, corresponding to Trace,\nDebug, Info, Warning, Error, Critical, and Never, with the default being\nWarning.\n\n * Log level setting rules:\n   \n   * If the occurring log level is greater than or equal to the set level, then\n     the log can be printed, otherwise, it will be shielded.\n   * The smaller the set log level, the more information is printed. For\n     example, if the log level is set to 3, which is the Warning level, then log\n     at levels 3, 4, 5 can all be printed. The default log level for the NN\n     module is the Warning level, so log messages at the Warning, Error, and\n     Critical levels can be printed.\n\n\nCustom Operator Library Settings Instructions#\n\nDNN supports model inference using custom operators. The settings are as\nfollows:\n\n\n\nNote\n\n * Multiple operator libraries are separated by colons.\n\n * If the same operator exists in different operator libraries, the operator in\n   the earlier operator library will be used first.\n\n * If the custom operator library does not exist, a warning message will be\n   printed and the loading of the operator library will be skipped.\n\n\nHBIR Model Inference Instructions#\n\nDNN supports inference of HBIR models in X86 environments. Since there may not\nbe a GPU in your environment, GPU acceleration is not used by default.\n\nIf there is a GPU on your machine, you can use GPU acceleration by setting the\nenvironment variable HB_NN_HBIR_GPU_ENABLE to true.\n\nNotes\n\n * When using GPU acceleration, make sure that the corresponding GPU driver and\n   CUDA environment are installed on your machine. For environment requirements,\n   please refer to Environment Deployment.\n\n * When using GPU acceleration, make sure libhbdnn.so is in the directory set by\n   LD_LIBRARY_PATH.\n\n * When using GPU acceleration, you can set the deviceId in the control\n   parameters to specify the GPU for calculation. Make sure the set value is in\n   the range [0, gpu_dvice_count).","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/configuration_info","lang":"en","toc":[{"text":"Environment Variables","id":"environment-variables","depth":2,"charIndex":3},{"text":"Log Level Setting Instruction","id":"log-level-setting-instruction","depth":2,"charIndex":30},{"text":"Custom Operator Library Settings Instructions","id":"custom-operator-library-settings-instructions","depth":2,"charIndex":812},{"text":"HBIR Model Inference Instructions","id":"hbir-model-inference-instructions","depth":2,"charIndex":1287}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":189,"title":"HB_DNN_TENSOR_MAX_DIMENSIONS","content":"#\n\n\n\nSet the maximum dimension of the tensor to 8.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/HB_DNN_TENSOR_MAX_DIMENSIONS","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":190,"title":"hbDNNDataType","content":"#\n\n\n\nTensor type.\n\nS stands for Signed, U for Unsigned, and F for Floating-point, followed by a\nnumber that stands for the number of bits.\n\n * Member\n\nMEMBER NAME                DESCRIPTION\nHB_DNN_TENSOR_TYPE_S4      Tensor type: Signed 4 bit.\nHB_DNN_TENSOR_TYPE_U4      Tensor type: Unsigned 4 bit.\nHB_DNN_TENSOR_TYPE_S8      Tensor type: Signed 8 bit.\nHB_DNN_TENSOR_TYPE_U8      Tensor type: Unsigned 8 bit.\nHB_DNN_TENSOR_TYPE_F16     Tensor type: 16 bit floating-point.\nHB_DNN_TENSOR_TYPE_S16     Tensor type: Signed 16 bit.\nHB_DNN_TENSOR_TYPE_U16     Tensor type: Unsigned 16 bit.\nHB_DNN_TENSOR_TYPE_F32     Tensor type: 32 bit floating-point.\nHB_DNN_TENSOR_TYPE_S32     Tensor type: Signed 32 bit.\nHB_DNN_TENSOR_TYPE_U32     Tensor type: Unsigned 32 bit.\nHB_DNN_TENSOR_TYPE_F64     Tensor type: 64 bit floating-point.\nHB_DNN_TENSOR_TYPE_S64     Tensor type: Signed 64 bit.\nHB_DNN_TENSOR_TYPE_U64     Tensor type: Unsigned 64 bit.\nHB_DNN_TENSOR_TYPE_BOOL8   Tensor type: Bool 8bit。\nHB_DNN_TENSOR_TYPE_MAX     Maximum tensor type number.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNDataType","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":191,"title":"hbDNNDescType","content":"#\n\n\n\nDescription information type, model or each input/output of the model can have\ndescription information to facilitate user use.\n\n * Member\n\nMEMBER NAME                DESCRIPTION\nHB_DNN_DESC_TYPE_UNKNOWN   Without description information.\nHB_DNN_DESC_TYPE_STRING    Description information of string type.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNDescType","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":192,"title":"hbDNNHandle_t","content":"#\n\n\n\nDNN handle, pointing to the single model.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNHandle_t","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":193,"title":"hbDNNPackedHandle_t","content":"#\n\n\n\nDNN handle, pointing to the packed multiple models.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNPackedHandle_t","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":194,"title":"hbDNNQuantiScale","content":"#\n\n\n\nQuantization/Dequantization scale data.\n\nInput: If the floating point data data is collected, the corresponding scale\ndata is scale, and the corresponding zero-point offset data is zeroPoint, then\nthe inference data sent to the model is $g((data / scale) + zeroPoint)$, in\nwhich, $g(x) = clip(round(x))$, clip is a truncation function, for example: U8:\n$g(x)∈[0, 255]$, S8: $g(x)∈[-128, 127]$.\n\nOutput: If the corresponding scale data of the inference result data is scale,\nand the corresponding zero-point offset data is zeroPoint, then the final\ninference result is $(data - zeroPoint) * scale$.\n\nThe scaleLen is determined by data according to per-axis or per-tensor\nquantization/dequantization. When the data is quantized/dequantized by\nper-tensor, the scaleLen is equal to 1, ignoring quantizeAxis. Otherwise the\nquantizeAxis represents the dimension index of the data quantization axis, and\nthe scaleLen is equal to the size of the quantizeAxis dimension of the data. The\nvalue of zeroPointLen is equal to scaleLen.\n\n * Member\n\nMEMBER NAME     DESCRIPTION\nscaleLen        Length of scale data.\nscaleData       Base address of scale data.\nzeropointLen    Length of zero-point offset data.\nzeropointData   Base address of zero-point offset data.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNQuantiScale","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":195,"title":"hbDNNQuantiType","content":"#\n\n\n\nFixed-point to floating-point conversion quantization/dequantization type.\n\nNONE: No data processing is needed.\n\nSCALE: The quantization/dequantization parameters corresponding to SCALE are\nstored in the hbDNNQuantiScale structure.\n\n * Member\n\nMEMBER NAME   DESCRIPTION\nNONE          No quantization.\nSCALE         Quantization type: SCALE.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNQuantiType","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":196,"title":"hbDNNTensor","content":"#\n\n\n\nTensor.\n\nUsed to store the input/output information.\n\n * Member","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensor","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":197,"title":"hbDNNTensorProperties","content":"#\n\n\n\nTensor information.\n\n * Member\n\nNote\n\nIf some dimensions in the validShape field are -1, it means that the validShape\nof the model is a dynamic input in this dimension, and you need to fill it in\naccording to the actual input before creating a task.\n\nThe quantizeAxis represents the dimension of the quantization axis index and is\nonly effective during per-axis quantization. When the data is quantized with\nper-tensor, quantizeAxis is negative.\n\nThe tensor information obtained through the interface is required by the model,\nyou can modify the corresponding tensor information according to the actual\ninput, at present, you are only allowed to modify the information of stride and\ntensorType, and must meet the requirements.\n\nstride:\n\n 1. If you prepare your input based on stride, you do not need to modify the\n    stride.\n\n 2. If you prepare your input based on validShape, you need to modify the value\n    of stride to be the step size of the jumps in each dimension of validShape,\n    and the inference library will internally padding the data.\n\n 3. If some dimensions of the stride obtained from the model are -1, it means\n    that the stride of the model is a dynamic input in this dimension, and you\n    need to fill it in according to the actual input before creating a task. For\n    the constraints of filling in, please refer to the Dynamic Input\n    Introduction section.\n\nThe alignedByteSize represents the size of the memory space occupied in the\ntensor, which can generally be used to apply for input and output memory.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensorProperties","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":198,"title":"hbDNNTensorShape","content":"#\n\n\n\nTensor shape.\n\nFor example, a tensor with numDimensions=4 and data layout 1x4x224x224 will have\nthe dimensionSize array storing the data in order as follows:\ndimensionSize[0]=1, dimensionSize[1]=4, dimensionSize[2]=224,\ndimensionSize[3]=224.\n\n * Member\n\nMEMBER NAME     DESCRIPTION\ndimensionSize   Size of each dimension of the tensor.\nnumDimensions   Dimension of the tensor.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/data_structure/hbDNNTensorShape","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":199,"title":"Dynamic input Introduction","content":"#\n\nWhen the model input tensor properties validShape or stride contains -1, it\nmeans that the model input is dynamic and you need to fill in the dynamic\ndimension according to the actual input.\n\nThe following rules should be followed when filling in stride:\n\n * Make sure the filled value satisfies $stride[idx] >= stride[idx+1] *\n   validShape.dimensionSize[idx+1]$, where idx represents the current dimension.\n\n * For stride of Y and UV inputs, due to some hardware constraints, dynamic\n   dimensions need to ensure 32-alignment, which means you need to align the\n   input according to stride.\n\nFor example, the model has three inputs:\n\n * input_y : validShape = [1,-1,-1,1]，stride = [-1,-1,1,1]\n * input_uv : validShape = [1,-1,-1,2]，stride = [-1,-1,2,1]\n * input_roi : validShape = [1,4]，stride = [16,4]\n\ninput_y and input_uv are dynamic inputs, and are Y and UV inputs. Assume that\nthe actually inputs: input_y's validShape = [1,220,220,1], input_uv's validShape\n= [1,110,110,2]. stride is calculated as follows to ensure that the dynamic\ndimension is aligned to 32, where ALIGN_32 represents 32-byte alignment:\n\n * input_y :\n   \n   stride[3] = 1;\n   \n   stride[2] = 1;\n   \n   stride[1] = ALIGN_32(stride[2] * validShape.dimensionSize[2]) = ALIGN_32(1 *\n   220) = 224;\n   \n   stride[0] = ALIGN_32(stride[1] * validShape.dimensionSize[1]) = ALIGN_32(224\n   * 220) = 49280;\n\n * input_uv :\n   \n   stride[3] = 1;\n   \n   stride[2] = 2;\n   \n   stride[1] = ALIGN_32(stride[2] * validShape.dimensionSize[2]) = ALIGN_32(2 *\n   110) = 224;\n   \n   stride[0] = ALIGN_32(stride[1] * validShape.dimensionSize[1]) = ALIGN_32(224\n   * 110) = 24640;\n\nYou need to set the following properties when preparing the input, and align the\ninput data according to stride:\n\n * input_y : validShape = [1,220,220,1]，stride = [49280,224,1,1]\n * input_uv : validShape = [1,110,110,2]，stride = [24640,224,2,1]\n * input_roi : validShape = [1,4]，stride = [16,4]","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/dynamic_input_introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":200,"title":"Model Inference","content":"#\n\n\nhbDNNInferV2#\n\n\n\nCreate synchronous/asynchronous inference tasks based on input parameters. For\nthe asynchronous inference task, the caller can use the returned taskHandle\nacross functions and threads.\n\n * Parameter\n   * [out] taskHandle Task handle pointer.\n   * [in/out] output Output of the inference task.\n   * [in] input Input of the inference task.\n   * [in] dnnHandle DNN handle pointer.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\nNote\n 1. If taskHandle is set to nullptr, a synchronous task will be created, as\n    returned by the interface, and the interface will be complete.\n 2. If *taskHandle is set to nullptr, an asynchronous task will be created, and\n    the taskHandle returned by the interface can be used for subsequent blocking\n    or callbacking.\n 3. If *taskHandle is not null and points to a previously created but\n    uncommitted task, a new task will be created and added to it.\n\nUp to 32 coexisting model tasks are supported.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/function_interface/model_inference","lang":"en","toc":[{"text":"hbDNNInferV2","id":"hbdnninferv2","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":201,"title":"Model Info","content":"#\n\n\nhbDNNGetModelNameList#\n\n\n\nGet the name list and number of the models that dnnPackedHandle points to.\n\n * Parameter\n   * [out] modelNameList List of model names.\n   * [out] modelNameCount Number of model names.\n   * [in] dnnPackedHandle Horizon DNN handle, pointing to multiple models.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetModelHandle#\n\n\n\nGet the handle of a model from the model list that dnnPackedHandle points to.\nThe caller can use the returned dnnHandle across functions and threads.\n\n * Parameter\n   * [out] dnnHandle DNN handle, pointing to one model.\n   * [in] dnnPackedHandle DNN handle, pointing to multiple models.\n   * [in] modelName Model name.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetInputCount#\n\n\n\nGet the number of the input tensors of the model that dnnHandle points to.\n\n * Parameter\n   * [out] inputCount Number of input tensors of the model.\n   * [in] dnnHandle DNN handle, pointing to one model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetInputName#\n\n\n\nGet the name of the input tensors of the model that dnnHandle points to.\n\n * Parameter\n   * [out] name Name of the input tensor of the model.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] inputIndex Index of the input tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetInputTensorProperties#\n\n\n\nGet the property of the specific input tensor of the model that dnnHandle points\nto.\n\n * Parameter\n   * [out] properties Info of the input tensor.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] inputIndex Index of the input tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetOutputCount#\n\n\n\nGet the number of the output tensors of the model that dnnHandle points to.\n\n * Parameter\n   * [out] outputCount Number of the output tensors of the model.\n   * [in] dnnHandle DNN handle, pointing to one model.\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetOutputName#\n\n\n\nGet the names of the output tensors of the model that dnnHandle points to.\n\n * Parameter\n   * [out] name Name of the output tensor of the model.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] outputIndex Index of the output tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNGetOutputTensorProperties#\n\n\n\nGet the property of the specific output tensor of the model that dnnHandle\npoints to.\n\n * Parameter\n   * [out] properties Info of the output tensor.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] outputIndex Index of the output tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\n\nGet the description information associated with the specific input of the model\nthat dnnHandle points to.\n\n * Parameter\n   * [out] desc Address of the description information.\n   * [out] size Size of the description information.\n   * [out] type Type of the description information, please refer to\n     hbDNNDescType.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] inputIndex Index of the input tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\n\nGet the description information associated with the specific output of the model\nthat dnnHandle points to.\n\n * Parameter\n   * [out] desc Address of the description information.\n   * [out] size Size of the description information.\n   * [out] type Type of the description information, please refer to\n     hbDNNDescType.\n   * [in] dnnHandle DNN handle, pointing to one model.\n   * [in] outputIndex Index of the output tensor of the model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\n\nGet the description information associated with the model that dnnHandle points\nto.\n\n * Parameter\n   * [out] desc Address of the description information.\n   * [out] size Size of the description information.\n   * [out] type Type of the description information, please refer to\n     hbDNNDescType.\n   * [in] dnnHandle DNN handle, pointing to one model.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/function_interface/model_info","lang":"en","toc":[{"text":"hbDNNGetModelNameList","id":"hbdnngetmodelnamelist","depth":2,"charIndex":3},{"text":"hbDNNGetModelHandle","id":"hbdnngetmodelhandle","depth":2,"charIndex":407},{"text":"hbDNNGetInputCount","id":"hbdnngetinputcount","depth":2,"charIndex":869},{"text":"hbDNNGetInputName","id":"hbdnngetinputname","depth":2,"charIndex":1215},{"text":"hbDNNGetInputTensorProperties","id":"hbdnngetinputtensorproperties","depth":2,"charIndex":1614},{"text":"hbDNNGetOutputCount","id":"hbdnngetoutputcount","depth":2,"charIndex":2030},{"text":"hbDNNGetOutputName","id":"hbdnngetoutputname","depth":2,"charIndex":2388},{"text":"hbDNNGetOutputTensorProperties","id":"hbdnngetoutputtensorproperties","depth":2,"charIndex":2793}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":202,"title":"Model Loading & Releasing","content":"#\n\n\nhbDNNInitializeFromFiles#\n\n\n\nComplete the creation and initialization of dnnPackedHandle from the file. The\ncaller can use the returned dnnPackedHandle across functions and threads.\n\n * Parameter\n   * [out] dnnPackedHandle Horizon DNN handle, pointing to multiple models.\n   * [in] modelFileNames Path of the model files.\n   * [in] modelFileCount Number of the model files.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNInitializeFromDDR#\n\n\n\nComplete the creation and initialization of dnnPackedHandle from the memory. The\ncaller can use the returned dnnPackedHandle across functions and threads.\n\n * Parameter\n   * [out] dnnPackedHandle Horizon DNN handle, pointing to multiple models.\n   * [in] modelData Pointer to the model file\n   * [in] modelDataLengths Length of the model data.\n   * [in] modelDataCount Length of the model data.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\n\nhbDNNRelease#\n\n\n\nRelease the model that dnnPackedHandle points to.\n\n * Parameter\n   * [in] dnnPackedHandle Horizon DNN handle, pointing to multiple models.\n * Return Value\n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/function_interface/model_loading_releasing","lang":"en","toc":[{"text":"hbDNNInitializeFromFiles","id":"hbdnninitializefromfiles","depth":2,"charIndex":3},{"text":"hbDNNInitializeFromDDR","id":"hbdnninitializefromddr","depth":2,"charIndex":496},{"text":"hbDNNRelease","id":"hbdnnrelease","depth":2,"charIndex":1037}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":203,"title":"Status Code","content":"#\n\n\nhbDNNGetErrorDesc#\n\n\n\nTranslate the error codes into natural language.\n\n * Parameter\n   * [in] errorCode dnn error status code.\n * Return Value\n   * return char * , translate internal error codes into natural language.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/function_interface/status_code","lang":"en","toc":[{"text":"hbDNNGetErrorDesc","id":"hbdnngeterrordesc","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":204,"title":"Version Info","content":"#\n\n\nhbDNNGetVersion#\n\n\n\nGet version info of DNN prediction library.\n\n * Return Value\n   * Return the version info.","routePath":"/en/guide/ucp/runtime/bpu_sdk_api/function_interface/version_info","lang":"en","toc":[{"text":"hbDNNGetVersion","id":"hbdnngetversion","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":205,"title":"Model Inference Application Development Guide","content":"#\n\n\nOverview#\n\nThis chapter introduces how to develop model inference applications on the\nHorizon platform and highlights relevant considerations you need to be aware of.\n\nAttention\n\nPrior to the application development, make sure that you have completed the\ndevelopment environment preparations as described in Environment Deployment .\n\nThe simplest application development can be divided into 3 stages: project\ncreation, project implementation, and project compilation and operation.\n\nHowever, given the fact that the development of actual business scenarios are\nmore complicated, we also provide explanations on multi-model control concepts\nand suggestions on application tuning.\n\n\nProject Creation #\n\nWe recommend using CMake to manage your application development engineering.\n\nAs described in the previous sections, by now, you should have installed CMake.\nBefore reading this section, we assume that you understand how to use CMake.\n\nThe Horizon Development Library provides relevant project dependencies. The\nspecific dependencies are listed below:\n\n * Horizon deployment library libdnn.so and libucp.so under\n   ${OE_DIR}/samples/ucp_tutorial/deps_aarch64/ucp/.\n * The aarch64-none-linux-gnu-gcc C compiler.\n * The aarch64-none-linux-gnu-g++ C++ compiler.\n\nNote\n\nThe $ above refers to the OE package path provided by Horizon.\n\nTo create a new project, you need to compile the CMakeLists.txt file.\n\nThe CMakeLists.txt file defines some compilation options, as well as the path to\nthe dependency libs and header files, as follows:\n\n\n\nNote\n\nIn the above sample, we did not specify the compiler location. We will specify\nit at the project compilation stage, as described in the section Project\nCompilation and Running .\n\n\nProject Implementation#\n\nThis section explains you how to run the hbm models on Horizon platforms.\n\nThe simplest procedure consists of model loading, input data preparations,\noutput memory preparations, inference and result parsing. The sample code for\nsimple model deployment are as follows:\n\n\n\nTo keep it simple, part of the model processing in above sample is described in\nthe form of comments. More details are explained in subsequent documents, such\nas:\n\nFor dynamic input instructions, please refer to section Dynamic Input\nInstruction.\n\nFor memory alignment rules, please refer to section Alignment Rule.\n\nFor more comprehensive instructions on the engineering implementation, refer to\nsections Model Inference API Instruction and Basic Sample User Guide.\n\n\nProject Compilation and Running #\n\nCombining with CMake engineering configurations as described in Project\nCreation, please refer to the following compilation script:\n\n\n\nAfter reading Environment Deployment, we assume that you have installed the\nrequired compiler on your dev PC, so here you only need to associate the\ncompiler configurations in the above script with your project.\n\nCopy the arm program to the Horizon board to run, note that the program\ndependencies also need to be copied to the board together, and configure the\ndependencies in the startup script. For example, our example program depends on\nthe following libraries: libucp.so, libdnn.so and other bsp libraries. These\ndependencies can be found in the OE package under the path\nucp_tutorial/deps_aarch64/ and need to be uploaded to the board's runtime\nenvironment. We recommend that you create a new lib path under the /userdata\npath on the board side and transfer the libraries to that directory, the paths\nto the dependency libraries that need to be specified before running the program\non the board side are as follows:\n\n\n\n\nMulti-model Control Strategy#\n\nIn the scenarios containing multiple models, as each model need to complete the\ninference with limited resources, they will inevitably compete for computing\nresources.\n\nTo help you control the execution of multiple models, we provide control\nstrategies for the model prioritization.\n\n\nModel Preemption Control #\n\nAttention\n\nThis feature is only supported on the dev board side and is not supported by the\nx86 emulator.\n\nThere isn't task preemption feature in the BPU computing unit hardware of the J6\nASIC. Each inference task, once put to the BPU and begins model computing, it\noccupies the BPU until the task is completed. At this time, other tasks have to\nwait in line. If the BPU is occupied by a large model inference task, then other\nhigh-priority model inference tasks cannot be executed.\n\nTo fix this, we added a software feature called BPU Resource Preemption in the\nRuntime SDK based on model priorities.\n\nPay attention to the following:\n\n * When executing inference in BPU, the compiled data command model is denoted\n   by 1 or more function calls. The function call means the atomic execution\n   unit of the BPU, and multiple function-call tasks are queued in the hardware\n   queue and processed in turn. A model inference task will be considered done\n   when all of it function calls are executed.\n * Based on the above descriptions, it is simpler to set the function call as\n   the preemption granularity of the BPU model task, that is, when the BPU\n   finishes a function call, it can temporarily suspend the existing model,\n   switch to another model, and then resume it when the latter is done. However,\n   there are 2 problems, the first is that the function calls of the model\n   compiled by the compiler are merged together to form a large function call\n   and cannot be preempted. The second problem is that the execution time of\n   each function call is relatively long or not fixed, which leads to unfixed\n   preemption timing, affecting the preemption results.\n\nTo solve these two problems, we provide supports in both model conversion and\nsystem software. The implementation principles and operation methods are as\nfollows:\n\n * Firstly, if you choose to process the model using the QAT scheme, then at the\n   model compilation stage, you need to add the max_time_per_fc option to the\n   extra parameter configurations in the compilation interface to set the\n   execution time (in microseconds) for each function call. The default value is\n   0 (no limits). By setting this option, you can control the execution time of\n   individual large function calls when they are running on-board. Suppose the\n   execution time of a function call is 10ms, and max_time_per_fc is set to 500\n   during model compilation, then this function call will be split into 20\n   function calls. If you are using the PTQ scheme to process the model, you can\n   add the max_time_per_fc parameter to the compiler-related parameters\n   (compiler_parameters) in the YAML configuration file of the model at the\n   model conversion stage.\n * Secondly, the hbUCPSchedParam.priority parameter needs to be set when the\n   reasoning task is submitted. High-optimization preemption nesting\n   capabilities can also be supported according to priority. For example, if you\n   configure the infer task with a priority less than 254, it is a normal task\n   and cannot preempt other tasks. Configure infer task with a priority equal to\n   254 to be a high preemption task, which can support preemption of normal\n   tasks. Configure infer task with a priority equal to\n   HB_DNN_PRIORITY_PREEMP(255) to be a urgent preemptive task, which can preempt\n   both normal and high preemptive tasks.\n\n\nSuggestions on Application Optimization#\n\nHorizon suggested application optimization strategy includes Engineering Task\nScheduling and Algorithm Task Integration.\n\nFor Engineering Task Scheduling, we recommend some workflow scheduling\nmanagement tools to fully utilize the parallel-processing capabilities at\ndifferent task stages.\n\nIn general, an application can be divided into 3 stages: pre-processing, model\ninference, and post-processing output.\n\nA simplified workflow is as follows:\n\nAfter making full use of the workflow management to achieve the parallel\nexecution of different task stages, the ideal task processing workflow can be as\nfollows:\n\nFor Algorithm Task Integration, we recommend multi-task models.\n\nOn one hand, it can avoid the difficulties brought by the management of\nmulti-model scheduling to a certain extent.\n\nOn the other hand, as multi-task model can fully share the computation of the\nbackbone, it can significantly reduce the amount of computation at the entire\napplication level compared to using independent models, and thereby achieve\nhigher overall performance.\n\nMultitasking is also a common application-level optimization strategy within\nHorizon Robotics and in the business practices of many collaborating customers.\n\n\nOther Dev Tools#\n\nThe hrt_model_exec is a model execution tool that can evaluate the inference\nperformance of the model and get the model information directly on the dev\nboard. On one hand, it allows you to get a realistic understanding of the\nmodel's real performance; On the other hand, it also helps you to learn the\nspeed limit that the model can achieve, which is useful information in\napplication tuning.\n\nThe hrt_model_exec provides three types of functions including model inference\ninfer, model performance analysis perf and viewing model information model_info.\nFor how to use the tool, please refer to hrt_model_exec Tool Introduction.\n\nUCP also provides performance analysis tools to assist you in locating\napplication performance bottlenecks. Among them, UCP Trace is used to analyze\nthe application pipline scheduling capability, and hrt_ucp_monitor is used to\nmonitor the occupancy rate of the monitoring hardware backend.\n\nPlease refer to the section UCP Trace Instructions and The hrt_ucp_monitor Tool\nIntroduction for how to use these tools.","routePath":"/en/guide/ucp/runtime/runtime_dev","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Project Creation","id":"project-creation","depth":2,"charIndex":-1},{"text":"Project Implementation","id":"project-implementation","depth":2,"charIndex":1726},{"text":"Project Compilation and Running","id":"project-compilation-and-running","depth":2,"charIndex":-1},{"text":"Multi-model Control Strategy","id":"multi-model-control-strategy","depth":2,"charIndex":3588},{"text":"Model Preemption Control","id":"model-preemption-control","depth":3,"charIndex":-1},{"text":"Suggestions on Application Optimization","id":"suggestions-on-application-optimization","depth":2,"charIndex":7297},{"text":"Other Dev Tools","id":"other-dev-tools","depth":2,"charIndex":8553}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":206,"title":"Common Operations","content":"#\n\n\nView Dev Board's Image Version#\n\nRun uname -a to check system version, as shown below:\n\n\n\nNote\n\n * SMP indicates that the system supports Symmetrical Multi-Processing.\n\n * PREEMPT indicates that the system supports the preemption core.\n\n * Oct 23 10:47:39 CST 2020 indicates the release time of the system image.\n\n * aarch64 indicates that the platform supported by the system is the aarch64.\n\n\nView System Logs#\n\nRun dmesg to check system logs, as shown below:\n\n\n\nIf a system error occurs while running a program on the board (such as a killed\nprogram or mem allocation failure), execute dmesg to check the exact cause of\nthe system error.\n\n\nView BPU Utilization#\n\nRun hrut_somstatus to check the BPU utilization of the current dev board, as\nshown below:\n\n","routePath":"/en/guide/ucp/runtime/tool_introduction/auxiliary_tool","lang":"en","toc":[{"text":"View Dev Board's Image Version","id":"view-dev-boards-image-version","depth":2,"charIndex":3},{"text":"View System Logs","id":"view-system-logs","depth":2,"charIndex":398},{"text":"View BPU Utilization","id":"view-bpu-utilization","depth":2,"charIndex":646}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":207,"title":"The hrt_model_exec Tool Introduction","content":"#\n\n\nTool Introduction#\n\nhrt_model_exec is a model execution tool that can evaluate the inference\nperformance of the model and get the model information directly on the\ndevelopment board.\n\nOn one hand, it allows you to get a realistic understanding of the model's real\nperformance; On the other hand, it also helps you to learn the speed limit that\nthe model can achieve, which is useful information in application tuning.\n\nhrt_model_exec tool source code is located in the\nsamples/ucp_tutorial/tools/hrt_model_exec path of the horizon_j6_open_explorer\npublication. The structure is as follows:\n\n\n\nhrt_model_exec provides three types of functions including model inference\ninfer, model performance analysis perf and viewing model information model_info,\nrespectively, as shown in the following table:\n\nNO.   SUBCOMMAND   DESCRIPTION\n1     model_info   Get model information, such as model input and output\n                   information, etc.\n2     infer        Perform model inference and get model inference results.\n3     perf         Perform model performance analysis and obtain performance\n                   analysis results.\n\nThe tool can view the tool's dnn prediction library version number with the -v\nor --version commands.\n\n\n\n\nParameters Description#\n\nPARAMETER*                TYPE     DESCRIPTION\nmodel_file                string   Model file path, multiple paths can be separated by commas.\nmodel_name                string   Specify the name of a model in the model.\ncore_id                   int      Specify the running core, 0 means arbitrary core, 1 means\n                                   core0; default 0.\ninput_file                string   Model input information. The input of the image type, it\n                                   must have one of the following file name suffixes: PNG / JPG\n                                   / JPEG / png / jpg / jpeg, the input suffix name of the\n                                   feature must be one of bin / txt. The inputs should be\n                                   separated by commas ,, such as xxx.jpg,input.txt.\ninput_img_properties      string   The color space information of the model image input. Each\n                                   image type input in input_file needs to specify a Y/UV type,\n                                   and each input color space needs to be separated by an\n                                   English character comma ,, such as: Y,UV.\ninput_valid_shape         string   Model dynamic validShape input information. If the model\n                                   input attribute validShape contains -1, the -1 part needs to\n                                   be completed, and multiple validShape are separated by\n                                   English semicolons. For example:\n                                   --input_valid_shape=\"1,376,376,1;1,188,188,2\".\ninput_stride              string   Model dynamic stride input information. If the model input\n                                   attribute stride contains -1, the -1 part needs to be\n                                   completed, and multiple strides are separated by English\n                                   semicolons. For example:\n                                   --input_stride=\"144384,384,1,1;72192,384,2,1\".\nframe_count               int      The number of running frames of the execution model.\ndump_intermediate         string   dump model each layer of input and output.\nenable_dump               bool     Enables dump model input and output, defaults to false.\ndump_precision            int      Controls the number of decimal places of the float type data\n                                   output in txt format, default is 9.\ndequantize_process        bool     Inverse quantization of model output, effective when\n                                   enable_dump is true, default is false.\nremove_padding_process    bool     Remove padding of model output, effective when enable_dump\n                                   is true, default is false.\ndump_format               string   The format of the dump model input and output.\ndump_txt_axis             int      Control line feed rules for txt format input and output.\nenable_cls_post_process   bool     Enables classification post-processing, defaults to false.\n                                   Used when the subcommand is infer. Currently, it only\n                                   supports post-processing of the ptq classification model and\n                                   printing of classification results.\nperf_time                 int      Execution model runtime.\nthread_num                int      Number of threads (parallelism), the value can indicate how\n                                   many tasks are processed in parallel at most.When testing\n                                   latency, the value needs to be set to 1 to avoid resource\n                                   preemption and get more accurate latency.When testing\n                                   throughput, it is recommended to set >2 (number of BPU\n                                   cores) to adjust the number of threads so that the BPU\n                                   utilization is as high as possible, and the throughput test\n                                   is more accurate.\nprofile_path              string   Statistical tool log generation path, run to generate\n                                   profiler.log and profiler.csv, analyze op time and\n                                   scheduling time consumption. Generally, just set\n                                   --profile_path=\".\", which means the log file will be\n                                   generated in the current directory.\ndump_path                 string   The path of dump model input and output, effective when\n                                   enable_dump or dump_intermediate is set.\n\nAfter setting the profile_path parameter and the tool runs normally,\nprofiler.log and profiler.csv files will be generated. The files include the\nfollowing parameters:\n\n * perf_result：Record perf results.\n\nPARAMETER         DESCRIPTIONS\nFPS               Frames processed per second.\naverage_latency   The average time it takes to run a frame.\n\n * running_condition：Operating environment information.\n\nPARAMETER     DESCRIPTIONS\ncore_id       The bpu core set by the program running.\nframe_count   The total number of frames the program runs.\nmodel_name    The name of the evaluation model.\nrun_time      Program running time.\nthread_num    The number of threads the program runs on.\n\n * model_latency： Model node time consumption statistics.\n\nPARAMETER                        DESCRIPTIONS\nNode-pad                         Model input padding takes time.\nNode-NodeIdx-NodeType-NodeName   Time consuming information of model nodes. Note: NodeIdx\n                                 Specifies the sequence number of the model node topology,\n                                 and NodeType is a specific node type, such as Dequantize,\n                                 and NodeName is a specific node name.\n\n * processor_latency：Model processor time consumption statistics.\n\nPARAMETER                 DESCRIPTIONS\nBPU_inference_time_cost   Inferencing BPU processor time per frame.\nCPU_inference_time_cost   Inference CPU processor time per frame.\n\n * task_latency：Model task time-consuming statistics.\n\nPARAMETER         DESCRIPTIONS\nTaskRunningTime   The actual running time of the task includes the time\n                  consumed by the UCP framework.\n\n\nUsage Instructions#\n\nThis tool provides three types of functions: model information acquisition,\nsingle-frame inference function, and multi-frame performance evaluation.\n\nRun hrt_model_exec, hrt_model_exec -h, or hrt_model_exec --help for tool usage\ndetails, as shown in the following:\n\n\n\n\nmodel_info#\n\nOverview#\n\nThis parameter is used to get the model information, supporting both QAT and PTQ\nmodels. This parameter is used together with model_file to get detailed\ninformation about the model, including model input and output information\nhbDNNTensorProperties.\n\nIf model_name is not specified, all the models in the model are outputted. If\nmodel_name is specified, only the information of the corresponding model is\noutputted.\n\nExample#\n\n 1. Single Model\n\n\n\n 2. Multi-model (output all model information)\n\n\n\n 3. Multi-model - pack model (output specified model information)\n\n\n\n\ninfer#\n\nOverview#\n\nThis parameter is used for model inference, where the input images are defined\nby user and one frame is inferred. This parameter should be used together with\ninput_file to specify the input image path, and the tool resizes the image\naccording to the model information and organizes the model input information.\n\nThe program runs a single frame of data in a single thread and outputs the time\nof the model execution.\n\nExample#\n\n 1. Single Model\n\n\n\n 2. Multi-model\n\n\n\nOptional Parameters#\n\nPARAMETER                 DESCRIPTION\ncore_id                   Specifies the core ID for model inference.\ninput_img_properties      Color space information of the model image input.\ninput_valid_shape         Model dynamic validShape input information.\ninput_stride              Model dynamic stride input information.\nframe_count               Sets the number of frames to run infer. Single frame\n                          repeated inference, can be used together with enable_dump to\n                          verify output consistency, defaults to 1.\ndump_intermediate         Dumps the input+output data of each model layer, default is\n                          0.\nenable_dump               Dumps the input and output data of the model, defaults to\n                          false.\ndump_precision            Controls the number of decimal places in the txt format to\n                          output float data, default is 9.\ndequantize_process        Inverse quantization of model output, effective when\n                          enable_dump is true, default is false.\nremove_padding_process    Remove padding of model output, effective when enable_dump\n                          is true, default is false.\ndump_format               Type of dump model output file, with optional parameters bin\n                          or txt, default is bin.\ndump_txt_axis             Line wrapping rule for txt format output of dump model. If\n                          output dimension = n, then parameter range: [0, n], defaults\n                          to -1, which means one data per row.\nenable_cls_post_process   Enables classification post-processing, currently only\n                          supports PTQ classification model, defaults to false.\ndump_path                 The path of dump model input and output, effective when\n                          enable_dump or dump_intermediate is set.\n\n\nperf#\n\nOverview#\n\nThis parameter is used to test the model performance.\n\nIn this mode, you does not need to input data, and the program automatically\nconstructs the input tensor according to the model, and the tensor data are\nrandom numbers.\n\nBy default, the program runs 200 frames of data in a single thread. When\nperf_time is specified, frame_count is disabled, and the program will run for\nthe specified period of time and then exit.\n\nOutputs the latency and the frame rate of the model. The program prints the\nperformance information every 200 frames: max, min, and average values of\nlatency. If < 200 frames, prints once before the programs ends.\n\nThe program finally outputs the running-related data, including number of\nprogram threads, number of frames, total model inference time, average latency\nof model inference, and frame rate.\n\nExample#\n\n 1. Single Model\n\n\n\n 2. Multi-model\n\n\n\nOptional Parameters#\n\nPARAMETER              DESCRIPTIONS\ncore_id                Specify the core id for model inference.\ninput_file             Model input information, multiple can be separated by\n                       commas.\ninput_img_properties   Color space information of the model image input.\ninput_valid_shape      Model dynamic validShape input information.\ninput_stride           Model dynamic stride input information.\nframe_count            Set perf the number of frames to run, takes effect when\n                       perf_time is 0, default value 200.\ndump_intermediate      dump model each layer of input data and output data, default\n                       value 0.\nperf_time              Set perf runtime in minutes, default value 0.\nthread_num             Set the number of threads to run, range [1, 8], default 1,\n                       if set to more than 8, it will be treated as 8 threads.\nprofile_path           Statistical tool log generation path, run to generate\n                       profiler.log and profiler.csv, analyze op time and\n                       scheduling time consumption.\n\n\nMulti-input Model Description#\n\nThe tool infer supports inference for multiple input models, supporting image\ninput, binary file input, and text file input, with input data separated by\ncommas. The model input information can be viewed via model_info.\n\nExample:\n\n\n\n\nDynamic input instructions#\n\nIf the model input is dynamic, you need to use the input_valid_shape and\ninput_stride parameters to complete the dynamic information according to the\nactual input situation. You can choose to specify the parameters in the\nfollowing two ways:\n\n * Only give the validShape or stride information of the dynamic input.\n\n * Give the validShape or stride information of all inputs. The information of\n   non-dynamic inputs must be consistent with the model information.\n\nNote\n\nThe tool will automatically complete the dynamic input information to the\nmaximum extent, which is convenient for you to perform performance evaluation\nmore easily. You can choose whether to let the tool automatically complete it\naccording to the actual situation.\n\n * If the input stride is dynamic and validShape is fixed, you can not specify\n   the input_stride parameter. The tool will automatically complete it according\n   to the minimum alignment rule and print the alignment information.\n\n * If both validShape and stride are dynamic:\n   \n   * If the input is a picture type, you need to specify the color space\n     information of the picture input_img_properties, and the other parameters\n     can be left unspecified. The tool will fill in the input_valid_shape and\n     input_stride information according to the size of the picture.\n   \n   * If the input is a type other than a picture or is not specified, you need\n     to set the input_valid_shape information, and the input_stride information\n     will be automatically filled in and printed.\n\nTaking the model in the Dynamic Input Introduction section as an example, you\ncan run the model with the following command:\n\n\n\n\nImage type input instructions#\n\nWhen input_file is given an image input, you need to use the\ninput_img_properties parameter to specify which color space of the image you\nwant to use as input to the model. Currently, only Y and UV color spaces are\nsupported.\n\n\n\n\nTool operation instructions#\n\nBuild#\n\nThere is a pre-configured compilation script build.sh in the\nucp_tutorial/tools/hrt_model_exec directory. The options -a x86 and -a aarch64\nsupport two compilation modes respectively. You can use this script and specify\nthe compilation options for compilation. In addition, the directory also\ncontains two compilation scripts, build_aarch64.sh and build_x86.sh, which\ncorrespond to two compilation options respectively. Compiling with these two\nscripts is equivalent to using the build.sh script and specifying the\ncompilation options.\n\n\n\nExecute#\n\nAfter building board-side hrt_model_exec tools, the output_shared_J6_aarch64\nfolder will be generated. You can use this tool by copying the folder to the\nboard environment and executing\noutput_shared_J6_aarch64/script/run_hrt_model_exec.sh.\n\nAfter building x86-side hrt_model_exec tools, the output_shared_J6_x86 folder\nwill be generated. You can use this tool on the x86 environment and executing\noutput_shared_J6_x86/script_x86/run_hrt_model_exec.sh.\n\nThe run_hrt_model_exec.sh script is divided into two parts: setting environment\nvariables and getting model information and inferring the model.\n\n\n\nNote\n\nBefore running, you need to modify the corresponding parameters of\nrun_hrt_model_exec.sh to ensure that the model and input files are correct. You\ncan also use other parameters flexibly to use more functions.\n\n\nFAQ#\n\n\nHow are Latency and FPS data calculated?#\n\nLatency refers to the average time spent by a single-process inference model. It\nfocuses on the average time it takes to infer one frame when resources are\nsufficient. This is reflected in the statistics of single-core and single-thread\nrunning on the board. The pseudo code of the statistical method is as follows:\n\n\n\nFPS refers to the average number of frames per second of model inference\nperformed by multiple processes at the same time, which focuses on the\nthroughput of the model when fully utilizing the resources.\n\nIn on-board running situation, it is represented as multi-core and\nmulti-threaded. The statistical method is to perform the model inference by\ninitiating multiple threads at the same time and calculate the total number of\nframes of the inference in average 1 second.\n\n\nWhy is the FPS estimated by Latency inconsistent with the FPS measured by the\ntool?#\n\nLatency and FPS are different in statistical scenarios. Latency is\nsingle-process (single-core, single-thread) inference, and FPS is multi-process\n(dual-core, multi-thread) inference, so the calculation is different. If the\nnumber of processes (threads) is set to 1 when counting the FPS, then the FPS\nestimated by Latency is consistent with the measured one.","routePath":"/en/guide/ucp/runtime/tool_introduction/hrt_model_exec","lang":"en","toc":[{"text":"Tool Introduction","id":"tool-introduction","depth":2,"charIndex":3},{"text":"Parameters Description","id":"parameters-description","depth":2,"charIndex":1238},{"text":"Usage Instructions","id":"usage-instructions","depth":2,"charIndex":7543},{"text":"model_info","id":"model_info","depth":3,"charIndex":7833},{"text":"Overview","id":"overview","depth":4,"charIndex":7846},{"text":"Example","id":"example","depth":4,"charIndex":8274},{"text":"infer","id":"infer","depth":3,"charIndex":8424},{"text":"Overview","id":"overview-1","depth":4,"charIndex":8432},{"text":"Example","id":"example-1","depth":4,"charIndex":8860},{"text":"Optional Parameters","id":"optional-parameters","depth":4,"charIndex":8909},{"text":"perf","id":"perf","depth":3,"charIndex":10826},{"text":"Overview","id":"overview-2","depth":4,"charIndex":10833},{"text":"Example","id":"example-2","depth":4,"charIndex":11670},{"text":"Optional Parameters","id":"optional-parameters-1","depth":4,"charIndex":11719},{"text":"Multi-input Model Description","id":"multi-input-model-description","depth":3,"charIndex":12840},{"text":"Dynamic input instructions","id":"dynamic-input-instructions","depth":3,"charIndex":13106},{"text":"Image type input instructions","id":"image-type-input-instructions","depth":3,"charIndex":14793},{"text":"Tool operation instructions","id":"tool-operation-instructions","depth":3,"charIndex":15055},{"text":"Build","id":"build","depth":4,"charIndex":15085},{"text":"Execute","id":"execute","depth":4,"charIndex":15632},{"text":"FAQ","id":"faq","depth":2,"charIndex":16461},{"text":"How are Latency and FPS data calculated?","id":"how-are-latency-and-fps-data-calculated","depth":3,"charIndex":16468},{"text":"Why is the FPS estimated by Latency inconsistent with the FPS measured by the tool?","id":"why-is-the-fps-estimated-by-latency-inconsistent-with-the-fps-measured-by-the-tool","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":208,"title":"HB_UCP_ALL_BACKENDS","content":"#\n\n\n\nDefine macros to specify all backends.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucp_all_backends","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":209,"title":"HB_UCP_INITIALIZE_SCHED_PARAM","content":"#\n\n\n\nInitialize the control parameter.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucp_initialize_sched_param","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":210,"title":"hbUCPBackend","content":"#\n\n\n\nTask execution hardware, can be used in combination, for example:\nHB_UCP_DSP_CORE_0 | HB_UCP_GDC_CORE_0 represents that the current task can use\neither the DSP 0 core or the GDC 0 core, the scheduling is leftto the UCP's own\ndecision based on the load.\n\nAttention\n\nHB_UCP_CORE_ANY must be used alone and cannot be ORed with other backends.\n\n * Member\n   \n   MEMBER NAME               DESCRIPTION\n   HB_UCP_CORE_ANY           Arbitrary executable hardware on Soc.\n   HB_UCP_BPU_CORE_0         BPU core 0.\n   HB_UCP_BPU_CORE_1         BPU core 1.\n   HB_UCP_BPU_CORE_2         BPU core 2.\n   HB_UCP_BPU_CORE_3         BPU core 3.\n   HB_UCP_BPU_CORE_ANY       Arbitrary BPU core.\n   HB_UCP_DSP_CORE_0         DSP core 0.\n   HB_UCP_DSP_CORE_1         DSP core 1.\n   HB_UCP_DSP_CORE_ANY       Arbitrary DSP core.\n   HB_UCP_GDC_CORE_0         GDC core 0.\n   HB_UCP_GDC_CORE_ANY       Arbitrary GDC core.\n   HB_UCP_STITCH_CORE_0      STITCH core 0.\n   HB_UCP_LKOF_CORE_0        LKOF core 0.\n   HB_UCP_JPU_CORE_0         JPU core 0.\n   HB_UCP_JPU_CORE_1         JPU core 1.\n   HB_UCP_JPU_CORE_2         JPU core 2.\n   HB_UCP_JPU_CORE_ANY       Arbitrary JPU core.\n   HB_UCP_VPU_CORE_0         VPU core 0.\n   HB_UCP_VPU_CORE_ANY       Arbitrary VPU core.\n   HB_UCP_PYRAMID_CORE_0     PYRAMID core 0.\n   HB_UCP_PYRAMID_CORE_1     PYRAMID core 1.\n   HB_UCP_PYRAMID_CORE_2     PYRAMID core 2.\n   HB_UCP_PYRAMID_CORE_ANY   Arbitrary PYRAMID core.\n   HB_UCP_ISP_CORE_0         ISP core 0.\n   HB_UCP_ISP_CORE_1         ISP core 1.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpbackend","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":211,"title":"hbUCPSchedParam","content":"#\n\n\n\nControl parameter structure.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   priority      Task priority.\n   customId      Custom priority, e.g. timestamp, frame id, etc. The smaller\n                 the value, the higher the priority. Priority: priority >\n                 customId.\n   backend       Task execution hardware, e.g. HB_UCP_BPU_CORE_0.\n   deviceId      Device ID.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpschedparam","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":212,"title":"hbUCPSysMem","content":"#\n\n\n\nSystem memory structure.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   phyAddr       Physical address.\n   virAddr       Virtual address.\n   memSize       Memory Size.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpsysmem","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":213,"title":"hbUCPSysMemFlushFlag","content":"#\n\n\n\nSystem memory and cache sync parameter.\n\n * Member\n   \n   MEMBER NAME                   DESCRIPTION\n   HB_SYS_MEM_CACHE_INVALIDATE   Synchronizes memory to cache, used before CPU read.\n   HB_SYS_MEM_CACHE_CLEAN        Synchronizes cached data to memory, used after CPU write.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucpsysmemflushflag","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":214,"title":"hbUCPTaskDoneCb","content":"#\n\n\n\nThe callback function that needs to be executed when the user-defined task is\ncompleted.\n\n * Parameter\n   \n   * [in] taskHandle Task handle pointer.\n   \n   * [in] status Status code returned by the task.\n   \n   * [in] userdata User-defined data.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskdonecb","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":215,"title":"hbUCPTaskHandle_t","content":"#\n\n\n\nUCP task handle pointing to the created task.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskhandle_t","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":216,"title":"hbUCPTaskPriority","content":"#\n\n\n\nSet the task priority, providing default parameter.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/hbucptaskpriority","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":217,"title":"Version Information Type","content":"#\n\nNote\n\nNote that the version numbers of the version information types in this\nsubsection vary with version changes. The version numbers here are for reference\nonly, and the actual version should be based on the release you have obtained.\n\n\nHB_UCP_VERSION_MAJOR #\n\n\n\nUCP master version number information.\n\n\nHB_UCP_VERSION_MINOR #\n\n\n\nUCP minor version number information.\n\n\nHB_UCP_VERSION_PATCH #\n\n\n\nUCP patch version number information.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_data_structure/version_information","lang":"en","toc":[{"text":"HB_UCP_VERSION_MAJOR","id":"hb_ucp_version_major","depth":2,"charIndex":-1},{"text":"HB_UCP_VERSION_MINOR","id":"hb_ucp_version_minor","depth":2,"charIndex":-1},{"text":"HB_UCP_VERSION_PATCH","id":"hb_ucp_version_patch","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":218,"title":"hbUCPFree","content":"#\n\n\n\nFree the system memory.\n\n * Parameter\n   \n   * [in] mem Memory pointer.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpfree","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":219,"title":"hbUCPGetErrorDesc","content":"#\n\n\n\nGet the natural language description of the error code.\n\n * Parameter\n   \n   * [in] errorCode The error code of UCP.\n\n * Return Value\n   \n   * Return the information of the description of the error code.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgeterrordesc","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":220,"title":"hbUCPGetSocName","content":"#\n\n\n\nObtain the soc name.\n\n * Return Value\n   \n   * Return the soc name.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgetsocname","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":221,"title":"hbUCPGetVersion","content":"#\n\n\n\nObtain the ucp version number.\n\n * Return Value\n   \n   * Return the version number.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpgetversion","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":222,"title":"hbUCPMalloc","content":"#\n\n\n\nApply the system memory.\n\n * Parameter\n   \n   * [out] mem Memory pointer.\n   * [in] size Size of the requested memory.\n   * [in] deviceId Reserved parameter.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmalloc","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":223,"title":"hbUCPMallocCached","content":"#\n\n\n\nApply the cached system memory.\n\n * Parameter\n   \n   * [out] mem Memory pointer.\n   * [in] size Size of the requested memory.\n   * [in] deviceId Reserved parameter.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmalloccached","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":224,"title":"hbUCPMemFlush","content":"#\n\n\n\nRefresh the cached system memory.\n\n * Parameter\n   \n   * [in] mem Memory pointer.\n   * [in] flag Refresh marker.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpmemflush","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":225,"title":"hbUCPReleaseTask","content":"#\n\n\n\nRelease UCP task resource.\n\n * Parameter\n   \n   * [in] taskHandle Task handle.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpreleasetask","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":226,"title":"hbUCPSetTaskDoneCb","content":"#\n\n\n\nRegister a callback function, this callback function is executed automatically\nafter the task is completed.\n\n * Parameter\n   \n   * [in] taskHandle Task handle pointer.\n   * [in] taskDoneCb Callback function pointer.\n   * [in] userdata User-defined data.\n\n * Return Value\n   \n   * If the return value is 0, the callback function is registered successfully,\n     otherwise the registration fails.\n\nNote\n\nThis interface allows you to register a callback function that will be called to\nperform a user-defined function when the task is completed. If you don't need to\ncustomize the input, you can set userdata to nullptr.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpsettaskdonecb","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":227,"title":"hbUCPSubmitTask","content":"#\n\n\n\nCommit the UCP task to the scheduler.\n\n * Parameter\n   \n   * [in] taskHandle Task handle pointer.\n   * [in] schedParam Task scheduling parameter.\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpsubmittask","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":228,"title":"hbUCPWaitTaskDone","content":"#\n\n\n\nWait for task complete or timeout.\n\n * Parameter\n   \n   * [in] taskHandle Task handle pointer.\n   * [in] timeout Timeout configuration (in milliseconds).\n\n * Return Value\n   \n   * If the return value is 0, the API is executed successfully, otherwise the\n     execution fails.\n\nNote\n 1. timeout > 0 represents the wait time.\n 2. timeout <= 0 represents keep waiting until the task is completed.","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_function_interface/hbucpwaittaskdone","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":229,"title":"Overview","content":"#\n\n\nData Structure#\n\n\nFunction Interface#","routePath":"/en/guide/ucp/ucp_api_reference/ucp_api_overview","lang":"en","toc":[{"text":"Data Structure","id":"data-structure","depth":2,"charIndex":3},{"text":"Function Interface","id":"function-interface","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":230,"title":"Environment Variable","content":"#\n\n","routePath":"/en/guide/ucp/ucp_api_reference/ucp_environment_variable","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":231,"title":"Error Code","content":"#\n\nNote\n\nThe error code has 7 digits: XX-XXXXX, the first two digits indicate the error\ntype, and the last 5 digits indicate the specific error description.\n\n","routePath":"/en/guide/ucp/ucp_faq_errorcode/ucp_error_code","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":232,"title":"FAQ","content":"#\n\n\nGeneral Issues#\n\n\nWhat is the reason for returning HB_UCP_INVALID_ARGUMENT error code after task\ncreation or submission?#\n\nPossible problems can be determined from the UCP error logs, which may exist as\nfollows:\n\n * Operator Constraint Problem: Most acceleration operators should satisfy usage\n   constraints when they are created, otherwise they will return error code.\n * If you encounter a log that prints op $1 of task has no proper backend, user\n   expect $2, indicating that there is no proper backend available for\n   execution, where $1 indicates the type of the task and $2 is the backend\n   parameter when the task is submitted in binary form, and the configuration\n   needs to be done according to the number of cores that each J6 series backend\n   can support.\n\n\nHow to understand the physical and virtual addresses of hbUCPSysMem?#\n\nIn the J6 processor architecture, all hardware share the DDR memory, and a\nphysically contiguous section of memory can be requested through the\nhbUCPMallocCached and hbUCPMalloc interfaczes.\n\nThe return values of these functions are wrapped in the hbUCPSysMem data\nstructure, and the phyAddr and virAddr fields correspond to the physical and\nvirtual addresses of its memory space, respectively.\n\nAs this memory space is contiguous, both physical and virtual addresses can be\nrepresented, read, and written by the first address. However, in practice, it is\nrecommended to use virtual addresses in preference in non-essential scenarios.\n\n\nHow to understand cacheable and non-cacheable hbmem?#\n\nUCP's memory management interface provides hbUCPMallocCached and hbUCPMalloc to\nallocate DDR read/write memory, which is physically contiguous and can be\naccessed and used by bpu/dsp and other ip accesses. Where hbUCPMallocCached\nrepresents the allocation of memory for the cacheable attribute and is\naccompanied by the hbUCPMemFlush function to refresh the Cache.\n\nThe cache mechanism is determined by the memory architecture of the platform, as\nshown in the following figure. The cache between CPU and memory is used as a\ndata cache, however, there is no cache between the BPU/DSP/JPU/VPU(Video Process\nUnit)/PYRAMID/STITCH/GDC backend hardware and main memory. Therefore, the misuse\nof the cache can cause problems in data reading/writing accuracy and efficiency.\n\n * When the CPU has finished writing data, it needs to actively flush the data\n   in the cache to the memory, otherwise other hardware accessing the same\n   memory space may read the old data from before.\n\n * The CPU also needs to actively INVALIDATE the data in the Cache before\n   accessing it when the other backend hardware has finished writing the data,\n   otherwise the CPU will preferentially read the old data previously cached.\n\n * In the continuous inference process of the model, those that need to be read\n   by the cpu, such as the model output, it is recommended to apply for memory\n   with cacheable to accelerate the efficiency of the CPU to read and write\n   repeatedly, while those that don't need to be read, and are only written,\n   such as the model input, can apply for non-cacheable memory.\n\n\nModel Inference#\n\n\nWhat are the possible causes of the timeout on the hbUCPWaitTaskDone interface\nfor model reasoning?#\n\n * The model itself takes a long time to execute, and an insufficient timeout\n   set by the asynchronous wait interface, or a long queuing time for the task\n   due to a high load on the current computational resources, may trigger an\n   interface timeout.\n * Memory leaks exist. Slow allocation of memory in case of insufficient system\n   memory may lead to inference timeout.\n * CPU load is too high. The scheduling thread can't get CPU, at this time, even\n   if the task is completed, it can't be synchronized to the user interface in\n   time, which leads to the reasoning timeout situation.\n\n\nReasons why model inference gets stuck#\n\nModel problem: The underlying running error caused by the model command reason,\nthe error is not reported, resulting in hang. At this point, the bpu task\nsituation can be viewed by cat /sys/devices/system/bpu/bpu0/task_running, as\nshown below:\n\nIf s_time is not null, it means that the task has started normally, while p_time\nis null, it means that it has not returned normally, which can be assumed that\nthe BPU task hangs, and can be solved by contacting sr or the compiler team.\n\n\nWhat are the ROI input model constraints?#\n\n * The size of the original image input to NV12 requires 1 < = Win < = 4096, and\n   the alignment in the W direction requires 32 < = stride < = 131072, and must\n   be a multiple of 32.\n * The size of the ROI is currently set to not exceed the boundaries of the\n   input image; this restriction will be relaxed in subsequent versions.\n * The ROI is represented by the upper left and lower right corner points, the\n   lower right corner point is included in the roi range, and the ROI tensor\n   input coordinates are four values of type int32 for left, top, right, and\n   bottom.\n * The output image size requirement after Resize is 2 < = Wout, 2 < = Hout.\n * The overall requirement for ROI size and output size is currently ROI_w *\n   ROI_h + Wout * Hout < 1.5MB, this limit will be relaxed in subsequent\n   versions.\n * ROI scaling multiplier limit 0 < = step < = 229375, the step formula here is\n   step = (src_len * 65536 + dst_len / 2) / dst_len, where src_len is the W or H\n   of the ROI and the W or H of the output image size after resize.\n\n\nCustom OP Development#\n\n\nDSP Related#\n\nWhat are the DSP models selected for the J6?#\n\nThe J6 uses Cadence's Tensilica Vision Q8 DSP IP (hereinafter referred to as\nQ8), a digital signal processor dedicated to vision/image processing, with the\nnumber of DSP IPs varying slightly depending on the development board model.\nMore information can be found: DSP Development Documentation or Cadence Official\nDocumentation。\n\nHow do to get the official Cadence documentation?#\n\nInstallation steps can be referred to: DSP Development Documentation section;\nAfter installing the Cadence Development Kit, you can view some of the\ndocumentation within the development package. To obtain the complete\ndocumentation package, please contact Horizon technical support staff.\n\nWhat computational accuracy does the DSP support?#\n\nSupport int8/int16/int32 integer calculations, as well as float32 and double\nfloating point calculations.\n\nHow to view the log output on the DSP side?#\n\nIn the X86 emulation environment, you can view the log output by modifying the\nlog printing level environment variable HB_DSP_VDSP_LOG_LEVEL when executing the\nsample script, and the method of setting the log level is the same as that of\nthe UCP, and the modification method, please refer to the contents of the\nscript;\n\nIn the development board, you can view the logs on the DSP side by monitoring\nthe log file. The specific method is as follows:\n\n 1. Add the environment variable export HB_DSP_WRITE_VDSP_LOG_TO_ARM=true to\n    enable DSP log output.\n\n\n\n 2. Modify the DSP log printing level environment variable\n    HB_DSP_VDSP_LOG_LEVEL.\n 3. Use the following command to enable file monitoring.\n\n\n\nCan the DSP samples be run directly?#\n\nIt is possible. In the X86 simulation environment, the OE development kit for J6\nprovides a pre-compiled DSP image in the samples/ucp_tutorial/deps_x86/ucp/bin\npath. In the VP samples, you can directly execute the build.sh script in the\nsamples/ucp_tutorial/vp/code/ directory to generate the executable file, and\nthen execute the samples/ucp_tutorial/vp/vp_samples/script_x86/01_basic_\nprocessing/run_basic_bin. processing/run_basic_processing.sh to run the sample.\n\nOn the development board, the samples provide build scripts for the\ncorresponding environments. In the VP sample, you can directly execute the\nbuild.sh script in the samples/ucp_tutorial/vp/code/ directory to generate an\nexecutable file, and then copy the generated object in the\nsamples/ucp_tutorial/vp/vp_samples/script directory to generate the executable,\nthen copy the build from the\nsamples/ucp_tutorial/vp/vp_samples/script/01_basic_processing/run_basic_processi\nng.sh directory to the development board to run the sample.\n\nHowever, for code development or compiling new images, the DSP development\nsoftware Xplorer and the corresponding License file are required, which can be\nobtained by contacting Horizon technical support.","routePath":"/en/guide/ucp/ucp_faq_errorcode/ucp_faq","lang":"en","toc":[{"text":"General Issues","id":"general-issues","depth":2,"charIndex":3},{"text":"What is the reason for returning HB_UCP_INVALID_ARGUMENT error code after task creation or submission?","id":"what-is-the-reason-for-returning-hb_ucp_invalid_argument-error-code-after-task-creation-or-submission","depth":3,"charIndex":-1},{"text":"How to understand the physical and virtual addresses of hbUCPSysMem?","id":"how-to-understand-the-physical-and-virtual-addresses-of-hbucpsysmem","depth":3,"charIndex":778},{"text":"How to understand cacheable and non-cacheable hbmem?","id":"how-to-understand-cacheable-and-non-cacheable-hbmem","depth":3,"charIndex":1486},{"text":"Model Inference","id":"model-inference","depth":2,"charIndex":3125},{"text":"What are the possible causes of the timeout on the hbUCPWaitTaskDone interface for model reasoning?","id":"what-are-the-possible-causes-of-the-timeout-on-the-hbucpwaittaskdone-interface-for-model-reasoning","depth":3,"charIndex":-1},{"text":"Reasons why model inference gets stuck","id":"reasons-why-model-inference-gets-stuck","depth":3,"charIndex":3842},{"text":"What are the ROI input model constraints?","id":"what-are-the-roi-input-model-constraints","depth":3,"charIndex":4367},{"text":"Custom OP Development","id":"custom-op-development","depth":2,"charIndex":5460},{"text":"DSP Related","id":"dsp-related","depth":3,"charIndex":5485},{"text":"What are the DSP models selected for the J6?","id":"what-are-the-dsp-models-selected-for-the-j6","depth":4,"charIndex":5499},{"text":"How do to get the official Cadence documentation?","id":"how-do-to-get-the-official-cadence-documentation","depth":4,"charIndex":5876},{"text":"What computational accuracy does the DSP support?","id":"what-computational-accuracy-does-the-dsp-support","depth":4,"charIndex":6218},{"text":"How to view the log output on the DSP side?","id":"how-to-view-the-log-output-on-the-dsp-side","depth":4,"charIndex":6377},{"text":"Can the DSP samples be run directly?","id":"can-the-dsp-samples-be-run-directly","depth":4,"charIndex":7125}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":233,"title":"Overview","content":"#\n\n\nGeneral Introduction#\n\n * What is UCP?\n   \n   The Unify Compute Platform(Unify Compute Platform, hereafter referred to UCP)\n   defines a unified set of heterogeneous programming interfaces, and provides\n   APIs(Application Programming Interface, hereafter referred to API) to enable\n   calls to all resources on the computing platform. UCP abstracts and\n   encapsulates the functional hardware on the SOC, and externally provides\n   function-based APIs for creating corresponding UCP tasks (e.g., VP operator\n   tasks), and supports the setting of a hardware backend to be submitted to the\n   UCP scheduler, which can complete the unified scheduling of tasks on the Soc\n   based on hardware resources.\n   \n   Specifically, the following features are provided: Vision Process, Neural\n   Network Inference, High Performance Library and Custom Operator Plugin\n   Development.\n   \n   For details of the Backend, you can refer to the section Backend Instruction\n   .\n\n * UCP scenarios:\n   \n   Call single operator: vision operators and high performance processing\n   operators in UCP can be used directly.\n   \n   Operator plugin development and usage: custom operator development is\n   available. Deep Learning Model Inference: deep learning model inference tasks\n   can be accomplished, and model parsing and hardware deployment are completed\n   within UCP.\n\n * UCP advantages:\n   \n   Highly abstract: for the function of single operator, no need to be troubled\n   by hardware differences, hardware that needs to be executed can be selected\n   by specifying the backend, reducing the difficulty of hardware deployment.\n   \n   Highly integrated: as Horizon's unified heterogeneous programming interface,\n   all the requirements development can be completed by a set of interfaces.\n\nNote\n\nThis section is used to guide you on how to use the UCP for hardware deployment\nof the model. Knowledge, experience, and skills in basic embedded development\ncan lead to a better understanding of the contents of this section.\n\n\nOperating Modes#\n\nThe UCP framework supports two main operating modes: Direct Mode and Relay Mode.\nBy default, the system operates in Direct Mode. In Relay Mode, UCP enables\nunified scheduling of multi-process tasks. To use Relay Mode, you must first\nstart the ucp_service, which is located at deps_aarch64/ucp/bin/service/. You\nalso need to enable Relay Mode by setting the environment variable\nHB_UCP_ENABLE_RELAY_MODE=true, allowing user processes to communicate via the\nrelay service. Regardless of whether Direct Mode or Relay Mode is used, the UCP\ninterface calls remain consistent, without impacting the programming logic. You\ncan flexibly choose between these two modes based on the system’s performance\nand flexibility requirements.\n\n\nInterface Usage Flow#\n\nThere are three ways to execute UCP task structure calls: synchronous execution,\nasynchronous execution, and registered callback function execution. Here is an\nexample of calling the VP interface hbVPRotate(hbVPRotate) to illustrate the\nthree task execution modes.\n\n\nSynchronous Execution#\n\nWhen creating a task, pass the corresponding parameter of the taskHandle into\nthe nullptr, which can be immediately synchronously executed, with the following\nreference code:\n\n\n\n\nAsynchronous Execution#\n\nWhen creating a task, the taskHandle parameter needs to be initialized to\nnullptr in advance. After submitting a UCP task (hbUCPSubmitTask), executing the\ninterface (hbUCPWaitTaskDone) at the specified location in the thread. Waiting\nfor the task to complete, with the following reference code:\n\n\n\n\nRegistered Callback Function Execution#\n\nWhen creating a task, the taskHandle parameter needs to be initialized to\nnullptr in advance. The callback function needs to be registered before the task\nis submitted (hbUCPSubmitTask).\n\nThe reference code for setting the callback function is as follows:\n\n\n\nNote\n 1. UCP has built-in the Neural Network Kernel, Vision Process Kernel, and High\n    Performance Kernel, which may be supported by different Backends.\n 2. UCP tasks are uniformly scheduled according to a priority-based scheduling\n    strategy, and the priority of task execution can be specified when\n    submitting tasks.\n 3. Synchronous execution does not support configuring the task control\n    parameters and selecting the backend. UCP will select the appropriate\n    hardware for execution based on the executable backend of the current task\n    and the hardware load information.\n 4. Asynchronous execution supports configuring task control parameters and\n    backend selection. If not specified, the backend is selected based on load\n    balancing across available backends.\n 5. Except for ISP and video encoding and decoding tasks, the input and output\n    memory for all tasks must be allocated and managed by you using the memory\n    interfaces provided by UCP.\n\n\nDEB Deployment Package#\n\nUCP provides a DEB deployment package located at\ndeps_aarch64/ucp/bin/deb/hbucp_aarch64_3.1.4.deb. This package is designed to\nsimplify deployment on the board, ensuring easier and more efficient system\nconfiguration. By automatically installing the necessary binaries and dependent\nlibraries, By automatically installing the necessary binaries and dependent\nlibraries, the UCP-related applications can be quickly set up and running.\n\nAfter successfully installing the DEB package, the binaries ucp_service and\nhrt_ucp_monitor will automatically be installed in the /usr/hobot/bin directory,\nallowing you to access these tools via the command line. The related dynamic\nlibraries will be installed in the /usr/hobot/lib directory, ensuring that all\nruntime dependencies are correctly configured.\n\nDuring the installation, the system automatically configures the required\nlibrary paths, removing the need for manual intervention. This ensures that all\ndependent libraries are correctly loaded when running applications, thus\nensuring the stability and reliability of the system.\n\n\nInstallation Command Example#\n\nWhen installing the DEB package for the first time, the system will display\n\n\n\n\nUpgrade Installation Command Example#\n\nWhen upgrading the DEB package, if the installation process fails, the system\nwill automatically roll back to the previous version.\n\n\n\n\nUninstalling the DEB Package#\n\nTo uninstall an already installed DEB package, refer to the following command:\n\n\n\n\nBackend Instruction #\n\nThe backend refers to the back-end computing hardware for UCP task execution.\nThe current backends supported by UCP include BPU, DSP, GDC, STITCH, JPU,\nVPU(Video Process Unit), PYRAMID, ISP.\n\nBACKEND   DESCRIPTION\nBPU       Brain Process Unit, the Horizon Neural Network computational\n          unit.\nDSP       Digital Signal Processor, a programmable hardware unit.\nGDC       Geometric Distortion Correction, a hardware IP on ARM that\n          can perform perspective transformation, distortion\n          correction, and image affine transformation on input images.\nSTITCH    The stitch is an IP unit of J6, which can crop and stitch\n          the input image, the stitching modes are: alpha fusion,\n          alpha beta fusion, direct copy.\nJPU       JPEG Processing Unit, mainly used to complete the JPEG\n          encoding and decoding functions.\nVPU       Video Processing Unit, a specialized visual processing unit.\nPYRAMID   Fully known as Image Pyramid, a hardware processing module\n          that reduces the entire original image.\nISP       Image Signal Processor, which is a hardware processing\n          module that can convert the RAW format rich in original\n          image information into the YUV format that is easy to\n          transmit and process.\n\n\nHardWare Feature Description#\n\n\nDSP#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       1\nmaximum input   4096x2160\nminimum input   32x16\nformat          Y/NV12\n\n\nDSP#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       1\nmaximum input   3840x2160\nminimum input   100x100\nformat          NV12\n\n\nSTITCH#\n\nHW FEATURE       FEATURE INDICATOR\nHW number        1\nmaximum input    2000x2000\nminimum input    16x2\nmaximum output   3840x3840\nminimum output   16x2\nformat           Y/NV12\n\n\nJPU#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       1\nmaximum input   8192x8192\nminimum input   32x32\nmax instance    64\nformat          NV12/YUV420/YUV444/YUV444_P\n\n\nVPU(Video Process Unit)#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       1\nmaximum input   8192x4096\nminimum input   256x128\nmax instance    32\nformat          NV12/YUV420\n\n\nPYRAMID#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       3\nmaximum input   4096x4096\nminimum input   32x32\nformat          Y/NV12\n\n\nISP#\n\nHW FEATURE      FEATURE INDICATOR\nHW number       2\nmaximum input   4096x2160\nminimum input   480x240\nformat          RAW12\n\n\nEnvironment and Tools#\n\nUCP is applicable for the Horizon J6 and higher architecture computing\nplatforms, you need to have basic embedded development knowledge and skills to\ncomplete the cross-compilation and deployment, the usage of the environment and\ntool requirements refer to the following table:\n\nENVIRONMENT/TOOLS      SUPPORTED VERSION\nOS                     Linux\nDevelopment board      J6 processor\nDevelopment language   C++11\nCross compiler         Linaro 12.2.0\nToolchain DSP          Cadence Vsion Q8 2023.11\n\nIn addition to the development board, UCP also provides the same development\nsupport in an x86 emulation environment as it does on the board.\n\n\nx86 Emulation Description#\n\n\nFunction Description#\n\nSimilar to the development board, UCP provides the same visual processing, model\ninference, and high-performance computing capabilities in an x86 architecture\nthrough emulation. All examples and interface code can be equivalently used in\nthe emulation environment. You can develop and debug code in the x86\nenvironment, gaining immediate feedback during the development process. This\nallows you to identify and resolve issues early, improving development\nefficiency and code quality, and ensuring seamless migration of the code to run\non SoC hardware.\n\nThe emulation methods for each backend supported by UCP are as follows:\n\n * BPU and DSP hardware use instruction-level emulation.\n * GDC, JPU, and VPU hardware use CModel executable file emulation.\n   * The CModel executable file for GDC hardware is gdc_cmodel .\n   * The CModel executable files for JPU hardware are Nieuport_JpgEnc and\n     Nieuport_JpgDec, used for JPEG encoding and decoding, respectively.\n   * The CModel executable files for VPU(Video Process Unit) hardware are\n     hevc_enc, hevc_dec, avc_enc and avc_dec. Among them, hevc_enc and hevc_dec\n     are used for H.265 encoding and decoding respectively, while avc_enc and\n     avc_dec are used for H.264 encoding and decoding respectively.\n * STITCH and PYRAMID hardware use emulation libraries.\n\n\nEnvironment Description#\n\nCompilation Environment#\n\nThe UCP emulation uses the compiler environment provided by the Docker image.\n\nRuntime Environment#\n\n 1. When using DSP hardware for emulation, you need to configure the Xtensa\n    development environment and specify the DSP emulation image path. For\n    environment configuration, refer to Install DSP Toolchain And Configure\n    Core. You can specify the DSP emulation image path by setting the\n    environment variable HB_DSP_CMODEL_IMAGE to ensure the application can\n    locate the correct emulation image file. The reference command is as\n    follows:\n\n\n\n 2. When running CModel executable files, you need to add the executable file\n    path to the PATH environment variable so that they can be run directly from\n    the terminal. The reference command is as follows:\n\n\n\n 3. No additional configuration is required for running emulations of the other\n    hardware.\n\n\nPerformance Description#\n\nHowever, the performance of the x86 emulation environment is usually lower than\nthat of the actual hardware due to the following reasons: As previously\nmentioned, the emulation methods used by the various backends supported by UCP\ninclude instruction-level emulation, CModel executable file emulation, and\nemulation libraries.\n\n * Instruction-level emulation: This method simulates and executes each\n   instruction individually, resulting in higher computational overhead and\n   lower emulation speed.\n * CModel executable file emulation: CModel executable files perform input and\n   output operations through file read/write processes. File I/O operations are\n   time-consuming and can impact the emulation speed to some extent.\n * Emulation libraries: These run on the CPU, which simulates the behavior of\n   hardware accelerators. However, the CPU's architecture and design are not\n   specifically optimized for these tasks, leading to lower execution\n   efficiency.\n\nAlthough the performance of the emulation environment is typically lower than\nthat of actual hardware, the emulation environment provides comprehensive API\nsupport and precise functional verification. This significantly enhances your\ndevelopment efficiency and code quality, helping you identify and resolve\npotential issues at an early stage.","routePath":"/en/guide/ucp/ucp_overview","lang":"en","toc":[{"text":"General Introduction","id":"general-introduction","depth":2,"charIndex":3},{"text":"Operating Modes","id":"operating-modes","depth":2,"charIndex":2014},{"text":"Interface Usage Flow","id":"interface-usage-flow","depth":2,"charIndex":2758},{"text":"Synchronous Execution","id":"synchronous-execution","depth":3,"charIndex":3048},{"text":"Asynchronous Execution","id":"asynchronous-execution","depth":3,"charIndex":3251},{"text":"Registered Callback Function Execution","id":"registered-callback-function-execution","depth":3,"charIndex":3575},{"text":"DEB Deployment Package","id":"deb-deployment-package","depth":2,"charIndex":4854},{"text":"Installation Command Example","id":"installation-command-example","depth":3,"charIndex":5958},{"text":"Upgrade Installation Command Example","id":"upgrade-installation-command-example","depth":3,"charIndex":6069},{"text":"Uninstalling the DEB Package","id":"uninstalling-the-deb-package","depth":3,"charIndex":6244},{"text":"Backend Instruction","id":"backend-instruction","depth":2,"charIndex":-1},{"text":"HardWare Feature Description","id":"hardware-feature-description","depth":2,"charIndex":7652},{"text":"DSP","id":"dsp","depth":3,"charIndex":7684},{"text":"DSP","id":"dsp-1","depth":3,"charIndex":7815},{"text":"STITCH","id":"stitch","depth":3,"charIndex":7946},{"text":"JPU","id":"jpu","depth":3,"charIndex":8133},{"text":"VPU(Video Process Unit)","id":"vpuvideo-process-unit","depth":3,"charIndex":8304},{"text":"PYRAMID","id":"pyramid","depth":3,"charIndex":8481},{"text":"ISP","id":"isp","depth":3,"charIndex":8616},{"text":"Environment and Tools","id":"environment-and-tools","depth":2,"charIndex":8748},{"text":"x86 Emulation Description","id":"x86-emulation-description","depth":2,"charIndex":9416},{"text":"Function Description","id":"function-description","depth":3,"charIndex":9445},{"text":"Environment Description","id":"environment-description","depth":3,"charIndex":10789},{"text":"Compilation Environment","id":"compilation-environment","depth":4,"charIndex":10815},{"text":"Runtime Environment","id":"runtime-environment","depth":4,"charIndex":10920},{"text":"Performance Description","id":"performance-description","depth":3,"charIndex":11714}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":234,"title":"The hrt_ucp_monitor Tool Introduction","content":"#\n\nhrt_ucp_monitor is a tool that monitors the hardware IP usage, and the supported\nIPs include BPU, DSP, GDC, STITCH, PYM, ISP, CODEC(VPU(Video Process Unit) and\nJPU). When submitting tasks, UCP can specify the Backend for task deployment,\nwhich corresponds to the hardware supported by UCP.\n\n\nParameters Description#\n\n   PARAMETER      TYPE     DESCRIPTION\n-b, --batch       null     Specified to run in non-interactive batch mode, which is the\n                           default mode for interactive mode.\n-d, --delay       int      Specify the update interval in milliseconds, with a default\n                           value of 1000 milliseconds and an effective range of [100,\n                           10000].\n-f, --freq        int      Specify the sampling frequency，defaluting to 1000 samples\n                           per second, only effective for BPU and DSP, valid range [10,\n                           1000].\n-n, --number      int      Specify the usage refresh rate, valid range is [1,\n                           INT32_MAX].\n-t, --time        int      Specify the tool running time in seconds, valid range is [1,\n                           INT32_MAX].\n-e, --enable      string   Specify the hardware IP to be monitored, valid values are\n                           bpu, dsp, gdc, stitch, pym, isp, jpu, vpu, case insensitive.\n                           Multiple parameters are separated by semicolons.\n\n\nUsage Instructions#\n\nRunning hrt_ucp_monitor -h or hrt_ucp_monitor --help to obtain tool usage\ndetails, as shown in the following:\n\n\n\nDuring the operation of hrt_ucp_monitor, you can early exit by pressing the Q\nkey or exectuing CTRL+C.\n\n\nDefault Parameters#\n\nOverview#\n\nwhen running hrt_ucp_monitor without specifying parameters, it will use default\nparameters to run. By default, all hardware IPs are monitored, the interactive\nmode is used, BPU and DSP samples are taken every 500 times per second, and the\nhardware IP utilization is refreshed every 1000ms.\n\nExample#\n\n\n\n\nbatch#\n\nOverview#\n\nIn the batch mode, it is not supported to receive user keystrokes. Instead, each\ntime hrt_ucp_monitor refreshed the data, it will output a new line and is\nsuitable for redirecting data to a file.\n\nExampe 1: Output results in terminal#\n\n\n\nExample 2: Redirect output results to file.#\n\n\n\n\ndelay#\n\nOverview#\n\nRefresh time for hardware IP usage, unit milliseconds, current supported setting\nrange is [100, 10000]. If the setting time is not within the valid ranges, it\nwill fail and print error prompt information.\n\nExample#\n\n\n\nThe refesh time is set to 3 seconds, indicating that the data will be refreshed\nevery 3 seconds.\n\n\nfreq#\n\nOverview#\n\nApplicable only to BPU and DSP, used to control the status sampling frequency of\nhardware IP. Currently supported range is [10, 1000], indicating a minimum\nsampling of 10 times and a maximum of 1000 samples per second. Due to different\nstatistical methods for utilization, other hardware IPs such as GDC, STITCH do\nnot require setting a sampling frequency. When setting a refresh time,\nutilization data for the specified period will be obtained.\n\nExample#\n\n\n\nSet the sampleling period for the busy state of BPU and DSP to 100 samples per\nsecond.\n\n\nnumber#\n\nOverview#\n\nSpecify the maximum refresh numbers for the hareware IP usage, and when the\nrefresh numbers reaches the maximum numbers, the hrt_ucp_monitor will\nautomatically exit. You can also press CTRL+C to exit early.\n\nExample#\n\n\n\n\ntime#\n\nOverview#\n\nFor specifying the tool running time in seconds, hrt_ucp_monitor will calculate\nthe maximum refresh numbers based on the refresh time. When the refresh numbers\nreaches the maximum refresh numbers, hrt_ucp_monitor will automatically exit.\nYou can also early exit by presing CTRL+C just like the number paramerte. The\ntime and number parameters can not be set simultaneously, an error message will\nbe prompted.\n\nExample#\n\n\n\nThe default refresh time of the tool is 1 second, and after running for 10\nseconds, it will exit.\n\n\nenable#\n\nOverview#\n\nYou can set the hardware IP to be monitored, which will only monitor the data\nyou care about and reduce the CPU usage of the tool.\n\nExample#\n\n\n\nThe tool will only output BPU usage data.\n\n\nverbose#\n\nOverview#\n\nDisplay more detailed log information such as tool parameters during the\noperation of hrt_ucp_monitor.\n\nExample#\n\n","routePath":"/en/guide/ucp/ucp_tools/ucp_monitor","lang":"en","toc":[{"text":"Parameters Description","id":"parameters-description","depth":2,"charIndex":294},{"text":"Usage Instructions","id":"usage-instructions","depth":2,"charIndex":1418},{"text":"Default Parameters","id":"default-parameters","depth":3,"charIndex":1657},{"text":"Overview","id":"overview","depth":4,"charIndex":1678},{"text":"Example","id":"example","depth":4,"charIndex":1980},{"text":"batch","id":"batch","depth":3,"charIndex":1993},{"text":"Overview","id":"overview-1","depth":4,"charIndex":2001},{"text":"Exampe 1: Output results in terminal","id":"exampe-1-output-results-in-terminal","depth":4,"charIndex":2209},{"text":"Example 2: Redirect output results to file.","id":"example-2-redirect-output-results-to-file","depth":4,"charIndex":2250},{"text":"delay","id":"delay","depth":3,"charIndex":2299},{"text":"Overview","id":"overview-2","depth":4,"charIndex":2307},{"text":"Example","id":"example-1","depth":4,"charIndex":2524},{"text":"freq","id":"freq","depth":3,"charIndex":2635},{"text":"Overview","id":"overview-3","depth":4,"charIndex":2642},{"text":"Example","id":"example-2","depth":4,"charIndex":3100},{"text":"number","id":"number","depth":3,"charIndex":3201},{"text":"Overview","id":"overview-4","depth":4,"charIndex":3210},{"text":"Example","id":"example-3","depth":4,"charIndex":3429},{"text":"time","id":"time","depth":3,"charIndex":3442},{"text":"Overview","id":"overview-5","depth":4,"charIndex":3449},{"text":"Example","id":"example-4","depth":4,"charIndex":3870},{"text":"enable","id":"enable","depth":3,"charIndex":3982},{"text":"Overview","id":"overview-6","depth":4,"charIndex":3991},{"text":"Example","id":"example-5","depth":4,"charIndex":4134},{"text":"verbose","id":"verbose","depth":3,"charIndex":4190},{"text":"Overview","id":"overview-7","depth":4,"charIndex":4200},{"text":"Example","id":"example-6","depth":4,"charIndex":4315}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":235,"title":"UCP Trace Instructions","content":"#\n\nUCP Trace provides the ability to in-depth analysis of the scheduling logic of\nUCP applications by embedding trace recording on the critical path executed by\nUCP. when performance anomalies occur, it can quickly locate the time point of\nthe anomaly by analyzing UCP trace.\n\nUCP trace provides two trace backend options: Perfetto Trace and Chrome Trace.\nYou can choose between them by setting an environment variable to meet your\nsepecific performance tracking needs.\n\n * Perfetto trace can retrieve ucp recorded traces, as well as system status,\n   ftrace information, etc.\n * Chrome trace can only retrieve ucp recoreded traces and is mainly used to\n   analyze UCP's scheduling logic.\n\nThe UCP trace tool and configuration files are located in the\nsamples/ucp_tutorial/tools directory path, with a directory structures as\nfollows:\n\n\n\n\nEnvironmental Variable#\n\nENVIRONMENTAL VARIABLE        RANGE OF VALUES                    DEFAULT VALUES   DESCRIPTION\nHB_UCP_ENABLE_PERFETTO        true，false                         false            Whether to enable the perfetto trace, defaults to not\n                                                                                  starting.\nHB_UCP_PERFETTO_CONFIG_PATH   Perfetto configuration file path   \"\"               Specify the path to the perfetto configuration file. By\n                                                                                  default, it is empty, and if you do not need UCP to\n                                                                                  initialize perfetto, you can ignore this enviromental\n                                                                                  variable.\nHB_UCP_TRACE_LOG_LEVEL        [0, 6]                             6                Specify the UCP trace log level, which defaults to 6 and is\n                                                                                  not output.\nHB_UCP_USE_ALOG               true，false                         false            Whether to enable the alog sink，defaults disabled. If\n                                                                                  enabled, logs will be output to the alog buffer and can be\n                                                                                  captured using logcat while logging is disabled for terminal\n                                                                                  output.\n\nNote\n\nPerfetto Trace has a higher priority and if export HB_UCP_ENABLE_PERFETTO=true\nwhile export UCP_TRACE_LOG_LEVEL=0 is also set, then only perfetto trace will be\nstarted and the ucp trace log will be ignored.\n\n\nUCP Trace Records#\n\nUCP add trace records in the application API and internal critical scheduling\npaths, including task trace records and operator records.\n\n\nTask Trace Records#\n\nNAME                DESCRIPTION\nhbDNNInfer          Create a model inference task\nhbVPxxx             Create a vision process task\nhbHPLxxx            Create a high performance compute task\nhbUCPSubmitTask     Submit Task\n${TaskType}::Wait   Wait task done\nTaskSetDone         Notify task done\nhbUCPReleaseTask    Release task\n\n\nOperator Trace Records#\n\nNAME       DESCRIPTION\nSubmitOp   Submit operator\nOpInfer    Operator inference\nOpFnish    Operator finish\n\n\nPerfetto Trace#\n\n\nOverview#\n\nPerfetto is a system analysis tool developed and open-sourced by Google, which\ncan collect performance data from different data sources and provides the\nPerfetto UI for data visualization and analysis. For more details on Perfetto,\nplease refer to the .\n\n\nConfiguration File#\n\nUCP Trace Configuration File#\n\nYou can configure UCP to use the Perfetto by specifying it through the\nenvironment variable HB_UCP_PERFETTO_CONFIG_PATH.\n\nParameters Description#\n\nPARAMETER      TYPE     RANGE OF VALUES                   DESCRIPTION\nbackend        string   \"in_process\"，\"system\"             in_process represents the process-internal mode, where the\n                                                          perfetto trace is directly saved to a file within the\n                                                          process. System represents the system mode, where trace\n                                                          capture is performed by the background service traced and\n                                                          traced_probe.\ntrace_config   string   configuration file for perfetto   It is available when the backend is set to in_process, the\n                                                          file is protobuf text format.\n\nNote\n\nThe UCP trace configuration file is not necessary when your application has\nalready initiaized Perfetto, you only need to export HB_UCP_ENABLE_PERFETTO=true\nto enable Perfetto.\n\nExample 1：in_process mode#\n\n\n\nExample 2：system mode#\n\n\n\nNote\n\nWhen selecting the system for backend, there is no need to specify trace_config\nseparately for UCP.\n\nPerfetto Configuration File#\n\nFor detailed information about perfetto configuration files, please refer to .\nUCP provides reference configuration files ucp_in_process.cfg and\nucp_system.cfg, which can be modified based on application scenario.\n\nExample：ucp_in_process.cfg#\n\n\n\n\nExample#\n\nin_process mode#\n\nIn the in_process mode, only trace within UCP process can be captured, and it is\nnot necessary to start the background process of perfetto.\n\nOperating Procedure#\n\n 1. Configure environment variables.\n\n\n\nNote\n\nIn the ucp_in_process.json, the configuration file for perfetto is specified as\nucp_in_process.cfg, and the output_path specifies the path for output trace\nfile. Due to the fact that Perfetto does not support directly overwriting\nexisting trace files, if the file already exists, it needs to be deleted first.\n\n 2. Running the UCP application, using hrt_model_exec as an example.\n\nDue to the specified file path is a relative path, the trace configuration file\nand scripts need to be placed in the same level directory as the running\nprogram. Also, you need to make sure that you configure the environment\nvariables and run the program in the same shell environment.\n\n\n\n 3. The generated trace is saved in the output file ucp.pftrace specified by the\n    perfetto command, and you can use to open it.\n\n 4. Clicking on the task in the timeline will show the complete scheduling\n    process from creation to release of the task.\n\n 5. The common operations of Perfetto UI are as follows, for more detailed\n    operation Instructions, please refer to the help interface.\n\nOPERATIONS                                      DESCRIPTION\nw or ctrl + scroll up with the mouse wheeel     Zoom in\ns or ctrl + scroll down with the mouse wheeel   zoom out\na or drag the time bar to the left              Pan left\nd or drag the time bar to the right             Pan right\n?                                               Show help\n\nsystem mode#\n\nIn system mode, UCP trace is just one of the data sources, so it it necessary to\nrun the corresponding commands for tracebox to complete the capture of trace.\n\nOperating Procedure#\n\n 1. Running the perfetto background process.\n\n\n\n 2. Trigger data capture.\n\n\n\n 3. Open a new terminal and configure the UCP environment variables.\n\n\n\n 4. Running the UCP application, using hrt_model_exec as an example.\n\nTo be able to capture complete data, it is necessary to ensure that the perfetto\nprocess does not exit before the hrt_model_exec execution is complete.\n\n\n\n 5. The generated trace is saved in ucp.pftrace, and you can use to open it.\n\n\nBPU Trace#\n\nIn system mode, BPU trace capture is supported. Simply add BPU trace data source\nto the perfetto configuration file. The ucp_bpu_trace.cfg file has already\ndefaultly included the BPU trace data source. The specific configuration items\nare shown below.\n\n\n\nThe bpu_trace_period_ms is used to set the period for reading the BPU trace. You\ncan adjust this parameter according to your actual usage scenario. When the BPU\nload is high, you can appropriately shorted the reading period to avoid the\nproblem of trace data being overwritten due to mismatched read and write speeds.\n\nTo demonstrate the BPU trace during the inference process of multiple models, an\nexample of a multi-process application is provided here. Except for the\ndifferent running programs being launched, the rest of the steps are the same as\nin the previous chapter.\n\n\n\nVisualization of BPU trace requires the use of the hbperfetto tool, which is\ncustom-developed by Horizon Robotics. You can obtain this tool by contacting the\nHorizon Robotics technical support personnel. The effect of opening a trace file\nusing hbperfetto is shown in the image below.\n\nThe scheduling of different model inference tasks presented in BPU trace is\nshown in the following figure.\n\nhbperfetto supports the association of UCP trace and BPU trace. The following\ndiagram illustrates the complete process from the creating, submission,\nscheduling and execution, to the task's completion and eventual releases.\n\n\nChrome Trace#\n\nChrome trace only supports capturing UCP trace, and does not support capturing\ndata sources. For capturing multiple data sources, please use Perfetto trace.\nThe characteristic of Chrome trace is simplicity and ease of use, using text\nlogs to record traces without depending on any extra thrid-party libraries or\ntools. If you are only interested in the scheduling logic of UCP, you can use\nChrome trace to capture it.\n\n\nExample#\n\n 1. Configure environment variables.\n\n\n\nNote\n\nBefore starting new capture, it is recommended to delete the old log files to\navoid interference from old data.\n\n 2. Running the UCP application, using hrt_model_exec as an example.\n\n\n\n 3. Execute trace capture script.\n\nAfter capturing the trace logs, run the catch_trace.sh provided in the UCP\ndistribution package to convert the raw trace logs into a json-formatted trace\nfile.\n\n\n\n 4. Open ucp_trace_task.json and ucp_trace_thread.json using or Chrome\n    UI(chrome://tracing/).\n\nOpen ucp_trace_task.json by Chrome UI:\n\nOpen ucp_trace_thread.json by Perfetto UI:","routePath":"/en/guide/ucp/ucp_tools/ucp_trace","lang":"en","toc":[{"text":"Environmental Variable","id":"environmental-variable","depth":2,"charIndex":838},{"text":"UCP Trace Records","id":"ucp-trace-records","depth":2,"charIndex":2647},{"text":"Task Trace Records","id":"task-trace-records","depth":3,"charIndex":2805},{"text":"Operator Trace Records","id":"operator-trace-records","depth":3,"charIndex":3155},{"text":"Perfetto Trace","id":"perfetto-trace","depth":2,"charIndex":3289},{"text":"Overview","id":"overview","depth":3,"charIndex":3307},{"text":"Configuration File","id":"configuration-file","depth":3,"charIndex":3574},{"text":"UCP Trace Configuration File","id":"ucp-trace-configuration-file","depth":4,"charIndex":3595},{"text":"Parameters Description","id":"parameters-description","depth":4,"charIndex":3748},{"text":"Example 1：in_process mode","id":"example-1in_process-mode","depth":4,"charIndex":4764},{"text":"Example 2：system mode","id":"example-2system-mode","depth":4,"charIndex":4794},{"text":"Perfetto Configuration File","id":"perfetto-configuration-file","depth":4,"charIndex":4927},{"text":"Example：ucp_in_process.cfg","id":"exampleucp_in_processcfg","depth":4,"charIndex":5172},{"text":"Example","id":"example","depth":3,"charIndex":5204},{"text":"in_process mode","id":"in_process-mode","depth":4,"charIndex":5214},{"text":"system mode","id":"system-mode","depth":4,"charIndex":6856},{"text":"BPU Trace","id":"bpu-trace","depth":3,"charIndex":7505},{"text":"Chrome Trace","id":"chrome-trace","depth":2,"charIndex":8973},{"text":"Example","id":"example-1","depth":3,"charIndex":9408}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":236,"title":"VP Overview","content":"#\n\nThe VP module mainly works in the preprocessing and postprocessing phases of the\nmodel in the NN-centered computing scheme.\n\nIn the horizon UCP, various hardware have been equipped with picture processing\noperators, and the VP module encapsulates the hardware calling related to image\nprocessing, and selects different hardware solutions by setting the backend (if\nyou don't specify the backend, the UCP will automatically adapt the processing\nunit with a lower load), which balances the load of the development board, fully\nexploits the board's computing power, and avoids the trouble of the difference\nbetween the different hardware calling, so that you can pay more attention to\nthe software functions.\n\nThe architecture is shown as below:\n\nAs shown in the above picture, through the task constructor functions of the\noperators provided by the VP module, such as hbVPResize, hbVPRotate, etc., the\napplication generates the task handle of the corresponding operator. The UCP\nprovides the Service which contains the modules of task scheduling, session\nmanagement, engine management, etc. After the task handles of the corresponding\noperators are generated, the operator tasks will be committed to the task queue\nthrough the UCP task scheduling interface, and assigned to the different\nunderlying hardware to realize the functional logic of the operators. The bottom\nlayer is the function of the operator encapsulated and implemented in different\nprocessing units, for example, the remap operator is implemented in both DSP and\nGDC, at this time, the remap operator in VP can be flexibly deployed on these\ntwo types of hardware.\n\nBefore reading this section, please make clear the following basic concepts,\nwhich may be mentioned several times in the following sections:\n\n * The Vision Algorithm, which is the function used to implement the picture\n   processing.\n * The VDSP, full name Vision Digital Signal Processing, which means the visual\n   digital signal processing.\n * The VP, full name Vision Process, which means the visual processing module in\n   the UCP.\n * The Backends, which means the allocatable processing unit in the UCP.\n * The Kernel, which means the filter kernel parameter of the VP operators.\n * The picture size, the operators have their own constraints on the input and\n   output picture sizes.","routePath":"/en/guide/ucp/vp/vp1_Introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":237,"title":"Quickstart","content":"#\n\nThis section describes how to use the sample of VP module in UCP sample package\nucp_tutorial, and how to configure the development environment, compile and run\nthe sample application code, which helps to get started with the VP function\nmodule in UCP quickly. The main architecture is as follows:\n\n\nSample Package Usage#\n\nThe structure of the sample package is shown as below:\n\n\n\nThe ucp_tutorial/vp folder contains the VP module samples, including basic image\nprocessing samples, image transformation samples, image feature extraction\nsamples, optical flow samples, and surround-view image stitching samples, video\ncodec sample, which can be compiled by both board-side running and x86\nemulation. For details, please refer to the section VP Sample.\n\n\nCompile Sample Operator#\n\nBefore compiling and running the sample application code, you need to ensure\nthat the environment meets the requirements, according to the guidelines in\nsection Environment Deployment, your development machine should already have the\nrelevant environment installed, the requirements are as follows:\n\n * cmake >= 3.0.\n * For board-side compilation, you need to specify the cross-compile toolchain,\n   for x86 emulation docker, you can use the compiler that comes with it.\n\nIn the ucp_tutorial/vp/code directory of the sample, there is a pre-configured\nbuild script build.sh, with the options -a x86 and -a aarch64 to support two\ntypes of builds respectively, and executing the build.sh script directly will\ncomplete the one-click build, and the generated files will be saved to the\nucp_tutorial/vp/vp_samples directory. Moreover, the directory encompasses two\ncompilation scripts, namely build_aarch64.sh and build_x86.sh, tailored to\ndistinct compilation configurations. Employing these scripts mirrors the\nfunctionality of the build.sh script. The command you need to execute to compile\nthe VP module running on the board side is as follows:\n\n\n\nAfter executing the compilation script, the executable programs and dependency\nfiles required to run the samples will be generated and saved in the\nucp_tutorial/vp/vp_samples directory.\n\nTaking the VP as an example, its generated objects are shown below, containing\nimage data, sample running scripts, running dependency libraries, executable\nfiles, and script directories for running the examples, which together form a\ncomplete running environment and running dependencies.\n\n\n\n\nRun Sample#\n\nAfter all the steps of compiling are completed correctly, the executable samples\nwill be configured and saved in the vp_samples folder. Depending on the\nexecution environment, the two ways of commands for executing the samples on the\nboard and simulation are introduced as follows.\n\nRun on Board#\n\nCopy the entire vp_samples folder to the development board, and go to the\nvp_samples/script folder and execute the provided running script in sample\nfolder directly to see the results of the sample run. The reference command for\nexecuting the script as follows:\n\n\n\nRun on x86 Emulation#\n\nGo to the vp_samples/script_x86 folder and execute the provided run script in\nsample folder directly to see the results of the sample run. The reference\ncommand for executing the script as follows:\n\n\n\nNote\n\nThe Horizon J6 SOC uses the Tensilica Vision Q8 DSP from Cadence, so the dsp\noperators running in the x86 simulation sample relies on a set of toolchain\nprovided by Cadence. The environment configuration can be found in the\nguidelines in section Install DSP Toolchain And Configure Core. The correct\nconfiguration of License and environment variable XTENSA_ROOT is required.\n\n\nOutput Description#\n\nTake the sample performing on board as an example, when the sample is running,\nthe process log will be printed on the console and the corresponding output file\nwill be generated. The log will contain the flow of all the operator callings,\nand the output will be saved in the script/01_basic_processing folder. The\noutput of the sample section is as follows:\n\n\n\nThe generation will be saved to the vp_samples/script/01_basic_processing\ndirectory with the following contents:\n\n\n\n\nUsage of VP Operator#\n\nThis section shows how to use VP encapsulated operator to realize the function\nof image processing through a simple operator calling. The main steps include\nimage load, task creation, task commit, task completion, task destruction, save\noutput and so on. You can read the corresponding source code and comments to\nlearn.\n\nThe role of the sample is to rotate the image by 90 degrees using the hbVPRotate\noperator, which is implemented as follows:\n\n","routePath":"/en/guide/ucp/vp/vp2_Quick_Start","lang":"en","toc":[{"text":"Sample Package Usage","id":"sample-package-usage","depth":2,"charIndex":301},{"text":"Compile Sample Operator","id":"compile-sample-operator","depth":3,"charIndex":754},{"text":"Run Sample","id":"run-sample","depth":3,"charIndex":2406},{"text":"Run on Board","id":"run-on-board","depth":4,"charIndex":2702},{"text":"Run on x86 Emulation","id":"run-on-x86-emulation","depth":4,"charIndex":2982},{"text":"Output Description","id":"output-description","depth":3,"charIndex":3589},{"text":"Usage of VP Operator","id":"usage-of-vp-operator","depth":2,"charIndex":4088}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":238,"title":"Module Architecture","content":"#\n\n\nVP Module Architecture#\n\n\n\nUsing the VP module's API, a task handle for the operator is created. Each\noperator interface has at least one hardware implementation (e.g., the remap\nperator can execute on DSP or GDC hardware). After creating the task handle,\nscheduling parameters can be set to specify the backend, task priority, device\nID, and custom ID, thereby submitting the task to the corresponding processing\nunit. Once the task is submitted, the API must be called to wait for the task to\ncomplete. After the task is completed, the API is used to release the task\nhandle and related resources, ensuring effective management and release of\nsystem resources. Throughout the process from task creation to submission and\nrelease, the UCP Service layer provides comprehensive interface and functional\nsupport at each stage.\n\n\nOperator Execution Process#\n\nThe Rotate operator's asynchronous execution is used as an example to\ndemonstrate the actual invocation process of the operator. The usage of other\noperators follows a similar process.\n\n\n\nNote\n\nThe dashed box indicates that the step is non-required, which can be omitted by\nreusing parameters, using default parameters, etc.\n\n 1. Prepare the input and output data: that is applying memory for the picture\n    and constructing the associated description.\n\n 2. Create an operator task: this step calls the operator task interface\n    directly, while pass the parameters required for the execution of the\n    operator, the UCP task handle will be output after the execution is\n    completed.\n\n 3. Commit task: commit operator tasks to different processing cores by passing\n    in scheduling parameters, task committing supports specifying the backend,\n    if you do not specify the backend, the system will automatically adapt.\n\n 4. Specify the interface to wait for the end of the task: at the end of the\n    task, the system will return different return values according to the\n    different execution status, at this time, you can check the result of the\n    task execution according to the return value.\n\n 5. Destroy the task: after the successful execution of the task, you need to\n    destroy the task and free the applied memory.","routePath":"/en/guide/ucp/vp/vp3_Architecture","lang":"en","toc":[{"text":"VP Module Architecture","id":"vp-module-architecture","depth":2,"charIndex":3},{"text":"Operator Execution Process","id":"operator-execution-process","depth":2,"charIndex":830}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":239,"title":"Performance Indicator","content":"#\n\nThis section provides you with VP performance test instructions and shows you\nthe performance data result statistics for reference.\n\n\nTest Platform#\n\nTest conditions (hardware and software platform requirements):\n\n * Test development board: J6 development board.\n * System software: LNX-PL4.0-DB-20240822 release 20240822-222755.\n * OpenCV version: 3.4.5.\n * OpenCV runtime environment: A78 single-threaded.\n\n\nTest Method#\n\n * Performance test method: Since the time consumed to run the operator will\n   fluctuate, here we take the average performance data of 100 runs of the\n   operator.\n * Test time with steady_clock.\n\n\n\n * Data description:\n   * Task time: from the creation of the containing task (e.g.\n     hbVPBilateralFilter) to the end of the releasing task (hbUCPReleaseTask).\n   * Baseline Time: OpenCV execution time\n   * Ratio: Baseline Avg / Task Avg\n * Error assessment: the error is mainly caused by the interpolation\n   implementation in the algorithm, edge processing, and the type of\n   computational data is different from some of the processes, the following\n   metrics can be used to assess the error.\n   * MaxAbsDev: absolute deviation of the maximum pixel value.\n   * SumAbsDev: sum of absolute deviations of pixel values.\n   * MSE: Mean square error of pixel value.\n\n\nPerformance Comparison#\n\nNote\n\nThe image size used for DSP GDC test is 1080P(1920x1080), Codec(3840x2160),\nLkof(1080P), Stitch(Input 2 sheets 320x320, Output 640x640), Pyramid(Input\n1080P, Output 960x540). The performance of pyramid core 2 is about 2/3 of core\n0/1.\n\nALGORITHM         PARAMETERS                                                   VISION PROCESSES(US)                         OPENCV 3.4.5 A78(US)   RATIO(DEFAULT DSP)             \nbilateralFilter   1920x1080 Y, kernelSize=5, sigmaColor=15, sigmaSpace=2       4224                                         54712                  13.0\nboxFilter         1920x1080 Y, kernelHW=3x3                                    510                                          1956                   3.833\ncanny             1920x1080 Y, kernelSize=3, threshold1=100, threshold2=400    3956                                         6069                   1.5\ncornerHarris      1920x1080 Y, blockSize=3, sensitivity=0.04, kernelSize=3     3748                                         27754                  6.442\ncvtColor          1920x1080 RGB, dst: y                                        939                                          2066                   2.2\ndilate            1920x1080 Y, kernelHW=3x3                                    537                                          608                    1.1\nequalizeHist      1920x1080 Y                                                  715                                          2697                   3.77\nerode             1920x1080 Y, kernelHW=3x3                                    534                                          610                    1.1\nfilter2D          1920x1080 Y, kernelHW=5x5                                    1144                                         22887                  20.0\nflip              1920x1080 Y, y-axis                                          495                                          1127                   2.28\ngaussianBlur      1920x1080 Y, kernelSize=3, sigmaX=0, sigmaY=0                508                                          2460                   4.8\nintegral          1920x1080 Y, y-axis                                          1204                                         1288                   1.07\nlaplacianFilter   1920x1080 Y, kernelSize=3, normalize=0                       836                                          5334                   6.37\nlkof              1920x1080 nv12, pyrLevel=5, winSize=7, criteriaEpsilon=0,    778                                          7429                   9.5\n                  maxIteration=5, minEigThreshold=1e-4\nmedianBlur        1920x1080 Y, maskWidth=7                                     24302                                        100173                 4.1\npyrDown           1920x1080 nv12, DSP:interpolation=gaussian                   DSP: 1211 PYM(normal): 1996 PYM(low): 2905   5243                   DSP: 4.3 PYM(normal): 2.6 PYM(low): 1.8\n                  PYM:interpolation=HB_VP_INTER_LINEAR\npyrUp             1920x1080 Y                                                  1171                                         6835                   5.8\nremap             1920x1080 nv12, interpolation=HB_VP_INTER_LINEAR             DSP: 10646 GDC: 3569                         20287                  DSP: 1.9 GDC: 5.3\nresize            1920x1080 Y, xScale=1.5 yScale=1.5,                          960                                          2692                   2.8\n                  interpolation=HB_VP_INTER_LINEAR\nrotate            1920x1080 Y, rotateCode=HB_VP_ROTATE_90_CLOCKWISE            558                                          3382                   6.06\nsepFilter2D       1920x1080 Y, kernelHW=1x5                                    551                                          12831                  23.2\nsobel             1920x1080 Y, kernelSize=3, scale=1, delta=0, dx=1, dy=0      1185                                         1840                   1.55\ntranspose         1920x1080 Y                                                  557                                          2316                   4.15\nwarpAffine        1920x1080 nv12, scale=1.0, rotate=45, translate=0            1064                                         5433                   5.1\nwarpPerspective   1920x1080 Y, interpolation=HB_VP_INTER_LINEAR,               1923                                         20435                  10.6\n                  transformMatrix=[0.9, 0.05, 15.0, 0.05, 0.9, 15.0, 0.0001,\n                  0.0001, 1.1]\nstitch            640x480 nv12, stitch num=4                                   369                                          969                    Stitch:2.6\njpegDecode        3840x2160 src: jpg, dst: nv12                                7398                                         127983                 Codec: 17.3\njpegEncode        3840x2160 src: nv12, dst: jpg                                9271                                         212206                 Codec: 22.9\nH265Decode        3840x2160 src: h.265, dst: nv12                              9390                                         -                      -\nH265Encode        3840x2160 src: nv12, dst: h.265                              10800                                        -                      -\nH264Decode        3840x2160 src: h.264, dst: nv12                              11104                                        -                      -\nH264Encode        3840x2160 src: nv12, dst: h.264                              10251                                        -                      -","routePath":"/en/guide/ucp/vp/vp4_Performance","lang":"en","toc":[{"text":"Test Platform","id":"test-platform","depth":2,"charIndex":136},{"text":"Test Method","id":"test-method","depth":2,"charIndex":412},{"text":"Performance Comparison","id":"performance-comparison","depth":2,"charIndex":1295}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":240,"title":"Bilateral Filter","content":"#\n\nBilinear filter is a nonlinear, edge-preserving smoothing filter that is often\nused to smooth pictures. It takes the weighted average of the intensity values\nof nearby pixels in each input image, and it calculates the weights by the\nEuclidean distance between pixels and color intensity differences, and finally\nobtains the picture processing effect of preserving the edges and smoothing the\nsimilar regions.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                             OUTPUT IMAGE\n              kernelSize = 5 sigmaColor = 15.0f sigmaSpace = 2.0f   \n              borderType = HB_VP_BORDER_REPLICATE\n\n\nPrinciple#\n\nThe main computational process of bilinear filtering is to convolve the input\npicture using a filter kernel, the main equation is as follows:\n\n$$dst(x)=\\frac\\displaystyle\\sum_$$\n\nAmong them, $dst$ is the output image, $src$ is the input image, $x$ is the\ncurrently computed pixel, and $\\Omega$ is the current filter window.\n\n$$f_r(x,xi)=e^$$\n\n$$g_s(x,xi)=e^$$\n\nAmong them, $\\vert$ is the absolute value, $\\Vert$ is the Euclidean distance,\n$\\sigma_r$ is the color sigma parameter for smoothing intensity differences\nacross the filter window, and $\\sigma_d$ is the spatial sigma parameter for\nsmoothing differences in spatial coordinates across the window.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPBilateralFilter.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/bilateral_filter","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":413},{"text":"Principle","id":"principle","depth":2,"charIndex":633},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1302},{"text":"Usage","id":"usage","depth":2,"charIndex":1395}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":241,"title":"Box Filter","content":"#\n\nThe box filter is a low-pass filter that generates pixel values by weighting and\naveraging the pixels around a pixel point, and is commonly used to eliminate\nimage details, remove noise, and blur edges.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                     OUTPUT IMAGE\n              kernelHeight = 3 kernelWidth = 3 pointLocX = -1 pointLocY =   \n              -1 normalize = 1 borderType = 0\n\n\nPrinciple#\n\nBox filtering uses the following filter kernel, which weights the pixel values\nwithin the filter window equally and calculates the pixel values of the output\npicture.\n\n$$\\beginbox_=\\frac\\begin1 & 1 & \\cdots & 1\\1 & 1 & \\cdots & 1\\\\vdots & \\vdots &\n\\ddots & \\vdots\\1 & 1 & \\cdots & 1\\\\end\\end$$\n\n$m$ and $n$ are the filter kernel sizes.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPBoxFilter.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/box_filter","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":207},{"text":"Principle","id":"principle","depth":2,"charIndex":439},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":789},{"text":"Usage","id":"usage","depth":2,"charIndex":876}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":242,"title":"Canny","content":"#\n\nCanny operator is an edge detector that uses a multi-stage algorithm to detect a\nlarge range of edges in an image.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                 OUTPUT IMAGE\n              threshold1 = 100 threshold2 = 400 kernelSize = 3 norm =   \n              HB_VP_NORM_L1 overlap = 64 borderType =\n              HB_VP_BORDER_REPLICATE\n\n\nPrinciple#\n\nThe computational process of the operator is divided into several stages:\n\n 1. Gradient calculation\n\nThe input image is convolved both vertically and horizontally using a Sobel\nkernel whose size is determined by the Sobel size parameter. Then it is filtered\nhorizontally and vertically using an edge detection filter that this produces Gx\nand Gy. The intensity and angle of each pixel is calculated as follows:\n\n$$Intensity=\\sqrt$$\n\n$$Angle=arctan(\\frac)$$\n\n 2. Non-maximum suppression\n\nFor each pixel in the image, check if the intensity is a local maximum in the\ngradient direction. If yes, keep it as an edge pixel, otherwise remove it as a\nnon-edge pixel.\n\n 3. Double thresholding\n\nThe intensity magnitude of each pixel is compared with a strong threshold\n(ℎ𝑖𝑔ℎ_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑) and a weak threshold (𝑙𝑜𝑤_𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑), and if\nthe intensity is greater than the strong threshold, it is labeled as a strong\nedge. If the intensity is less than the weak threshold, it is labeled as no\nedge. Intensity values between the strong and weak thresholds are labeled as\nweak edges.\n\n 4. Edge Tracking\n\nIf a weak edge is connected to a strong edge, then that weak edge becomes a\nstrong edge. Repeat this process until all weak edges connected to strong edges\nare found. Then the remaining weak edges are labeled as non-edges, the edge\ndetection result for the whole image is obtained.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPCanny.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/canny","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":119},{"text":"Principle","id":"principle","depth":2,"charIndex":388},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1788},{"text":"Usage","id":"usage","depth":2,"charIndex":1871}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":243,"title":"Corner Harris","content":"#\n\nThe algorithm implements the Harris keypoint detection operator for detecting\nkeypoints and inferring image features.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                     OUTPUT IMAGE\n              blockSize = 3 sensitivity = 0.04 kernelSize = 3borderType =   \n              HB_VP_BORDER_REPLICATE\n\n\nPrinciple#\n\nThe calculation process is as follows:\n\n$$dst(x,y)=det(A)-k*trace(A)^2$$\n\n$dst$ is the output picture.\n\n$$trace(A)=\\displaystyle\\sum_A ^2+\\displaystyle\\sum_A ^2 $$\n\n$$det(A)=\\displaystyle\\sum_A\\displaystyle\\sum_A-(\\displaystyle\\sum_A)^2$$\n\nAmong them, $G_x$ is the sobel convolution in the x-direction, $G_y$ is the\nsobel convolution in the y-direction, and $A$ is the convolution window.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPCornerHarris.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/corner_harris","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":122},{"text":"Principle","id":"principle","depth":2,"charIndex":345},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":748},{"text":"Usage","id":"usage","depth":2,"charIndex":838}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":244,"title":"Cvtcolor","content":"#\n\nThe operator converts the input picture from one color space to another,\nsupporting the conversion of color channels, formats and depths, often to\nconvert the picture format.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nThere are a variety of formulas to choose from for the color space conversion\nprocess, and one method is listed as belows:\n\nRGB to Gray\n\n$$Y=0.299R+0.587G+0.114*B$$\n\nRGB/BGR $\\Leftrightarrow$ YUV420\n\n$$Y=16+0.257R+0.504G+0.098*B$$\n\n$$Cb=128-0.148R-0.291G+0.439*B$$\n\n$$Cr=128+0.439R-0.368G-0.071*B$$\n\n$$R=1.164*(Y-16)+1.596*(Cr-128)$$\n\n$$G=1.164*(Y-16)-0.392*(Cb-128)-0.812*(Cr-128)$$\n\n$$B=1.164*(Y-16)+2.016*(Cb-128)$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPCvtColor.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/cvtcolor","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":179},{"text":"Principle","id":"principle","depth":2,"charIndex":265},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":698},{"text":"Usage","id":"usage","depth":2,"charIndex":784}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":245,"title":"Dilate","content":"#\n\nThe dilation (expansion) algorithm performs a two-dimensional filtering\noperation on the input image using the supplied 2D Boolean kernel; the kernel\nsize of the Boolean defines the pixel neighborhood of the filtering operation.\nAs a morphological operation operator, equivalent to the maximize operation,\nwhich increases or enlarges the foreground area in the picture (in this case,\nthe white color in the figure below). Identifies and fills holes in the picture.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\n$$\\begindst(x,y)=\\max_\nsrc(x-\\lfloor\\frac\\rfloor+i,y-\\lfloor\\frac\\rfloor+j)\\end$$\n\nMorphological operations are a set of image processing operations that process\ndigital images based on the shape of the image. The dilation filter is\nequivalent to the max operation and will expand the bright structures in the\nimage. Another morphological filter is Erode, which is equivalent to the\nminimization operation. An erosion filter may be used after an expansion filter\nto form a morphological closure of the image.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPDilate.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/dilate","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":469},{"text":"Principle","id":"principle","depth":3,"charIndex":555},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1078},{"text":"Usage","id":"usage","depth":2,"charIndex":1162}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":246,"title":"Equalizehist","content":"#\n\nHistogram equalization is an image processing method that uses a histogram of\nthe image intensity distribution for contrast adjustment.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nTo achieve the equalization effect, the image needs to be remapped and its\ncumulative distribution function is calculated as follows:\n\n$$H'(i)=\\displaystyle\\sum_H(j)$$\n\nAfter obtaining the cumulative distribution it needs to be normalized to obtain\nthe functional form as follows:\n\n\n\nFinally the equalized image is obtained using the remapping function, which is\nas follows:\n\n$$dst(x,y)=H'(src(x,y))$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPEqualizeHist.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/equalizehist","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":140},{"text":"Principle","id":"principle","depth":2,"charIndex":226},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":642},{"text":"Usage","id":"usage","depth":2,"charIndex":732}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":247,"title":"Erode","content":"#\n\nThe erosion (corrosion) algorithm performs a 2D filtering operation on the input\nimage using the provided 2D Boolean kernel. The Boolean kernel size defines the\npixel neighborhood of the filtering operation. As a morphological operation\noperator, equivalent to a minimization operation, it reduces or shrinks the\nforeground area in the picture (in this case the white color in the figure\nbelow), the causes unwanted small or thin objects to disappear, recognizing and\nfilling holes in the picture.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\n$$\\begindst(x,y)=\\min_\nsrc(x-\\lfloor\\frac\\rfloor+i,y-\\lfloor\\frac\\rfloor+j)\\end$$\n\nThe erosion filter is equivalent to the minimum operation and will shrink the\nbright structures in the image, another morphological filter is Dilate, which is\nequivalent to the maximum operation. Expansion filters may be used after erosion\nfilters to form morphological closures of the image.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPErode.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/erode","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":502},{"text":"Principle","id":"principle","depth":2,"charIndex":588},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":978},{"text":"Usage","id":"usage","depth":2,"charIndex":1061}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":248,"title":"Filter2d","content":"#\n\nFilters (convolves) a 2D image using the specified filter kernel, where the\nanchor point defaults to the center of the convolution kernel.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                    OUTPUT IMAGE\n              $$\\beginkernel = \\begin 0.00390625& 0.015625& 0.02734375&    \n              0.015625& 0.00390625\\ 0.015625& 0.0625& 0.1015625& 0.0625&\n              0.015625&\\ 0.02734375& 0.1015625& 0.09375& 0.1015625&\n              0.02734375\\ 0.015625& 0.0625& 0.1015625& 0.0625& 0.015625\\\n              0.00390625& 0.015625& 0.02734375& 0.015625&\n              0.00390625\\end\\end$$\n\n\nPrinciple#\n\n$$\\begindst(x,y)=\\sum_kernel(x',y')*src(x+x'-anchor.x,y+y'-anchor.y)\\end$$\n\nAmong them, $dst$ is the output image, $kernel$ is the filter kernel, and\n$anchor$ is the anchor coordinates, the default anchor is (-1, -1).\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPFilter2D.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/filter2d","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":143},{"text":"Principle","id":"principle","depth":2,"charIndex":634},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":866},{"text":"Usage","id":"usage","depth":2,"charIndex":952}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":249,"title":"Flip","content":"#\n\nThe Flip operator realizes the function of image flipping through the remapping\nof pixels in the form of horizontal flipping and vertical flipping.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              mode=0      \n              mode=1      \n\n\nPrinciple#\n\nThe horizontal flip formula is as follows:\n\n$$dst(x,y)=src(width-x,y)$$\n\nThe vertical flip formula is as follows:\n\n$$dst(x,y)=src(x,height-y)$$\n\n$dst$ is the output image, $src$ is the input image, $height$ is the image\nheight, and $width$ is the image width.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPFlip.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/flip","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":152},{"text":"Principle","id":"principle","depth":2,"charIndex":265},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":539},{"text":"Usage","id":"usage","depth":2,"charIndex":621}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":250,"title":"Gaussian Blur","content":"#\n\nGaussian filtering uses a specific filter kernel to convolve the image and is a\nlinear smoothing filter that is widely used in the noise removal process of\nimage processing.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER        OUTPUT IMAGE\n              kernelSize = 3   \n\n\nPrinciple#\n\nThe 3X3 Gaussian filter kernel is as follows:\n\n$$\\beginkernel = \\frac\\begin 1 & 2 & 1 \\2 & 4 & 2 \\1 & 2 & 1 \\end\\end$$\n\nThe 5X5 Gaussian filter kernel is as follows:\n\n$$\\beginkernel = \\frac\\begin1 & 4 & 6 & 4 & 1 \\4 & 16 & 24 & 16 & 4 \\6 & 24 & 36\n& 24 & 6 \\4 & 16 & 24 & 16 & 4 \\1 & 4 & 6 & 4 & 1 \\end\\end$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPGaussianBlur.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/gaussian_blur","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":178},{"text":"Principle","id":"principle","depth":2,"charIndex":274},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":597},{"text":"Usage","id":"usage","depth":2,"charIndex":687}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":251,"title":"Integral","content":"#\n\nCalculate the integral map of the image.\n\n\nOperator Effect#\n\nSCHEMATIC IMAGE\n\n\n\nPrinciple#\n\nThe principle of the operator is to accumulate the pixel values in the rectangle\nfrom the origin to the point (X, Y) as the pixel values at the point (X, Y) with\nthe following formula:\n\n$$sum(X,Y)=\\displaystyle\\sum_src(x,y)$$\n\nA common application scenario for integral maps in neural networks is to obtain\nthe integral of the pixel values of a piece of area, which is calculated as\nfollows:\n\n$$\\displaystyle\\sum_image(x,y)=sum(x_2,y_2)-sum(x_1,y_2)-sum(x_2,y_1)+sum(x_1,y_\n1)$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPIntegral.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/integral","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":45},{"text":"Principle","id":"principle","depth":2,"charIndex":82},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":575},{"text":"Usage","id":"usage","depth":2,"charIndex":661}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":252,"title":"ISP","content":"#\n\nThe ISP operator is used to convert a RAW image rich in original image data into\na YUV image which is more widely used in video processing.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                    OUTPUT IMAGE\n              hbVPISPCtxParam.bufNum = 3 hbVPISPCtxParam.bufCached = 0     \n              hbVPISPCtxParam.backend = HB_UCP_ISP_CORE_0\n              hbVPISPCtxParam.width = 1920 hbVPISPCtxParam.height = 1080\n\n\nPrinciple#\n\nThe ISP processes RAW images, including but not limited to the following\nsubmodules:\n\n * BLC (Black Level Correction)：The sensor circuit itself has dark current,\n   which causes the pixel unit to have a certain output voltage when there is no\n   light. BLC adjusts the black level offset in the image sensor output so that\n   the dark area of the image reflects the brightness of the scene more\n   realistically.\n\n * LSC (Lens Shading Correction)：In the actual image capture process, the uneven\n   refraction of light by the lens will cause shadows to appear around the lens.\n   LSC corrects the image brightness and color. In the actual image processing\n   process, the light attenuation characteristics of the lens are measured, a\n   correction model is generated and applied to each pixel of the image, and the\n   image brightness and color distribution are made more uniform by increasing\n   the brightness of edge pixels and adjusting the intensity of color channels.\n\n * DPC (Defect Pixel Correction)：Due to defects in the sensor photosensitive\n   array process or errors in the conversion of light signals, the pixel values\n   in the image are inaccurate. DPC usually uses neighboring interpolation and\n   pattern matching to make the bad pixels consistent with the surrounding\n   normal pixels.\n\n * 2DNR/3DNR (2D/3D Noise Reduce)：Image noise is usually generated by the\n   imaging device itself and the external environment during the acquisition,\n   transmission and recording of signals. ISP includes 2D and 3D noise reduction\n   modules, which perform spatial noise reduction on single-frame images and use\n   time domain filtering technology to reduce temporal noise.\n\n * Demosaic：Converting raw pixel Data captured by a monochrome image sensor\n   (such as a Bayer filter array) into a full-color image. Most digital cameras\n   and image sensors use a Bayer filter, which separates the sensor's pixels\n   into three color channels: red, green, and blue. The process of demosaicing\n   is to recover the full RGB color image from these monochrome sampled data.\n\n * CSC (Color space conversion)：The CSC module converts the image data format\n   from RGB to YUV. Color space conversion is usually achieved through matrix\n   transformation or nonlinear transformation.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPISP。\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/isp","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":144},{"text":"Principle","id":"principle","depth":2,"charIndex":459},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":2748},{"text":"Usage","id":"usage","depth":2,"charIndex":2829}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":253,"title":"JPEG Codec","content":"#\n\n\nOperator Effect#\n\nSCHEMATIC IMAGE\n\n\n\nPrinciple#\n\nJPEG (Joint Photographic Experts Group) is a widely used lossy compression\nstandard, developed by the Joint Photographic Experts Group, and has become an\ninternationally recognized standard for image compression. JPEG is not only\napplicable for static image encoding but has also been extended for intra-frame\ncompression in television image sequences. The JPEG standard supported by JPU\nhardware is the Baseline Sequential mode of ISO/IEC 10918-1.\n\n\nJPEG Encoding Principle#\n\nJPEG encoding, encode YUV format images into JPEG compressed format image files,\nsuch as *.jpg.\n\nSCHEMATIC IMAGE\n\n\n\nJPEG encoding principle#\n\nJPEG decoding, realize the decoding of .jpg, .jpeg, .JPG, .JPEG image files.\n\nSCHEMATIC IMAGE\n\n\nThe main process of JPEG encoding is described by taking an 8x8 subregion of an\nimage as an example.The values of the 8x8 image subregion are shown as belows:\n\n$$\\begin\\begin52 & 55 & 61 & 66 & 70 & 61 & 64 & 73\\ 63 & 59 & 55 & 90 & 109 &\n85 & 69 & 72\\62 & 59 & 68 & 113 & 144 & 104 & 66 & 73\\63 & 58 & 71 & 122 & 154 &\n106 & 70 & 69\\67 & 61 & 68 & 104 & 126 & 88 & 68 & 70\\79 & 65 & 60 & 70 & 77 &\n68 & 58 & 75\\85 & 71 & 64 & 59 & 55 & 61 & 65 & 83\\87 & 79 & 69 & 68 & 65 & 76 &\n78 & 94\\\\end\\end$$\n\nA two-dimensional Discrete Cosine Transform (DCT) is first performed on the 8x8\nsubregion with the aim of transforming the YUV color space into the frequency\ndomain space. Since the range of values accepted for discrete cosine variation\nis [-128, 127], the following matrix is obtained by subtracting -128 from the\nvalues of the 8x8 image sub-region:\n\n$$\\begin\\begin -76&-73&-67&-62&-58&-67&-64&-55\\\n-65&-69&-73&-38&-19&-43&-59&-56\\-66&-69&-60&-15&16&-24&-62&-55\\-65&-70&-57&-6&26\n&-22&-58&-59\\-61&-67&-60&-24&-2&-40&-60&-58\\-49&-63&-68&-58&-51&-60&-70&-53\\-43&\n-57&-64&-69&-73&-67&-63&-45\\-41&-49&-59&-60&-63&-52&-50&-34\\end\\end$$\n\nThe normalized matrix is subjected to DCT and the transformationformula:\n\n$$F(u,v)=\\cfracC(u)C(v)[\\displaystyle\\sum_^\n\\displaystyle\\sum_^f(m,n)cos\\cfraccos\\cfrac]$$\n\nThe DCT coefficient matrix is obtained after transformation:\n\n$$\\begin\\begin -415&-30&-61&27&56&-20&-2&0\\\n4&-22&-61&10&13&-7&-9&5\\-47&7&77&-25&-29&10&5&-6\\-49&12&34&-15&-10&6&2&2\\12&-7&-\n13&-4&-2&2&-3&3\\-8&3&2&-6&-2&1&4&2\\-1&0&0&-2&-1&-3&4&-1\\0&0&-1&-4&-1&0&1&2\\end\\e\nnd$$\n\nThe coefficients at the points (0, 0) are called the direct current components\n(DC coefficients) and the coefficients at the remaining 63 points are called the\nalternating current components (AC coefficients).\n\nNext, the DCT coefficients of the luminance and chrominance components are\nquantized, i.e., the DCT coefficients are divided by the quantization table and\nrounded to the nearest integer. Since the human eye is more sensitive to\nluminance signals than to chromaticity signals, two quantization scales, for the\nluminance component and the chromaticity component, are used. The default\nquantization tables are derived from extensive experiments, and custom\nquantization tables are also available.\n\nThe default quantization table for the luminance component:\n\n$$\\begin\\begin 16& 11& 10& 16& 24& 40& 51& 61 \\12& 12& 14& 19& 26& 58& 60& 55\n\\14& 13& 16& 24& 40& 57& 69& 56\\14& 17& 22& 29& 51& 87& 80& 62\\18& 22& 37& 56&\n68& 109& 103& 77\\24& 35& 55& 64& 81& 104& 113& 92\\49& 64& 78& 87& 103& 121& 120&\n101\\72& 92& 95& 98& 112& 100& 103& 99\\\\end\\end$$\n\nThe default quantization table for colorimetric components:\n\n$$\\begin\\begin 17 & 18 & 24 & 47 & 99 & 99 & 99 & 99\\18 & 21 & 26 & 66 & 99 & 99\n& 99 & 99\\24 & 26 & 56 & 99 & 99 & 99 & 99 & 99\\47 & 66 & 99 & 99 & 99 & 99 & 99\n& 99\\99 & 99 & 99 & 99 & 99 & 99 & 99 & 99\\99 & 99 & 99 & 99 & 99 & 99 & 99 &\n99\\99 & 99 & 99 & 99 & 99 & 99 & 99 & 99\\99 & 99 & 99 & 99 & 99 & 99 & 99 &\n99\\\\end\\end$$\n\nThe quantized DCT coefficients can be obtained by dividing the DCT coefficient\nmatrix obtained earlier with the luminance table of the default values and\nrounding up:\n\n$$\\begin\\begin -26 & -3 & -6 & 2 & 2 & -1 & 0 & 0\\0 & -2 & -4 & 1 & 1 & 0 & 0 &\n0\\-3 & 1 & 5 & -1 & -1 & 0 & 0 & 0\\-4 & 1 & 2 & -1 & 0 & 0 & 0 & 0\\1 & 0 & 0 & 0\n& 0 & 0 & 0 & 0\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\0 &\n0 & 0 & 0 & 0 & 0 & 0 & 0\\\\end\\end$$\n\nObserve that the quantized DCT coefficients of the quantized data are a bit\nlarger relative to the AC coefficients and that the AC coefficients kind of\ncontain a large number of zeros. Therefore, using Z-shaped encoding allows you\nto concatenate a large number of zeros together to reduce the size after\nencoding. The main idea is to organize the quantized DCT coefficients in a\nzigzag pattern starting from the first pixel in the upper left corner of the\nquantized DCT coefficients:\n\nSCHEMATIC IMAGE\n\n\nSince the DC coefficients of the DCT coefficients after zigzag coding have large\nvalues and the DC coefficients of the neighboring 8x8 image regions do not vary\nmuch, therefore, differential pulse coding technique is used to encode the\ndifference of DC coefficients between neighboring image regions; for characters\nwith AC coefficients that are repeated and occur many times in a row, the stroke\nlength coding is used. Both encoding methods have intermediate formats that are\nintended to further minimize storage.\n\nAfter obtaining the intermediate format of DC coefficients and the intermediate\nformat of AC coefficients, entropy coding of both is required for further\ncompression of image data. Compression is achieved by encoding characters with a\nhigher probability of occurrence with a smaller number of bits. The JPEG basic\nsystem specifies the Huffman coding method. Different Huffman coding tables are\nused for DC coefficient and AC coefficient for Huffman coding, and different\nHuffman coding tables are used for luminance and chrominance. Therefore, 4\nHuffman coding tables are needed to complete the entropy coding, and waiting\nuntil the specific Huffman coding is done efficiently by using table lookups.\nHowever, there is no default Huffman table defined in the JPEG standard, so you\nare free to choose one according to the actual application. It is also possible\nto use the Huffman table recommended by the JPEG standard, or to predefine a\ngeneric Huffman table. It is also possible to compute the value of the Huffman\ntable for a particular image by collecting its statistical features before\ncompression coding.\n\n\nAPI Interface#\n\n\nJPEG Encoding Interface#\n\n\n\n\nJPEG Decoding Interface#\n\n\n\nFor detailed interface information, please refer to hbVPJPEGEncode and\nhbVPJPEGDecode.\n\n\nUsage#\n\n\nJPEG Encoding Usage#\n\n\n\n\nJPEG Decoding Usage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/jpeg_codec","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":3},{"text":"Principle","id":"principle","depth":2,"charIndex":40},{"text":"JPEG Encoding Principle","id":"jpeg-encoding-principle","depth":3,"charIndex":503},{"text":"JPEG encoding principle","id":"jpeg-encoding-principle-1","depth":3,"charIndex":-1},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":6367},{"text":"JPEG Encoding Interface","id":"jpeg-encoding-interface","depth":3,"charIndex":6384},{"text":"JPEG Decoding Interface","id":"jpeg-decoding-interface","depth":3,"charIndex":6413},{"text":"Usage","id":"usage","depth":2,"charIndex":6530},{"text":"JPEG Encoding Usage","id":"jpeg-encoding-usage","depth":3,"charIndex":6539},{"text":"JPEG Decoding Usage","id":"jpeg-decoding-usage","depth":3,"charIndex":6564}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":254,"title":"Laplacian Filter","content":"#\n\nLaplacian filter is a second derivative operator commonly used in image\nprocessing, mainly for edge detection. It highlights regional changes in an\nimage by calculating the second derivative of the image grayscale value. Since\nit is very sensitive to noise, the image is usually first Gaussian smoothed to\nreduce the impact of noise. The calculation of the Laplacian operator involves a\nconvolution kernel on the image to obtain the second-order derivative value of\neach pixel in the image. These values represent the rate of change of the pixel\nvalue in the image. This process can effectively detect the edges in the image.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                     OUTPUT IMAGE\n              kernelSize = 1 borderType = HB_VP_BORDER_CONSTANT normalize   \n              = 0\n\n\nPrinciple#\n\nThe main calculation process of Laplacian filter is to convolve the input image\nusing a specific filter kernel. The main formula is as follows:\n\n$$ = \\Delta \\text = \\frac + \\frac$$\n\nThe $dst$ is the output image, $src$ is the input image. Currently vp only\nsupports OpenCV ksize = 1, [0, 1, 0, 1, -4, 1, 0, 1, 0].\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPLaplacianFilter.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/laplacian_filter","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":630},{"text":"Principle","id":"principle","depth":2,"charIndex":834},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1162},{"text":"Usage","id":"usage","depth":2,"charIndex":1255}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":255,"title":"Median Blur","content":"#\n\nMedian filtering performs a two-dimensional filtering operation on the input\nimage using the provided 2D filter kernel, the size of which is the pixel\nneighborhood range used for the filtering operation. The filter performs a\nnonlinear operation, computed by weighting the input pixel values according to\nthe filter kernel weights and taking the median value as the output. Median\nfiltering operations are often used for impulse noise reduction, image\nsmoothing, analysis, and so on.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nThe median filtering formula is as follows:\n\n\n\n$dst$ is the output image, $src$ is the input image, $K$ is the filter kernel,\nand $k_w, k_h$ are the width and height of the filter kernel. The specific\ncalculation process is shown as belows:\n\n\n\nOne of the 5X5 kind of filter kernel is shown below:\n\n\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPMedianBlur.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/median_blur","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":488},{"text":"Principle","id":"principle","depth":2,"charIndex":574},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":887},{"text":"Usage","id":"usage","depth":2,"charIndex":975}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":256,"title":"OpticalFlowPyrLK","content":"#\n\nThe OpticalFlowPyrLK operator is used to calculate the changes of specific pixel\npoints in consecutive multi-frame images, which is commonly used in scenarios\nsuch as target feature point tracking and improving BOX stability.\n\n\nOperator Effect#\n\nINPUT CONTINUOUS FRAME IMAGE   PARAMETER   OUTPUT IMAGE\n                               -           \n\n\nPrinciple#\n\nThe sparse optical flow algorithm, known as the Lucas-Kanade algorithm, is a\nmotion estimation method commonly used in computer vision to estimate the\ndirection and speed of motion of pixels in an image sequence. The Lucas-Kanade\nalgorithm is based on two assumptions:\n\n 1. Brightness constancy assumption: the brightness of pixels of the same object\n    remains constant over a short period of time.\n\nAssuming that the object located at $$(x,y)$$ pixel position at time $$t$$ is\nlocated at $$(x+u,y+v)$$ position at time $$t+Δt$$, based on the assumption of\nbrightness invariance we have.\n\n$$I(x,y,t)=I(x+u,y+v,t+\\Delta t)$$\n\nA first-order Taylor expansion of the right-hand side of the equation yields:\n\n$$I(x+u,y+v,t+\\Delta t)=I(x,y,t) + I'_xu + I'_yv+I'_t\\Delta t$$\n\nIt can be obtained from the above two equations:\n\n$$I'_xu + I'_yv+I'_t\\Delta t=0$$\n\nThe collation can be expressed as:\n\n$$\\begin\\begin I'_x , I'_y \\end\\begin u \\v \\end=-\\Delta I_t\\end$$\n\nAmong them, $$I'_x$$ and $$I'_y$$ are the partial derivatives of the image\nluminance in the $$x$$ direction and the $$y$$ direction at the $$(x,y)$$ pixel\npoint, respectively, the gradient of the image in the $x$ and $y$ directions.\n\n 2. Neighborhood optical flow similarity assumes that the direction and size of\n    pixel movement in a small image region is essentially the same.\n\nWith the help of this assumption, all pixels in the field of pixel point\n$$(x,y)$$ have the following equation.\n\n$$\\begin\\begin I' , I' \\I' , I' \\end\\begin u \\v \\end= \\begin -\\Delta I_ \\-\\Delta\nI_ \\end\\end$$\n\nThe above equation, which is of the form $$Ax=b$$, leads to a least squares\nsolution for the optical flow vector:\n\n$$x=(A^TA)^A^Tb$$\n\nOne of the requirements is that $$A^TA$$ is invertible, and in order to fulfill\nthis requirement, the Lucas-Kanade method selects corner points as feature\npoints. In addition to being based on the luminance invariance assumption and\nthe neighborhood optical flow similarity assumption, the Lucas-Kanade algorithm\nsolves for the case of large image offsets with the help of image pyramids,\nwhere large offsets become small offsets on high-level low-resolution images,\nthus solving for the optical flow.\n\nTherefore, the OpticalFlowPyrLK operator requires the pyramid layers of the\nprevious and previous frames, and the feature points of the previous frame as\ninput, where the feature points are usually chosen to be corner points.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPOpticalFlowPyrLK.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/opticalflowpyrlk","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":230},{"text":"Principle","id":"principle","depth":2,"charIndex":350},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":2777},{"text":"Usage","id":"usage","depth":2,"charIndex":2871}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":257,"title":"PyrDown","content":"#\n\nThe Pyrdowm operator is used to obtain different resolution representations of\nthe image. There are currently two types of interpolation supported by this\noperator: Gaussian interpolation and bilinear interpolation. The pyramids\ncorresponding to the two interpolation methods are called Gaussian and bilinear\npyramids, respectively.\n\n * Gaussian pyramid downsamples the picture through processes such as Gaussian\n   smoothing and sampling to produce an image whose length and width are half\n   the size of the original.\n * The bilinear pyramid directly generates a new image of half the size of the\n   previous image of the current layer in length and width through the processes\n   of bilinear interpolation and downsampling.\n * The pyramid supports 5 levels of computation.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                              OUTPUT IMAGE\n              hbVPPymParam.levels = 5 hbVPPymParam.Interpolation =   \n              HB_VP_INTER_LINEAR\n\n\nPrinciple#\n\nBased on the principle of downsampling, there are two methods: Gaussian\ndownsampling and bilinear downsampling. The implementation process of\ndownsampling with Gaussian pyramid is to first perform Gaussian smoothing on the\noriginal image and then remove the even rows and columns to finally get the\ntarget image.\n\nThe Gaussian filter kernel used in the Gaussian smoothing stage is as follows:\n\n$$\\beginkernel = \\frac\\begin1 & 4 & 6 & 4 & 1 \\4 & 16 & 24 & 16 & 4 \\6 & 24 & 36\n& 24 & 6 \\4 & 16 & 24 & 16 & 4 \\1 & 4 & 6 & 4 & 1 \\end\\end$$\n\nBilinear pyramid uses bilinear interpolation to accomplish the downsampling\ncalculation of the image, the principle of bilinear interpolation is shown in\nthe figure below:\n\n\n\nIn the interpolation process, it is assumed that the interpolated pixel point to\nbe output is P, and the distance between the four to-be-interpolated source\npoints P1,P2,P3,P4 adjacent to P is one. Since the interpolation method is\nlinear, the order of interpolation in the horizontal and vertical directions\ndoes not affect the final result. The specific interpolation point formula is as\nfollows:\n\n$$P_ = (1 - a) * P_1 + a * P_2$$\n\n$$P_ = (1 - a) * P_3 + a * P_4$$\n\n$$P = (1 - a) * (1 - b) * P_1 + a * (1 - b) * P_2 + (1 - a) * b * P_3 + a * b *\nP_4$$\n\nThe coefficients of the coordinates of each point in Eq. can be considered as\nthe weight values of the interpolated source points for the output pixel points.\nThe pyramid interface presented in this operator uses a bilinear interpolation\ndefaulting to equal distances from the output point to each interpolated source\npoint. The input image data is downsampled by one-half of its aspect size. After\nobtaining a 2*2 image region data, the corresponding homogenization is performed\nto obtain a resultant pixel. The maximum number of layers for the op is up to 5\nlayers. The width and height of the image size of each layer is 1/2 of the\nprevious layer. The specific calculations are as follows:\n\n$$BL_(i,j) = \\left( \\text(2i, 2j) + \\text(2i + 1, 2j) + \\text(2i, 2j + 1) +\n\\text(2i + 1, 2j + 1) + 2 \\right) >> 2$$\n\nAmong them, SRC is the input image pixel, i, j are the pixel coordinates of the\noutput bl layer, and BL is the bilinear downsampling result of the current layer\noutput.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPPyrDown.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/pyrdown","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":780},{"text":"Principle","id":"principle","depth":2,"charIndex":985},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":3247},{"text":"Usage","id":"usage","depth":2,"charIndex":3332}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":258,"title":"PyrUp","content":"#\n\nPyrup operator through the smoothing, sampling and other processes will be\nup-sampled picture, to generate the length and width of the original picture\ntwice the size of the picture, commonly used in the construction of the image\npyramid, access to high-resolution original picture and other scenarios.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nThe implementation process of up-sampling is to expand the original image first,\ninserting zeros on the even rows and columns of the original image, and then\nperforming Gaussian smoothing on the image to finally obtain the target image.\nThe length and width conform to the following constraints:\n\n$$dst.width=src.width*2$$\n\n$$dst.height=src.height*2$$\n\nThe Gaussian filter kernel used in the Gaussian smoothing stage is as follows:\n\n$$\\beginkernel = \\frac\\begin1 & 4 & 6 & 4 & 1 \\4 & 16 & 24 & 16 & 4 \\6 & 24 & 36\n& 24 & 6 \\4 & 16 & 24 & 16 & 4 \\1 & 4 & 6 & 4 & 1 \\end\\end$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPPyrUp.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/pyrup","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":307},{"text":"Principle","id":"principle","depth":2,"charIndex":393},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":982},{"text":"Usage","id":"usage","depth":2,"charIndex":1065}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":259,"title":"Remap","content":"#\n\nThe Remap operator remaps the image according to the input information based on\nthe incoming specific mapping relationships, and can be applied to scenarios\nwhere the image is transformed in various ways in 2D space and 3D space. This\noperator is usually a bit less efficient in execution compared to specific image\ntransformation operators such as transpose, resize, due to different\nacceleration implementations.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nThe principle of remapping operator is to rearrange the input picture on the\noutput picture according to the mapping relationship in the incoming parameter\nmap to realize the reconstruction of the picture, the main formula is as\nfollows:\n\n$$dst(x,y)=src(map_x(x,y),map_y(x,y))$$\n\nAmong them, $dst$ is the output picture and $src$ is the input picture.\n\n\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPRemap.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/remap","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":419},{"text":"Principle","id":"principle","depth":2,"charIndex":505},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":873},{"text":"Usage","id":"usage","depth":2,"charIndex":956}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":260,"title":"Resize","content":"#\n\nThe Resize operator is used to resize the picture by interpolating the algorithm\nand scaling the picture to the target size according to the user's needs.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER               OUTPUT IMAGE\n              xscale=1.5 yscale=1.5   \n\n\nPrinciple#\n\nThe operator is formulated as follows, with different implementation details\ndepending on the interpolation algorithm:\n\n$$\\begin\\begindst_=xscalesrc_\\dst_=yscalesrc_\\end\\end$$\n\nFor the bilinear interpolation method, the pixel mapping principle is as\nfollows:\n\n\n\n$$dst(x,y)=Q*(1-b)+R*b$$\n\n$$Q=src(m,n)*(1-a)+src(m,n+1)*a$$\n\n$$R=src(m+1,n)*(1-a)+src(m+1,n+1)*a$$\n\nAmong them, $src$ is the input image, $dst$ is the output image, and $m,n,x,y$\nare the pixel point coordinates.\n\nFor the nearest neighbor interpolation method, the pixel mapping principle is as\nfollows:\n\n$$\\begin\\beginsrc_x=xscaledst_x\\src_y=yscaledst_y\\end\\end$$\n\nAmong them, $x,y$ are the coordinates of the picture.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPResize.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/resize","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":159},{"text":"Principle","id":"principle","depth":2,"charIndex":269},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":964},{"text":"Usage","id":"usage","depth":2,"charIndex":1048}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":261,"title":"Roi Resize","content":"#\n\nThe function of RoiResize operator is to crop the input picture according to the\nROI area, then scale the cropped ROI area to the predefined target size, and\nfinally padding the short side according to the predefined parameters, which is\ncommonly used to extract the ROI area in the picture and adjust the output size.\nThe operator supports both bilinear interpolation and closest proximity\ninterpolation, and the results of the pictures generated by different\ninterpolation methods will be different.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              ROI=        \n\n\nPrinciple#\n\nThe principle of operator realization is as follows:\n\nAmong these, please refer to the hbVPResize operator for the resize procedure.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPRoiResize.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/roi_resize","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":506},{"text":"Principle","id":"principle","depth":2,"charIndex":592},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":739},{"text":"Usage","id":"usage","depth":2,"charIndex":826}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":262,"title":"Rotate","content":"#\n\nThe Rotate operator calculates the input image rotated by a certain angle by\nestablishing the mapping relationship between pixels in the image, which is\ncommonly used for picture adjustment.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER             OUTPUT IMAGE\n              90clockwise           \n              180clockwise          \n              90counter_clockwise   \n\n\nPrinciple#\n\nThe principle of operator realization is as follows:\n\nThe input picture is mapped to different rotational positions according to\ndifferent rotation angles, and the length and width of the operator conforms to\nthe following constraints:\n\n$\\begindst.width=src.width\\dst.height=src.height\\end180clockwise$\n\n$\\begindst.height=src.width\\dst.width=src.height\\end90clockwise、90counter_clockw\nise$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPRotate.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/rotate","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":195},{"text":"Principle","id":"principle","depth":2,"charIndex":375},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":779},{"text":"Usage","id":"usage","depth":2,"charIndex":863}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":263,"title":"SepFilter2d","content":"#\n\nThe operator applies separable linear filters to the image, decomposing the\nfiltering of a two-dimensional image into a horizontal one-dimensional filter\nand a vertical one-dimensional filter. Each row is convolved with a\none-dimensional filter kernelX, and then each column of the result is convolved\nwith a one-dimensional filter kernelY. Decomposable filter kernels include, but\nare not limited to, Sobel derivative kernel, Gaussian filter kernel, box filter\nkernel, median filter kernel, and so on.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                  OUTPUT IMAGE\n              kernel=[0.0833333,0.0833333,0.66666,0.0833333,0.0833333]   \n\n\nPrinciple#\n\nThe computational efficiency can be improved by splitting a 2D filter kernel\ninto two separable 1D filter kernels. The specific principle is that a\ndecomposable filtering kernel can be understood as two one-dimensional kernels.\nThe x-filtering kernel is called first and then the y-filtering kernel is called\nduring convolution. The consumption incurred by the convolution of two matrices\ncan be estimated as the product of the areas of the two matrices, such that the\ntime required to convolve an image of area A with an n×n convolution kernel is\nA*n^2, but if it is decomposed into two kernels of n×1 and 1×n, then the cost is\nA*n + A*n = 2A*n,. Thus decomposing the convolution kernel improves the\nefficiency of convolution computation.\n\nNote\n\nThis calculation is efficient as long as n is not less than 3, and the benefit\nbecomes more apparent as n increases.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPSepFilter2D.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/sepfilter2d","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":507},{"text":"Principle","id":"principle","depth":2,"charIndex":687},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1565},{"text":"Usage","id":"usage","depth":2,"charIndex":1654}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":264,"title":"Sobel","content":"#\n\nThe sobel operator is used to obtain a first order gradient image of a digital\nimage and a common application is edge detection. The algorithm is based on the\nweighted difference of the gray values of the four fields of each pixel in the\nimage, up, down, left and right, to reach the extreme values at the edges thus\ndetecting the edges.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                     OUTPUT IMAGE\n              $$\\beginkernel = \\begin-1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1   \n              \\end\\end$$\n\n\nPrinciple#\n\nThe 3X3 convolution kernel is shown below, which is only applicable to gradient\ncomputation in one direction. The first convolution kernel applies to the\nconvolution in the X direction, when the difference between the pixel values to\nthe left and to the right of the pixel point is large, the convolution results\nin a larger target pixel value, i.e., a convex outline. The second convolution\nkernel is used for convolution in the Y direction, and the principle is the\nsame.\n\n$$\\begin&kernel\\quad X\\quad direction = \\begin-1 & 0 & 1\\ -2 & 0 & 2\\ -1 & 0 & 1\n\\end\\ &kernel\\quad Y\\quad direction = \\begin -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 &\n1 \\end\\end$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPSobel.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/sobel","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":342},{"text":"Principle","id":"principle","depth":2,"charIndex":553},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1216},{"text":"Usage","id":"usage","depth":2,"charIndex":1299}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":265,"title":"Stitch","content":"#\n\nThrough stitch, realize the effect of stitching and fusion of one picture with\nanother picture, commonly used in APA and panorama stitching and other\nscenarios.\n\n\nOperator Effect#\n\n\nPrinciple#\n\nIf it's a stitch mode, where you do a stitch of two pictures and paste them into\na dst layer, then the formula is as follows:\n\n$$dst(x, y) = src0(x, y) * alpha + src1(x, y) * (255-alpha)$$\n\nAmong them, dst is the output picture, src0 is the input picture 0, and src1 is\nthe input picture 1. The src0 pixel points are multiplied by the alpha\ncoefficient and the corresponding pixel points of the other image are multiplied\nby the beta coefficient, and the two results are added together to update the\npixel points of the dst layer.\n\nIf you are in paste original mode, pasting a picture to a dst layer, then the\nformula is as follows:\n\n$$dst(x, y) = src(x, y)$$\n\nAmong them, dst is the output picture and src is the input picture. Updates the\npixel points of the dst layer, using the src pixel points.\n\nThe lut table, which is composed of the above n alpha, is the table that can\nrepresent the fusion of each pixel point in the w*h region. By configuring\ndifferent alpha values for each pixel point, different stitching values can be\nrealized.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPStitch .\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/stitch","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":165},{"text":"Principle","id":"principle","depth":2,"charIndex":184},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1240},{"text":"Usage","id":"usage","depth":2,"charIndex":1325}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":266,"title":"Threshold","content":"#\n\nThresholding function to process the image by different thresholding methods.\nThis function applies a fixed level of thresholding to a multi-channel array. It\nis typically used to obtain a binary image from a grayscale image or for noise\nremoval, i.e., filtering out pixels with values that are too small or too large.\nThis function currently supports the HB_VP_THRESH_TOZERO type of threshold.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                          OUTPUT IMAGE\n              type = HB_VP_THRESH_TOZERO thresh = 0 maxVal = 0   \n\n\nPrinciple#\n\nDifferent threshold types have different processing effects, which are based on\nthe following principles:\n\nThe principle expression formula is as follows:\n\n$$\\begin\\beginTHRESH\\quad BINARY : \\quad dst(x,y)&=\\begin maxval & if\nsrc(x,y)>thresh \\ 0 & otherwise \\end \\THRESH\\quad BINARY\\quad INV : \\quad\ndst(x,y)&=\\begin 0 & if src(x,y)>thresh \\ maxval & otherwise \\end \\THRESH\\quad\nTRUNC : \\quad dst(x,y)&=\\begin threshold & if src(x,y)>thresh \\ src(x,y) &\notherwise \\end \\THRESH\\quad TOZERO: \\quad dst(x,y)&=\\begin src(x,y) & if\nsrc(x,y)>thresh \\ 0 & otherwise \\end \\THRESH\\quad TOZERO\\quad INV: \\quad\ndst(x,y)&=\\begin 0 & if src(x,y)>thresh \\ src(x,y) & otherwise \\end\\end\\end$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPThreshold.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/threshold","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":399},{"text":"Principle","id":"principle","depth":2,"charIndex":563},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1255},{"text":"Usage","id":"usage","depth":2,"charIndex":1342}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":267,"title":"Transpose","content":"#\n\nRealizes the transposition of the input picture by mapping between coordinate\nrelations. Commonly used for picture adjustment.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER   OUTPUT IMAGE\n              -           \n\n\nPrinciple#\n\nThe principle of the operator is the mapping between image pixels and the\nmapping principle is as follows:\n\nThe dimensions of the input and output picture conform to the following\nconstraints:\n\n$$\\begin&dst.width&=src.height \\&dst.height&=src.width\\end$$\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPTranspose.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/transpose","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":131},{"text":"Principle","id":"principle","depth":2,"charIndex":217},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":486},{"text":"Usage","id":"usage","depth":2,"charIndex":573}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":268,"title":"Video Codec","content":"#\n\n\nPrinciple#\n\nThe principle of video coding is the elimination of redundant information and\ncompression of video signals to reduce the amount of data and facilitate storage\nand transmission. The implementation of video coding usually relies on specific\nvideo coding standards, such as H.264, H.265, and so on. These standards specify\nspecific coding algorithms and parameters for efficient video compression and\ntransmission.\n\n\nVideo Encoding Principle#\n\nTaking the H.265 coding protocol as an example, video coding involves the\nfollowing key steps:\n\n\n\n 1. Segmentation\n\nH.265 first divides the video into a number of sequences, and a sequence is\ndivided into a number of Groups of Picture (GOP), each GOP represents a set of\nconsecutive video frames.\n\n 2. Spatial prediction\n\nH.265 uses spatial prediction to remove redundant information between image\nblocks. As shown in the figure below, from a spatial point of view, the\ndifference in pixel values between pixel points inside a single video frame is\nvery small. From a temporal perspective, there are also many identical pixel\npoints between two consecutive video frames.\n\nSPATIAL SAMPLING SCHEMATIC IMAGE   TIME SAMPLING SCHEMATIC IMAGE\n                                   \n\nPredictive coding is a method of data compression based on the statistical\nproperties of an image that utilizes the temporal and spatial correlation of an\nimage to predict the pixels that are currently being coded from the pixel data\nthat has been reconstructed.\n\nIntra-frame prediction means that both the pixels used for prediction and the\npixels currently being encoded are within the same video frame and are generally\nin neighboring regions. Due to the strong correlation between neighboring\npixels, the pixel values are generally very close to each other, and the\nprobability of a mutation is very small, with the differences all being zero or\nvery small numbers. Therefore, what is transmitted after intra-frame prediction\ncoding is the difference between the predicted value and the true value, i.e.,\nthe value around 0, which is called the prediction error or residual. This\nenables compression to be achieved with fewer bits transmitted.\n\nH.265 intra-frame predictive coding is done on a block basis, using the rebuilt\nvalues of neighboring blocks that have been rebuilt for the block being coded.\nThe prediction component is divided into two, luminance and chrominance, and the\ncorresponding prediction blocks are luminance prediction block and chrominance\nprediction block, respectively. In order to adapt to the content characteristics\nof HD video and improve the prediction accuracy, H.265 adopts richer prediction\nblock sizes and prediction modes.\n\nInter-frame prediction means that the pixels used for prediction and the pixels\ncurrently being encoded are not in the same video frame, but are generally in\nadjacent or nearby locations. In general, inter-frame predictive coding provides\nbetter compression than intra-frame prediction, mainly because of the very\nstrong correlation between video frames. If the rate of change of moving objects\nin a video frame is slow, then the pixel differences between video frames are\nsmall and the temporal redundancy is very large.\n\nThe method of inter-frame prediction to evaluate the motion condition of a\nmoving object is motion estimation, and its main idea is to search for a\nmatching block for the prediction block from a given range of the reference\nframe, and calculate the relative displacement between the matching block and\nthe prediction block, and this relative displacement is the motion vector. After\nobtaining the motion vector, the prediction needs to be corrected, also known as\nmotion compensation. Input the motion vectors into the motion compensation\nmodule and \"compensate\" the reference frame to get the prediction frame of the\ncurrent encoded frame. The difference between the predicted frame and the\ncurrent frame is the inter-frame prediction error.\n\nIf the inter-frame prediction only uses the previous frame image, it is called\nforward inter-frame prediction or unidirectional prediction. This predicted\nframe is also known as a P-frame, and the P-frame can refer to the previous\nI-frame or P-frame.\n\nInter-frame prediction is bidirectional if it uses not only the previous frame\nimage to predict the current block, but also the subsequent frame image. This\nprediction frame is also known as the B-frame, which can refer to the preceding\nI-frame or P-frame and the following P-frame. Since P-frames need to refer to\nprevious I-frames or P-frames, and B-frames need to refer to previous I-frames\nor P-frames and later P-frames, if in a video stream, the B-frame arrives first,\nand the dependent I-frames and P-frames haven't arrived yet, then the B-frame\ncan't be decoded immediately, so what should be done to ensure the playback\norder?\n\nActually, PTS and DTS are generated during video encoding. Typically, after\ngenerating an I-frame, the encoder skips backward a few frames and encodes a\nP-frame using the preceding I-frame as a reference frame, and the frame between\nthe I-frame and the P-frame is encoded as a B-frame. The video frame order of\nthe push stream is already coded at the time of encoding in the dependent order\nof I-frames, P-frames, and B-frames, and the data is directly decoded upon\nreceipt. Therefore, it is not possible to receive B-frames first and then\ndependent I-frames and P-frames.\n\nSCHEMATIC IMAGE\n\n\n 3. Transformation and quantification\n\nIn H.265, transform and quantization are used to further compress the data. By\ntransforming the prediction error, the data can be converted from the time\ndomain to the frequency domain for better removal of data redundancy. The\ntransformed data is then quantized to map the data to a lower precision, thus\nfurther compressing the data. The process can be referred to the JPEG coding\nprocess.\n\n 4. Entropy encoding\n\nIn the final step of the encoding process, H.265 uses entropy coding to\nlosslessly compress the data. The main purpose of entropy coding is to minimize\nthe redundancy of the coded data in order to improve the data compression\nefficiency. The process can be referred to JPEG coding process.\n\n\nVideo Decoding Principle#\n\nVideo decoding refers to the process of converting compressed video data back to\nthe original video format. The process of video decoding is mainly divided into\nentropy decoding, inverse quantization, inverse transformation, motion\ncompensation and deconvolution, and post-processing, each of which is designed\nto recover as close as possible to the original video picture from the highly\ncompressed data. Since the goal of video encoding is to minimize the file size,\nthe decoding process must perform each step of the encoding process precisely in\nreverse to recover the video content:\n\n 1. Entropy decoding\n\nEntropy decoding is the process of converting compressed data back into a more\nmanageable video format. In the process of video encoding, entropy coding\ntechniques such as Hoffman coding or arithmetic coding are usually used to\nreduce the amount of data. And the purpose of entropy decoding is to recover the\nsymbols used in the video coding process to prepare for the next step of inverse\nquantization.\n\n 2. Inverse quantization\n\nInverse quantization is the process of flipping the quantization (the step in\nthe encoding process that reduces the precision of the data in order to achieve\nspace savings) in order to restore the precision of the original data, a step\nthat is critical to restoring image quality.\n\n 3. Inverse transform\n\nInverse tranform is the process of reversing the changes used in coding (e.g.,\nthe Discrete Cosine Transform, DCT) in order to restore the data from the\ntransform domain (e.g., the frequency domain) to the spatial domain (i.e., the\noriginal image), and this step is a key step in image reconstruction.\n\n 4. Motion compensation & deblocking\n\nRun compensation means that, for predicted frames (i.e., frames generated based\non the previous/next frame), the complete frame needs to be reconstructed using\nthe run vector data. Deconvolution, on the other hand, is the process of\nremoving block artifacts generated during the coding process. These steps are\nessential to restore smooth and consistent video playback.\n\n 5. Post-processing\n\nPost-processing, as the final step, involves some techniques to enhance video\nquality such as denoising, sharpening, etc. This step is optional, and whether\nor not it is carried out depends on the requirements of video playback and\nwhether or not the hardware capability can support it.\n\n\nAPI Interface#\n\n\nVideo Encoding Interface#\n\n\n\n\nVideo Decoding Interface#\n\n\n\nFor detailed interface information, please refer to hbVPVideoEncode and\nhbVPVideoDecode.\n\n\nUsage#\n\n\nVideo Encoding Usage#\n\n\n\n\nVideo Decoding Usage#\n\n\n\n\nAdditional notes#\n\nBit Rate Control Mode#\n\nThe encoder supports both H.264 and H.265 standards, offering five bitrate\ncontrol modes: CBR (Constant Bitrate), VBR (Variable Bitrate), AVBR (Adaptive\nVariable Bitrate), FixQp, and QpMap. These modes ensure flexibility in balancing\nbitrate stability, image quality, and compression efficiency. For example, CBR\nmaintains a consistent overall bitrate, while VBR focuses on stable image\nquality. FixQp keeps the quantization parameter (QP) fixed for specific frames,\nand QpMap assigns QP values to blocks within frames, with H.265 using 32x32\nblocks and H.264 using 16x16. The following explanation uses H.265 as an example\nfor rate control settings.\n\n 1. Constant Bit Rate(CBR)\n\nCBR (Constant Bitrate) refers to maintaining a stable bitrate throughout the\nencoding process, ensuring that the overall bitrate of the video stream remains\nconstant. The following are the meanings of the parameters used in CBR mode:\n\nPARAMETER     DESCRIPTION                                                   RANGE OF VALUES   RECOMMENDED VALUE\nintraPeriod   I frame interval                                              [0, 2047]         28\nbitRate       The target average bitrate of the encoded data in kbps        [1, 700000]       1000\nframeRate     The target frame rate of the encoded data in fps              [1, 240]          30\ninitialRcQp   Specify the initial QP value for rate control. If the value   [0, 51]           63\n              is not within the [0,51] range, the encoder will internally\n              determine the initial value\n\n 2. Variable Bit Rate(VBR)\n\nVBR (Variable Bitrate) dynamically adjusts the bitrate during the encoding\nprocess based on the complexity of the scene. In simple scenes, a larger QP\n(Quantization Parameter) is assigned to achieve higher compression. In complex\nscenes, a smaller QP is allocated to maintain stable image quality. The\nfollowing are the meanings of the parameters used in VBR mode:\n\nPARAMETER     DESCRIPTION                                        RANGE OF VALUES   RECOMMENDED VALUE\nintraPeriod   I frame interval                                   [0, 2047]         28\nintraQp       A quantization parameter of intra picture          [0, 51]           30\nframeRate     The target frame rate of the encoded data in fps   [1, 240]          30\n\n 3. Average Variable Bit Rate(AVBR)\n\nAVBR (Adaptive Variable Bitrate) is a strategy that maintains a stable average\nbitrate throughout the encoding process. It combines the advantages of both CBR\nand VBR: in simple scenes, a lower bitrate is allocated, while in complex\nscenes, enough bitrate is provided to ensure image quality. This ensures that\nthe bitrate is reasonably distributed across different scenes. At the same time,\nAVBR ensures that the average bitrate approaches the preset target bitrate over\na certain period, helping to control the output file size. AVBR can be viewed as\na compromise between CBR and VBR, resulting in a stream with relatively stable\nbitrate and image quality. The following are the meanings of the parameters used\nin AVBR mode:\n\nPARAMETER     DESCRIPTION                                                   RANGE OF VALUES   RECOMMENDED VALUE\nintraPeriod   I frame interval                                              [0, 2047]         28\nbitRate       The target average bitrate of the encoded data in kbps        [1, 700000]       1000\nframeRate     The target frame rate of the encoded data in fps              [1, 240]          30\ninitialRcQp   Specify the initial QP value for rate control. If the value   [0, 51]           63\n              is not within the [0,51] range, the encoder will internally\n              determine the initial value\n\n 4. Fix QP\n\nIn FixQp (Fixed QP) mode, the encoder assigns a fixed Quantization Parameter\n(QP) value to each I-frame, P-frame, and B-frame without making any dynamic\nadjustments. This mode is typically used in scenarios where strict control over\nimage quality and compression ratio is required. The following are the meanings\nof the parameters used in FixQp mode:\n\nPARAMETER     DESCRIPTION                                            RANGE OF VALUES   RECOMMENDED VALUE\nintraPeriod   I frame interval                                       [0, 2047]         28\nframeRate     he target frame rate of the encoded data in fps        [1, 240]          30\nqpI           A force picture quantization parameter for I picture   [0, 51]           0\nqpP           A force picture quantization parameter for P picture   [0, 51]           0\nqpB           A force picture quantization parameter for B picture   [0, 51]           0\n\n 5. Qp Map\n\nThe QpMap mode allows different Quantization Parameter (QP) values to be\nassigned to each macroblock within a frame, enabling more fine-grained control\nover the bitrate. For H.265 encoding, the size of each macroblock is 32x32. The\nfollowing are the meanings of the parameters used in QpMap mode:\n\nPARAMETER         DESCRIPTION                                                 RANGE OF VALUES          RECOMMENDED VALUE\nintraPeriod       I frame interval                                            [0, 2047]                28\nframeRate         The target frame rate of the encoded data in fps            [1, 240]                 30\nqpMapArray        Specify the qp map. The QP map array should be written a    指针地址                     -\n                  series of 1 byte QP values for each subCTU in raster scan\n                  order.\nqpMapArrayCount   Specify the qp map number. It's related with the picture    [0, 8192x4096/(32x32)]   -\n                  width and height，(ALIGN64(width)>>5)*(ALIGN64(height)>>5)\n\nGOP Structure#\n\nH.264 and H.265 encoding support the configuration of GOP structures, allowing\nyou to choose from preset GOP structures. Below are the descriptions of the\npreset GOP structures:\n\nGOPPRESETIDX   GOP STRUCTURE   LOW DELAY   GOP SIZE   ENCODING ORDER    DESCRIPTION\n1              I               Yes         1          I0-I1-I2-I3,…     I-frames only, no cross-referencing\n2              P               Yes         1          I-P0-P1-P2,…      Only I-frames and P-frames, and P-frames refer to 2 forward\n                                                                        reference frames\n3              B               Yes         1          I-B0-B1-B2,…      Only I-frames and B-frames, and B-frames refer to 2 forward\n                                                                        reference frames\n6              PPPP            Yes         4          I-P0-P1-P2-P3,…   Only I-frames and P-frames, and P-frames refer to 2 forward\n                                                                        reference frames\n7              BBBB            Yes         4          I-B0-B1-B2-B3,…   Only I-frames and B-frames, and B-frames refer to 2 forward\n                                                                        reference frames\n9              P               Yes         1          I-P0,…            Only I-frames and P-frames, and P-frames refer to 1 forward\n                                                                        reference frames","routePath":"/en/guide/ucp/vp/vp5_op/video_codec","lang":"en","toc":[{"text":"Principle","id":"principle","depth":2,"charIndex":3},{"text":"Video Encoding Principle","id":"video-encoding-principle","depth":3,"charIndex":429},{"text":"Video Decoding Principle","id":"video-decoding-principle","depth":3,"charIndex":6188},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":8584},{"text":"Video Encoding Interface","id":"video-encoding-interface","depth":3,"charIndex":8601},{"text":"Video Decoding Interface","id":"video-decoding-interface","depth":3,"charIndex":8631},{"text":"Usage","id":"usage","depth":2,"charIndex":8751},{"text":"Video Encoding Usage","id":"video-encoding-usage","depth":3,"charIndex":8760},{"text":"Video Decoding Usage","id":"video-decoding-usage","depth":3,"charIndex":8786},{"text":"Additional notes","id":"additional-notes","depth":3,"charIndex":8812},{"text":"Bit Rate Control Mode","id":"bit-rate-control-mode","depth":4,"charIndex":8831},{"text":"GOP Structure","id":"gop-structure","depth":4,"charIndex":14475}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":269,"title":"Overview","content":"#\n\nOPERATOR NAME      DESCRIPTION                  DSP   GDC   STITCH   JPU   VIDEO PROCESSING UNIT   PYRAMID   ISP\nBilateral Filter   Bilinear Filter              Y     N/A   N/A      N/A   N/A                     N/A       N/A\nBox Filter         Box Filter                   Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCanny              Canny Edge Detection         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCvtColor           Color Space Conversion       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nCorner Harris      Harris Corner Detection      Y     N/A   N/A      N/A   N/A                     N/A       N/A\nDilate             Dilate                       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nEqualizehist       Histogram Equalization       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nErode              Erode                        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nFilter2d           Filter-2D                    Y     N/A   N/A      N/A   N/A                     N/A       N/A\nFlip               Flip                         Y     N/A   N/A      N/A   N/A                     N/A       N/A\nGaussian Blur      Gaussian Filter              Y     N/A   N/A      N/A   N/A                     N/A       N/A\nIntegral           integral                     Y     N/A   N/A      N/A   N/A                     N/A       N/A\nISP                Image Signal Processor       N/A   N/A   N/A      N/A   N/A                     N/A       Y\nJPEG Codec         JPEG Codec                   N/A   N/A   N/A      Y     N/A                     N/A       N/A\nLaplacian Filter   Laplacian Filter             Y     N/A   N/A      N/A   N/A                     N/A       N/A\nMedian Blur        Median Filter                Y     N/A   N/A      N/A   N/A                     N/A       N/A\nOpticalFlowPyrLK   Lucas-Kanade Optical Flow    Y     N/A   N/A      N/A   N/A                     N/A       N/A\nPyrDown            Downsample                   Y     N/A   N/A      N/A   N/A                     Y         N/A\nPyrUp              Upsample                     Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRemap              Remap                        Y     Y     N/A      N/A   N/A                     N/A       N/A\nResize             Resize                       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRoi Resize         Roi Resize                   Y     N/A   N/A      N/A   N/A                     N/A       N/A\nRotate             Rotate                       Y     N/A   N/A      N/A   N/A                     N/A       N/A\nSepFilter2d        Separate Filter-2D           Y     N/A   N/A      N/A   N/A                     N/A       N/A\nStitch             Stitch                       N/A   N/A   Y        N/A   N/A                     N/A       N/A\nSobel              Sobel                        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nThreshold          Threshold                    Y     N/A   N/A      N/A   N/A                     N/A       N/A\nTranspose          Transpose                    Y     N/A   N/A      N/A   N/A                     N/A       N/A\nVideo Codec        Video Codec                  N/A   N/A   N/A      N/A   Y                       N/A       N/A\nWarp Affine        Affine Transformation        Y     N/A   N/A      N/A   N/A                     N/A       N/A\nWarp Perspective   Perspective Transformation   Y     N/A   N/A      N/A   N/A                     N/A       N/A","routePath":"/en/guide/ucp/vp/vp5_op/vp5_Overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":270,"title":"Warp Affine","content":"#\n\nAn affine transformation is the process of transforming a vector space into\nanother vector space by performing a linear transformation (multiplying by a\nmatrix) and adding a translation (adding a vector). The WarpAffine operator\nfunctions as an affine transformation of the input picture, including planar\noperations such as picture rotation, scaling, and translation. Project the\npicture as any parallelogram.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                   OUTPUT IMAGE\n              rotation_angle = 45.0f interpolation = HB_VP_INTER_LINEAR   \n              borderType = HB_VP_BORDER_CONSTANT\n\n\nPrinciple#\n\nThe operator maps the input picture in the form of affine by means of a\ntransformation matrix parameter of size 2X3 with the following mapping relation:\n\n$$\\begin\\beginX \\Y \\end=\\begina & b \\ c & d \\end\\beginx \\y \\end+\\begine \\f\n\\end\\end$$\n\nAmong them x,y are the input image pixel coordinates, X,Y are the output image\npixel coordinates, and a,b,c,d,e,f are the transformation matrices.\n\nFor the input picture, it is decomposed into multiple sub-pictures (red areas in\nthe figure below) during the accelerated computation process, and each\nsub-picture has an independent mapping relationship, and the conversion\ncoefficients are consistent with the original picture. The display is shown\nbelow:\n\n\n\nWhere the right side is the input picture and the left side is the output\npicture.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPWarpAffine.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/warp_affine","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":415},{"text":"Principle","id":"principle","depth":2,"charIndex":646},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":1442},{"text":"Usage","id":"usage","depth":2,"charIndex":1530}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":271,"title":"Warp Perspective","content":"#\n\nPerspective transformation is in accordance with the object imaging projection\nlaw for transformation, that is, the object is re-projected to the new imaging\nplane, commonly used in visual aberration correction.\n\n\nOperator Effect#\n\nINPUT IMAGE   PARAMETER                                                      OUTPUT IMAGE\n              $$\\beginkernel = \\begin0.9 & 0.05 & 15.0 \\ 0.05 & 0.9 & 15.0   \n              \\ 0.0001 & 0.0001 & 1.1 \\end\\end$$\n\n\nPrinciple#\n\nThe operator maps the input picture in the form of a perspective by means of a\ntransformation matrix parameter of size 3X3 with the following mapping\nrelationship:\n\n$$dst(x,y)=src(\\frac,\\frac)$$\n\nWhere $dst$ is the output picture, $src$ is the input picture and $M$ is the 3X3\ntransformation matrix.\n\n\nAPI Interface#\n\n\n\nFor detailed interface information, please refer to hbVPWarpPerspective.\n\n\nUsage#\n\n","routePath":"/en/guide/ucp/vp/vp5_op/warp_perspective","lang":"en","toc":[{"text":"Operator Effect","id":"operator-effect","depth":2,"charIndex":216},{"text":"Principle","id":"principle","depth":2,"charIndex":453},{"text":"API Interface","id":"api-interface","depth":2,"charIndex":767},{"text":"Usage","id":"usage","depth":2,"charIndex":860}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":272,"title":"Surround View Image Stitching Sample","content":"#\n\nThis sample describes the use of hbVPStitch, hbVPRemap, hbVPJPEGDecode,\nhbVPJPEGEncode and other APIs, which involves stitches, gdc, and codec hardware\nip, to complete a 360-view scene stitching application.\n\nThe function realized in this sample is to de-distort and splice the four\nfront-view, right-view, rear-view and left-view distortion maps captured by the\nfisheye camera on the car, and to complete the application of 360 surround-view\nscene splicing once. First a map will be created for the four distortion maps,\nthe codec hardware ip will be used to read the fisheye distortion maps and\nconvert them to nv12 data, then the gdc hardware ip will be run to turn the\ndistortion maps into top views, and then the stitch hardware ip will be run to\nsplice the top views into a ring view splice.\n\nThe code is mainly divided into the following steps:\n\n 1. Initialize log and environment variables.\n 2. Set the camera parameters for back, front, left, and right views, and call\n    the ProcessIpmRois() interface to get the top view's roi on the IPM (the\n    coordinate system for image perspective mapping).\n 3. Use the gdc hardware to generate the top view map for the four views, use\n    the codec hardware ip to read the fisheye aberration map to convert it to\n    nv12 data, convert the fisheye aberration data to the top view data, and\n    release the four maps.\n 4. Create the lut table and prepare the information for the splice zone.\n 5. Use the stitch hardware to generate a ring-view stitch map.\n 6. Free the lut table and the requested memory.\n\nConcept Statement\n\n 1. VCS (Vehicle Coordinate System) the coordinate system of the vehicle:\n\n\n\nThe VCS coordinate system describes the coordinate system of the vehicle. The\ncoordinate axes x correspond to the front of the vehicle, y to the left of the\nvehicle, and z to the top of the vehicle. The origin of this coordinate system\nis not the center of the vehicle, but the center of the vehicle's rear axle.\n\n 1. IPM (Image Perspective Mapping) a coordinate system for image perspective\n    mapping:\n\n\n\nThe IPM coordinate system describes the coordinate system for image perspective\nmapping.\n\nFisheye Aberration Map Conversion Map Generation Details\n\nThe main purpose of generating a map is to use the remap function, which can\ngenerate pixel mapping relationships:.\n\n\n\nThe core function that creates the map that converts the fisheye distortion map\nto a top view is TransformMapGenerate() , and the main logic is shown in the\nflowchart:\n\nThe CalculateExtrinsicTransform() used in TransformMapGenerate() corresponds to\nthe penultimate step in the flowchart, which converts the vehicle coordinate\nsystem data to the camera coordinate system, and the main logic is shown in the\nflowchart.\n\nIncluded among these:\n\n 1. IntrinsicXYZ2R() acts as: converts the euler angle representation of a\n    rotation (Roll-Pitch-Yaw) into a rotation matrix for describing rotational\n    transformations in three-dimensional space. A rotation matrix is able to\n    apply rotation operations to vectors in the form of matrix multiplication.\n 2. Adas camera to the camera coordinate system in the transformation matrix.\n\n * The first line indicates the orientation of the new x-Run command line\n   instructions in the imager coordinate system, pointing in the direction of\n   the z-axis in the ADAS imager coordinate system.\n * The second line indicates the orientation of the new y-axis in the imager\n   coordinate system, pointing in the direction of the x-axis in the ADAS imager\n   coordinate system.\n * The third line indicates the orientation of the new z-axis in the imager\n   coordinate system, pointing in the direction of the y-axis in the ADAS imager\n   coordinate system.\n * The last line indicates the panning part, where the last element is 1,\n   indicating no panning.\n\nThe function of this matrix is to transform points under the ADAS camera\ncoordinate system to the camera coordinate system, enabling the transformation\nand alignment of the coordinate system. This transformation is often used in\ncomputer graphics and computer vision.\n\nstitch lut table generation details\n\nThe lut table function serves to indicate the fusion status of each pixel point,\nwith the main purpose of generating transition bands to create a blurring\neffect.\n\nExplanation: The dump out of the lut table is shown below. 0 and 255 are\nopposites, 255 is taken as src0 and 0 is taken as src1. 0\n\n\n\nOperating Methods\n\nThe sample is located in the vp_samples/script/05_avm directory and contains the\nfollowing script:\n\n\n\nRun command line instructions.\n\nrun_avm.sh: this script realizes the sample function of stitching four fisheye\naberration maps in a circular view. run_avm.sh script contains the following\ncommand line:\n\n\n\n${bin}: compiled executable file\n\nTo use it, go to the vp_samples/script/05_avm directory, and execute run_avm.sh\nas shown in the following code block:\n\n\n\nDescription of results\n\nThe visualization image is generated in the current directory at the end of the\nexample run. Some of the results are shown as belows:\n\nOriginal picture:\n\nDe-distortion + top view:\n\nSplice around view:\n\nAttention\n\n 1. The sample remap runs on gdc by default.\n\n 2. If you need to replace the picture, please change the camera parameter\n    configuration data corresponding to the picture, otherwise there will be\n    problems with the operation.\n\n 3. Environment variables:\n    \n    HB_AVM_GDC_MAP_DUMP choose whether dump gdc's map.\n    \n    \n    \n    HB_AVM_GDC_RES_DUMP choose whether to dump the results of the top view.\n    \n    \n    \n    HB_AVM_LOG_LEVEL log level, 1 for debug.\n    \n    \n\n 4. When the YUV image is dumped as a JPG file, if the width or height of the\n    YUV image does not satisfy the Codec alignment rules, it will be dumped by\n    default using the opencv method.","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_avm","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":273,"title":"Basic Image Processing Sample","content":"#\n\nThe basic image processing sample script is located in the\nucp_tutorial/vp/vp_samples/script/01_basic_processing directory. The sample\nshows how to perform related processing on the picture, including picture\nfiltering, picture morphological processing, picture equalization, the detailed\nimplementation method, please combine with the sample source code to compare the\npractice.\n\nIn this sample, direct execution of the sample script performs the default\npicture processing flow: filtering with sepFilter2D, detecting morphological\nedges, and picture equalization. If you want to change the processing flow of\nthe picture, you can control the flow of the execution by appending parameter\nwhen executing the sample script, the rules for appending parameters are as\nfollows:\n\n\n\nIncluded among these:\n\n * filter type: optional parameter, filter operator.\n * morphology type: optional parameter, morphological operatio.\n\nA list of all available append parameters can also be obtained with the append\n-help command.\n\nThe result of the picture processing is saved in the\nvp_samples/script/01_basic_processing directory after the algorithm is executed,\nand the contents of the generated object for this sample are as follows:\n\n\n\nThe execution flow of the sample is shown below:\n\nAs shown in the execution flowchart of the sample, the processing of the picture\nby the sample can be roughly divided into the following four stages:\n\n 1. Filtering of the noised picture. According to the different filtering\n    algorithms, they can be categorized into bilinear filtering, box filtering,\n    gaussian filtering, median filtering, two-dimensional filtering, and\n    separated two-dimensional filtering.\n\n 2. Morphological processing of filtered picture. According to the processing\n    methods can be categorized as corrosion, expansion, open operation, closed\n    operation, and detection of edges.\n\n 3. Histogram equalization of morphologically processed pictures.\n\nImage filter\n\nIn the image filter section, the sample encapsulates the six filtering\nalgorithms in the corresponding functions, and uses std::map to establish the\nconnection between the strings and the filtering functions. As shown below, the\nparameter filter_op holds the mapping between the keywords and the functions.\n\n\n\nDuring the execution of the sample code, according to the different append\nkeywords in the argv parameter, it is automatically adapted to the corresponding\nfiltering algorithm. The core logic is as follows:\n\n\n\nTake sepFilter2D filtering algorithm as an example, the processing effect on the\nnoise-added pictures is shown as belows:\n\nBefore filtering:\n\n\n\nAfter filtering:\n\n\n\nMorphological process\n\nMorphological processing operators include erosion, dilation, open and close\noperations, top-hat, black-hat, and so on. In this example, by calling the\nhbVPDilate and hbVPErode interfaces. This example encapsulates some of these\nmorphological functions, please check the function source code for the specific\nimplementation. Similar to the filtering session, the morphology_op parameter is\ncreated here to store the link between the string and the morphology function.\nThe code is as follows:\n\n\n\nThe same string adaptation is used for scheduling morphological functions, and\nthe core logic is as follows:\n\n\n\nTaking the morphological gradient edge algorithm (the edge function in the\nexample) as an example, the effect on the filtered image is shown as belows:\n\nFiltered picture:\n\n\n\nMorphological gradient edge detection picture:\n\n\n\nHistogram equalization\n\nIn order to enhance the contrast and bring out the details of the picture, the\nsample performs histogram equalization on the morphology processed picture\nthrough the equalizeHist function. The equalization effect is shown as belows:\n\nOriginal picture:\n\n\n\nEqualized effect picture:\n\n","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_basic_process","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":274,"title":"Video Encoding and Decoding Sample","content":"#\n\nThe video encoding and decoding sample demonstrates typical use cases of video\nprocessing with UCP, including the following:\n\n * Decoding Sample Based on UCP Encoding Output: This showcases how to decode a\n   stream generated by the UCP encoder, providing a complete encoding-decoding\n   workflow demonstration.\n\n * Encoding Sample Based on UCP Decoding Output: This demonstrates decoding from\n   a provided stream file, followed by re-encoding the decoded data using the\n   UCP encoder. It is suitable for scenarios involving the processing and\n   transformation of existing video data.\n\nThe sample scripts allow to configure encoding parameters according to their\nneeds, enabling optimization of video quality, compression ratio, and\nperformance to suit different application scenarios. Through this sample, you\ncan learn and practice how to use the UCP framework for video encoding and\ndecoding operations. The implementation details can be fine-tuned according to\nspecific needs, and the accompanying source code provides further guidance for\npractical application.\n\n\nFunction Overview#\n\n\nDecoding Sample Based on UCP Encoding Output#\n\n\n\nThis sample demonstrates a complete video encoding and decoding process. The\ninput video sequence is in YUV420 format, and after encoding it with the H.265\nencoder, the corresponding stream is generated. During the encoding process,\neach frame's stream is fed into the decoder in real-time, and after H.265\ndecoding, the corresponding video stream is obtained. It's important to note\nthat the video encoding process is lossy, meaning that the YUV video sequence\nobtained after decoding is not identical to the original input sequence.\n\nThe steps involved in the sample code are as follows (refer to the source code\nfor detailed implementation):\n\n 1. Prepare Encoder and Decoder Parameters.\n\n 2. Create Encoder and Decoder Contexts.\n\n 3. Prepare Encoder Input Memory.\n\n 4. Feed Input YUV420 Video into the Encoder: The YUV420 video sequence is sent\n    frame by frame into the encoder for encoding. Once the encoding task is\n    completed, the generated H.265 bitstream is sent to the decoder for\n    decoding.\n\n 5. Free Input Memory After Encoding and Decoding.\n\n 6. Release Encoder and Decoder Contexts.\n\n\nEncoding Sample Based on UCP Decoding Output#\n\n\n\nThis sample demonstrates how to decode an H.265 bitstream file and, after\ndecoding, use the UCP encoder to re-encode the video. This is useful for\nscenarios where existing video data needs to be processed and converted. The\ninput video sequence is an H.265 bitstream, which is first decoded into a YUV420\nformat video sequence using an H.265 decoder. Once decoded, the YUV420 video\nsequence is fed frame by frame into the encoder, where it is re-encoded back\ninto an H.265 bitstream.\n\nThe steps involved in the code can be summarized as follows (refer to the source\ncode for detailed implementation):\n\n 1. Prepare Decoder and Encoder Parameters.\n\n 2. Create Decoder and Encoder Contexts.\n\n 3. Prepare Decoder Input Memory.\n\n 4. Use Parse Functions for Decoding: Frame by frame, parse the H.265 bitstream\n    using avformat functions and send each frame to the decoder. Once decoding\n    is complete, send the resulting YUV data frame by frame to the encoder.\n\n 5. Release Input Memory.\n\n 6. Release Encoder and Decoder Contexts.\n\n\nRun Guide#\n\nThe video encoding and decoding sample scripts are located in the\nucp_tutorial/vp/vp_samples/script/06_codec directory. The directory includes the\nfollowing scripts:\n\n\n\nThe command line contents in the run_codec.sh script are as follows：\n\n\n\nWhen executing, navigate to the vp_samples/script/06_codec directory and simply\nrun run_codec.sh. The execution method and log output are as follows:\n\n\n\nIn this sample, running the script directly will use the default parameters for\nH.265 encoding. If you wish to modify the H.265 encoding parameters, you can\nappend additional parameters when executing the script to control the execution\nprocess. The rules for appending parameters are as follows:\n\n\n\nAmong the parameters, rc_mode is optional and is used to specify the rate\ncontrol mode for the H.265 encoder, while gop_preset_idx is also optional and is\nused to specify the GOP (Group of Pictures) structure preset for the H.265\nencoder.\n\nAdditionally, you can use the -help command to view a list of all available\nparameters that can be appended. The encoding parameters rc_mode and\ngop_preset_idx are defined as follows:：\n\n\n\n\nDescription of the results#\n\nAfter the execution of the sample, the image processing result will be saved in\nthe vp_samples/script/06_codec directory, and the content of the generated\nobject is as follows under the default parameter of this sample：\n\n","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_codec","lang":"en","toc":[{"text":"Function Overview","id":"function-overview","depth":2,"charIndex":1074},{"text":"Decoding Sample Based on UCP Encoding Output","id":"decoding-sample-based-on-ucp-encoding-output","depth":3,"charIndex":1095},{"text":"Encoding Sample Based on UCP Decoding Output","id":"encoding-sample-based-on-ucp-decoding-output","depth":3,"charIndex":2251},{"text":"Run Guide","id":"run-guide","depth":2,"charIndex":3331},{"text":"Description of the results","id":"description-of-the-results","depth":2,"charIndex":4466}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":275,"title":"Feature Extraction Sample","content":"#\n\nThe image conversion sample execution scripts are located in the\nucp_tutorial/vp/vp_samples/script/03_feature_extraction directory. The image\nfeature extraction is usually categorized into point features and edge features,\nand this sample mainly demonstrates different edge feature extraction methods,\nincluding morphological processing to obtain edges, combining XY-direction\ngradient to obtain edges, and canny algorithm. Detailed implementation method\nplease combine with the sample source code comparison practice.\n\nIn this sample, executing the sample script directly will calculate the edges of\nthe picture by the canny algorithm by default. If you want to change the method\nof detecting the edges of the picture, you can control the flow of execution by\nappending parameters when executing the sample script, the rules for appending\nparameters are as follows:\n\n\n\nAmong these, edge operate is an optional parameter, edge detection algorithm. A\nlist of all available append parameters can also be obtained with the append\n-help command.\n\nThe result of the picture processing is saved in the\nvp_samples/script/03_feature_extraction directory after the algorithm is\nexecuted, and the contents of the generated object for this sample are as\nfollows:\n\n\n\nIn the sample implementation, in order to establish a link between the string\nand the edge detection function, the parameter edge_op is defined as follows :\n\n\n\nThe method of adapting the append parameter is consistent with the other\nsamples, and the code is as follows:\n\n\n\nmorphology_edge\n\nThe edge features of the pictures are extracted using morphological methods,\nmainly using hbVPDilate and hbVPErode operators, and the processing steps are as\nfollows:\n\n * The original picture is processed using the dilate operator to obtain an\n   inflated image, and the inflation operation will make the light-colored\n   regions in the original picture more prominent.\n * The original picture is processed using the erode operator to obtain the\n   corroded image, and the corrosion operation will make the darker regions in\n   the original picture more prominent.\n * Subtracting the corroded image from the expanded image gives you the\n   demarcation line between the dark and light areas, i.e., the edges of the\n   image.\n\nThe core source code is as follows:\n\n\n\nThe effect of edge extraction is shown as belows:\n\n\n\nsobel_edge\n\nThe edge features of the pictures are extracted using the hbVPSobel operator and\nthe processing steps are as follows:\n\n * Use the hbVPSobel operator on the original picture to obtain an X-direction\n   gradient image.\n * Use the hbVPSobel operator on the original picture to obtain an Y-direction\n   gradient image.\n * The edge features in the XY direction are obtained by combining the gradient\n   image in the XY direction.\n\nOne of the core source codes for XY direction gradient image combination is as\nfollows:\n\n\n\nThe effect of edge extraction is shown as belows:\n\n\n\ncanny_edge\n\nUse hbVPCanny operator to extract the edge features of the picture, please refer\nto hbVPCanny for the operator principle and interface.","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_feature_extraction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":276,"title":"Optical Flow Sample","content":"#\n\nThe optical flow processing sample uses the hbVPPyrDown, hbVPCornerHarris and\nhbVPOpticalFlowPyrLK interfaces. It shows how to perform optical flow tracking\non consecutive frame images for detailed implementation, please compare the\npractice with the sample source code.\n\nFunction overview\n\n\n\nThis sample realizes the function of feature point tracking on the image input\nof consecutive frames. The input image is of NV12 type, which is converted to\ngray type and then used hbVPPyrDown to generate 5 layers of pyramid layers. The\nfirst frame uses hbVPCornerHarris to generate harris corner points, and\nsubsequent frames track the corner points generated in the first frame using\nhbVPOpticalFlowPyrLK. Every 5 frames hbVPCornerHarris is called again to\ngenerate a priori eigenpoints to calibrate the tracking results.\n\nThe sample code process can be roughly divided into the following four steps,\nthe detailed implementation of the sample source code please combine.\n\n 1. Allocate memory for input image and input-output feature points.\n\n 2. Prepare parameters for optical flow calculations.\n\n 3. The computations for each frame are performed sequentially, including\n    generation of pyramid layers, corner detection and optical flow tracking.\n\n 4. Release memory resources.\n\nOperation guide\n\nThe optical flow processing sample execution script is located in the\nucp_tutorial/vp/vp_samples/script/04_optical_flow directory and contains the\nfollowing script:\n\n\n\nThe command line in the run_optical_flow.sh script reads as follows:\n\n\n\nThe meanings of the command line parameters are as follows:\n\n * $: Compiled executable file.\n * image_file_list: Input image list。\n * frame_num: The number of running frames can be adjusted to the number of\n   calculated frames according to the actual needs.\n * lkof_threshold: Optical flow confidence threshold for filtering the results\n   of optical flow calculations, user adjustable.\n * harris_top_k: Filtering top k number results of corner point calculations,\n   user adjustable.\n\nTo use it, go to the vp_samples/script/04_optical_flow directory, and then\ndirectly execute run_optical_flow.sh. The execution method and log output are as\nfollows:\n\n\n\nDescription of the results\n\nAt the end of the sample run, a visualization picture will be generated in the\nvp_samples/script/04_optical_flow directory, and the contents of the generated\nobject for this sample is as follows:\n\n\n\nThe first frame will show the results of the harris feature point detection and\nthe subsequent frames will show the optical flow tracking trajectory. Among\nthem, the blue points are the feature points of the previous frame image, the\nred points are the feature points of the current frame image, and the green\ncolor is the optical flow trajectory. The tracking effect of the first and nth\nframe is shown as belows:\n\nFIRST FRAME   FRAME N\n              \n\nAttention\n 1. The PyrDown and CornerHarris calculations for this samples are processed on\n    a DSP, so please configure your DSP environment in advance.\n 2. If you need to replace a picture, please make sure that the replacement\n    input picture is a continuous frame image, otherwise the result of the\n    optical flow calculation may be inaccurate.","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_optical_flow","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":277,"title":"Image Conversion Sample","content":"#\n\nThe image conversion sample execution scripts are located in the\nucp_tutorial/vp/vp_samples/script/02_transformation directory. The sample mainly\nshows the application effect of different conversion classes of operators,\nincluding picture scaling, up and down sampling, rotation, flip, transpose,\naffine, perspective, the detailed implementation method, please combine with the\nsample source code to compare the practice.\n\nIn this sample, executing the sample script directly will execute the default\npicture transformation sample: generate a picture comparing the effect of resize\nand PyrUp. If you want to change the processing method of the picture, you can\ncontrol the flow of execution by appending parameters when executing the sample\nscript, the rules for appending parameters are as follows:\n\n\n\nAmong these, transform operate is an optional parameter, sample algorithm.\n\nA list of all available append parameters can also be obtained with the append\n-help command.\n\nThe result of the picture processing is saved in the\nvp_samples/script/02_transformation directory after the algorithm is executed,\nand the contents of the generated object for this sample are as follows:\n\n\n\nDifferent append parameters will cause the sample to call different interfaces\nand generate different processing results, which are implemented as follows:\n\ncompare_resize_pyrUp\n\nThe input picture is divided into left and right parts, and half of the input\npicture is processed through the hbVPPyrUp and hbVPResize interfaces\nrespectively, and the result is stitched into the output picture. The left side\nof the output picture is the effect of pyrUp processing, and the right side is\nthe effect of resize processing, the specific output is as follows:\n\ncompare_pyrDown_rotate_flip_transpose\n\nIn this sample flow, the input picture is divided into four parts, in which the\nupper left part is processed by hbVPPyrDown, the upper right part is processed\nby hbVPTranspose, the lower left part is processed by hbVPRotate, and the lower\nright part is processed by hbVPFlip, and finally the results are combined into\nthe output picture according to the parts. The specific output is shown below:\n\naffine\n\nThis sample flow of the input picture affine transformation, the picture is\nrotated 45 ° counterclockwise, and the rotated picture is placed in the length\nand width of the input picture are twice the memory, the specific output is\nshown as belows:\n\nperspective\n\nThis sample flow of the input picture perspective transformation, the picture\nwill be transformed in accordance with a specific transformation matrix, if you\nneed to change the form of the change, please manually change the relevant\nsource code in the transformation matrix, the specific output is shown as\nbelows:","routePath":"/en/guide/ucp/vp/vp6_sample/op_sample_transformation","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":278,"title":"Overview","content":"#\n\nThis section introduces you the various sample functions of the VP module, the\nrealization process and implementation details, which will help you quickly\nunderstand how to use the VP module, and the sample introduction is based on the\noperation of the board as an example. For sample to compile and run, please\nrefer to Quick Start section.\n\nSample list, where the columns dsp, gdc, and so on indicate whether or not the\ncorresponding example uses the corresponding backend, (Y means used, N means not\nused).\n\nSAMPLE NAM           DESCRIPTION                            DSP   GDC   STITCH   JPU   VPU\nbasic process        Basic image processing sample          Y     N     N        N     N\ntransformation       Image conversion sample                Y     N     N        N     N\nfeature extraction   Feature extraction sample              Y     N     N        N     N\noptical flow         Optical flow sample                    Y     N     N        Y     N\navm                  Surround view image stitching sample   Y     Y     Y        Y     N\ncodec                Video Codec Sample                     N     N     N        N     Y","routePath":"/en/guide/ucp/vp/vp6_sample/vp6_Overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":279,"title":"Overview","content":"#\n\n\nData structure#\n\n\nFunctional Interface#","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_Overview","lang":"en","toc":[{"text":"Data structure","id":"data-structure","depth":2,"charIndex":3},{"text":"Functional Interface","id":"functional-interface","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":280,"title":"hbVPArray","content":"#\n\n\n\nDynamic array structure.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   phyAddr       Physical address of the array.\n   virAddr       Virtual address of the array.\n   memSize       Memory size of the array.\n   capacity      The maximum number of elements the array can store.\n   size          The number of elements actually stored in the array.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvparray","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":281,"title":"hbVPBorderType","content":"#\n\n\n\nFill Type.\n\n * Member\n   \n   MEMBER NAME              DESCRIPTION\n   HB_VP_BORDER_CONSTANT    Value fill, iiiiii|abcdefgh|iiiiiii.\n   HB_VP_BORDER_REPLICATE   Boundary copy fill, aaaaaa|abcdefgh|hhhhhhh.\n\nNote\n\nWhen HB_VP_BORDER_CONSTANT is used, only 0 is supported as the padding value.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpbordertype","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":282,"title":"hbVPFilterKernel","content":"#\n\n\n\nThe filter kernel structure of the operator.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   dataType      Data element type.\n   width         Filter kernel width.\n   height        Filter kernel height.\n   dataPhyAddr   Filter kernel physical address.\n   dataVirAddr   Filter kernel virtual address.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpfilterkernel","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":283,"title":"hbVPImage","content":"#\n\n\n\nPicture Structure. Only BPU memory is supported.\n\nPicture channel description: take HB_VP_IMAGE_FORMAT_RGB as an example, when the\nnumber of channels is 3, the imageType is HB_VP_IMAGE_TYPE_U8C3. When the number\nof channels is 4, the data exists an extra memory due to alignment requirements,\nat this time it is necessary to specify the imageType as HB_VP_IMAGE_TYPE_U8C4\n(RRR* GGG* BBB*).\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   imageFormat   Picture format.\n   imageType     Picture type.\n   width         The pixel width of the picture.\n   height        The pixel height of the picture.\n   stride        The pixel offset of the picture, expressed as a number of\n                 bytes.\n   dataVirAddr   The logical address of the picture on the arm side.\n   dataPhyAddr   The physical address of the picture.\n   uvVirAddr     The uv data logical address when the picture format is NV12.\n   uvPhyAddr     The uv data physical address when the picture format is\n                 NV12.\n   uvStride      The offset of the uv data when the picture format is NV12,\n                 expressed as the number of bytes.\n\nOffset description: stride is in the range [bytes_per_pixel * width,\nbytes_per_pixel * MAX_IMAGE_WIDTH] and satisfies the bytes_per_pixel alignment.\nThe bytes_per_pixel indicate the number of bytes per pixel, the size of which\ndepends on the picture format imageFormat, width indicates the pixel width of\nthe picture, MAX_IMAGE_WIDTH indicates the maximum width of the picture pixels.\nWhen the picture format is NV12, uvStride ranges from [2*uv_width,\nMAX_IMAGE_WIDTH] and must be even. When using pyramid and gdc hardware, the\nvalues of stride and uvStride should be greater than the width and meet the\n16-byte alignment requirement.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimage","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":284,"title":"hbVPImageFormat","content":"#\n\n\n\nFormat enumeration of picture.\n\n * Member\n   \n   MEMBER NAME                   DESCRIPTION\n   HB_VP_IMAGE_FORMAT_Y          Grayscale format.\n   HB_VP_IMAGE_FORMAT_NV12       NV12 format with channels arranged in YYYYYYYYY... UVUV...,\n                                 the picture Y and UV need to specify the memory address.\n   HB_VP_IMAGE_FORMAT_RGB_P      RGB_P format with channels arranged in RRRR.... .GGGG...\n                                 .BBBB....\n   HB_VP_IMAGE_FORMAT_RGB        RGB format with channels arranged in RGBRGBRGB... (C3) or\n                                 RGB*RGB*RGB*... (C4).\n   HB_VP_IMAGE_FORMAT_BGR_P      BGR_P format with channels arranged in BBBB.... .GGGG...\n                                 .RRRR.\n   HB_VP_IMAGE_FORMAT_BGR        BGR format with channels arranged in BGRBGRBGR... (C3) or\n                                 BGR*BGR*BGR*... (C4).\n   HB_VP_IMAGE_FORMAT_YUV444     YUV format with channels arranged in YUVYUVYUV... or\n                                 YUV*YUV*YUV*....\n   HB_VP_IMAGE_FORMAT_YUV444_P   YUV_P format with channels arranged in YYYY.... .UUUUU...\n                                 .VVVVV....\n   HB_VP_IMAGE_FORMAT_YUV420     YUV format with channels arranged in YYYY.... .U... .V....","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimageformat","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":285,"title":"hbVPImageType","content":"#\n\n\n\nEnumeration of the type of picture, the type of picture includes the depth of\nthe picture and the number of channels. Take a picture with uint8 data type as\nan example, when the imageFormat of the picture is HB_VP_IMAGE_FORMAT_RGB and\nits channel number is 3, its imageType is HB_VP_IMAGE_TYPE_U8C3. When its\nchannel number is 4, its imageType is HB_VP_IMAGE_TYPE_U8C4; when the\nimageFormat of the image is HB_VP_IMAGE_FORMAT_Y and its channel number is 1,\nits imageType is HB_VP_IMAGE_TYPE_U8C1.\n\n * Member\n   \n   MEMBER NAME              DESCRIPTION\n   HB_VP_IMAGE_TYPE_U8C1    Single channel uint_8 type.\n   HB_VP_IMAGE_TYPE_U8C3    Three channel uint_8 type.\n   HB_VP_IMAGE_TYPE_U8C4    Four channel uint_8 type.\n   HB_VP_IMAGE_TYPE_S16C1   Single channel int_16 type.\n   HB_VP_IMAGE_TYPE_S16C2   Dual channel int_16 type.\n   HB_VP_IMAGE_TYPE_S16C3   Three channel int_16 type.\n   HB_VP_IMAGE_TYPE_S32C1   Single channel int_32 type.\n   HB_VP_IMAGE_TYPE_F32C1   Single channel float_32 type.\n   HB_VP_IMAGE_TYPE_F64C1   Single channe float_64 type.\n   HB_VP_IMAGE_TYPE_F64C2   Doublr channe float_64 type.\n   HB_VP_IMAGE_TYPE_U10C1   Single CHANNE uint_10 type.\n   HB_VP_IMAGE_TYPE_U12C1   Single CHANNE uint_12 type.\n   HB_VP_IMAGE_TYPE_U16C1   Single CHANNE uint_16 type.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpimagetype","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":286,"title":"hbVPInterpolationType","content":"#\n\n\n\nInterpolation method.\n\n * Member\n   \n   MEMBER NAME            DESCRIPTION\n   HB_VP_INTER_NEAREST    Nearest neighbor interpolation.\n   HB_VP_INTER_LINEAR     Bilinear interpolation.\n   HB_VP_INTER_GAUSSIAN   Gaussian interpolation.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvpinterpolationtype","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":287,"title":"hbVPRoi","content":"#\n\n\n\nThe picture ROI region structure, the parameters are the four sides subscripts\nof the ROI region, numbered from 0 . The width of the ROI region is equal to\nright - left + 1 , height = bottom - top + 1 .\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   left          The left subscript of the ROI area.\n   top           The upper subscript of the ROI area.\n   right         The right subscript of the ROI area.\n   bottom        The lower subscript of the ROI area.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_data_structure/hbvproi","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":288,"title":"Environment Variable","content":"#\n\n\n\n\nLog Level Setting Instruction#\n\n * Log level:\n\nThe logs in VP module are mainly divided into 7 levels:\n\nThe log level can be set to 0, 1, 2, 3, 4, 5 and 6, corresponding to Verbose,\nDebug, Info, Warning, Error, Critical, and Never, with the default being\nWarning.\n\nThe logs in DSP module ARM side are mainly divided into 7 levels:\n\nThe log level can be set to 0, 1, 2, 3, 4, 5 and 6, corresponding to Verbose,\nDebug, Info, Warning, Error, Critical, and Never, with the default being\nWarning.\n\nThe logs in DSP module DSP side are mainly divided into 5 levels:\n\nThe log level can be set to 1, 2, 3, 4, 5, corresponding to Debug, Info,\nWarning, Error, Always, with the default being Warning.\n\n * Log level setting rules:\n   \n   * If the occurring log level is greater than or equal to the set level, then\n     the log can be printed, otherwise, it will be shielded.\n   \n   * The smaller the set log level, the more information is printed. For\n     example, if the log level is set to 3, which is the Warning level, then log\n     at levels 3, 4, 5 can all be printed. The default log level for the NN\n     module is the Warning level, so log messages at the Warning, Error, and\n     Critical levels can be printed.\n\nNote\n\nDSP side logs can be obtained by following the steps as belows:\n\n * Configure environment variables to enable DSP logging outputs.\n   \n   \n\n * Start the log listening service.\n   \n   ","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_environment_variable","lang":"en","toc":[{"text":"Log Level Setting Instruction","id":"log-level-setting-instruction","depth":2,"charIndex":5}],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":289,"title":"hbVPBilateralFilter","content":"#\n\n\n\nThe BilateralFilter operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   sigmaColor    The sigma filter parameter in color space.\n   sigmaSpace    The sigma filter parameters in coordinate space.\n   kernelSize    The filter kernel size supports size 5 or 9.\n   borderType    The fill type supports types in hbVPBorderType.\n\n\n\nThe API for calling the BilateralFilter.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The Input picture, type supports U8C1, format supports Y.\n   * [in] bilateralParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpbilateralfilter","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":290,"title":"hbVPBoxFilter","content":"#\n\n\n\nBoxFilter operator parameter.\n\n * Member\n   \n   MEMBER NAME    DESCRIPTION\n   kernelHeight   The height of the filter kernel, the value is an odd number\n                  between [3,31) and the kernel height and width are equal.\n   kernelWidth    The width of the filter kernel, the value is an odd number\n                  between [3,31) and the kernel width and height are equal.\n   pointLocX      Reserved parameter.\n   pointLocY      Reserved parameter.\n   normalize      Reserved parameter.\n   borderType     The fill type supports type in hbVPBorderType .\n\n\n\nThe API for calling the boxFilter.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The Input picture, type supports U8C1, format supports Y.\n   * [in] boxFilterParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpboxfilter","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":291,"title":"hbVPCanny","content":"#\n\n\n\nCanny operator parameter.\n\n * Member\n   \n   ENUMERATION NAME   DESCRIPTION\n   HB_VP_NORM_L1      norm L1 filter.\n\n\n\nCanny operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   threshold1    Low thresholds.\n   threshold2    High thresholds.\n   kernelSize    Filter kernel size, supported sizes are 3, 5, or 7.\n   norm          Supports the types in hbVPCannyNorm.\n   overlap       Reserved parameter.\n   borderType    The fill type supports type in hbVPBorderType.\n\nNote\n\nDue to the differences in algorithm implementation, currently, the results are\nonly generally consistent with OpenCV when the borderType is specified as\nHB_VP_BORDER_REPLICATE.\n\n\n\nThe API for calling the Canny.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The Input picture, type supports U8C1, format supports Y.\n   * [in] cannyParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcanny","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":292,"title":"hbVPCornerHarris","content":"#\n\n\n\nCornerHarris operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   borderType    The fill type supports type in hbVPBorderType .\n   kernelSize    Filter kernel size, supported sizes are 3, 5, or 7.\n   blockSize     Neighborhood dimensions take odd values within [3, 27]\n   sensitivity   Detector free parameter are recommended values in the range\n                 [0.04, 0.06].\n\n\n\nThe API for calling the CornerHarris.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture format is the same as the input picture,\n     the type supports S32C1, and the size is the same as the input.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] cornerHarrisParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcornerharris","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":293,"title":"hbVPCvtColor","content":"#\n\n\n\nThe API for calling the CvtColor supports RGB converts to GRAY, RGB and BGR\nconverts to NV12.\n\n * Parameter\n   \n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture size is the same as the input picture, type\n     supports U8C1, format supports Y and nv12.\n   * [in] srcImg The input picture type supports U8C3 and format supports RGB\n     and BGR.\n\n * Return Value\n   \n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n * Conversion Support Table\n   \n   SRCFMT/DSTFMT   GRAY   NV12   RGB_P   RGB   BGR_P   BGR   YUV_P   YUV\n   gray            N      N      N       N     N       N     N       N\n   nv12            N      N      N       N     N       N     N       N\n   rgb_p           N      N      N       N     N       N     N       N\n   rgb             Y      Y      N       N     N       N     N       N\n   bgr_p           N      N      N       N     N       N     N       N\n   bgr             N      Y      N       N     N       N     N       N\n   yuv_p           N      N      N       N     N       N     N       N\n   yuv             N      N      N       N     N       N     N       N","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpcvtcolor","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":294,"title":"hbVPDilate","content":"#\n\n\n\nDilate operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pointLocX     Reserved parameter.\n   pointLocY     Reserved parameter.\n   iterations    Reserved parameter.\n   borderType    The fill type supports type in hbVPBorderType.\n   borderValue   Reserved parameter.\n\n\n\nThe API for calling the Dilate.\n\n * Parameter\n   * [out] task Task handles are responsible for the interaction of the operator\n     with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] dilateKernel The operator processing kernel type supports U8C1 with a\n     positive odd width and height less than or equal to 9.\n   * [in] dilateParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpdilate","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":295,"title":"hbVPEqualizehist","content":"#\n\n\n\nThe API for calling the EqualizeHist.\n\n * Parameter\n   * [out] task Task handles are responsible for the interaction of the operator\n     with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpequalizehist","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":296,"title":"hbVPErode","content":"#\n\n\n\nErode operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pointLocX     Reserved parameter.\n   pointLocY     Reserved parameter.\n   iterations    Reserved parameter.\n   borderType    The fill type supports type in hbVPBorderType.\n   borderValue   Reserved parameter.\n\n\n\nThe API for calling the Erode.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] erodeKernel The operator processing kernel type supports U8C1 with a\n     positive odd width and height less than or equal to 9.\n   * [in] erodeParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvperode","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":297,"title":"hbVPFilter2d","content":"#\n\n\n\nFilter2D operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   delta         Reserved parameter.\n   pointLocX     Reserved parameter.\n   pointLocY     Reserved parameter.\n   borderType    The fill type supports type in hbVPBorderType.\n\n\n\nThe API for calling the Filter2D.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] filterKernel The operator processing kernel type supports F32C1 with a\n     positive odd width and height less than or equal to 9.\n   * [in] filter2DParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpfilter2d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":298,"title":"hbVPFlip","content":"#\n\n\n\nThe API for calling the Flip.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n   * [in] flipMode A parameter value of 0 in the operator parameters indicates a\n     flip along the x-axis, and a value of a positive integer indicates a flip\n     along the y-axis.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpflip","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":299,"title":"hbVPGaussianBlur","content":"#\n\n\n\nGaussianBlur operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   sigmaX        Reserved parameter.\n   sigmaY        Reserved parameter.\n   kernelSize    Filter kernel size, supported sizes are 3 or 5.\n   borderType    The fill type supports type in hbVPBorderType.\n\n\n\nThe API for calling the GaussianBlur.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] gaussianParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpgaussianblur","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":300,"title":"hbVPGetVersion","content":"#\n\n\n\nGet the vp version number.\n\n * Return Value\n   * Return the version number.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpgetversion","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":301,"title":"hbVPIntegral","content":"#\n\n\n\nThe API for calling the Integral.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [in/out] dstImg The output picture format is the same as the input picture,\n     type supports S32C1, its width is the width of the input picture plus one,\n     and its height is the height of the input picture plus one.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpintegral","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":302,"title":"hbVPISP","content":"#\n\n\n\nISP configuration parameter structure.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   bufNum        The ISP bottom layer allocates the number of output buffers,\n                 supporting a minimum of 3 and a maximum of 10.\n   bufCached     Whether to use cached buffer, 0: do not activate cache, 1:\n                 activate cache\n   backend       Select the backend that performs ISP tasks\n   width         The width of the input image\n   height        The height of the input image\n\nNote\n\nWhen using ISP, the width of the input image must meet the limit of [480, 4096],\nand the height must meet the limit of [240, 2160]. The execution backend\nspecified when submitting the ISP task must be consistent with the backend\nparameter in the configuration parameters.\n\n\n\nCreate an ISP context API. Before creating it, you need to initialize the\ncontext to nullptr.\n\n * Parameter\n   * [out] context ISP context.\n   * [in] ispCtxParam ISP configuration structure.\n * Return Value\n   * Returning 0 means the API is executed successfully, otherwise it fails.\n\n\n\nThe API for calling ISP.\n\n * Parameter\n   * [out] taskHandle The task handle is responsible for the interaction between\n     the operator and the UCP architecture.\n   * [in] srcImg Input image for ISP.\n   * [in] context ISP context。\n * Return Value\n   * Returning 0 means the API is executed successfully, otherwise it fails.\n * Description of interface limitations\n   * When executing ISP, the Image Format of the input image is required to be\n     HB_VP_IMAGE_FORMAT_Y, the Image Type of the input image is required to be\n     HB_VP_IMAGE_TYPE_U12C1.\n   * Only asynchronous task creation is supported.\n\n\n\nThe API for get ISP output buffer.\n\n * Parameter\n   * [out] outImg Output image of ISP.\n   * [in] taskHandle The task handle is responsible for the interaction between\n     the operator and the UCP architecture.\n * Return Value\n   * Returning 0 means the API is executed successfully, otherwise it fails.\n * Description of interface limitations\n   * The image buffer dstImg is allocated internally by ISP.\n   * When the task is successfully completed, the data in the output buffer is\n     valid; in the task release phase, the output buffer will be released.\n\n\n\nRelease ISP context API.\n\n * Parameter\n   * [in] context ISP context。\n * Return Value\n   * Returning 0 means the API is executed successfully, otherwise it fails.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpisp","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":303,"title":"hbVPJPEGDecode","content":"#\n\n\n\nCreates the decoder context API and initialize context to nullptr in advance\nbefore creation.\n\n * Parameter\n   * [out] context The encoder context.\n   * [in] outBufCount The count of JPU internal output buffers. The range of\n     values is [1, 31], typically set to 5.\n   * [in] imageFormat Output picture format. imageFormat supports\n     HB_VP_IMAGE_FORMAT_NV12, HB_VP_IMAGE_FORMAT_YUV420,\n     HB_VP_IMAGE_FORMAT_YUV444 and HB_VP_IMAGE_FORMAT_YUV444_P format.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Due to hardware limitations, the decoder does the decoding process\n     internally in 16x16 units, so the decoded resolution changes to the 16x16\n     aligned resolution.\n   * JPU supports the creation of contexts for the highest 64-way encoding or\n     decoding.\n   * JPU only support 8bit data decoding.\n\n\n\nRelease the encoder context API.\n\n * Parameter\n   * [in] context The encoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling the JPEG decoding.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg Pointer the memory address where the decoded picture is\n     stored.\n   * [in] srcBuf Pointer the memory address where JPEG data is stored.\n   * [in] context The decoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * When JPEG decoding, if the decoding format is HB_VP_IMAGE_FORMAT_YUV444,\n     the input JPEG data is required to be in the format of\n     HB_VP_IMAGE_FORMAT_YUV444 or HB_VP_IMAGE_FORMAT_YUV444_P.\n   * When decoding JPEG images, the supported resolution ranges from 32x32 to\n     8192x8192 pixels, and the input image size must be greater than 1024 bytes.\n   * Only asynchronous task creation is supported.\n   * To avoid wasting system resources and improve JPU decoding performance, it\n     is recommended to reuse input addresses.\n\n\n\nThe API for calling the JPEG decoding output buffer.\n\n * Parameter\n   * [out] outImg Pointer the memory address where the decoded picture is\n     stored.\n   * [in] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The Image buffer outImg is allocated internally by the JPU.\n   * The buffer contains valid data upon the task is successfully completed, and\n     is released during the task release phase.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpjpegdecode","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":304,"title":"hbVPJPEGEncode","content":"#\n\n\n\nhbVPJPEGEncode operator and the context description handle for the\nhbVPJPEGDecode operator, the handle contains the necessary description\ninformation to run on the JPU and can be reused.\n\n\n\nEncoding parameter for JPEG coding.\n\n * Member\n   \n   MEMBER NAME          DESCRIPTION\n   extendedSequential   Only 8bit encoding is supported, the parameter value is 0..\n   imageFormat          Input image format. Currently supports\n                        HB_VP_IMAGE_FORMAT_NV12, HB_VP_IMAGE_FORMAT_YUV420,\n                        HB_VP_IMAGE_FORMAT_YUV444 and HB_VP_IMAGE_FORMAT_YUV444_P\n                        format.\n   width                The width of the input picture. The range of values is [32,\n                        8192].\n   height               The height of the input picture. The range of values is [32,\n                        8192].\n   qualityFactor        The encoding quality takes values in the range [1, 100],\n                        recommended value 50, the smaller the value the worse the\n                        picture quality.\n   outBufCount          The count of JPU internal output buffers. The range of\n                        values is [1, 1000], recommended value 5.\n\n\n\nCreate the encoder context API and initialize context to nullptr in advance\nbefore creation.\n\n * Parameter\n   * [out] context The encoder context.\n   * [in] param Pointer to the encoding parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * When imageFormat is in HB_VP_IMAGE_FORMAT_NV12 or HB_VP_IMAGE_FORMAT_YUV420\n     format, it is required that width satisfies 16 alignment and height\n     satisfies 8 alignment.\n   * When imageFormat is in HB_VP_IMAGE_FORMAT_YUV444 or\n     HB_VP_IMAGE_FORMAT_YUV444_P format, it is required that width satisfies 8\n     alignment and height satisfies 8 alignment.\n   * Due to hardware limitations, the encoder internally encodes in 16x16 units.\n     When the data to be encoded is not 16x16 aligned, there will be a\n     difference in the last part of the encoded data that is randomly filled,\n     but it will not affect the valid data, and the encoded data resolution will\n     not be changed. So you need to pay attention to this point when doing md5\n     comparison.\n   * JPU supports the creation of contexts for the highest 64-way encoding or\n     decoding.\n   * JPU only support 8bit data encoding.\n\n\n\nRelease the encoder context API.\n\n * Parameter\n   * [in] context The encoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling the JPEG encoding.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [in] srcImg The memory address where the encoded picture is stored.\n   * [in] context The encoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Only asynchronous task creation is supported.\n   * To avoid wasting system resources and improve JPU encoding performance, it\n     is recommended to reuse input addresses.\n\n\n\nThe API for calling the JPEG encoding output buffer.\n\n * Parameter\n   * [out] outBuf The memory address where the encoded JPEG data is stored.\n   * [in] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The JPEG buffer outBuf is allocated internally by the JPU.\n   * The buffer contains valid data upon the task is successfully completed, and\n     is released during the task release phase.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpjpegencode","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":305,"title":"hbVPLaplacianFilter","content":"#\n\n\n\nLaplacianFilter operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   kernelSize    filter kernel，only supports 1.\n   borderType    the fill type supports types in hbVPBorderType.\n   normalize     normalize, non-zero represent true.\n\n\n\nThe API for calling the LaplacianFilter.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, format and size are the same as the input\n     picture. type support S16C1.\n   * [in] srcImg The Input picture, type supports U8C1, format supports Y.\n   * [in] laplacianParam operator parameters.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvplaplacianfilter","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":306,"title":"hbVPMedianBlur","content":"#\n\n\n\nThe API for calling the medianBlur API。\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] maskWidth Operator parameters, the filter kernel size can be\n     configured to 3, 5, or 7.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpmedianblur","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":307,"title":"hbVPOpticalFlowPyrLK","content":"#\n\n\n\nFeature point structure.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   x             The x-coordinate of the feature point.\n   y             The y-coordinate of the feature point.\n\n\n\nSparse optical flow task parameter structure.\n\n * Member\n   \n   MEMBER NAME       DESCRIPTION\n   pyrLevels         The number of pyramid layers used for tcalculation.\n   winSize           The size of the optical flow window in the range of value is\n                     3 ~ 23 and is an odd number, and the optical flow window\n                     size needs to be larger than the corner detection window\n                     size.\n   criteriaEpsilon   The optimization termination threshold takes values in the\n                     range of value is 0 ~ 255.\n   maxIteration      The maximum number of iterations takes values in the range\n                     of value is 1 ~ 10.\n   minEigThreshold   The minimum eigen value threshold.\n   confEnable        Confidence output switch,non zero: enable, zero: disable, j6\n                     is not supported, use default value\n\nNote\n\nJ6 has no lkof backend, use dsp backend.\n\n\n\nThe API for calling the OpticalFlowPyrLK.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] currPoints Output feature point, Memory element type is hbVPKeyPoint,\n     memory size is datasize * sizeof(hbVPKeyPoint), size is the same as\n     prevPoints's size.\n   * [out] currPointsStatus track point status,0: inconvergent, 1: convergence,\n     memory element type is uint8_t, memory size is size * sizeof(uint8_t), size\n     is the same as currPoints.\n   * [out] currPointsConf optical flow confidence, dsp unsupport it, J6 DSP is\n     not supported, it can be nullptr.\n   * [in] currPym Input the pyramid layer of the current frame picture and needs\n     to be consistent with the format of the previous frame.\n   * [in] prevPym Input the pyramid layer of the previous picture frame. The\n     image format is HB_VP_IMAGE_FORMAT_Y or HB_VP_IMAGE_FORMAT_NV12, and the\n     image type is HB_VP_IMAGE_TYPE_U8C1.\n   * [in] prevPoints The feature point of the previous frame.\n   * [in] lkofParam The sparse optical flow task parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpopticalflowpyrlk","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":308,"title":"hbVPPyrDown","content":"#\n\n\n\nPym configuration parameter structure.\n\n * Member\n   \n   MEMBER NAME     DESCRIPTION\n   levels          Select the number of pyramid activation layers.\n   interpolation   Selects a pyramid interpolation method that supports the\n                   GAUSSIAN and LINEAR types in hbVPInterpolationType.\n\n\n\nNote\n\nWhen using the bilinear pyramid, the size of the input image and the size of\neach layer of the output image are required to meet the range limit of [32,\n4096].\n\nThe API for calling the PyrDown.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImgs Outputs an array of pictures, with the type and format of\n     each layer consistent with the input pictures, and the width and height of\n     each layer being half of the previous picture，respectively. The rounding\n     strategies are: upward rounding for Gaussian pyramids and downward rounding\n     for bilinear pyramids.\n   * [in] srcImg Input picture, type supports U8C1, Gaussian pyramid format\n     supports Y and nv12, bilinear pyramid format supports nv12.\n   * [in] pymCfg Input Pyramid Configuration Structures, Pyramid supports 1 to 5\n     levels of computation.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvppyrdown","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":309,"title":"hbVPPyrUp","content":"#\n\n\n\nThe API for calling the PyrUp.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type and format are the same as the input\n     picture, and its length and width are twice of the input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvppyrup","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":310,"title":"hbVPRemap","content":"#\n\n\n\nThe visual operator hbVPRemap needs to prepare hbVPRemapParam to get hbVPMapWrap\nfor executing the remap task before doing the creation of the task.\n\nNote\n * The underlying gdc only supports bilinear interpolation, and does not need to\n   configure borderType padValue parameter;\n * Since the implementation principle of gdc is different from that of dsp, the\n   accuracy of the image output by gdc and dsp from the same map will be\n   different;\n * GDC has the following requirements for map:\n   * Map coordinates must be positive numbers;\n   * Try to keep the sampling points in the same row with a smooth transition.\n     If the transition between two points changes sharply, the accuracy of the\n     spline will not be enough to handle it;\n   * Four sampling points cannot form a triangle;\n   * If the sampling accuracy is insufficient, it may cause aliasing,\n     distortion, etc. In this case, it is recommended to use subpixel\n     coordinates.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   mapPhyAddr    Physical address of map data, data type double.\n   mapVirAddr    The map data virtual address.\n   srcWidth      Input width.\n   srcHeight     Input height.\n   mapWidth      map width (i.e., target width).\n   mapHeight     map height (i.e., target height).\n   interpoType   Interpolation type in Remap, supports NEAREST and LINEAR\n                 types in hbVPInterpolationType.\n   borderType    Reserved parameter.\n   padValue      Reserved parameter.\n   dataType      Data types in map that supports the type\n                 HB_VP_IMAGE_TYPE_F64C2 in hbVPImageType.\n\nNote\n\nNote that the data layout in map (i.e. the memory data under the physical\naddress of phyAddr) is x_0, y_0, x_1, y_1, ..., and the memory length should be\n2 * mapWidth * mapHeight * sizeof(dataType), please refer to the sample for\ndetailed usage.\n\n 1. When the task is deployed on a GDC, srcWidth and mapWidth are in the range\n    [100, 3840], and srcHeight and mapHeight are in the range [100, 2160] and\n    satisfy: mapWidth <= srcWidth, mapHeight <= srcHeight. Due to the GDC's\n    alignment constraints, the input stride must be aligned to 16 pixels in the\n    width direction. Thus, the input and output stride are in the range\n    [112,3840]. When allocating memory, please use the height and stride size as\n    the criteria.\n 2. When the task is deployed on a DSP, srcWidth and dstWidth range [32, 4096]\n    and srcHeight and dstHeight range [16, 2160].\n\n\n\nhbVPRemap the operator's argument description handle, which contains the\ndescription information necessary to run on different Backend, can be reused.\n\n\n\nCreates the mapWrap parameter of the Remap.\n\n * Parameter\n   * [out] mapWrap Created parameter that are used in the interface, the\n     parameter must point to nullptr.\n   * [in] param The remap parameter, which is used to create a uniform mapWrap\n     parameter.\n   * [in] backend Select the backend hardware, different hardware backend need\n     to prepare different hardware resources for the map.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\nNote\n 1. hbVPCreateMapWrap is used to create map resources for remap. Since different\n    backends require different hardware resources to be prepared, it is\n    necessary to specify the backend here to determine which resource the map\n    applies to.\n 2. Interface support hardware resources for GDC, DSP. Due to the multi-core\n    nature of the DSP, when specifying DSP deployment resources, When specifying\n    DSP deployment resources, backend needs to be set according to the hardware\n    configuration of the development board. For example, on a single-core DSP\n    development board, backend can be: HB_UCP_DSP_CORE_ANY, HB_UCP_DSP_CORE_0.\n 3. Supports specifying both dsp and gdc backends at the same time, creating two\n    hardware map resources respectively, such as:\n    backend=HB_UCP_GDC_CORE_0|HB_UCP_DSP_CORE_ANY (specifies the hardware\n    resources to create two kinds of backends, GDC and DSP, and the parameter\n    needs to satisfy the constraints on the sizes of GDC and DSP at the same\n    time, HB_UCP_CORE_ANY cannot be ORed with other backends).\n\n\n\nRelease the mapWrap parameter of the Remap.\n\n * Parameter\n   * [in] mapWrap The mapWrap parameter to be released.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling the Remap.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type and format are the same as the input\n     picture, the size is the same as the map.\n   * [in] srcImg Input picture, type support U8C1, format dsp hardware support Y\n     and nv12, gdc support nv12.\n   * [in] mapWrap Point to the map parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpremap","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":311,"title":"hbVPResize","content":"#\n\n\n\nThe API for calling the Resize. When the image format is Y, the effective\nscaling range of the length and width is [0.25, 4], and when the image format is\nNV12, the effective scaling range of the length and width is (0.25, 4].\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture, the picture size can be calculated from the input picture\n     and the zoom ratio.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n   * [in] interpolation Operator interpolation type.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpresize","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":312,"title":"hbVPRoiResize","content":"#\n\n\n\nParameters of the RoiResize operator.\n\n * Member\n   \n   MEMBER NAME       DESCRIPTION\n   interpolation     Interpolation types support the NEAREST and LINEAR types in\n                     hbVPInterpolationType.\n   paddingValue[4]   Padding values, each channel corresponds to one value, NV12\n                     uses three channels.\n\n\n\nThe API for calling the RoiResize, , where the valid scaling range for the width\nand height is [0.25, 4].\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y adn\n     nv12.\n   * [in] roi ROI region, the valid range takes the intersection of the ROI\n     region and srcImg.\n   * [in] roiResizeParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvproiresize","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":313,"title":"hbVPRotate","content":"#\n\n\n\nRotate operator parameter。\n\n\n\nThe API for calling the Rotate.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type and format are the same as the input\n     picture, and its size is decided according to the rotation angle and the\n     size of the input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n   * [in] rotateDegree Operator parameter, picture rotation angle.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvprotate","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":314,"title":"hbVPSepFilter2d","content":"#\n\n\n\nSepFilter2D operator parameter。\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   delta         Reserved parameter.\n   pointLocX     Reserved parameter.\n   pointLocY     Reserved parameter.\n   borderType    Fill type, supports types in hbVPBorderType.\n\n\n\nThe API for calling the SepFilter2D.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] filterKernelX The filter coefficients per line of the operator, TYPE\n     support F32C1, are of size 1xN and N is a positive odd number less than or\n     equal to 9.\n   * [in] filterKernelY The filter coefficients per line of the operator, TYPE\n     support F32C1, are of size 1xN and N is a positive odd number less than or\n     equal to 9.\n   * [in] sepFilter2DParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpsepfilter2d","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":315,"title":"hbVPSobel","content":"#\n\n\n\nSobel operator parameter。\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   scale         Reserved parameter.\n   delta         Reserved parameter.\n   dx            The order of derivative X.\n   dy            The order of derivative YUV_P.\n   kernelSize    sobel extends the core size to support 3 and 5.\n   borderType    Fill type, supports types in hbVPBorderType.\n\n\n\nThe API for calling the Sobel.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, format and size are the same as the input\n     picture, type supports S16C1.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] sobelParam Operator parameter.。\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpsobel","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":316,"title":"hbVPStitch","content":"#\n\n\n\nA data structure describing the position of a point. hbVPStitch needs to prepare\nhbVPPoint describing the position of the src source map on the target map for\nthe execution of the stitch task before the creation of the task is performed on\nit.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   x             Coordinate x of the upper left corner.\n   y             Coordinate y of the upper left corner.\n\n\n\nThe API for calling the alpha-blend lut table for creating a Stitch.\n\n * Parameter\n   * [out] alphaBlendLut Handle pointing to lut table information with stitch\n     alpha-blend lut table information, the parameter must point to nullptr.\n   * [in] alphaDatas Memory for the lut table, passed in by the user.\n   * [in] alphaBlendRegions Area coordinates for alpha-blend are needed in the\n     dst layer. A single roi is supported to the maximum width of 2000 and the\n     maximum height of 2000.\n   * [in] alphaBlendRegionNum The number of regions that need to be\n     alpha-blended in the dst layer.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\nNote\n 1. The alpha-blend lut table is a coefficient that operates on each pixel, and\n    by configuring the lut table, arbitrary blending effects can be realized for\n    two pictures;\n 2. The alpha value in the alpha-blend lut table will be assigned to the\n    specified src map according to the order in which src is given into it. src\n    order is: src0, src1; then the alpha value in the blend-reg region belongs\n    to src0 and (255-alpha) belongs to src1; as follows:\n\n\n\nThe API for calling to release Stitch's alpha-blend lut table.\n\n * Parameter\n   * [in] alphaBlendLut A handle to the lut table information.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling to Stitch.\n\nNote\n 1. In the operator, the position of the source graph on the target graph is\n    only needed to provide the x and y coordinates, and the values of w and h\n    will be obtained from the src.\n 2. Call once hbVPStitch interface can not achieve the results of the\n    overlapping region of the fusion of the three maps (for example, you want to\n    overlap the region to achieve the effect of src0 20%, src1 30%, src2 50%\n    splicing), you need to execute the hbVPStitch interface many times.\n 3. Stitch interface, the call supports maximum four input src maps at a time,\n    internally it will first stitch the src maps to the specified region.\n 4. If the stitched image is in nv12 format, the x and y coordinates specified\n    by the dstPoses parameter, as well as the width and height of the stitched\n    image, must be even numbers.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg Output picture.\n   * [in] srcImgs Input picture array.\n   * [in] dstPoses An array of coordinates of the input picture on the output\n     picture starting point, the number and order of the points need to be the\n     same as the input picture.\n   * [in] srcImgCount The number of input pictures.\n   * [in] alphaBlendLut The lut table information used for splicing.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * dstImg: type supports U8C1, format supports Y and nv12. dst picture's width\n     supports 3840 at maximum, 16 at minimum; height supports 3840 at maximum, 2\n     at minimum. width needs to be 16 bytes aligned.\n   * srcImgs: type supports U8C1, format supports Y and nv12, and the type and\n     format need to be the same as the output picture, support the maximum four\n     input picturessrc. The width of the image supports the maximum of 2000 and\n     the minimum of 16; the height of the image supports the maximum of 2000 and\n     the minimum of 2. The width needs to be aligned by 16 bytes.\n * Instructions for use\n\nWhen using less than or equal to two src graphs to do stitching:\n\n 1. pipline process (src0—copy,src1-copy).\n\n1). First, it will paste src0 onto the dst layer.\n\n2). Then paste src1 onto the dst layer.\n\n3). Since the overlapping regions do not need to be blended, the overlapping\nregions are covered by the region of src1 in pipline order.\n\n\n\n 2. pipline process (src0—copy,src1-copy ,src0-src1-blend).\n\n1). First, it will paste src0 onto the dst layer.\n\n2). Then paste src1 onto the dst layer.\n\n3). Since there are overlapping areas and the overlapping areas require a blend\noperation, overwrite the overlapped area as the result of blending the area\nspecified by src0 and src1.\n\n\n\nWhen using greater than two src graphs to do stitching:\n\n 1. pipline process (src0—copy, src1-copy, src2-copy, src0-src1-blend).\n\n1). First, it will paste src0 onto the dst layer.。\n\n2). Next, paste src1 onto the dst layer.\n\n3). Then paste src2 onto the dst layer.\n\n4). Finally, since there are overlapping regions and the overlapping regions\nneed to perform a blend operation to overwrite the overlapping regions as the\nresult of blending the regions specified by src0 and src1.\n\n\n\n 2. pipline process\n    (src0—copy,src1-copy,src2-copy,src0-src1-blend,src0-src2-blend).\n\nNote\n\nCall once the interface does not support, because the stitch is according to the\npipline process order refresh mechanism, can not be done at once pipline to\ncomplete the fusion of the three map results show ( for example, if you want to\noverlap the region to achieve the effect of src0 20%, src1 30%, src2 50%\nsplicing). There are internal judgment mechanisms that will intercept and report\nerrors for the above scenarios.\n\nCorrect approach (split into two pipline executions):\n\npipline0 process (src0—copy,src1-copy, src0-src1-blend).\n\n1). First, it will paste src0 onto the dst layer.\n\n2). Next, paste src1 onto the dst layer.\n\n3). Finally, since there are overlapping regions and the overlapping regions\nneed to perform a blend operation to overwrite the overlapping regions as the\nresult of blending the regions specified by src0 and src1.\n\n\n\npipline1 process (src3-copy,src2-copy,src3-src2-blend).\n\n1). First, it will paste src3 onto the dst layer.。\n\n2). Next, paste src2 onto the dst layer.\n\n3). src3 and src2 have overlapping areas, and the overlapping areas need to\nperform a blend operation to overwrite the overlapping areas as the result of\nblending the specified areas of src3 and src2.\n\n","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpstitch","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":317,"title":"hbVPThreshold","content":"#\n\n\n\nThreshold operator parameter.\n\n\n\nThreshold operator parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   thresh        Threshold, value less than or equal to 255.\n   maxVal        Reserved parameter.\n   type          ThresholdType, supports the types in hbVPThresholdType.\n\n\n\nThe API for calling the Threshold API.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, support U8C1, format Y,type, and size are\n     the same as the input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y.\n   * [in] thresholdParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpthreshold","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":318,"title":"hbVPTranspose","content":"#\n\n\n\nThe API for calling the Transpose.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type and format are the same as the input\n     picture, its width is equal to the input picture's height, and its height\n     is equal to the input picture's width.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvptranspose","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":319,"title":"hbVPVideoDecode","content":"#\n\n\n\nParameter for Video decoding.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pixelFormat   The pixel format of the input image. The\n                 HB_VP_IMAGE_FORMAT_NV12 and HB_VP_IMAGE_FORMAT_YUV420\n                 formats are currently supported.\n   inBufSize     Specifies the size of the internal input buffer of Video\n                 Process Unit, its size needs to satisfy 1024 alignment.\n                 Value range [1024, 2^31-1), recommended value 10 * 1024 *\n                 1024.\n   outBufCount   Specifies the count of output buffers the Video Process\n                 Unit. Value range [1, 31], recommended value 5.\n   videoType     The type of encoding protocol.\n\n\n\nGet the decoder's default decoding parameter，where the pixelFormat parameter is\nHB_VP_IMAGE_FORMAT_YUV420 , the inBufSize parameter is set to 10 * 1024 * 1024\n(which equals 10 MB), and the outBufCount parameter is set to 5.\n\n * Parameter\n   * [in] param Decode the parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Before calling the interface, you need to specify the videoType as\n     HB_VP_VIDEO_TYPE_H264 or HB_VP_VIDEO_TYPE_H265 .\n\n\n\nThe API for calling to create decoder context and initialize context to nullptr\nin advance before creation.\n\n * Parameter\n   * [out] context Decoder context.\n   * [in] param Decode the parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The Video Process Unit highest supports the creation of 32 encoding or\n     decoding contexts.\n   * The first frame of the decoder must be an IDR frame containing the header\n     information of VPS+SPS+PPS, otherwise it cannot be decoded properly.\n   * The decoder does not support decoding of B-frame streams containing\n     backward reference frames.\n\n\n\nThe API for calling to release the decoder context.\n\n * Parameter\n   * [in] context Decoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling the Video Process Unit decoding\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [in] srcBuf Pointer the memory address where the H.264 or H.265 data is\n     stored.\n   * [in] context Decoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Create the task using asynchronous methods is only supported.\n   * During video decoding, the supported resolution ranges from a minimum of\n     256x128 to a maximum of 8192x4096.\n   * Due to hardware limitations of the Video Processing Unit, the Video\n     Processing Unit supports a maximum of 31 tasks being submitted\n     simultaneously for either encoding or decoding.\n   * To avoid wasting system resources and improve Video Processing Unit\n     decoding performance, it is recommended to reuse input addresses.\n\n\n\nThe API calling for to get the decoded data buffer.\n\n * Parameter\n   * [out] outImg Pointer the memory address that holds the decoded data.\n   * [in] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The output buffer outImg is requested internally by the Video Process Unit.\n   * The buffer contains valid data upon the task is successfully completed, and\n     is released during the task release phase.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpvideodecode","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":320,"title":"hbVPVideoEncode","content":"#\n\n\n\nhbVPVideoEncode operator and hbVPVideoDecode operator's context description\nhandle, which contains the description information necessary to run on the Video\nProcess Unit. necessary to run on the Video Process Unit, and can be reused.\n\n\n\nThe video encoding type enumeration.\n\n * Member\n   \n   MEMBER NAME             DESCRIPTION\n   HB_VP_VIDEO_TYPE_H264   The H.264 video coding protocol.\n   HB_VP_VIDEO_TYPE_H265   The H.265 video coding protocol.\n\n\n\nCode rate control mode enumeration.\n\n * Member\n   \n   MEMBER NAME                      DESCRIPTION\n   HB_VP_VIDEO_RC_MODE_H264_CBR     H.264 coding protocol CBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H264_VBR     H.264 coding protocol VBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H264_AVBR    H.264 coding protocol AVBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H264_FIXQP   H.264 coding protocol FixQp bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H264_QPMAP   H.264 coding protocol QPMap bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H265_CBR     H.265 coding protocol CBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H265_VBR     H.265 coding protocol VBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H265_AVBR    H.265 coding protocol AVBR bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H265_FIXQP   H.265 coding protocol FIXQP bit rate control mode.\n   HB_VP_VIDEO_RC_MODE_H265_QPMAP   H.265 coding protocol QPMAP bit rate control mode.\n\n\n\nH.264 coding protocol constant bit rate parameters.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   intraPeriod   I Frame Interval. Value range [0, 2047], recommended value\n                 28.\n   bitRate       Target average bit rate in kbps. Value range [1, 700000],\n                 recommended value 1000.\n   frameRate     Target frame rate in fps. Value range [1, 240], recommended\n                 value 30.\n   initialRcQp   Initial QP value, when the value is not in the range of [0,\n                 51], The larger the parameter value, the worse the encoding\n                 quality becomes. If the parameter value exceeds 51, the\n                 specific value will be determined by the Video Processing\n                 Unit hardware.\n\n\n\nH.264 coding protocol variable bit rate parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   intraPeriod   I Frame Interval. Value range [0, 2047], recommended value\n                 28.\n   intraQp       Frame QP value. Value range [0, 51], recommended value 30.\n   frameRate     Target frame rate in fps. Value range [1, 240], recommended\n                 value 30.\n\n\n\nThe H.264 coding protocol fixes the quantization parameters.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   intraPeriod   I Frame Interval. Value range [0, 2047], recommended value\n                 28.\n   frameRate     Target frame rate in fps. Value range [1, 240], recommended\n                 value 30.\n   qpI           Forces the QP value of the I-frame. Value range [0, 51],\n                 recommended value 0.\n   qpP           Forces the QP value of the P-frame. Value range [0, 51],\n                 recommended value 0.\n   qpB           Forces the QP value of the B-frame. Value range [0, 51],\n                 recommended value 0.\n\n\n\nH.264 coding protocol QP mapping parameter.\n\n * Member\n   \n   MEMBER NAME       DESCRIPTION\n   intraPeriod       I Frame Interval. Value range [0, 2047], recommended value\n                     28.\n   frameRate         Target frame rate in fps. Value range [1, 240], recommended\n                     value 30.\n   qpMapArrayCount   Specifies the count of the QP Map. The calculation formula\n                     is (ALIGN16(width)>>4) * (ALIGN16(height)>>4), with a value\n                     range of [0, 8192x4096/(16x16)], and a recommended value of\n                     0.\n   qpMapArray        Specify the QP Map, the subCTU size of H.264 coding protocol\n                     is 16x16, you need to specify a QP value for each subCTU.\n                     Each QP value occupies one byte and is ordered by raster\n                     scan direction. The recommended value nullptr.\n\n\n\nH.264 coding protocol constant average target bit rate parameter.\n\n * Member\n   \n   MEMBER NAME       DESCRIPTION\n   intraPeriod       I Frame Interval. Value range [0, 2047], recommended value\n                     28.\n   frameRate         Target frame rate in fps. Value range [1, 240], recommended\n                     value 30.\n   qpMapArrayCount   Specifies the count of the QP Map. The calculation formula\n                     is (ALIGN16(width)>>4) * (ALIGN16(height)>>4), with a value\n                     range of [0, 8192x4096/(16x16)], and a recommended value of\n                     0.\n   qpMapArray        Specify the QP Map, the subCTU size of H.264 coding protocol\n                     is 16x16, you need to specify a QP value for each subCTU.\n                     Each QP value occupies one byte and is ordered by raster\n                     scan direction. The recommended value nullptr.\n\n\n\nH.265 coding protocol variable bit rate parameter, the parameter range is\nrestricted in the same way as hbVPVideoH264Cbr.\n\n\n\nH.265 coding protocol variable bit rate parameter, the parameter range is\nrestricted in the same way as hbVPVideoH264Vbr.\n\n\n\nH.265 coding protocol constant average target bit rate parameter, the parameter\nrange is restricted in the same way as hbVPVideoH264AVbr.\n\n\n\nH.265 coding protocol fixed quantization parameter, the parameter range is\nrestricted in the same way as hbVPVideoH264FixQp.\n\n\n\nH.265 coding protocol QP mapping parameter. The calculation formula for\nqpMapArrayCount is\n\n(ALIGN64(width)>>5) * (ALIGN64(height)>>5) , with a value range of [0,\n8192x4096/(32x32)] and a recommended value of 0. The macroblock size is 32x32 ,\nand each macroblock requires a specified QP value. The parameter range is\nrestricted in the same way as hbVPVideoH264QpMap.\n\n\n\nVideo Process Unit coding protocol code rate control parameter.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   mode          Bit rate control mode.\n   h264Cbr       The parameter of the H.264 coding protocol CBR bit rate\n                 control mode.\n   h264Vbr       The parameter of the H.264 coding protocol CBR bit rate\n                 control mode.\n   h264Avbr      The parameter of the H.264 coding protocol AVBR bit rate\n                 control mode.\n   h264QpMap     The parameter of the H.264 coding protocol QpMap bit rate\n                 control mode.\n   h264FixQp     The parameter of the H.264 coding protocol FixQp bit rate\n                 control mode.\n   h265Cbr       The parameter of the H.265 coding protocol CBR bit rate\n                 control mode.\n   h265Vbr       The parameter of the H.265 coding protocol VBR bit rate\n                 control mode.\n   h265Avbr      The parameter of the H.265 coding protocol AVBR bit rate\n                 control mode.\n   h265QpMap     The parameter of the H.265 coding protocol QpMap bit rate\n                 control mode.\n   h265FixQp     The parameter of the H.265 coding protocol FixQp bit rate\n                 control mode.\n\n\n\nGOP structural parameter.\n\n * Member\n   \n   MEMBER NAME           DESCRIPTION\n   decodingRefreshType   Specifies the type of decoding refresh to apply at the intra\n                         frame period. Range value [0, 2], recommended value 2.\n   gopPresetIdx          GOP structure, recommended value 2. gopPresetIdx supports 1,\n                         2, 3, 6, 7 and 9.\n\n\n\nParameter for Video encoding.\n\n * Member\n   \n   MEMBER NAME   DESCRIPTION\n   pixelFormat   The pixel format of the input picture. The currently\n                 supported formats include HB_VP_IMAGE_FORMAT_NV12 and\n                 HB_VP_IMAGE_FORMAT_YUV420.\n   width         The width of the input image should be aligned to 32. Range\n                 Values[256, 8192].\n   height        The height of the input image should be aligned to 8. Range\n                 Values[128, 4096].\n   outBufCount   Specifies the number of output Buffers inside the Video\n                 Process Unit. Range Values[1, 1000], recommended value 5.\n   videoType     The type of encoding protocol.\n   rcParam       Bit rate control parameter.\n   gopParam      GOP parameter.\n\n\n\nGets the encoder default encoding parameter. The pixelFormat is\nHB_VP_IMAGE_FORMAT_YUV420 , and both width and height are 0, requiring you to\nspecify the actual dimensions. The bitrate control method of rcParam is\nhbVPVideoH265Cbr. Refer to the hbVPVideoEncParam structure for the recommended\nvalues of the other parameters.\n\n * Parameter\n   * [in] param Pointer the encoding parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Before calling the interface, you need to specify the videoType as\n     HB_VP_VIDEO_TYPE_H264 or HB_VP_VIDEO_TYPE_H265 .\n\n\n\nCreate the encoder context API and initialize context to nullptr in advance\nbefore creation.\n\n * Parameter\n   * [out] context Encoder context.\n   * [in] param Pointer the encoding parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The Video Process Unit supports the creation of the highest 32 encoding or\n     decoding contexts.\n   * The Video Process Unit only supports H265 encoding.\n\n\n\nRelease the encoder context API.\n\n * Parameter\n   * [in] context Encoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n\n\n\nThe API for calling the Video Process Unit encoding\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [in] srcImg Pointer the memory address where the encoded picture is stored.\n   * [in] context Encoder context.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * Creating tasks using asynchronous methods is only supported.\n   * Due to hardware limitations of the Video Processing Unit (VPU), the VPU\n     supports a maximum of 31 tasks being submitted simultaneously for encoding\n     or decoding operations.\n   * To avoid wasting system resources and improve Video Processing Unit\n     encoding performance, it is recommended to reuse input addresses.\n\n\n\nThe API for calling to get the encoded data buffer.\n\n * Parameter\n   * [out] outBuf Pointer the memory address that holds the encoded data.\n   * [in] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.\n * Description of interface limitations\n   * The output buffer outBuf is requested internally by the Video Process Unit.\n   * The buffer contains valid data upon the task is successfully completed, and\n     is released during the task release phase.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpvideoencode","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":321,"title":"hbVPWarpAffine","content":"#\n\n\n\nWarpAffine operator parameter.\n\n * Member\n   \n   MEMBER NAME          DESCRIPTION\n   transformMatrix[6]   WarpAffine transformation matrix (dst->src), 2X3 matrix.\n   interpolation        Interpolation types in WarpAffine, only support NEAREST and\n                        LINEAR types in hbVPInterpolationType.\n   borderType           Reserved parameter.\n   borderValue          Reserved parameter.\n   isInverse            Flag whether transformMatrix is an inverse matrix, non-zero\n                        means inverse matrix (src = M * dst), 0 means positive\n                        matrix (dst = M * src)\n\n\n\nThe API calling the WarpAffine.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture, the dimensional size is calculated from the conversion\n     matrix.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n   * [in] affineParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpwarpaffine","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":322,"title":"hbVPWarpPerspective","content":"#\n\n\n\nWarpPerspective operator parameter.\n\n * Member\n   \n   MEMBER NAME          DESCRIPTION\n   transformMatrix[9]   WarpPerspective transformation matrix, 3X3 matrix.\n   interpolation        Interpolation types in WarpPerspective, support NEAREST and\n                        LINEAR types in hbVPInterpolationType.\n   borderType           Reserved parameter.\n   borderValue          Reserved parameter.\n   isInverse            Flag whether transformMatrix is an inverse matrix, non-zero\n                        means inverse matrix (src = M * dst), 0 means positive\n                        matrix (dst = M * src)\n\n\n\nThe API for calling the WarpPerspective.\n\n * Parameter\n   * [out] taskHandle Task handles are responsible for the interaction of the\n     operator with the UCP architecture.\n   * [out] dstImg The output picture, type, format and size are the same as the\n     input picture.\n   * [in] srcImg The input picture type supports U8C1 and format supports Y and\n     nv12.\n   * [in] perspectiveParam Operator parameter.\n * Return Value\n   * Return 0 means the API was successfully executed, otherwise the execution\n     failed.","routePath":"/en/guide/ucp/vp/vp7_reference/vp7_function_interface/hbvpwarpperspective","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"},{"id":324,"title":"","content":"Currently, the Horizon Journey 6 Algorithmic Toolchain is in a trial version. It\nis only available to trial customers. So if you need it, you can contact the\nrelevant Horizon project representatives to obtain it at this stage.","routePath":"/en/oe_obtain","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"3.0.22"}]