<!doctype html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Rspress v1.23.1"><title data-rh="true">Horizon OpenExplorer</title><meta data-rh="true" name="description" content="地平线OpenExplorer算法工具链"/><script>{const e=localStorage.getItem("rspress-theme-appearance"),t=window.matchMedia("(prefers-color-scheme: dark)").matches,a=e&&"auto"!==e?"dark"===e:t;document.documentElement.classList.toggle("dark",a),document.documentElement.style.colorScheme=a?"dark":"light"}</script><title>Horizon OpenExplorer</title><link rel="icon" href="/favicon.ico"><script defer="defer" src="/static/js/styles.47a58349.js"></script><script defer="defer" src="/static/js/lib-polyfill.f3f8d881.js"></script><script defer="defer" src="/static/js/lib-lodash.6d460d4e.js"></script><script defer="defer" src="/static/js/lib-react.57316932.js"></script><script defer="defer" src="/static/js/lib-router.687d37e2.js"></script><script defer="defer" src="/static/js/11162.ccfbaa80.js"></script><script defer="defer" src="/static/js/index.de38ed8a.js"></script><link href="/static/css/styles.d4060814.css" rel="stylesheet"></head><body ><div id="root"><div><div class="navContainer_f6cde rspress-nav px-6 " style="position:sticky"><div class="container_f6cde flex justify-between items-center h-full"><div class="navBarTitle_f6cde"><a href="/en" class="flex items-center w-full h-full text-base font-semibold transition-opacity duration-300 hover:opacity-60"><img src="/logo-light.png" alt="logo" id="logo" class="mr-4 rspress-logo dark:hidden"/><img src="/logo-dark.svg" alt="logo" id="logo" class="mr-4 rspress-logo hidden dark:block"/></a></div><div class="flex flex-1 justify-end items-center"><div class="rightNav_f6cde"><div class="flex sm:flex-1 items-center sm:pl-4 sm:pr-2"><div class="rspress-nav-search-button navSearchButton_6e282"><button><svg width="18" height="18" viewBox="0 0 32 32"><path fill="var(--rp-c-gray)" d="m29 27.586-7.552-7.552a11.018 11.018 0 1 0-1.414 1.414L27.586 29ZM4 13a9 9 0 1 1 9 9 9.01 9.01 0 0 1-9-9Z"></path></svg><p class="searchWord_6e282">Search Docs</p><div style="opacity:0"><span></span><span>K</span></div></button></div><div class="mobileNavSearchButton_6e282"><svg width="24" height="24" viewBox="0 0 32 32"><path fill="var(--rp-c-gray)" d="m29 27.586-7.552-7.552a11.018 11.018 0 1 0-1.414 1.414L27.586 29ZM4 13a9 9 0 1 1 9 9 9.01 9.01 0 0 1-9-9Z"></path></svg></div></div><div class="rspress-nav-menu menu h-14"><a href="https://auto-developer.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Smart Car Developer Community</div></a><a href="https://en.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Official Website</div></a><a class="link_03735  cursor-pointer" target="" href="/en/index.html"><div class="rspress-nav-menu-item singleItem_f6cde activeItem_f6cde text-sm font-medium mx-1.5 px-3 py-2 flex items-center">version: 3.0.22</div></a></div><div class="flex-center flex-row"><div class="translation menu-item_f6cde flex text-sm font-bold items-center px-3 py-2"><div><div class="relative flex-center h-14"><button class="rspress-nav-menu-group-button flex-center items-center font-medium text-sm text-text-1 hover:text-text-2 transition-colors duration-200"><span class="text-sm font-medium flex" style="margin-right:2px"><svg width="18" height="18" viewBox="0 0 32 32" style="width:18px;height:18px"><path fill="currentColor" d="M27.85 29H30l-6-15h-2.35l-6 15h2.15l1.6-4h6.85zm-7.65-6 2.62-6.56L25.45 23zM18 7V5h-7V2H9v3H2v2h10.74a14.71 14.71 0 0 1-3.19 6.18A13.5 13.5 0 0 1 7.26 9h-2.1a16.47 16.47 0 0 0 3 5.58A16.84 16.84 0 0 1 3 18l.75 1.86A18.47 18.47 0 0 0 9.53 16a16.92 16.92 0 0 0 5.76 3.84L16 18a14.48 14.48 0 0 1-5.12-3.37A17.64 17.64 0 0 0 14.8 7z"></path></svg></span><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M16 22 6 12l1.4-1.4 8.6 8.6 8.6-8.6L26 12z"></path></svg></button><div class="rspress-nav-menu-group-content absolute mx-0.8 transition-opacity duration-300" style="opacity:0;visibility:hidden;right:0;top:52px"><div class="p-3 pr-2 w-full h-full max-h-100vh whitespace-nowrap" style="box-shadow:var(--rp-shadow-3);z-index:100;border:1px solid var(--rp-c-divider-light);border-radius:var(--rp-radius-large);background:var(--rp-c-bg)"><div><div class="font-medium my-1"><a class="link_03735  cursor-pointer" target="" href="/guide/appendix/supported_op_list/torch_operator_support_list_html.html"><div class="rounded-2xl hover:bg-mute" style="padding:0.4rem 1.5rem 0.4rem 0.75rem"><div class="flex"><span>中文</span></div></div></a></div></div><div><div class="rounded-2xl my-1 flex" style="padding:0.4rem 1.5rem 0.4rem 0.75rem"><span class="text-brand">English</span></div></div></div></div></div></div></div><div class="mx-2"><div class="md:mr-2 rspress-nav-appearance"><div class="p-1 border border-solid border-gray-300 text-gray-400  cursor-pointer rounded-md hover:border-gray-600 hover:text-gray-600 dark:hover:border-gray-200 dark:hover:text-gray-200 transition-all duration-300 w-7 h-7"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 24 24" class="dark:hidden" width="18" height="18" fill="currentColor"><path d="M12 18c-3.3 0-6-2.7-6-6s2.7-6 6-6 6 2.7 6 6-2.7 6-6 6zm0-10c-2.2 0-4 1.8-4 4s1.8 4 4 4 4-1.8 4-4-1.8-4-4-4zM12 4c-.6 0-1-.4-1-1V1c0-.6.4-1 1-1s1 .4 1 1v2c0 .6-.4 1-1 1zM12 24c-.6 0-1-.4-1-1v-2c0-.6.4-1 1-1s1 .4 1 1v2c0 .6-.4 1-1 1zM5.6 6.6c-.3 0-.5-.1-.7-.3L3.5 4.9c-.4-.4-.4-1 0-1.4s1-.4 1.4 0l1.4 1.4c.4.4.4 1 0 1.4-.1.2-.4.3-.7.3zM19.8 20.8c-.3 0-.5-.1-.7-.3l-1.4-1.4c-.4-.4-.4-1 0-1.4s1-.4 1.4 0l1.4 1.4c.4.4.4 1 0 1.4-.2.2-.5.3-.7.3zM3 13H1c-.6 0-1-.4-1-1s.4-1 1-1h2c.6 0 1 .4 1 1s-.4 1-1 1zM23 13h-2c-.6 0-1-.4-1-1s.4-1 1-1h2c.6 0 1 .4 1 1s-.4 1-1 1zM4.2 20.8c-.3 0-.5-.1-.7-.3-.4-.4-.4-1 0-1.4l1.4-1.4c.4-.4 1-.4 1.4 0s.4 1 0 1.4l-1.4 1.4c-.2.2-.4.3-.7.3zM18.4 6.6c-.3 0-.5-.1-.7-.3-.4-.4-.4-1 0-1.4l1.4-1.4c.4-.4 1-.4 1.4 0s.4 1 0 1.4l-1.4 1.4c-.2.2-.5.3-.7.3z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 24 24" class="hidden dark:block" width="18" height="18" fill="currentColor"><path d="M12.1 22h-.9c-5.5-.5-9.5-5.4-9-10.9.4-4.8 4.2-8.6 9-9 .4 0 .8.2 1 .5.2.3.2.8-.1 1.1-2 2.7-1.4 6.4 1.3 8.4 2.1 1.6 5 1.6 7.1 0 .3-.2.7-.3 1.1-.1.3.2.5.6.5 1-.2 2.7-1.5 5.1-3.6 6.8-1.9 1.4-4.1 2.2-6.4 2.2zM9.3 4.4c-2.9 1-5 3.6-5.2 6.8-.4 4.4 2.8 8.3 7.2 8.7 2.1.2 4.2-.4 5.8-1.8 1.1-.9 1.9-2.1 2.4-3.4-2.5.9-5.3.5-7.5-1.1-2.8-2.2-3.9-5.9-2.7-9.2z"></path></svg></div></div></div></div></div><div class="mobileNavMenu_f6cde"><div class="navScreen_457e8 " id="navScreen"><div class="container_457e8"><div class="navMenu_457e8"><div class="navMenuItem_457e8 w-full"><a href="https://auto-developer.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Smart Car Developer Community</div></a></div><div class="navMenuItem_457e8 w-full"><a href="https://en.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Official Website</div></a></div><div class="navMenuItem_457e8 w-full"><a class="link_03735  cursor-pointer" target="" href="/en/index.html"><div class="rspress-nav-menu-item singleItem_f6cde activeItem_f6cde text-sm font-medium mx-1.5 px-3 py-2 flex items-center">version: 3.0.22</div></a></div></div><div class="flex-center flex-col gap-2"><div class="mt-2 navAppearance_457e8 flex justify-center"></div><div class="flex text-sm font-bold justify-center"><div class="mx-1.5 my-1"><div class=" navScreenMenuGroup_457e8 relative"><button class="button_457e8"><span class="buttonSpan_457e8"><svg width="18" height="18" viewBox="0 0 32 32" style="width:18px;height:18px"><path fill="currentColor" d="M27.85 29H30l-6-15h-2.35l-6 15h2.15l1.6-4h6.85zm-7.65-6 2.62-6.56L25.45 23zM18 7V5h-7V2H9v3H2v2h10.74a14.71 14.71 0 0 1-3.19 6.18A13.5 13.5 0 0 1 7.26 9h-2.1a16.47 16.47 0 0 0 3 5.58A16.84 16.84 0 0 1 3 18l.75 1.86A18.47 18.47 0 0 0 9.53 16a16.92 16.92 0 0 0 5.76 3.84L16 18a14.48 14.48 0 0 1-5.12-3.37A17.64 17.64 0 0 0 14.8 7z"></path></svg></span><svg width="1em" height="1em" viewBox="0 0 32 32" class=" down_457e8 "><path fill="currentColor" d="M16 22 6 12l1.4-1.4 8.6 8.6 8.6-8.6L26 12z"></path></svg></button><div><div class="items_457e8"><div><div class="py-1 font-medium"><a class="link_03735  cursor-pointer" target="" href="/guide/appendix/supported_op_list/torch_operator_support_list_html.html"><div><div class="flex justify-center"><span>中文</span></div></div></a></div></div><div><div class="p-1 text-center"><span class="text-brand">English</span></div></div></div></div></div></div></div></div></div></div><button aria-label="mobile hamburger" class=" navHamburger_e7b06 text-gray-500"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" fill="currentColor"><circle cx="8" cy="16" r="2" fill="currentColor"></circle><circle cx="16" cy="16" r="2" fill="currentColor"></circle><circle cx="24" cy="16" r="2" fill="currentColor"></circle></svg></button></div></div></div></div><section><div class="docLayout_edeb4 pt-0"><div class="rspress-sidebar-menu"><button class="flex-center mr-auto"><div class="text-md mr-2"><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M4 6h24v2H4zm0 18h24v2H4zm0-12h24v2H4zm0 6h24v2H4z"></path></svg></div><span class="text-sm">Menu</span></button><button class="flex-center ml-auto"><span class="text-sm">On This Page</span><div class="text-md mr-2" style="transform:rotate(0deg);transition:transform 0.2s ease-out;margin-top:2px"><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M22 16 12 26l-1.4-1.4 8.6-8.6-8.6-8.6L12 6z"></path></svg></div></button></div><aside class="sidebar_71eca rspress-sidebar "><div class="navTitleMask_71eca"><div class="navBarTitle_f6cde"><a href="/en" class="flex items-center w-full h-full text-base font-semibold transition-opacity duration-300 hover:opacity-60"><img src="/logo-light.png" alt="logo" id="logo" class="mr-4 rspress-logo dark:hidden"/><img src="/logo-dark.svg" alt="logo" id="logo" class="mr-4 rspress-logo hidden dark:block"/></a></div></div><div class="rspress-scrollbar sidebarContent_71eca"><nav class="pb-2"></nav></div></aside><div class="content_edeb4 rspress-doc-container flex flex-shrink-0 mx-auto"><div class="w-full flex-1"><div><div class="rspress-doc"><!--$--><span data-dark="false" class="rp-reading-time">Estimated reading time: less than 1 minute</span>
<!-- -->
<div class="table-container"><table class="scrollable-table"><thead><tr><th>Torch Operator</th><th>Eager Mode Operator</th><th>Map Description &amp; Graph Fusion Description</th><th>HBIR Operator Name</th><th>BPU Support Constraints</th></tr></thead><tbody><tr><td rowspan="1">torch.abs<br/>torch.Tensor.abs</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.abs&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.abs</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.acos</td><td rowspan="1">horizon.nn.Acos</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.acosh</td><td rowspan="1">horizon.nn.Acosh</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">torch.add<br/>torch.Tensor.add</td><td rowspan="3">torch.nn.quantized.FloatFunctional  OR <br/>horizon.nn.quantized.FloatFunctional</td><td rowspan="1">if alpha == 1:<br/>func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.add&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="2">if alpha != 1:<br/>func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.mul&quot;(%arg1, %0)<br/>  %2 = &quot;hbir.add&quot;(%arg0, %1)<br/>  return %2<br/>}</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.argmax<br/>torch.Tensor.argmax</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_argmax&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_argmax</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.argmin<br/>torch.Tensor.argmin</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_argmin&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_argmin</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.asin</td><td rowspan="1">horizon.nn.Asin</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">torch.asinh</td><td rowspan="3">horizon.nn.Asinh</td><td rowspan="3">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.max&quot;(%arg0, %0)<br/>  %2 = &quot;hbir.clip&quot;(%1)<br/>  %3 = &quot;qnt.quantize&quot;(%2)<br/>  %4 = &quot;hbir.constant&quot;()<br/>  %5 = &quot;b30.lut&quot;(%3, %4)<br/>  %6 = &quot;qnt.dequantize&quot;(%5)<br/>  %7 = &quot;qnt.const_fake_quant&quot;(%6)<br/>  return %7<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.clip</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.max</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.atan</td><td rowspan="1">horizon.nn.Atan</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.atanh</td><td rowspan="1">horizon.nn.Atan</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.ceil</td><td rowspan="1">horizon.nn.Ceil</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">torch.clamp<br/>torch.clip<br/>torch.Tensor.clamp<br/>torch.Tensor.clip</td><td rowspan="3">　</td><td rowspan="1">if isinstance(args, scalar):<br/>func.func @forward(%arg0, %arg1, %arg2){<br/>  %0 = &quot;hbir.clip&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.clip</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">if isinstance(args, Tensor):<br/>func.func @forward(%arg0, %arg1, %arg2){<br/>  %0 = &quot;hbir.max&quot;(%arg0, %arg1)<br/>  %1 = &quot;hbir.min&quot;(%0, %arg2)<br/>  return %1<br/>}</td><td>hbir.min</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.max</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.cat<br/>torch.concat<br/>torch.concatenate</td><td rowspan="1">torch.nn.quantized.FloatFunctional  OR <br/>horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.concat&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.cos</td><td rowspan="1">horizon.nn.Cos</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.cosh</td><td rowspan="1">horizon.nn.Cosh</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">torch.div</td><td rowspan="2">horizon.nn.Div</td><td rowspan="2">if qat:<br/>func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;qnt.quantize&quot;(%arg1)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  %5 = &quot;hbir.mul&quot;(%arg0, %4)<br/>  %6 = &quot;qnt.const_fake_quant&quot;(%5)<br/>  return %6<br/>}<br/>else:<br/>func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.div&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.eq<br/>torch.Tensor.eq</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.equal&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.equal</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">torch.gt<br/>torch.greater<br/>torch.Tensor.gt<br/>torch.Tensor.greater</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.greater&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.greater</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">torch.lt<br/>torch.less<br/>torch.Tensor.lt<br/>torch.Tensor.less</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.less&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.less</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">torch.erf</td><td rowspan="1">horizon.nn.Erf</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.exp</td><td rowspan="1">horizon.nn.Exp</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">torch.Tensor.expand</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/> %0 = &quot;hbir.reshape&quot;(%arg0)<br/> %1 = &quot;hbir.concat&quot;(%0, %0, %0)<br/> return %1<br/>}</td><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.flatten<br/>torch.Tensor.flatten</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.flip<br/>torch.Tensor.flip</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.flip&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.flip</td><td><b>input:</b><br/>Type: int8<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.floor</td><td rowspan="1">horizon.nn.Floor</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.index_select<br/>torch.Tensor.index_select</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.index&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.index</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*]<br/>Dim: dims ∈ [1, 65536]<br/><b>index:</b><br/>Type: int8, int16, int32, int64<br/>Shape: [*]<br/><b>output:</b><br/>Same as input except Dim constraints<br/></td></tr><tr><td rowspan="1">torch.log</td><td rowspan="1">horizon.nn.HardLog</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.logical_and</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.logical_and&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.logical_and</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">torch.logical_not</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.logical_not&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.logical_not</td><td><b>input:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">torch.Tensor.masked_fill</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.where&quot;(%0, %1, %arg0)<br/>  return %2<br/>}</td><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.matmul</td><td rowspan="1">horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.matmul&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.matmul</td><td><b>lhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,M,C]<br/>Dim: * ∈ [1, 4096], M,C ∈ [1, 8192]<br/><b>rhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,C,N]<br/>Dim: * ∈ [1, 4096]; C,N ∈ [1, 8192]<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Shape: [*,M,N]<br/>Other constraints: Same as lhs<br/></td></tr><tr><td rowspan="3">torch.max<br/>torch.Tensor.max</td><td rowspan="3">　</td><td rowspan="2">if dim is not None:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_max&quot;(%arg0)<br/>  %1 = &quot;hbir.reduce_argmax&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_argmax</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">if dim is None:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_max&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.maximum</td><td rowspan="1">horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.max&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.max</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.mean</td><td rowspan="1">horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_mean&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="3">torch.min<br/>torch.Tensor.min</td><td rowspan="3">　</td><td rowspan="2">if dim is not None:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_min&quot;(%arg0)<br/>  %1 = &quot;hbir.reduce_argmin&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_min</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reduce_argmin</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">if dim is None:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_min&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_min</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.minimum</td><td rowspan="1">horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/> %0 = &quot;hbir.min&quot;(%arg0, %arg1)<br/> return %0<br/>}</td><td>hbir.min</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.mul<br/>torch.Tensor.mul</td><td rowspan="1">torch.nn.quantized.FloatFunctional or <br/>horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.mul&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.neg<br/>torch.negative<br/>torch.Tensor.neg<br/>torch.Tensor.negative</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>    %0 = &quot;hbir.neg&quot;(%arg0)<br/>    return %0<br/>  }</td><td>hbir.neg</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.permute<br/>torch.Tensor.permute</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torch.pow</td><td rowspan="3">horizon.nn.Pow</td><td rowspan="1">if exponent != 2 and qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">if exponent == 2:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.mul&quot;(%arg0, %arg0)<br/>  return %0<br/>}<br/></td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">if exponent != 2 and float:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.pow&quot;(%arg0, %0)<br/>  return %1<br/>}</td><td>hbir.pow</td><td>Unsupported<br/></td></tr><tr><td rowspan="1">torch.reciprocal</td><td rowspan="1">horizon.nn.Reciprocal</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.repeat<br/>torch.Tensor.repeat</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.tile&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.tile</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.reshape<br/>torch.Tensor.reshape<br/>torch.view<br/>torch.Tensor.view</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.sin</td><td rowspan="1">horizon.nn.Sin</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.sinh</td><td rowspan="1">horizon.nn.Sinh</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.split</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.slice&quot;(%arg0)<br/>  %1 = &quot;hbir.slice&quot;(%arg0)<br/>  %2 = &quot;hbir.slice&quot;(%arg0)<br/>  %3 = &quot;hbir.slice&quot;(%arg0)<br/>  return %0, %1, %2, %3<br/>}</td><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torch.sqrt</td><td rowspan="3">horizon.nn.Sqrt</td><td rowspan="3">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.max&quot;(%arg0, %0)<br/>  %2 = &quot;hbir.clip&quot;(%1)<br/>  %3 = &quot;qnt.quantize&quot;(%2)<br/>  %4 = &quot;hbir.constant&quot;()<br/>  %5 = &quot;b30.lut&quot;(%3, %4)<br/>  %6 = &quot;qnt.dequantize&quot;(%5)<br/>  %7 = &quot;qnt.const_fake_quant&quot;(%6)<br/>  return %7<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.clip</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.max</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.squeeze<br/>torch.Tensor.squeeze</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torch.sub</td><td rowspan="3">horizon.nn.quantized.FloatFunctional</td><td rowspan="2">if alpha != 1:<br/>func.func @forward(%arg0, %arg1){<br/> %0 = &quot;hbir.constant&quot;()<br/> %1 = &quot;hbir.mul&quot;(%arg1, %0)<br/> %2 = &quot;hbir.sub&quot;(%arg0, %1)<br/> return %2<br/>}</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">if alpha == 1:<br/>func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.sub&quot;(%arg0, %arg1)<br/>  return %0<br/>}</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.sum</td><td rowspan="1">horizon.nn.quantized.FloatFunctional</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reduce_sum&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.tan</td><td rowspan="1">horizon.nn.Tan</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.tile<br/>torch.Tensor.tile</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.tile&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.tile</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.transpose<br/>torch.Tensor.transpose</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.tril</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.where&quot;(%0, %arg0, 1)<br/>  return %2<br/>}</td><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.triu</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.where&quot;(%0, %arg0, 1)<br/>  return %2<br/>}</td><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.unsqueeze<br/>torch.Tensor.unsqueeze</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.where<br/>torch.Tensor.where</td><td rowspan="1">horizon.nn.Where</td><td rowspan="1">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.where&quot;(%0, %arg0, %arg1)<br/>  return %1<br/>}</td><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.zeros_like<br/>torch.ones_like<br/>torch.full_like<br/>torch.rand_like</td><td rowspan="1"></td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  return %0<br/>}</td><td>hbir.constant</td><td>No limits<br/></td></tr><tr><td rowspan="6">torch.linalg.norm</td><td rowspan="6">horizon.nn.LinalgNorm</td><td rowspan="3">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.const_fake_quant&quot;(%arg0)<br/>  %1 = &quot;hbir.mul&quot;(%0, %0)<br/>  %2 = &quot;qnt.const_fake_quant&quot;(%1)<br/>  %3 = &quot;hbir.reduce_sum&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  %5 = &quot;qnt.quantize&quot;(%4)<br/>  %6 = &quot;hbir.constant&quot;()<br/>  %7 = &quot;b30.lut&quot;(%5, %6)<br/>  %8 = &quot;qnt.dequantize&quot;(%7)<br/>  %9 = &quot;qnt.const_fake_quant&quot;(%8)<br/>  return %9<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="3">if float:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.mul&quot;(%arg0, %arg0)<br/>  %1 = &quot;hbir.reduce_sum&quot;(%0)<br/>  %2 = &quot;hbir.sqrt&quot;(%1)<br/>  return %3<br/>}</td><td>hbir.sqrt</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="2">torch.nn.functional.adaptive_avg_pool1d<br/>torch.nn.AdaptiveAvgPool1d</td><td rowspan="2">torch.nn.AdaptiveAvgPool1d</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  %1 = &quot;hbir.avg_pool2d&quot;(%0)<br/>  %2 = &quot;hbir.reshape&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.avg_pool2d</td><td>Unsupported<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.nn.functional.adaptive_avg_pool2d<br/>torch.nn.AdaptiveAvgPool2d</td><td rowspan="2">torch.nn.AdaptiveAvgPool2d</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.avg_pool2d&quot;(%0)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.avg_pool2d</td><td>Unsupported<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torch.nn.functional.affine_grid</td><td rowspan="3">　</td><td rowspan="3">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.transpose&quot;(%arg0)<br/>  %2 = &quot;hbir.matmul&quot;(%0, %1)<br/>  %3 = &quot;hbir.reshape&quot;(%2)<br/>  return %3<br/>}</td><td>hbir.matmul</td><td><b>lhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,M,C]<br/>Dim: * ∈ [1, 4096], M,C ∈ [1, 8192]<br/><b>rhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,C,N]<br/>Dim: * ∈ [1, 4096]; C,N ∈ [1, 8192]<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Shape: [*,M,N]<br/>Other constraints: Same as lhs<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.nn.functional.avg_pool2d<br/>torch.nn.AvgPool2d</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.avg_pool2d&quot;(%0)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.avg_pool2d</td><td>Unsupported<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.nn.Embedding</td><td rowspan="2"></td><td rowspan="2">func.func @forward(%arg0){<br/>    %0 = &quot;hbir.constant&quot;()<br/>    %1 = &quot;hbir.reshape&quot;(%arg0)<br/>    %2 = &quot;hbir.gather_nd&quot;(%0, %1)<br/>    %3 = &quot;hbir.reshape&quot;(%2)<br/>    return %3<br/>}</td><td>hbir.gather_nd</td><td>Unsupported<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.nn.functional.gelu<br/>torch.nn.GELU</td><td rowspan="1">torch.nn.GELU</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">torch.nn.functional.glu<br/>torch.nn.GLU</td><td rowspan="3">torch.nn.GLU</td><td rowspan="3">module {<br/>  func.func @forward(%arg0){<br/>    %0 = &quot;qnt.const_fake_quant&quot;(%arg0)<br/>    %1 = &quot;hbir.slice&quot;(%0)<br/>    %2 = &quot;hbir.slice&quot;(%0)<br/>    %3 = &quot;qnt.quantize&quot;(%2)<br/>    %4 = &quot;hbir.constant&quot;()<br/>    %5 = &quot;b30.lut&quot;(%3, %4)<br/>    %6 = &quot;qnt.dequantize&quot;(%5)<br/>    %7 = &quot;qnt.const_fake_quant&quot;(%6)<br/>    %8 = &quot;hbir.mul&quot;(%1, %7)<br/>    %9 = &quot;qnt.const_fake_quant&quot;(%8)<br/>    return %9<br/>  }<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="2">torch.nn.functional.grid_sample</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.grid_sample&quot;(%0, %arg1)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.grid_sample</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/>Dim: H,W ∈ [1, 1024]; H*W ≤ 720*1024; other dims ∈ [1, 65536]<br/><b>grid:</b><br/>Type: int16<br/>Shape: [*,H,W,2]<br/><b>output:</b><br/>Same as input except Dim constraints<br/></td></tr><tr><td rowspan="1">torch.nn.functional.hardsigmoid<br/>torch.nn.HardSigmoid</td><td rowspan="1">torch.nn.HardSigmoid</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">torch.nn.functional.interpolate<br/>torch.nn.Upsample<br/>torch.nn.UpsamplingNearest2d<br/>torch.nn.UpsamplingBilinear2d</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.resize2d&quot;(%0)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.resize2d</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.nn.functional.leaky_relu<br/>torch.nn.LeakyReLU</td><td rowspan="1">torch.nn.LeakyReLU</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.nn.functional.pad<br/>torch.nn.ConstantPad1d<br/>torch.nn.ConstantPad2d<br/>torch.nn.ConstantPad3d<br/>torch.nn.ReplicationPad1d<br/>torch.nn.ReplicationPad2d<br/>torch.nn.ReplicationPad3d<br/>torch.nn.ZeroPad2d</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.pad&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.pad</td><td><b>input:</b><br/>Type: int64, uint64 and f64 are not supported when expansionMode is &#x27;constant&#x27; else no constraints<br/>Dim: all dims &lt; 737280 when expansionMode is not &#x27;constant&#x27; else no constraints<br/><b>output:</b><br/>Same as input<br/><b>begin/end:</b><br/>Value should be in range [1, 4096]<br/></td></tr><tr><td rowspan="2">torch.nn.functional.pixel_shuffle<br/>torch.nn.PixelShuffle</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  %1 = &quot;hbir.transpose&quot;(%0)<br/>  %2 = &quot;hbir.reshape&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.nn.functional.pixel_unshuffle<br/>torch.nn.PixelUnshuffle</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.reshape&quot;(%arg0)<br/>  %1 = &quot;hbir.transpose&quot;(%0)<br/>  %2 = &quot;hbir.reshape&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torch.nn.functional.prelu<br/>torch.nn.PReLU</td><td rowspan="3">torch.nn.PReLU</td><td rowspan="1">if isinstance(weight, scalar):<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.prelu&quot;(%arg0, %0)<br/>  return %1<br/>}<br/></td><td>hbir.prelu</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="2">if not isinstance(weight, scalar):<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.transpose&quot;(%arg0)<br/>  %2 = &quot;hbir.prelu&quot;(%1, %0)<br/>  %3 = &quot;hbir.transpose&quot;(%2)<br/>  return %3<br/>}</td><td>hbir.prelu</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.nn.functional.relu<br/>torch.nn.ReLU</td><td rowspan="1">torch.nn.ReLU</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.relu&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.relu</td><td><b>input:</b><br/>Type: int8, int16, int32<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.nn.functional.relu6(fused)<br/>torch.nn.ReLU6(fused)</td><td rowspan="1">torch.nn.ReLU6</td><td rowspan="1">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.clip&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.clip</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">torch.nn.functional.silu<br/>torch.nn.SiLU</td><td rowspan="1">torch.nn.SiLU</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="7">torch.nn.functional.softmax<br/>torch.nn.Softmax</td><td rowspan="7">torch.nn.Softmax</td><td rowspan="1">if float:<br/>func.func @forward(%arg0) {<br/>  %0 = &quot;hbir.softmax&quot;(%arg0)<br/>  return %0<br/>}</td><td>hbir.softmax</td><td>Unsupported<br/></td></tr><tr><td rowspan="6">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.cast_type&quot;(%arg0)<br/>  %1 = &quot;hbir.reduce_max&quot;(%0)<br/>  %2 = &quot;hbir.reduce_argmax&quot;(%0)<br/>  %3 = &quot;hbir.sub&quot;(%0, %1)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  %5 = &quot;qnt.quantize&quot;(%4)<br/>  %6 = &quot;hbir.constant&quot;()<br/>  %7 = &quot;b30.lut&quot;(%5, %6)<br/>  %8 = &quot;qnt.dequantize&quot;(%7)<br/>  %9 = &quot;qnt.const_fake_quant&quot;(%8)<br/>  %10 = &quot;hbir.reduce_sum&quot;(%9)<br/>  %11 = &quot;qnt.const_fake_quant&quot;(%10)<br/>  %12 = &quot;qnt.quantize&quot;(%11)<br/>  %13 = &quot;hbir.constant&quot;()<br/>  %14 = &quot;b30.lut&quot;(%12, %13)<br/>  %15 = &quot;qnt.dequantize&quot;(%14)<br/>  %16 = &quot;qnt.const_fake_quant&quot;(%15)<br/>  %17 = &quot;hbir.mul&quot;(%9, %16)<br/>  %18 = &quot;qnt.const_fake_quant&quot;(%17)<br/>  return %18<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reduce_argmax</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">torch.nn.functional.softplus<br/>torch.nn.Softplus</td><td rowspan="1"></td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">torch.nn.BatchNorm2d</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.constant&quot;()<br/>  %3 = &quot;hbir.constant&quot;()<br/>  %4 = &quot;hbir.transpose&quot;(%arg0)<br/>  %5 = &quot;hbir.batchnorm&quot;(%4, %0, %1, %2, %3)<br/>  %6 = &quot;hbir.transpose&quot;(%5)<br/>  return %6<br/>}</td><td>hbir.batchnorm</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*,H,W,C]<br/><b>mean:</b><br/>Type: f32<br/>Shape: [C]<br/><b>var:</b><br/>Type: f32<br/>Shape: [C]<br/><b>weight:</b><br/>Type: f32<br/>Shape: [C]<br/><b>bias:</b><br/>Type: f32<br/>Shape: [C]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="4">torch.nn.Conv2d</td><td rowspan="4">　</td><td rowspan="2">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;qnt.const_fake_quant&quot;(%0)<br/>  %2 = &quot;hbir.constant&quot;()<br/>  %3 = &quot;hbir.transpose&quot;(%arg0)<br/>  %4 = &quot;hbir.transpose&quot;(%1)<br/>  %5 = &quot;hbir.conv2d&quot;(%3, %4, %2)<br/>  %6 = &quot;hbir.transpose&quot;(%5)<br/>  %7 = &quot;qnt.const_fake_quant&quot;(%6)<br/>  return %7<br/>}</td><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">if float:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.transpose&quot;(%arg0)<br/>  %3 = &quot;hbir.transpose&quot;(%0)<br/>  %4 = &quot;hbir.conv2d&quot;(%2, %3, %1)<br/>  %5 = &quot;hbir.transpose&quot;(%4)<br/>  return %5<br/>}</td><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="4">torch.nn.ConvTranspose2d</td><td rowspan="4">　</td><td rowspan="2">if float:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.transpose&quot;(%arg0)<br/>  %3 = &quot;hbir.transpose&quot;(%0)<br/>  %4 = &quot;hbir.conv2dtranspose&quot;(%2, %3, %1)<br/>  %5 = &quot;hbir.transpose&quot;(%4)<br/>  return %5<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.conv2dtranspose</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 128]; H,W ∈ [1, 65536]; C ∈ [1, 2048]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: N,C ∈ [1, 2048]; KH,KW ∈ [1, 14]; KH,KW cannot both be 1<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as input, the type additionally supports int32<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 14]; SH &lt; KH; SW &lt; KW;<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_left,P_bottom,P_right ∈ [0, 256]<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ {1}<br/></td></tr><tr><td rowspan="2">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;qnt.const_fake_quant&quot;(%0)<br/>  %2 = &quot;hbir.reshape&quot;(%1)<br/>  %3 = &quot;hbir.transpose&quot;(%2)<br/>  %4 = &quot;hbir.reshape&quot;(%3)<br/>  %5 = &quot;hbir.constant&quot;()<br/>  %6 = &quot;hbir.transpose&quot;(%arg0)<br/>  %7 = &quot;hbir.transpose&quot;(%4)<br/>  %8 = &quot;hbir.conv2dtranspose&quot;(%6, %7, %5)<br/>  %9 = &quot;hbir.transpose&quot;(%8)<br/>  %10 = &quot;qnt.const_fake_quant&quot;(%9)<br/>  return %10<br/>}</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.conv2dtranspose</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 128]; H,W ∈ [1, 65536]; C ∈ [1, 2048]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: N,C ∈ [1, 2048]; KH,KW ∈ [1, 14]; KH,KW cannot both be 1<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as input, the type additionally supports int32<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 14]; SH &lt; KH; SW &lt; KW;<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_left,P_bottom,P_right ∈ [0, 256]<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ {1}<br/></td></tr><tr><td rowspan="6">torch.nn.LayerNorm<br/>torch.nn.InstanceNorm2d</td><td rowspan="6">　</td><td rowspan="1">if float:<br/>func.func @forward(%arg0){<br/>   %0 = &quot;hbir.layernorm&quot;(%arg0)<br/>   return %0<br/>}</td><td>hbir.layernorm</td><td>Unsupported<br/></td></tr><tr><td rowspan="5">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.cast_type&quot;(%arg0)<br/>  %1 = &quot;hbir.reduce_mean&quot;(%0)<br/>  %2 = &quot;qnt.const_fake_quant&quot;(%1)<br/>  %3 = &quot;hbir.sub&quot;(%0, %2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  %5 = &quot;hbir.mul&quot;(%4, %4)<br/>  %6 = &quot;qnt.const_fake_quant&quot;(%5)<br/>  %7 = &quot;hbir.reduce_mean&quot;(%6)<br/>  %8 = &quot;qnt.const_fake_quant&quot;(%7)<br/>  %9 = &quot;qnt.quantize&quot;(%8)<br/>  %10 = &quot;hbir.constant&quot;()<br/>  %11 = &quot;b30.lut&quot;(%9, %10)<br/>  %12 = &quot;qnt.dequantize&quot;(%11)<br/>  %13 = &quot;qnt.const_fake_quant&quot;(%12)<br/>  %14 = &quot;hbir.mul&quot;(%4, %13)<br/>  %15 = &quot;qnt.const_fake_quant&quot;(%14)<br/>  %16 = &quot;hbir.constant&quot;()<br/>  %17 = &quot;qnt.const_fake_quant&quot;(%16)<br/>  %18 = &quot;hbir.mul&quot;(%15, %17)<br/>  %19 = &quot;qnt.const_fake_quant&quot;(%18)<br/>  %20 = &quot;hbir.constant&quot;()<br/>  %21 = &quot;qnt.const_fake_quant&quot;(%20)<br/>  %22 = &quot;hbir.add&quot;(%19, %21)<br/>  %23 = &quot;qnt.const_fake_quant&quot;(%22)<br/>  return %23<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="11">horizon.nn.LayerNorm</td><td rowspan="11">　</td><td rowspan="5">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.cast_type&quot;(%arg0)<br/>  %1 = &quot;hbir.reduce_mean&quot;(%0)<br/>  %2 = &quot;qnt.const_fake_quant&quot;(%1)<br/>  %3 = &quot;hbir.sub&quot;(%0, %2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  %5 = &quot;hbir.mul&quot;(%4, %4)<br/>  %6 = &quot;qnt.const_fake_quant&quot;(%5)<br/>  %7 = &quot;hbir.reduce_mean&quot;(%6)<br/>  %8 = &quot;qnt.const_fake_quant&quot;(%7)<br/>  %9 = &quot;qnt.quantize&quot;(%8)<br/>  %10 = &quot;hbir.constant&quot;()<br/>  %11 = &quot;b30.lut&quot;(%9, %10)<br/>  %12 = &quot;qnt.dequantize&quot;(%11)<br/>  %13 = &quot;qnt.const_fake_quant&quot;(%12)<br/>  %14 = &quot;hbir.mul&quot;(%4, %13)<br/>  %15 = &quot;qnt.const_fake_quant&quot;(%14)<br/>  %16 = &quot;hbir.constant&quot;()<br/>  %17 = &quot;qnt.const_fake_quant&quot;(%16)<br/>  %18 = &quot;hbir.mul&quot;(%15, %17)<br/>  %19 = &quot;qnt.const_fake_quant&quot;(%18)<br/>  %20 = &quot;hbir.constant&quot;()<br/>  %21 = &quot;qnt.const_fake_quant&quot;(%20)<br/>  %22 = &quot;hbir.add&quot;(%19, %21)<br/>  %23 = &quot;qnt.const_fake_quant&quot;(%22)<br/>  return %23<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="5">if float and dim is not None:<br/>  func.func @forward(%arg0){<br/>    %0 = &quot;hbir.reduce_mean&quot;(%arg0)<br/>    %1 = &quot;hbir.sub&quot;(%arg0, %0)<br/>    %2 = &quot;hbir.mul&quot;(%1, %1)<br/>    %3 = &quot;hbir.reduce_mean&quot;(%2)<br/>    %4 = &quot;hbir.rsqrt&quot;(%3)<br/>    %5 = &quot;hbir.mul&quot;(%1, %4)<br/>    %6 = &quot;hbir.constant&quot;()<br/>    %7 = &quot;hbir.mul&quot;(%5, %6)<br/>    %8 = &quot;hbir.constant&quot;()<br/>    %9 = &quot;hbir.add&quot;(%7, %8)<br/>    return %9<br/>  }</td><td>hbir.rsqrt</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">if float and dim is None:<br/>func.func @forward(%arg0){<br/>   %0 = &quot;hbir.layernorm&quot;(%arg0)<br/>   return %0<br/>}</td><td>hbir.layernorm</td><td>Unsupported<br/></td></tr><tr><td rowspan="3">torch.nn.Linear</td><td rowspan="3">　</td><td rowspan="1">if float:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;hbir.linear&quot;(%arg0, %0, %1)<br/>  return %2<br/>}</td><td>hbir.linear</td><td><b>input:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [*,C_in]<br/>Dim: *, C_in ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [C_out, C_in]<br/>Dim: C_in,C_out ∈ [1, 8192]; C_in × C_out ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as input<br/></td></tr><tr><td rowspan="2">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;qnt.const_fake_quant&quot;(%0)<br/>  %2 = &quot;hbir.reshape&quot;(%1)<br/>  %3 = &quot;hbir.constant&quot;()<br/>  %4 = &quot;hbir.linear&quot;(%arg0, %2, %3)<br/>  %5 = &quot;qnt.const_fake_quant&quot;(%4)<br/>  return %5<br/>}</td><td>hbir.linear</td><td><b>input:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [*,C_in]<br/>Dim: *, C_in ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [C_out, C_in]<br/>Dim: C_in,C_out ∈ [1, 8192]; C_in × C_out ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.nn.functional.max_pool2d<br/>torch.nn.MaxPool2d<br/>torch.nn.functional.adaptive_max_pool2d<br/>torch.nn.AdaptiveMaxPool2d</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.max_pool2d&quot;(%0)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.max_pool2d</td><td>Unsupported<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="13">torch.nn.MultiheadAttention</td><td rowspan="13"></td><td rowspan="13">func.func @forward(%arg0, %arg1, %arg2, %arg3, %arg4){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.transpose&quot;(%arg1)<br/>  %2 = &quot;hbir.transpose&quot;(%arg2)<br/>  %3 = &quot;hbir.constant&quot;()<br/>  %4 = &quot;hbir.constant&quot;()<br/>  %5 = &quot;hbir.linear&quot;(%0, %3, %4)<br/>  %6 = &quot;hbir.constant&quot;()<br/>  %7 = &quot;hbir.constant&quot;()<br/>  %8 = &quot;hbir.linear&quot;(%1, %6, %7)<br/>  %9 = &quot;hbir.constant&quot;()<br/>  %10 = &quot;hbir.constant&quot;()<br/>  %11 = &quot;hbir.linear&quot;(%2, %9, %10)<br/>  %12 = &quot;hbir.constant&quot;()<br/>  %13 = &quot;hbir.concat&quot;(%8, %12)<br/>  %14 = &quot;hbir.constant&quot;()<br/>  %15 = &quot;hbir.concat&quot;(%11, %14)<br/>  %16 = &quot;hbir.pad&quot;(%arg4)<br/>  %17 = &quot;hbir.pad&quot;(%arg3)<br/>  %18 = &quot;hbir.reshape&quot;(%5)<br/>  %19 = &quot;hbir.transpose&quot;(%18)<br/>  %20 = &quot;hbir.reshape&quot;(%13)<br/>  %21 = &quot;hbir.transpose&quot;(%20)<br/>  %22 = &quot;hbir.reshape&quot;(%15)<br/>  %23 = &quot;hbir.transpose&quot;(%22)<br/>  %24 = &quot;hbir.constant&quot;()<br/>  %25 = &quot;hbir.concat&quot;(%21, %24)<br/>  %26 = &quot;hbir.constant&quot;()<br/>  %27 = &quot;hbir.concat&quot;(%23, %26)<br/>  %28 = &quot;hbir.pad&quot;(%16)<br/>  %29 = &quot;hbir.pad&quot;(%17)<br/>  %30 = &quot;hbir.reshape&quot;(%28)<br/>  %31 = &quot;hbir.reshape&quot;(%29)<br/>  %32 = &quot;hbir.logical_or&quot;(%30, %31)<br/>  %33 = &quot;hbir.constant&quot;()<br/>  %34 = &quot;hbir.constant&quot;()<br/>  %35 = &quot;hbir.where&quot;(%32, %34, %33)<br/>  %36 = &quot;hbir.constant&quot;()<br/>  %37 = &quot;hbir.mul&quot;(%19, %36)<br/>  %38 = &quot;hbir.transpose&quot;(%25)<br/>  %39 = &quot;hbir.matmul&quot;(%37, %38)<br/>  %40 = &quot;hbir.tile&quot;(%35)<br/>  %41 = &quot;hbir.reshape&quot;(%40)<br/>  %42 = &quot;hbir.add&quot;(%39, %41)<br/>  %43 = &quot;hbir.softmax&quot;(%42)<br/>  %44 = &quot;hbir.matmul&quot;(%43, %27)<br/>  %45 = &quot;hbir.transpose&quot;(%44)<br/>  %46 = &quot;hbir.reshape&quot;(%45)<br/>  %47 = &quot;hbir.constant&quot;()<br/>  %48 = &quot;hbir.constant&quot;()<br/>  %49 = &quot;hbir.linear&quot;(%46, %47, %48)<br/>  %50 = &quot;hbir.reshape&quot;(%43)<br/>  %51 = &quot;hbir.reduce_mean&quot;(%50)<br/>  %52 = &quot;hbir.transpose&quot;(%49)<br/>  return %52, %51<br/>}</td><td>hbir.matmul</td><td><b>lhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,M,C]<br/>Dim: * ∈ [1, 4096], M,C ∈ [1, 8192]<br/><b>rhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,C,N]<br/>Dim: * ∈ [1, 4096]; C,N ∈ [1, 8192]<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Shape: [*,M,N]<br/>Other constraints: Same as lhs<br/></td></tr><tr><td>hbir.softmax</td><td>Unsupported<br/></td></tr><tr><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.tile</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.linear</td><td><b>input:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [*,C_in]<br/>Dim: *, C_in ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16, input and weight cannot both be int16<br/>Shape: [C_out, C_in]<br/>Dim: C_in,C_out ∈ [1, 8192]; C_in × C_out ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as input<br/></td></tr><tr><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.pad</td><td><b>input:</b><br/>Type: int64, uint64 and f64 are not supported when expansionMode is &#x27;constant&#x27; else no constraints<br/>Dim: all dims &lt; 737280 when expansionMode is not &#x27;constant&#x27; else no constraints<br/><b>output:</b><br/>Same as input<br/><b>begin/end:</b><br/>Value should be in range [1, 4096]<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.logical_or</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">torch.nn.functional.selu<br/>torch.nn.SELU</td><td rowspan="1">torch.nn.SELU</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.nn.functional.sigmoid<br/>torch.nn.Sigmoid</td><td rowspan="1">torch.nn.Sigmoid</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.nn.Softplus</td><td rowspan="1"></td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">torch.tanh<br/>torch.nn.Tanh</td><td rowspan="1">torch.nn.Tanh</td><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.quantize&quot;(%arg0)<br/>  %1 = &quot;hbir.constant&quot;()<br/>  %2 = &quot;b30.lut&quot;(%0, %1)<br/>  %3 = &quot;qnt.dequantize&quot;(%2)<br/>  %4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>  return %4<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">torch.quantization.DeQuantStub</td><td rowspan="2">　</td><td rowspan="1">if float:<br/>func.func @forward(%arg0){<br/> return %arg0<br/>}</td><td></td><td><br/></td></tr><tr><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.barrier&quot;(%arg0)<br/>  return %0<br/>}</td><td>qnt.barrier</td><td>Unsupported<br/></td></tr><tr><td rowspan="2">torch.quantization.QuantStub</td><td rowspan="2">　</td><td rowspan="1">if float:<br/>func.func @forward(%arg0){<br/> return %arg0<br/>}</td><td></td><td><br/></td></tr><tr><td rowspan="1">if qat:<br/>func.func @forward(%arg0){<br/>  %0 = &quot;qnt.const_fake_quant&quot;(%arg0)<br/>  return %0<br/>}</td><td>qnt.const_fake_quant</td><td>Unsupported<br/></td></tr><tr><td rowspan="1">horizon.nn.AnchorGenerator</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/> %0 = &quot;hbir.constant&quot;()<br/> return %0<br/>}</td><td>hbir.constant</td><td>No limits<br/></td></tr><tr><td rowspan="1">horizon.nn.BaseGridGenerator</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/> %0 = &quot;hbir.constant&quot;()<br/> return %0<br/>}</td><td>hbir.constant</td><td>No limits<br/></td></tr><tr><td rowspan="2">horizon.nn.functional.bev_pool_v2</td><td rowspan="2">horizon.nn.BevPoolV2</td><td rowspan="2">func.func @forward(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6){<br/>  %0 = &quot;hbir.transpose&quot;(%arg1)<br/>  %1 = &quot;hbir.bev_pool_v2&quot;(%arg0, %0, %arg2, %arg3, %arg4, %arg5, %arg6)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.bev_pool_v2</td><td>Unsupported<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">horizon.nn.GridSample</td><td rowspan="2"></td><td rowspan="2">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.transpose&quot;(%arg0)<br/>  %1 = &quot;hbir.warp&quot;(%0, %arg1)<br/>  %2 = &quot;hbir.transpose&quot;(%1)<br/>  return %2<br/>}</td><td>hbir.warp</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/>Dim: H,W ∈ [1, 1024]; H*W ≤ 720*1024; other dims ∈ [1, 65536]<br/><b>grid:</b><br/>Type: int16<br/>Shape: [*,H,W,2]<br/><b>output:</b><br/>Same as input except Dim constraints<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">horizon.nn.SegmentLUT</td><td rowspan="1">　</td><td rowspan="1">func.func @forward(%arg0){<br/>%0 = &quot;qnt.quantize&quot;(%arg0)<br/>%1 = &quot;hbir.constant&quot;()<br/>%2 = &quot;b30.lut&quot;(%0, %1)<br/>%3 = &quot;qnt.dequantize&quot;(%2)<br/>%4 = &quot;qnt.const_fake_quant&quot;(%3)<br/>return %4</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">horizon.nn.functional.filter</td><td rowspan="2">　</td><td rowspan="2">func.func @forward(%arg0){<br/>%0 = &quot;hbir.transpose&quot;(%arg0)<br/>%maxValue, %maxIndex, %filterCoord, %filterData = &quot;hbir.filter&quot;(%0)<br/>return %maxValue, %maxIndex, %filterCoord, %filterData<br/>}</td><td>hbir.filter</td><td>Unsupported<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">torchvision.ops.DeformConv2d</td><td rowspan="3"></td><td rowspan="3">func.func @forward(%arg0, %arg1, %arg2){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.transpose&quot;(%arg0)<br/>  %2 = &quot;hbir.transpose&quot;(%arg1)<br/>  %3 = &quot;hbir.transpose&quot;(%arg2)<br/>  %4 = &quot;hbir.transpose&quot;(%0)<br/>  %5 = &quot;hbir.deform_conv2d&quot;(%1, %4, %2, %3)<br/>  %6 = &quot;hbir.transpose&quot;(%5)<br/>  return %6<br/>}</td><td>hbir.constant</td><td>No limits<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.deform_conv2d</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/>Dim: H,W ∈ [1, 1024]; H&amp;timesW ≤ 720&amp;times1024 other dims ∈ [1, 65536]<br/><b>offset:</b><br/>Type: int16<br/>Shape: [*,OH,OW,2&amp;timesoffsetGroupNum&amp;timesKH&amp;timesKW]<br/>Size: 2&amp;timesoffsetGroupNum&amp;timesKH&amp;timesKW ∈ [2, 256], OH&amp;timesKH&amp;timesOW&amp;timesKW ≤ 720&amp;time1024<br/><b>mask:</b><br/>Type: int8<br/>Shape: [*,OH,OW,offsetGroupNum&amp;timesKH&amp;timesKW]<br/>Size: offsetGroupNum&amp;timesKH&amp;timesKW ∈ [1, 128]<br/><b>weight:</b><br/>Type: int8<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 8]; N ∈ [1, 4096]<br/>Size: KH&amp;timesKW&amp;timesC ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1]<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>offsetGroupNum:</b><br/>fin.c is divisible by offset group number<br/>Size: offsetGroupNum ∈ [1, 2]<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1]<br/><b>others:</b><br/>For each group, fin.c ∈ [1, 8192], KH&amp;timesKW&amp;timesfin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td rowspan="5">torch.Tensor.__getitem__</td><td rowspan="5">　</td><td rowspan="3">if use as slice:<br/>func.func @forward(%arg0){<br/>%0 = &quot;hbir.slice&quot;(%arg0)<br/>%1 = &quot;hbir.select&quot;(%0)<br/>%2 = &quot;hbir.reshape&quot;(%1)<br/>return %2<br/>}</td><td>hbir.select</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">if use as index:<br/>func.func @forward(%arg0){<br/>%0 = &quot;hbir.select&quot;(%arg0)<br/>%1 = &quot;hbir.reshape&quot;(%0)<br/>return %1<br/>}</td><td>hbir.select</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">torch.Tensor.__setitem__</td><td rowspan="2">horizon.nn.SetItem</td><td rowspan="2">func.func @forward(%arg0, %arg1){<br/>  %0 = &quot;hbir.constant&quot;()<br/>  %1 = &quot;hbir.reshape&quot;(%arg1)<br/>  %2 = &quot;hbir.scatter_nd&quot;(%arg0, %0, %1)<br/>  return %2<br/>}</td><td>hbir.scatter_nd</td><td>Unsupported<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr></tbody></table></div><!--/$--></div><div class="rspress-doc-footer"><footer class="mt-8"><div class="xs:flex pb-5 px-2 justify-end items-center"></div><div class="flex flex-col"></div><div class="flex flex-col sm:flex-row sm:justify-around gap-4 pt-6"><div class="prev_e7091 flex flex-col"></div><div class="next_e7091 flex flex-col"></div></div></footer></div></div></div><div class="aside-container_edeb4"><div><div class="flex flex-col"><div class="hidden"><div id="aside-container" class="relative text-sm font-medium"><div class="leading-7 block text-sm font-semibold pl-3">On This Page</div><nav class="mt-1"><ul class="relative"></ul></nav></div></div></div></div></div></div></div></section></div></div><div id="search-container"></div></body></html>