<!doctype html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Rspress v1.23.1"><title data-rh="true">Horizon OpenExplorer</title><meta data-rh="true" name="description" content="地平线OpenExplorer算法工具链"/><script>{const e=localStorage.getItem("rspress-theme-appearance"),t=window.matchMedia("(prefers-color-scheme: dark)").matches,a=e&&"auto"!==e?"dark"===e:t;document.documentElement.classList.toggle("dark",a),document.documentElement.style.colorScheme=a?"dark":"light"}</script><title>Horizon OpenExplorer</title><link rel="icon" href="/favicon.ico"><script defer="defer" src="/static/js/styles.47a58349.js"></script><script defer="defer" src="/static/js/lib-polyfill.f3f8d881.js"></script><script defer="defer" src="/static/js/lib-lodash.6d460d4e.js"></script><script defer="defer" src="/static/js/lib-react.57316932.js"></script><script defer="defer" src="/static/js/lib-router.687d37e2.js"></script><script defer="defer" src="/static/js/11162.ccfbaa80.js"></script><script defer="defer" src="/static/js/index.de38ed8a.js"></script><link href="/static/css/styles.d4060814.css" rel="stylesheet"></head><body ><div id="root"><div><div class="navContainer_f6cde rspress-nav px-6 " style="position:sticky"><div class="container_f6cde flex justify-between items-center h-full"><div class="navBarTitle_f6cde"><a href="/en" class="flex items-center w-full h-full text-base font-semibold transition-opacity duration-300 hover:opacity-60"><img src="/logo-light.png" alt="logo" id="logo" class="mr-4 rspress-logo dark:hidden"/><img src="/logo-dark.svg" alt="logo" id="logo" class="mr-4 rspress-logo hidden dark:block"/></a></div><div class="flex flex-1 justify-end items-center"><div class="rightNav_f6cde"><div class="flex sm:flex-1 items-center sm:pl-4 sm:pr-2"><div class="rspress-nav-search-button navSearchButton_6e282"><button><svg width="18" height="18" viewBox="0 0 32 32"><path fill="var(--rp-c-gray)" d="m29 27.586-7.552-7.552a11.018 11.018 0 1 0-1.414 1.414L27.586 29ZM4 13a9 9 0 1 1 9 9 9.01 9.01 0 0 1-9-9Z"></path></svg><p class="searchWord_6e282">Search Docs</p><div style="opacity:0"><span></span><span>K</span></div></button></div><div class="mobileNavSearchButton_6e282"><svg width="24" height="24" viewBox="0 0 32 32"><path fill="var(--rp-c-gray)" d="m29 27.586-7.552-7.552a11.018 11.018 0 1 0-1.414 1.414L27.586 29ZM4 13a9 9 0 1 1 9 9 9.01 9.01 0 0 1-9-9Z"></path></svg></div></div><div class="rspress-nav-menu menu h-14"><a href="https://auto-developer.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Smart Car Developer Community</div></a><a href="https://en.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Official Website</div></a><a class="link_03735  cursor-pointer" target="" href="/en/index.html"><div class="rspress-nav-menu-item singleItem_f6cde activeItem_f6cde text-sm font-medium mx-1.5 px-3 py-2 flex items-center">version: 3.0.22</div></a></div><div class="flex-center flex-row"><div class="translation menu-item_f6cde flex text-sm font-bold items-center px-3 py-2"><div><div class="relative flex-center h-14"><button class="rspress-nav-menu-group-button flex-center items-center font-medium text-sm text-text-1 hover:text-text-2 transition-colors duration-200"><span class="text-sm font-medium flex" style="margin-right:2px"><svg width="18" height="18" viewBox="0 0 32 32" style="width:18px;height:18px"><path fill="currentColor" d="M27.85 29H30l-6-15h-2.35l-6 15h2.15l1.6-4h6.85zm-7.65-6 2.62-6.56L25.45 23zM18 7V5h-7V2H9v3H2v2h10.74a14.71 14.71 0 0 1-3.19 6.18A13.5 13.5 0 0 1 7.26 9h-2.1a16.47 16.47 0 0 0 3 5.58A16.84 16.84 0 0 1 3 18l.75 1.86A18.47 18.47 0 0 0 9.53 16a16.92 16.92 0 0 0 5.76 3.84L16 18a14.48 14.48 0 0 1-5.12-3.37A17.64 17.64 0 0 0 14.8 7z"></path></svg></span><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M16 22 6 12l1.4-1.4 8.6 8.6 8.6-8.6L26 12z"></path></svg></button><div class="rspress-nav-menu-group-content absolute mx-0.8 transition-opacity duration-300" style="opacity:0;visibility:hidden;right:0;top:52px"><div class="p-3 pr-2 w-full h-full max-h-100vh whitespace-nowrap" style="box-shadow:var(--rp-shadow-3);z-index:100;border:1px solid var(--rp-c-divider-light);border-radius:var(--rp-radius-large);background:var(--rp-c-bg)"><div><div class="font-medium my-1"><a class="link_03735  cursor-pointer" target="" href="/guide/appendix/supported_op_list/onnx_operator_support_list_html.html"><div class="rounded-2xl hover:bg-mute" style="padding:0.4rem 1.5rem 0.4rem 0.75rem"><div class="flex"><span>中文</span></div></div></a></div></div><div><div class="rounded-2xl my-1 flex" style="padding:0.4rem 1.5rem 0.4rem 0.75rem"><span class="text-brand">English</span></div></div></div></div></div></div></div><div class="mx-2"><div class="md:mr-2 rspress-nav-appearance"><div class="p-1 border border-solid border-gray-300 text-gray-400  cursor-pointer rounded-md hover:border-gray-600 hover:text-gray-600 dark:hover:border-gray-200 dark:hover:text-gray-200 transition-all duration-300 w-7 h-7"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 24 24" class="dark:hidden" width="18" height="18" fill="currentColor"><path d="M12 18c-3.3 0-6-2.7-6-6s2.7-6 6-6 6 2.7 6 6-2.7 6-6 6zm0-10c-2.2 0-4 1.8-4 4s1.8 4 4 4 4-1.8 4-4-1.8-4-4-4zM12 4c-.6 0-1-.4-1-1V1c0-.6.4-1 1-1s1 .4 1 1v2c0 .6-.4 1-1 1zM12 24c-.6 0-1-.4-1-1v-2c0-.6.4-1 1-1s1 .4 1 1v2c0 .6-.4 1-1 1zM5.6 6.6c-.3 0-.5-.1-.7-.3L3.5 4.9c-.4-.4-.4-1 0-1.4s1-.4 1.4 0l1.4 1.4c.4.4.4 1 0 1.4-.1.2-.4.3-.7.3zM19.8 20.8c-.3 0-.5-.1-.7-.3l-1.4-1.4c-.4-.4-.4-1 0-1.4s1-.4 1.4 0l1.4 1.4c.4.4.4 1 0 1.4-.2.2-.5.3-.7.3zM3 13H1c-.6 0-1-.4-1-1s.4-1 1-1h2c.6 0 1 .4 1 1s-.4 1-1 1zM23 13h-2c-.6 0-1-.4-1-1s.4-1 1-1h2c.6 0 1 .4 1 1s-.4 1-1 1zM4.2 20.8c-.3 0-.5-.1-.7-.3-.4-.4-.4-1 0-1.4l1.4-1.4c.4-.4 1-.4 1.4 0s.4 1 0 1.4l-1.4 1.4c-.2.2-.4.3-.7.3zM18.4 6.6c-.3 0-.5-.1-.7-.3-.4-.4-.4-1 0-1.4l1.4-1.4c.4-.4 1-.4 1.4 0s.4 1 0 1.4l-1.4 1.4c-.2.2-.5.3-.7.3z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" viewBox="0 0 24 24" class="hidden dark:block" width="18" height="18" fill="currentColor"><path d="M12.1 22h-.9c-5.5-.5-9.5-5.4-9-10.9.4-4.8 4.2-8.6 9-9 .4 0 .8.2 1 .5.2.3.2.8-.1 1.1-2 2.7-1.4 6.4 1.3 8.4 2.1 1.6 5 1.6 7.1 0 .3-.2.7-.3 1.1-.1.3.2.5.6.5 1-.2 2.7-1.5 5.1-3.6 6.8-1.9 1.4-4.1 2.2-6.4 2.2zM9.3 4.4c-2.9 1-5 3.6-5.2 6.8-.4 4.4 2.8 8.3 7.2 8.7 2.1.2 4.2-.4 5.8-1.8 1.1-.9 1.9-2.1 2.4-3.4-2.5.9-5.3.5-7.5-1.1-2.8-2.2-3.9-5.9-2.7-9.2z"></path></svg></div></div></div></div></div><div class="mobileNavMenu_f6cde"><div class="navScreen_457e8 " id="navScreen"><div class="container_457e8"><div class="navMenu_457e8"><div class="navMenuItem_457e8 w-full"><a href="https://auto-developer.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Smart Car Developer Community</div></a></div><div class="navMenuItem_457e8 w-full"><a href="https://en.horizon.cc/" target="_blank" rel="noopener noreferrer" class="link_03735 "><div class="rspress-nav-menu-item singleItem_f6cde  text-sm font-medium mx-1.5 px-3 py-2 flex items-center">Horizon Official Website</div></a></div><div class="navMenuItem_457e8 w-full"><a class="link_03735  cursor-pointer" target="" href="/en/index.html"><div class="rspress-nav-menu-item singleItem_f6cde activeItem_f6cde text-sm font-medium mx-1.5 px-3 py-2 flex items-center">version: 3.0.22</div></a></div></div><div class="flex-center flex-col gap-2"><div class="mt-2 navAppearance_457e8 flex justify-center"></div><div class="flex text-sm font-bold justify-center"><div class="mx-1.5 my-1"><div class=" navScreenMenuGroup_457e8 relative"><button class="button_457e8"><span class="buttonSpan_457e8"><svg width="18" height="18" viewBox="0 0 32 32" style="width:18px;height:18px"><path fill="currentColor" d="M27.85 29H30l-6-15h-2.35l-6 15h2.15l1.6-4h6.85zm-7.65-6 2.62-6.56L25.45 23zM18 7V5h-7V2H9v3H2v2h10.74a14.71 14.71 0 0 1-3.19 6.18A13.5 13.5 0 0 1 7.26 9h-2.1a16.47 16.47 0 0 0 3 5.58A16.84 16.84 0 0 1 3 18l.75 1.86A18.47 18.47 0 0 0 9.53 16a16.92 16.92 0 0 0 5.76 3.84L16 18a14.48 14.48 0 0 1-5.12-3.37A17.64 17.64 0 0 0 14.8 7z"></path></svg></span><svg width="1em" height="1em" viewBox="0 0 32 32" class=" down_457e8 "><path fill="currentColor" d="M16 22 6 12l1.4-1.4 8.6 8.6 8.6-8.6L26 12z"></path></svg></button><div><div class="items_457e8"><div><div class="py-1 font-medium"><a class="link_03735  cursor-pointer" target="" href="/guide/appendix/supported_op_list/onnx_operator_support_list_html.html"><div><div class="flex justify-center"><span>中文</span></div></div></a></div></div><div><div class="p-1 text-center"><span class="text-brand">English</span></div></div></div></div></div></div></div></div></div></div><button aria-label="mobile hamburger" class=" navHamburger_e7b06 text-gray-500"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" fill="currentColor"><circle cx="8" cy="16" r="2" fill="currentColor"></circle><circle cx="16" cy="16" r="2" fill="currentColor"></circle><circle cx="24" cy="16" r="2" fill="currentColor"></circle></svg></button></div></div></div></div><section><div class="docLayout_edeb4 pt-0"><div class="rspress-sidebar-menu"><button class="flex-center mr-auto"><div class="text-md mr-2"><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M4 6h24v2H4zm0 18h24v2H4zm0-12h24v2H4zm0 6h24v2H4z"></path></svg></div><span class="text-sm">Menu</span></button><button class="flex-center ml-auto"><span class="text-sm">On This Page</span><div class="text-md mr-2" style="transform:rotate(0deg);transition:transform 0.2s ease-out;margin-top:2px"><svg width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M22 16 12 26l-1.4-1.4 8.6-8.6-8.6-8.6L12 6z"></path></svg></div></button></div><aside class="sidebar_71eca rspress-sidebar "><div class="navTitleMask_71eca"><div class="navBarTitle_f6cde"><a href="/en" class="flex items-center w-full h-full text-base font-semibold transition-opacity duration-300 hover:opacity-60"><img src="/logo-light.png" alt="logo" id="logo" class="mr-4 rspress-logo dark:hidden"/><img src="/logo-dark.svg" alt="logo" id="logo" class="mr-4 rspress-logo hidden dark:block"/></a></div></div><div class="rspress-scrollbar sidebarContent_71eca"><nav class="pb-2"></nav></div></aside><div class="content_edeb4 rspress-doc-container flex flex-shrink-0 mx-auto"><div class="w-full flex-1"><div><div class="rspress-doc"><!--$--><span data-dark="false" class="rp-reading-time">Estimated reading time: less than 1 minute</span>
<!-- -->
<div class="table-container"><table class="scrollable-table"><thead><tr><th>ONNX Operator Name</th><th>HMCT convert description</th><th>PTQ ONNX Operator</th><th>Map Description &amp; Graph Fusion Description</th><th>HBIR Operator Name</th><th>BPU Support Constraints</th></tr></thead><tbody><tr><td rowspan="1">Abs</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Acos</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Acosh</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Add</td><td rowspan="1">None</td><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">And</td><td rowspan="1">None</td><td rowspan="1">And</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    # lhs = adaptor.operands[0].value.astype(np.int8)<br/>    # rhs = adaptor.operands[1].value.astype(np.int8)<br/>    return hbir.logical_and(lhs, rhs, output_type=y)</td><td>hbir.logical_and</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">ArgMin</td><td rowspan="1">None</td><td rowspan="1">ArgMin</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *, axis=0, keepdims=1<br/>):<br/>    return hbir.reduce_argmin(<br/>        x,<br/>        dims=[axis],<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )</td><td>hbir.reduce_argmin</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">Asin</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Asinh</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Atan</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Atanh</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">AveragePool</td><td rowspan="2">None</td><td rowspan="2">AveragePool</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    kernel_shape,<br/>    pads=[0, 0],<br/>    strides=[1],<br/>    auto_pad=&quot;NOTSET&quot;,<br/>    ceil_mode=0,<br/>    count_include_pad=0,<br/>):<br/>    if auto_pad != &quot;NOTSET&quot;:<br/>        raise ValueError(<br/>            &quot;Operator AveragePool does not support attribute auto_pad. It is a deprecated attribute. &quot;<br/>        )<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim &lt; 1 or kernel_dim &gt; 2:<br/>        raise ValueError(<br/>            f&quot;Operator AveragePool does not support kernel_dim {kernel_dim}&quot;<br/>        )<br/>    dilations = [1]<br/>    if kernel_dim == 1:<br/>        x = hbir.transpose(x, [0, 2, 1])<br/>    elif kernel_dim == 2:<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        if dilations == [1]:<br/>            dilations = dilations * kernel_dim<br/>        if strides == [1]:<br/>            strides = strides * kernel_dim<br/>        if pads == [0, 0]:<br/>            pads = pads * kernel_dim<br/>    # input trans: nchw-&gt;nhwc<br/>    # output trans: nhwc-&gt;nchw<br/>    x = hbir.avg_pool(<br/>        x, kernel_shape, strides, pads, dilation=dilations, ceilMode=bool(ceil_mode)<br/>    )<br/><br/>    if kernel_dim == 1:<br/>        return hbir.transpose(x, [0, 2, 1], output_type=y)<br/>    return hbir.transpose(x, [0, 3, 1, 2], output_type=y)</td><td>hbir.avg_pool</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*,H,W,C]<br/><b>output:</b><br/>Same as input<br/><b>kernel:</b><br/>Shape: [KH,KW]<br/>Dim: KH, KW ∈ [1, 256]<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH, SW ∈ [1, 256]<br/><b>pad:</b><br/>Shape: [PN,PH,PW,PC]<br/>PN,PH,PW,PC ∈ [-3, 256]<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Cast</td><td rowspan="1">None</td><td rowspan="1">Cast</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *, to):<br/>    return hbir.cast_type(x, output_type=y)</td><td>hbir.cast_type</td><td><b>input:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Ceil</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Celu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Clip</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Concat</td><td rowspan="1">None</td><td rowspan="1">Concat</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, *args, axis):<br/>    return hbir.concat(args, dim=axis, output_type=y)</td><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="3">Conv</td><td rowspan="3">None</td><td rowspan="3">Conv</td><td rowspan="3">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    w: mlir.Value,<br/>    b: Optional[mlir.Value] = None,<br/>    *,<br/>    dilations=(1, 1),<br/>    group=1,<br/>    kernel_shape,<br/>    pads=(0, 0, 0, 0),<br/>    strides=(1, 1),<br/>):<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim == 2:<br/>        # input trans: nchw-&gt;nhwc<br/>        # output trans: nhwc-&gt;nchw<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 1])<br/>        x = hbir.conv2d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    elif kernel_dim == 3:<br/>        # input trans: ncdhw-&gt;ndhwc<br/>        # output trans: ndhwc-&gt;ncdhw<br/>        x = hbir.transpose(x, [0, 2, 3, 4, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 4, 1])<br/>        x = hbir.conv3d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 4, 1, 2, 3], output_type=y)<br/>    else:<br/>        raise ValueError(&quot;Only Conv2d and Conv3d supported for now.&quot;)</td><td>hbir.conv3d</td><td><b>lhs:</b><br/>Type: int8<br/>Shape: [*,D,H,W,C]<br/>Dim: * ∈ [1, 128]; D,H,W ∈ [1, 65536]; C ∈ [1, 4096];<br/><b>rhs:</b><br/>Type: int8<br/>Shape: [N,KD,KH,KW,C]<br/>N ∈ [1, 65536]; KD,KH,KW ∈ [1, 9]; Dim: C ∈ [1, 4096];<br/>Size: KD × KH × KW × C ∈ [1, 131072]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as lhs<br/><b>stride:</b><br/>Shape: [SD,SH,SW]<br/>Dim: SD,SH,SW must be 1 or 2 and equal to each other.<br/><b>pad:</b><br/>Shape: [P_front, P_top, P_left, P_back, P_bottom, P_right]<br/>Dim: P_front,P_back ∈ [0, KD/2], P_top,P_bottom ∈ [0, KH/2], P_left,P_right ∈ [0, KW/2]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DD,DH,DW]<br/>DD,DH,DW = 1 when group number more than 1<br/></td></tr><tr><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ConvTranspose</td><td rowspan="2">None</td><td rowspan="2">ConvTranspose</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    w: mlir.Value,<br/>    b: Optional[mlir.Value] = None,<br/>    *,<br/>    auto_pad=&quot;NOTSET&quot;,<br/>    dilations=(1, 1),<br/>    group=1,<br/>    kernel_shape=None,<br/>    output_padding=None,<br/>    output_shape=None,<br/>    pads=(0, 0, 0, 0),<br/>    strides=(1, 1),<br/>):<br/>    auto_pad = auto_pad if type(auto_pad) == str else auto_pad.decode()<br/>    if auto_pad != &quot;NOTSET&quot;:<br/>        raise ValueError(<br/>            f&quot;Operator ConvTranspose does not support auto_pad with value: {auto_pad}&quot;<br/>        )<br/><br/>    if output_padding is not None:<br/>        for idx, p in enumerate(output_padding):<br/>            pads[(len(pads) // 2) + idx] -= p<br/><br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim != 2:<br/>        raise ValueError(<br/>            f&quot;Only support ConvTranspose 2D Operator, got kernel_dim={kernel_dim}&quot;<br/>        )<br/>    # input trans: nchw-&gt;nhwc<br/>    # output trans: nhwc-&gt;nchw<br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    w = hbir.transpose(w, [0, 2, 3, 1])<br/>    x = hbir.conv2dtranspose(<br/>        x,<br/>        weight=w,<br/>        stride=strides,<br/>        pad=pads,<br/>        dilation=dilations,<br/>        groupNum=group,<br/>        bias=b,<br/>        illegalWeight=True,<br/>    )<br/>    x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    return x</td><td>hbir.conv2dtranspose</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 128]; H,W ∈ [1, 65536]; C ∈ [1, 2048]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: N,C ∈ [1, 2048]; KH,KW ∈ [1, 14]; KH,KW cannot both be 1<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as input, the type additionally supports int32<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 14]; SH &lt; KH; SW &lt; KW;<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_left,P_bottom,P_right ∈ [0, 256]<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ {1}<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Cos</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Cosh</td><td rowspan="1">None</td><td rowspan="1">Cosh</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.cosh(x, output_type=y)</td><td>hbir.cosh</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">CumSum</td><td rowspan="1">None</td><td rowspan="1">CumSum</td><td rowspan="1">//opset11<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    axis: mlir.Value,<br/>    *,<br/>    exclusive=0,<br/>    reverse=0,<br/>):<br/>    axis = adaptor.operands[1].value.tolist()<br/>    return hbir.cumsum(x, axis, exclusive=exclusive, reverse=reverse)</td><td>hbir.cumsum</td><td><b>input:</b><br/>Type: int8<br/>            Shape: [*, dim[axis], *]<br/>Dim: * ∈ [1, 65536]; dim[axis] ∈ [1, 8192]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">Div</td><td rowspan="2">The operator is splited</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Elu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Equal</td><td rowspan="1">None</td><td rowspan="1">Equal</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.equal(lhs, rhs, output_type=y)</td><td>hbir.equal</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">Erf</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Exp</td><td rowspan="1">None</td><td rowspan="1">Exp</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>):<br/>    return hbir.exp(<br/>        x,<br/>        output_type=y,<br/>    )</td><td>hbir.exp</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">Expand</td><td rowspan="2">None</td><td rowspan="2">Expand</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, data: mlir.Value, shape: mlir.Value<br/>):<br/>    original_shape = adaptor.operands[0].type.shape<br/>    new_shape = adaptor.operands[1].value<br/>    original_shape = [<br/>        1,<br/>    ] * (len(new_shape) - len(original_shape)) + original_shape<br/><br/>    repeat_list = []<br/>    for i in range(len(original_shape)):<br/>        if (<br/>            original_shape[i] != new_shape[i]<br/>            and original_shape[i] != 1<br/>            and new_shape[i] != 1<br/>        ):<br/>            raise ValueError(<br/>                f&quot;Operator Expand does not support this shape. original_shape: {original_shape}, new_shape: {new_shape}&quot;<br/>            )<br/>        if original_shape[i] == 1:<br/>            repeat_list.append(new_shape[i])<br/>        else:<br/>            repeat_list.append(1)<br/>    data = hbir.reshape(<br/>        data,<br/>        original_shape,<br/>        output_type=mlir.UnrankedTensorType.get(mlir.ShapedType(y).element_type),<br/>    )<br/>    return hbir.tile(data, repeat_list, output_type=y)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.tile</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Flatten</td><td rowspan="1">The operator is replaced</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Floor</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="10">GRU</td><td rowspan="10">The operator is splited</td><td rowspan="1">Split</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0, split<br/>):<br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    axis = axis if axis &gt;= 0 else (dim + axis)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(shape=dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset13<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        num_outputs = len(adaptor.results)<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i]<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    axis=0,<br/>    num_outputs=0,<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list</td><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Transpose</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, perm):<br/>    return hbir.transpose(x, perm, output_type=y)</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">MatMul</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, output_type: mlir.Type, a: mlir.Value, b: mlir.Value<br/>):<br/>    lhs_shape = np.array(adaptor.operands[0].type.shape)<br/>    rhs_shape = np.array(adaptor.operands[0].type.shape)<br/>    lhs_rank = len(lhs_shape)<br/>    rhs_rank = len(rhs_shape)<br/>    if lhs_rank == 1 and rhs_rank == 1:<br/>        # the output of 1D matmul is a scalar, the out_shape is none, so add reshape<br/>        out_shape = adaptor.results[0].type.shape<br/>        return hbir.reshape(hbir.matmul(a, b), out_shape, output_type=output_type)<br/>    else:<br/>        return hbir.matmul(a, b, output_type=output_type)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.matmul</td><td><b>lhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,M,C]<br/>Dim: * ∈ [1, 4096], M,C ∈ [1, 8192]<br/><b>rhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,C,N]<br/>Dim: * ∈ [1, 4096]; C,N ∈ [1, 8192]<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Shape: [*,M,N]<br/>Other constraints: Same as lhs<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Concat</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, *args, axis):<br/>    return hbir.concat(args, dim=axis, output_type=y)</td><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Gather</td><td rowspan="1">None</td><td rowspan="1">Gather</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    indices: mlir.Value,<br/>    axis=0,<br/>):<br/>    return hbir.index(x, index=indices, dim=axis, output_type=y)<br/>//opset11<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    indices: mlir.Value,<br/>    axis=0,<br/>):<br/>    return hbir.index(x, index=indices, dim=axis, output_type=y)<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    indices: mlir.Value,<br/>    axis=0,<br/>):<br/>    return hbir.index(x, index=indices, dim=axis, output_type=y)</td><td>hbir.index</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*]<br/>Dim: dims ∈ [1, 65536]<br/><b>index:</b><br/>Type: int8, int16, int32, int64<br/>Shape: [*]<br/><b>output:</b><br/>Same as input except Dim constraints<br/></td></tr><tr><td rowspan="1">Gelu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">Gemm</td><td rowspan="3">The operator is replaced</td><td rowspan="3">Conv</td><td rowspan="3">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    w: mlir.Value,<br/>    b: Optional[mlir.Value] = None,<br/>    *,<br/>    dilations=(1, 1),<br/>    group=1,<br/>    kernel_shape,<br/>    pads=(0, 0, 0, 0),<br/>    strides=(1, 1),<br/>):<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim == 2:<br/>        # input trans: nchw-&gt;nhwc<br/>        # output trans: nhwc-&gt;nchw<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 1])<br/>        x = hbir.conv2d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    elif kernel_dim == 3:<br/>        # input trans: ncdhw-&gt;ndhwc<br/>        # output trans: ndhwc-&gt;ncdhw<br/>        x = hbir.transpose(x, [0, 2, 3, 4, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 4, 1])<br/>        x = hbir.conv3d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 4, 1, 2, 3], output_type=y)<br/>    else:<br/>        raise ValueError(&quot;Only Conv2d and Conv3d supported for now.&quot;)</td><td>hbir.conv3d</td><td><b>lhs:</b><br/>Type: int8<br/>Shape: [*,D,H,W,C]<br/>Dim: * ∈ [1, 128]; D,H,W ∈ [1, 65536]; C ∈ [1, 4096];<br/><b>rhs:</b><br/>Type: int8<br/>Shape: [N,KD,KH,KW,C]<br/>N ∈ [1, 65536]; KD,KH,KW ∈ [1, 9]; Dim: C ∈ [1, 4096];<br/>Size: KD × KH × KW × C ∈ [1, 131072]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as lhs<br/><b>stride:</b><br/>Shape: [SD,SH,SW]<br/>Dim: SD,SH,SW must be 1 or 2 and equal to each other.<br/><b>pad:</b><br/>Shape: [P_front, P_top, P_left, P_back, P_bottom, P_right]<br/>Dim: P_front,P_back ∈ [0, KD/2], P_top,P_bottom ∈ [0, KH/2], P_left,P_right ∈ [0, KW/2]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DD,DH,DW]<br/>DD,DH,DW = 1 when group number more than 1<br/></td></tr><tr><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">GlobalAveragePool</td><td rowspan="1">None</td><td rowspan="1">GlobalAveragePool</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.reduce_mean(x, dims=[-2, -1], keepDim=True, output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">GlobalMaxPool</td><td rowspan="1">None</td><td rowspan="1">GlobalMaxPool</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    # do not use transpose, transpose dims directly<br/>    input_shape = adaptor.operands[0].type.shape<br/>    input_rank = len(input_shape)<br/>    if input_rank &lt; 3:<br/>        raise ValueError(&quot;input size can not be less than 3&quot;)<br/>    dims = []<br/>    for i in range(input_rank - 2):<br/>        dims.append(-(i + 1))<br/>    return hbir.reduce_max(x, dims=dims, keepDim=True, output_type=y)</td><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">Greater</td><td rowspan="1">None</td><td rowspan="1">Greater</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.greater(lhs, rhs, output_type=y)</td><td>hbir.greater</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">GreaterOrEqual</td><td rowspan="1">None</td><td rowspan="1">GreaterOrEqual</td><td rowspan="1">//opset12<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.greater_equal(lhs, rhs, output_type=y)</td><td>hbir.greater_equal</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="2">GridSample</td><td rowspan="2">None</td><td rowspan="2">GridSample</td><td rowspan="2">//opset16<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    Grid: mlir.Value,<br/>    *,<br/>    align_corners=0,<br/>    mode=&quot;bilinear&quot;,<br/>    padding_mode=&quot;zeros&quot;,<br/>):<br/>    if isinstance(mode, bytes):<br/>        mode = mode.decode()<br/>    # Currently only supports nearest and bilinear<br/>    if mode not in [&quot;nearest&quot;, &quot;bilinear&quot;]:<br/>        raise ValueError(<br/>            f&quot;Operator GridSample does not support resize mode: {mode}&quot;<br/>        )<br/>    padding_mode = (<br/>        padding_mode if type(padding_mode) == str else padding_mode.decode()<br/>    )<br/><br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    if padding_mode == &quot;zeros&quot;:<br/>        x = hbir.grid_sample(<br/>            x, Grid, mode=mode, alignCorner=bool(align_corners), padValue=0<br/>        )<br/>    elif padding_mode == &quot;border&quot;:<br/>        x = hbir.grid_sample(<br/>            x,<br/>            Grid,<br/>            mode=mode,<br/>            alignCorner=bool(align_corners),<br/>            expansionMode=padding_mode,<br/>        )<br/>    else:<br/>        raise ValueError(<br/>            f&quot;Operator GridSample does not support padding_mode {padding_mode}&quot;<br/>        )<br/>    return hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>//horizon<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    Grid: mlir.Value,<br/>    *,<br/>    align_corners=0,<br/>    mode=&quot;bilinear&quot;,<br/>    padding_mode=&quot;zeros&quot;,<br/>):<br/>    padding_mode = (<br/>        padding_mode if type(padding_mode) == str else padding_mode.decode()<br/>    )<br/><br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    if padding_mode == &quot;zeros&quot;:<br/>        x = hbir.grid_sample(<br/>            x, Grid, mode=mode, alignCorner=bool(align_corners), padValue=0<br/>        )<br/>    elif padding_mode == &quot;border&quot;:<br/>        x = hbir.grid_sample(<br/>            x,<br/>            Grid,<br/>            mode=mode,<br/>            alignCorner=bool(align_corners),<br/>            expansionMode=padding_mode,<br/>        )<br/>    else:<br/>        raise ValueError(<br/>            f&quot;Operator GridSample does not support padding_mode {padding_mode}&quot;<br/>        )<br/>    return hbir.transpose(x, [0, 3, 1, 2], output_type=y)</td><td>hbir.grid_sample</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/>Dim: H,W ∈ [1, 1024]; H*W ≤ 720*1024; other dims ∈ [1, 65536]<br/><b>grid:</b><br/>Type: int16<br/>Shape: [*,H,W,2]<br/><b>output:</b><br/>Same as input except Dim constraints<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="7">GroupNormalization</td><td rowspan="7">The operator is splited</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceMean</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_mean(x, dims=axes, keepDim=bool(keepdims), output_type=y)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">HardSigmoid</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">HardSwish</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="6">InstanceNormalization</td><td rowspan="6">The operator is splited</td><td rowspan="2">ReduceMean</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_mean(x, dims=axes, keepDim=bool(keepdims), output_type=y)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="9">LSTM</td><td rowspan="9">The operator is splited</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Split</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0, split<br/>):<br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    axis = axis if axis &gt;= 0 else (dim + axis)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(shape=dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset13<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        num_outputs = len(adaptor.results)<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i]<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    axis=0,<br/>    num_outputs=0,<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list</td><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Concat</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, *args, axis):<br/>    return hbir.concat(args, dim=axis, output_type=y)</td><td>hbir.concat</td><td><b>input:</b><br/>Arg Number: input number ∈ [1, 1024]<br/>Dim: all dims &lt; 131072 <br/>size &lt; 2G<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Transpose</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, perm):<br/>    return hbir.transpose(x, perm, output_type=y)</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="3">Conv</td><td rowspan="3">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    w: mlir.Value,<br/>    b: Optional[mlir.Value] = None,<br/>    *,<br/>    dilations=(1, 1),<br/>    group=1,<br/>    kernel_shape,<br/>    pads=(0, 0, 0, 0),<br/>    strides=(1, 1),<br/>):<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim == 2:<br/>        # input trans: nchw-&gt;nhwc<br/>        # output trans: nhwc-&gt;nchw<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 1])<br/>        x = hbir.conv2d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    elif kernel_dim == 3:<br/>        # input trans: ncdhw-&gt;ndhwc<br/>        # output trans: ndhwc-&gt;ncdhw<br/>        x = hbir.transpose(x, [0, 2, 3, 4, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 4, 1])<br/>        x = hbir.conv3d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 4, 1, 2, 3], output_type=y)<br/>    else:<br/>        raise ValueError(&quot;Only Conv2d and Conv3d supported for now.&quot;)</td><td>hbir.conv3d</td><td><b>lhs:</b><br/>Type: int8<br/>Shape: [*,D,H,W,C]<br/>Dim: * ∈ [1, 128]; D,H,W ∈ [1, 65536]; C ∈ [1, 4096];<br/><b>rhs:</b><br/>Type: int8<br/>Shape: [N,KD,KH,KW,C]<br/>N ∈ [1, 65536]; KD,KH,KW ∈ [1, 9]; Dim: C ∈ [1, 4096];<br/>Size: KD × KH × KW × C ∈ [1, 131072]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as lhs<br/><b>stride:</b><br/>Shape: [SD,SH,SW]<br/>Dim: SD,SH,SW must be 1 or 2 and equal to each other.<br/><b>pad:</b><br/>Shape: [P_front, P_top, P_left, P_back, P_bottom, P_right]<br/>Dim: P_front,P_back ∈ [0, KD/2], P_top,P_bottom ∈ [0, KH/2], P_left,P_right ∈ [0, KW/2]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DD,DH,DW]<br/>DD,DH,DW = 1 when group number more than 1<br/></td></tr><tr><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="4">LayerNormalization</td><td rowspan="4">The operator is splited</td><td rowspan="2">ReduceMean</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_mean(x, dims=axes, keepDim=bool(keepdims), output_type=y)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">GlobalAveragePool</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.reduce_mean(x, dims=[-2, -1], keepDim=True, output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">LeakyRelu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Less</td><td rowspan="1">None</td><td rowspan="1">Less</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.less(lhs, rhs, output_type=y)</td><td>hbir.less</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">LessOrEqual</td><td rowspan="1">None</td><td rowspan="1">LessOrEqual</td><td rowspan="1">//opset12<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/><br/>    return hbir.less_equal(lhs, rhs, output_type=y)</td><td>hbir.less_equal</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">Log</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="5">MatMul</td><td rowspan="5">The operator is splited</td><td rowspan="2">MatMul</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, output_type: mlir.Type, a: mlir.Value, b: mlir.Value<br/>):<br/>    lhs_shape = np.array(adaptor.operands[0].type.shape)<br/>    rhs_shape = np.array(adaptor.operands[0].type.shape)<br/>    lhs_rank = len(lhs_shape)<br/>    rhs_rank = len(rhs_shape)<br/>    if lhs_rank == 1 and rhs_rank == 1:<br/>        # the output of 1D matmul is a scalar, the out_shape is none, so add reshape<br/>        out_shape = adaptor.results[0].type.shape<br/>        return hbir.reshape(hbir.matmul(a, b), out_shape, output_type=output_type)<br/>    else:<br/>        return hbir.matmul(a, b, output_type=output_type)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.matmul</td><td><b>lhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,M,C]<br/>Dim: * ∈ [1, 4096], M,C ∈ [1, 8192]<br/><b>rhs:</b><br/>Type: int8, int16; lhs and rhs cannot both be int16<br/>Shape: [*,C,N]<br/>Dim: * ∈ [1, 4096]; C,N ∈ [1, 8192]<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Shape: [*,M,N]<br/>Other constraints: Same as lhs<br/></td></tr><tr><td rowspan="3">Conv</td><td rowspan="3">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    w: mlir.Value,<br/>    b: Optional[mlir.Value] = None,<br/>    *,<br/>    dilations=(1, 1),<br/>    group=1,<br/>    kernel_shape,<br/>    pads=(0, 0, 0, 0),<br/>    strides=(1, 1),<br/>):<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim == 2:<br/>        # input trans: nchw-&gt;nhwc<br/>        # output trans: nhwc-&gt;nchw<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 1])<br/>        x = hbir.conv2d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    elif kernel_dim == 3:<br/>        # input trans: ncdhw-&gt;ndhwc<br/>        # output trans: ndhwc-&gt;ncdhw<br/>        x = hbir.transpose(x, [0, 2, 3, 4, 1])<br/>        w = hbir.transpose(w, [0, 2, 3, 4, 1])<br/>        x = hbir.conv3d(x, w, strides, pads, dilations, group, bias=b)<br/>        return hbir.transpose(x, [0, 4, 1, 2, 3], output_type=y)<br/>    else:<br/>        raise ValueError(&quot;Only Conv2d and Conv3d supported for now.&quot;)</td><td>hbir.conv3d</td><td><b>lhs:</b><br/>Type: int8<br/>Shape: [*,D,H,W,C]<br/>Dim: * ∈ [1, 128]; D,H,W ∈ [1, 65536]; C ∈ [1, 4096];<br/><b>rhs:</b><br/>Type: int8<br/>Shape: [N,KD,KH,KW,C]<br/>N ∈ [1, 65536]; KD,KH,KW ∈ [1, 9]; Dim: C ∈ [1, 4096];<br/>Size: KD × KH × KW × C ∈ [1, 131072]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Same as lhs<br/><b>stride:</b><br/>Shape: [SD,SH,SW]<br/>Dim: SD,SH,SW must be 1 or 2 and equal to each other.<br/><b>pad:</b><br/>Shape: [P_front, P_top, P_left, P_back, P_bottom, P_right]<br/>Dim: P_front,P_back ∈ [0, KD/2], P_top,P_bottom ∈ [0, KH/2], P_left,P_right ∈ [0, KW/2]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DD,DH,DW]<br/>DD,DH,DW = 1 when group number more than 1<br/></td></tr><tr><td>hbir.conv2d</td><td><b>input:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [*,H,W,C]<br/>Dim: * ∈ [1, 4096]; H,W,C ∈ [1, 65536]<br/><b>weight:</b><br/>Type: int8, int16; input and weight cannot both be int16<br/>Shape: [N,KH,KW,C]<br/>Dim: C ∈ [1, 8192]; KH,KW ∈ [1, 31]; N ∈ [1, 65536] if fout is the last layer of conv else [1, 8192]<br/>Size: KH × KW × C ∈ [1, 65536]<br/><b>bias:</b><br/>Type: f32<br/><b>output:</b><br/>Type: int8, int16, int32<br/>Other constraints: Same as fin<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH,SW ∈ [1, 256]; SH,SW ∈ {1} if dilation &gt; 1<br/><b>pad:</b><br/>Shape: [P_top,P_left,P_bottom,P_right]<br/>Dim: P_top,P_bottom ∈ [-H/2, 256], P_left,P_right ∈ [-W/2, 256]<br/><b>groupNum:</b><br/>fin.c is divisible by group number<br/><b>dilation:</b><br/>Shape: [DH,DW]<br/>Dim: DH,DW ∈ [1, 18]<br/><b>others:</b><br/>Stride only support odd number and 2 when conv is a int16 depthwise conv<br/>For each group, fin.c ∈ [1, 8192], KH × KW × fin.c ∈ [1, 65535], fin.c = C when group = 1<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Max</td><td rowspan="1">None</td><td rowspan="1">Max</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    lhs: mlir.Value,<br/>    rhs: mlir.Value,<br/>    *args,<br/>):<br/>    if len(args) != 0:<br/>        raise ValueError(f&quot;Operator Max expects 2 inputs, but got {len(args)+2}&quot;)<br/>    return hbir.max(lhs, rhs, output_type=y)</td><td>hbir.max</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="2">MaxPool</td><td rowspan="2">None</td><td rowspan="2">MaxPool</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    auto_pad=&quot;NOTSET&quot;,<br/>    ceil_mode=0,<br/>    dilations=[1],<br/>    kernel_shape,<br/>    pads=[0, 0],<br/>    storage_order=0,<br/>    strides=[1],<br/>):<br/>    # auto_pad is a DEPRECATED attribute, so it is ignored here.<br/>    # onnx maxpool supports a second output, and it is the index of the first output. storage_order is used to control whether it is row-major order or column-major order. Since second output is not supported, so attribute storage_order is ignored here.<br/>    assert len(adaptor.results) == 1<br/>    kernel_dim = len(kernel_shape)<br/>    if kernel_dim &lt; 1 or kernel_dim &gt; 2:<br/>        raise ValueError(<br/>            f&quot;Operator MaxPool does not support kernel_dim {kernel_dim}&quot;<br/>        )<br/>    if kernel_dim == 1:<br/>        x = hbir.transpose(x, [0, 2, 1])<br/>    elif kernel_dim == 2:<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        if dilations == [1]:<br/>            dilations = dilations * kernel_dim<br/>        if strides == [1]:<br/>            strides = strides * kernel_dim<br/>        if pads == [0, 0]:<br/>            pads = pads * kernel_dim<br/>    x = hbir.max_pool(<br/>        x,<br/>        kernel=kernel_shape,<br/>        stride=strides,<br/>        pad=pads,<br/>        dilation=dilations,<br/>        ceilMode=bool(ceil_mode),<br/>    )<br/>    if kernel_dim == 1:<br/>        return hbir.transpose(x, [0, 2, 1], output_type=y)<br/>    return hbir.transpose(x, [0, 3, 1, 2], output_type=y)</td><td>hbir.max_pool</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*,H,W,C]<br/><b>output:</b><br/>Same as input<br/><b>kernel:</b><br/>Shape: [KH,KW]<br/>Dim: KH, KW ∈ [1, 256]<br/><b>stride:</b><br/>Shape: [SH,SW]<br/>Dim: SH, SW ∈ [1, 256]<br/><b>pad:</b><br/>Shape: [PN,PH,PW,PC]<br/>PN,PH,PW,PC ∈ [-3, 256]<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Min</td><td rowspan="1">None</td><td rowspan="1">Min</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    lhs: mlir.Value,<br/>    rhs: mlir.Value,<br/>    *args,<br/>):<br/>    if len(args) != 0:<br/>        raise ValueError(f&quot;Operator Min expects 2 inputs, but got {len(args)+2}&quot;)<br/>    return hbir.min(lhs, rhs, output_type=y)</td><td>hbir.min</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Mish</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">None</td><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Neg</td><td rowspan="1">None</td><td rowspan="1">Neg</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.Neg(x)<br/>//opset11<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>):<br/>    return hbir.neg(x, output_type=y)</td><td>hbir.neg</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Not</td><td rowspan="1">None</td><td rowspan="1">Not</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.logical_not(x, output_type=y)</td><td>hbir.logical_not</td><td><b>input:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">Or</td><td rowspan="1">None</td><td rowspan="1">Or</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    # lhs = adaptor.operands[0].value.astype(np.int8)<br/>    # rhs = adaptor.operands[1].value.astype(np.int8)<br/>    return hbir.logical_or(lhs, rhs, output_type=y)</td><td>hbir.logical_or</td><td><b>lhs:</b><br/>Type: int8, int16, bool8<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Type: bool8<br/></td></tr><tr><td rowspan="1">PRelu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Pow</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Reciprocal</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="3">ReduceL1</td><td rowspan="3">The operator is splited</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">ReduceSum</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    axes=None,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = adaptor.operands[1].value.tolist() if axes is not None else None<br/><br/>    if axes is None:<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )</td><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="4">ReduceL2</td><td rowspan="4">The operator is splited</td><td>Pow</td><td></td><td></td><td>Unsupported</td></tr><tr><td rowspan="2">ReduceSum</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    axes=None,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = adaptor.operands[1].value.tolist() if axes is not None else None<br/><br/>    if axes is None:<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )</td><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sqrt</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value):<br/>    return hbir.sqrt(x, output_type=y)</td><td>hbir.sqrt</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceMax</td><td rowspan="2">None</td><td rowspan="2">ReduceMax</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_max(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceMean</td><td rowspan="2">None</td><td rowspan="2">ReduceMean</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_mean(x, dims=axes, keepDim=bool(keepdims), output_type=y)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_mean</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceMin</td><td rowspan="2">None</td><td rowspan="2">ReduceMin</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_min(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_min</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceSum</td><td rowspan="2">None</td><td rowspan="2">ReduceSum</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    axes=None,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = adaptor.operands[1].value.tolist() if axes is not None else None<br/><br/>    if axes is None:<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )</td><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Reshape</td><td rowspan="1">None</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">Resize</td><td rowspan="2">None</td><td rowspan="2">Resize</td><td rowspan="2">//opset10<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    scales: mlir.Value = None,<br/>    *,<br/>    mode=&quot;nearest&quot;,<br/>):<br/>    input_shape = np.array(adaptor.operands[0].type.shape)<br/>    output_shape = np.array(adaptor.results[0].type.shape)<br/>    # Only supports resizing operations with 4-dimensional features<br/>    assert len(input_shape) == 4<br/>    scale_tensor = adaptor.operands[1].value<br/>    resize_scale = scale_tensor[2:]<br/>    if isinstance(mode, bytes):<br/>        mode = mode.decode()<br/>    if mode == &quot;linear&quot;:<br/>        mode = &quot;bilinear&quot;<br/>    if mode not in [&quot;nearest&quot;, &quot;bilinear&quot;]:<br/>        raise ValueError(f&quot;Operator Resize does not support resize mode: {mode}&quot;)<br/>    # resize 10 only supports one resize method, which is the most basic resize method<br/>    step = 1 / resize_scale<br/>    initial_offset = np.array([-0.5, -0.5])<br/><br/>    # 1. when there is a ratio parameter, if the value of ratio is negative, you need to correct the value of initialOffset;<br/>    # 2. when there is a size parameter, if the value of step is negative, you need to correct the value of initialOffset;<br/>    rank = len(input_shape)<br/>    numOfResizeAxis = 2<br/>    for i in range(numOfResizeAxis):<br/>        axis = rank - numOfResizeAxis - 1 + i<br/>        if step[i] &lt; 0:<br/>            initial_offset[i] = float(input_shape[axis])<br/><br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    x = hbir.resize2d(<br/>        x, step, initial_offset, mode, size=output_shape[2:], expansionMode=&quot;border&quot;<br/>    )<br/>    x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    return x<br/>//opset11<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    roi: mlir.Value = None,<br/>    scales: mlir.Value = None,<br/>    sizes: mlir.Value = None,<br/>    *,<br/>    coordinate_transformation_mode=&quot;half_pixel&quot;,<br/>    cubic_coeff_a=&quot;-0.75&quot;,<br/>    exclude_outside=0,<br/>    extrapolation_value=0,<br/>    mode=&quot;nearest&quot;,<br/>    nearest_mode=&quot;round_prefer_floor&quot;,<br/>):<br/>    input_shape = np.array(adaptor.operands[0].type.shape)<br/>    output_shape = np.array(adaptor.results[0].type.shape)<br/>    rank = len(input_shape)<br/>    roi = adaptor.operands[1].value<br/>    # 仅支持feature为4维的resize操作<br/>    assert len(input_shape) == 4<br/>    scale_tensor = adaptor.operands[2].value<br/><br/>    if scale_tensor.size &gt; 0:<br/>        resize_shape = input_shape * scale_tensor<br/>    else:<br/>        resize_shape = adaptor.operands[3].value<br/>        scale_tensor = np.array(<br/>            [<br/>                1,<br/>                1,<br/>                resize_shape[2] / input_shape[2],<br/>                resize_shape[3] / input_shape[3],<br/>            ]<br/>        )<br/><br/>    length_original = input_shape[2:].astype(np.int32)<br/>    length_resized = resize_shape[2:].astype(np.int32)<br/>    coordinate_transformation_mode = (<br/>        coordinate_transformation_mode.decode()<br/>        if isinstance(coordinate_transformation_mode, bytes)<br/>        else coordinate_transformation_mode<br/>    )<br/>    if coordinate_transformation_mode != &quot;tf_crop_and_resize&quot; and np.array_equal(<br/>        length_original, length_resized<br/>    ):<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        x = hbir.resize2d(<br/>            x, step=[1.0, 1.0], initialOffset=[0.0, 0.0], ratio=(1.0, 1.0)<br/>        )<br/>        x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>        return x<br/><br/>    # resize 计算公式:<br/>    # hw_in = initial_offset + hw_out * step<br/>    resize_scale = scale_tensor[2:]<br/>    if coordinate_transformation_mode in [&quot;pytorch_half_pixel&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale - 0.5<br/>        for index, resized_num in enumerate(length_resized):<br/>            if resized_num &lt;= 1:<br/>                step[index] = 0.0<br/>                initial_offset[index] = 0.0<br/><br/>    elif coordinate_transformation_mode in [&quot;half_pixel&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale - 0.5<br/>    elif coordinate_transformation_mode in [&quot;align_corners&quot;]:<br/>        step = (<br/>            (length_original[0] - 1) / (length_resized[0] - 1),<br/>            (length_original[1] - 1) / (length_resized[1] - 1),<br/>        )<br/>        initial_offset = [0.0, 0.0]<br/>    elif coordinate_transformation_mode in [&quot;asymmetric&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = [0.0, 0.0]<br/><br/>    elif coordinate_transformation_mode in [&quot;tf_half_pixel_for_nn&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale<br/>    elif coordinate_transformation_mode in [&quot;tf_crop_and_resize&quot;]:<br/>        start_x = roi[rank - 2 : rank]<br/>        end_x = roi[2 * rank - 2 : 2 * rank]<br/>        step = [1, 1]<br/>        initial_offset = [0, 0]<br/>        for index in range(len(length_resized)):<br/>            if length_resized[index] &gt; 1:<br/>                step[index] = (<br/>                    (end_x[index] - start_x[index])<br/>                    * (length_original[index] - 1)<br/>                    / (length_resized[index] - 1)<br/>                )<br/>                initial_offset[index] = start_x[index] * (<br/>                    length_original[index] - 1<br/>                )<br/>            else:<br/>                step[index] = 0<br/>                initial_offset[index] = (<br/>                    0.5<br/>                    * (start_x[index] + end_x[index])<br/>                    * (length_original[index] - 1)<br/>                )<br/>    else:<br/>        raise ValueError(<br/>            f&quot;Operator Resize does not support coordinate_transformation_mode: {coordinate_transformation_mode}&quot;<br/>        )<br/><br/>    if isinstance(mode, bytes):<br/>        mode = mode.decode()<br/>    if isinstance(nearest_mode, bytes):<br/>        nearest_mode = nearest_mode.decode()<br/><br/>    if mode == &quot;linear&quot;:<br/>        mode = &quot;bilinear&quot;<br/>    if mode == &quot;cubic&quot;:<br/>        mode = &quot;bicubic&quot;<br/>    if mode not in [&quot;nearest&quot;, &quot;bilinear&quot;, &quot;bicubic&quot;]:<br/>        raise ValueError(f&quot;Operator Resize does not support resize mode: {mode}&quot;)<br/>    if mode == &quot;nearest&quot;:<br/>        if nearest_mode == &quot;round_prefer_ceil&quot;:<br/>            initial_offset += np.array(<br/>                [np.finfo(np.float32).eps, np.finfo(np.float32).eps]<br/>            )<br/>        if nearest_mode == &quot;round_prefer_floor&quot;:<br/>            initial_offset -= np.array(<br/>                [np.finfo(np.float32).eps, np.finfo(np.float32).eps]<br/>            )<br/>        elif nearest_mode == &quot;floor&quot;:<br/>            initial_offset -= np.array([0.5, 0.5])<br/>        elif nearest_mode == &quot;ceil&quot;:<br/>            initial_offset += np.array([0.5, 0.5])<br/>        else:<br/>            raise ValueError(<br/>                f&quot;Operator Resize does not support nearest_mode mode: {nearest_mode}&quot;<br/>            )<br/><br/>    # 1. when there is a ratio parameter, if the value of ratio is negative, you need to correct the value of initialOffset;<br/>    # 2. when there is a size parameter, if the value of step is negative, you need to correct the value of initialOffset;<br/>    rank = len(input_shape)<br/>    numOfResizeAxis = 2<br/>    for i in range(numOfResizeAxis):<br/>        axis = rank - numOfResizeAxis - 1 + i<br/>        if step[i] &lt; 0:<br/>            initial_offset[i] = float(input_shape[axis])<br/><br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    x = hbir.resize2d(<br/>        x, step, initial_offset, mode, size=output_shape[2:], expansionMode=&quot;border&quot;<br/>    )<br/>    x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    return x<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    roi: mlir.Value = None,<br/>    scales: mlir.Value = None,<br/>    sizes: mlir.Value = None,<br/>    *,<br/>    antialias=0,<br/>    axes=None,<br/>    coordinate_transformation_mode=&quot;half_pixel&quot;,<br/>    cubic_coeff_a=&quot;-0.75&quot;,<br/>    exclude_outside=0,<br/>    extrapolation_value=0,<br/>    keep_aspect_ratio_policy=&quot;stretch&quot;,<br/>    mode=&quot;nearest&quot;,<br/>    nearest_mode=&quot;round_prefer_floor&quot;,<br/>):<br/>    input_shape = np.array(adaptor.operands[0].type.shape)<br/>    length_original = input_shape[2:].astype(np.int32)<br/>    output_shape = np.array(adaptor.results[0].type.shape)<br/>    rank = len(input_shape)<br/><br/>    # 仅支持feature为4维的resize操作<br/>    assert len(input_shape) == 4<br/>    if roi is not None and len(adaptor.operands[1].value) &gt; 0:<br/>        roi = adaptor.operands[1].value<br/>    scale_tensor = (<br/>        None<br/>        if (scales is None or (len(adaptor.operands[2].value) == 0))<br/>        else adaptor.operands[2].value<br/>    )<br/>    output_size = (<br/>        None<br/>        if (sizes is None or (len(adaptor.operands[3].value) == 0))<br/>        else adaptor.operands[3].value<br/>    )<br/>    coordinate_transformation_mode = (<br/>        coordinate_transformation_mode.decode()<br/>        if isinstance(coordinate_transformation_mode, bytes)<br/>        else coordinate_transformation_mode<br/>    )<br/>    if isinstance(mode, bytes):<br/>        mode = mode.decode()<br/>    if isinstance(nearest_mode, bytes):<br/>        nearest_mode = nearest_mode.decode()<br/>    if isinstance(keep_aspect_ratio_policy, bytes):<br/>        keep_aspect_ratio_policy = keep_aspect_ratio_policy.decode()<br/><br/>    if antialias != 0:<br/>        raise ValueError(&quot;Operator Resize does not support antialias mode.&quot;)<br/>    if output_size is None and scale_tensor is None:<br/>        raise ValueError(<br/>            &quot;Operator Resize: one of scales and sizes must be specified.&quot;<br/>        )<br/>    if output_size is not None and scale_tensor is not None:<br/>        raise ValueError(<br/>            &quot;Operator Resize: only one of scales and sizes can be specified.&quot;<br/>        )<br/><br/>    if axes is not None:<br/>        if len(axes) &gt; 2:<br/>            raise ValueError(&quot;Operator Resize supports specifying up to 2 axes.&quot;)<br/>        axes_num = len(axes)<br/>        if output_size is not None:<br/>            new_output_size = np.copy(input_shape)<br/>            new_output_size[axes] = output_size<br/>            output_size = new_output_size<br/><br/>        if scale_tensor is not None:<br/>            new_scales = np.ones(rank, dtype=np.float32)<br/>            new_scales[axes] = scale_tensor<br/>            scale_tensor = new_scales<br/><br/>        if roi is not None:<br/>            new_roi_begin = np.zeros(rank, dtype=np.float32)<br/>            new_roi_end = np.ones(rank, dtype=np.float32)<br/>            new_roi_begin[axes] = roi[:axes_num]<br/>            new_roi_end[axes] = roi[axes_num:]<br/>            roi = np.concatenate((new_roi_begin, new_roi_end))<br/><br/>    if scale_tensor is not None:<br/>        resize_shape = input_shape * scale_tensor<br/>        length_resized = resize_shape[2:].astype(np.int32)<br/>    else:<br/>        resize_shape = output_size<br/>        length_resized = resize_shape[2:].astype(np.int32)<br/>        scale_tensor = np.array(<br/>            [<br/>                1,<br/>                1,<br/>                resize_shape[2] / input_shape[2],<br/>                resize_shape[3] / input_shape[3],<br/>            ]<br/>        )<br/><br/>        if keep_aspect_ratio_policy != &quot;stretch&quot;:<br/>            if keep_aspect_ratio_policy == &quot;not_larger&quot;:<br/>                scale = np.min(resize_shape[2:] / input_shape[2:])<br/>            elif keep_aspect_ratio_policy == &quot;not_smaller&quot;:<br/>                scale = np.max(resize_shape[2:] / input_shape[2:])<br/>            else:<br/>                raise ValueError(<br/>                    f&quot;Invalid keep_aspect_ratio_policy={keep_aspect_ratio_policy!r}&quot;<br/>                )<br/><br/>            scale_tensor[2] = scale_tensor[3] = scale<br/><br/>            resize_shape = input_shape * scale_tensor<br/>            length_resized = [int(elem + 0.5) for elem in resize_shape[2:]]<br/><br/>    if coordinate_transformation_mode != &quot;tf_crop_and_resize&quot; and np.array_equal(<br/>        length_original, length_resized<br/>    ):<br/>        x = hbir.transpose(x, [0, 2, 3, 1])<br/>        x = hbir.resize2d(<br/>            x, step=[1.0, 1.0], initialOffset=[0.0, 0.0], ratio=(1.0, 1.0)<br/>        )<br/>        x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>        return x<br/><br/>    # resize calculation formula:<br/>    # hw_in = initial_offset + hw_out * step<br/>    resize_scale = scale_tensor[2:]<br/>    if coordinate_transformation_mode in [&quot;pytorch_half_pixel&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale - 0.5<br/>        for index, resized_num in enumerate(length_resized):<br/>            if resized_num &lt;= 1:<br/>                step[index] = 0.0<br/>                initial_offset[index] = 0.0<br/>    elif coordinate_transformation_mode in [&quot;half_pixel_symmetric&quot;]:<br/>        adjustment = length_resized / resize_shape[2:]<br/>        center = length_original / 2<br/>        offset = center * (1 - adjustment)<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale - 0.5 + offset<br/>    elif coordinate_transformation_mode in [&quot;half_pixel&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = 0.5 / resize_scale - 0.5<br/>    elif coordinate_transformation_mode in [&quot;align_corners&quot;]:<br/>        step = (<br/>            (length_original[0] - 1) / (length_resized[0] - 1),<br/>            (length_original[1] - 1) / (length_resized[1] - 1),<br/>        )<br/>        initial_offset = [0.0, 0.0]<br/>    elif coordinate_transformation_mode in [&quot;asymmetric&quot;]:<br/>        step = 1 / resize_scale<br/>        initial_offset = [0.0, 0.0]<br/>    elif coordinate_transformation_mode in [&quot;tf_crop_and_resize&quot;]:<br/>        start_x = roi[rank - 2 : rank]<br/>        end_x = roi[2 * rank - 2 : 2 * rank]<br/>        step = [1, 1]<br/>        initial_offset = [0, 0]<br/>        for index in range(len(length_resized)):<br/>            if length_resized[index] &gt; 1:<br/>                step[index] = (<br/>                    (end_x[index] - start_x[index])<br/>                    * (length_original[index] - 1)<br/>                    / (length_resized[index] - 1)<br/>                )<br/>                initial_offset[index] = start_x[index] * (<br/>                    length_original[index] - 1<br/>                )<br/>            else:<br/>                step[index] = 0<br/>                initial_offset[index] = (<br/>                    0.5<br/>                    * (start_x[index] + end_x[index])<br/>                    * (length_original[index] - 1)<br/>                )<br/>    else:<br/>        raise ValueError(<br/>            f&quot;Operator Resize does not support coordinate_transformation_mode: {coordinate_transformation_mode}&quot;<br/>        )<br/><br/>    # Currently only nearest and bilinear are supported.<br/>    if mode == &quot;linear&quot;:<br/>        mode = &quot;bilinear&quot;<br/>    if mode == &quot;cubic&quot;:<br/>        mode = &quot;bicubic&quot;<br/>    if mode not in [&quot;nearest&quot;, &quot;bilinear&quot;, &quot;bicubic&quot;]:<br/>        raise ValueError(f&quot;Operator Resize does not support resize mode: {mode}&quot;)<br/>    if mode == &quot;nearest&quot;:<br/>        if nearest_mode == &quot;round_prefer_ceil&quot;:<br/>            initial_offset += np.array(<br/>                [np.finfo(np.float32).eps, np.finfo(np.float32).eps]<br/>            )<br/>        if nearest_mode == &quot;round_prefer_floor&quot;:<br/>            initial_offset -= np.array(<br/>                [np.finfo(np.float32).eps, np.finfo(np.float32).eps]<br/>            )<br/>        elif nearest_mode == &quot;floor&quot;:<br/>            initial_offset -= np.array([0.5, 0.5])<br/>        elif nearest_mode == &quot;ceil&quot;:<br/>            initial_offset += np.array([0.5, 0.5])<br/>        else:<br/>            raise ValueError(<br/>                f&quot;Operator Resize does not support nearest_mode mode: {nearest_mode}&quot;<br/>            )<br/><br/>    # 1. when there is a ratio parameter, if the value of ratio is negative, you need to correct the value of initialOffset;<br/>    # 2. when there is a size parameter, if the value of step is negative, you need to correct the value of initialOffset;<br/>    rank = len(input_shape)<br/>    numOfResizeAxis = 2<br/>    for i in range(numOfResizeAxis):<br/>        axis = rank - numOfResizeAxis - 1 + i<br/>        if step[i] &lt; 0:<br/>            initial_offset[i] = float(input_shape[axis])<br/><br/>    x = hbir.transpose(x, [0, 2, 3, 1])<br/>    x = hbir.resize2d(<br/>        x, step, initial_offset, mode, size=output_shape[2:], expansionMode=&quot;border&quot;<br/>    )<br/>    x = hbir.transpose(x, [0, 3, 1, 2], output_type=y)<br/>    return x</td><td>hbir.resize2d</td><td><b>input:</b><br/>Type: int8<br/>Shape: [*,H,W,C]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Round</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Selu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Sigmoid</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Sign</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Sin</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Sinh</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">Slice</td><td rowspan="2">None</td><td rowspan="2">Slice</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    data: mlir.Value,<br/>    starts: mlir.Value,<br/>    ends: mlir.Value,<br/>    axes: Optional[mlir.Value] = None,<br/>    steps: Optional[mlir.Value] = None,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    input_rank = len(input_shape)<br/>    starts = adaptor.operands[1].value<br/>    ends = adaptor.operands[2].value<br/>    steps = (<br/>        np.ones(input_rank, dtype=np.int64)<br/>        if steps is None<br/>        else adaptor.operands[3].value<br/>        if axes is None<br/>        else adaptor.operands[4].value<br/>    )<br/>    axes = (<br/>        [i for i in range(input_rank)]<br/>        if axes is None<br/>        else adaptor.operands[3].value<br/>    )<br/><br/>    assert len(starts) == len(<br/>        ends<br/>    ), &quot;Incompatible attributes starts and ends for Slice.&quot;<br/><br/>    new_start = np.zeros(input_rank, dtype=np.int64)  # start from zero<br/>    new_end = np.array(input_shape)  # end with original shape limit<br/>    new_step = np.ones(input_rank, dtype=np.int64)  # step default 1<br/>    if len(starts) == len(axes):<br/>        for idx, axis in enumerate(axes):<br/>            new_start[axis] = starts[idx]<br/>            new_end[axis] = ends[idx]<br/>            new_step[axis] = steps[idx]<br/>    elif len(starts) == 1:<br/>        for index in range(input_rank):<br/>            if index in axes:<br/>                new_start[index] = starts[0]<br/>                new_end[index] = ends[0]<br/>                new_step[index] = steps[0]<br/>    else:<br/>        raise ValueError(&quot;Incompatible attributes starts and axes for Slice.&quot;)<br/><br/>    return hbir.slice(<br/>        data, begin=new_start, end=new_end, step=new_step, output_type=y<br/>    )<br/>//opset10<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    data: mlir.Value,<br/>    starts: mlir.Value,<br/>    ends: mlir.Value,<br/>    axes: Optional[mlir.Value] = None,<br/>    steps: Optional[mlir.Value] = None,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    input_rank = len(input_shape)<br/>    starts = adaptor.operands[1].value<br/>    ends = adaptor.operands[2].value<br/>    steps = (<br/>        np.ones(input_rank, dtype=np.int64)<br/>        if steps is None<br/>        else adaptor.operands[3].value<br/>        if axes is None<br/>        else adaptor.operands[4].value<br/>    )<br/>    axes = (<br/>        [i for i in range(input_rank)]<br/>        if axes is None<br/>        else adaptor.operands[3].value<br/>    )<br/><br/>    assert len(starts) == len(<br/>        ends<br/>    ), &quot;Incompatible attributes starts and ends for Slice.&quot;<br/><br/>    new_start = np.zeros(input_rank, dtype=np.int64)  # start from zero<br/>    new_end = np.array(input_shape)  # end with original shape limit<br/>    new_step = np.ones(input_rank, dtype=np.int64)  # step default 1<br/>    if len(starts) == len(axes):<br/>        for idx, axis in enumerate(axes):<br/>            new_start[axis] = starts[idx]<br/>            new_end[axis] = ends[idx]<br/>            new_step[axis] = steps[idx]<br/>    elif len(starts) == 1:<br/>        for index in range(input_rank):<br/>            if index in axes:<br/>                new_start[index] = starts[0]<br/>                new_end[index] = ends[0]<br/>                new_step[index] = steps[0]<br/>    else:<br/>        raise ValueError(&quot;Incompatible attributes starts and axes for Slice.&quot;)<br/><br/>    # torch2onnx, axes is all -1. need transpose to hbir<br/>    if (steps == -1).any():<br/>        return hbir.flip(data, axes)<br/><br/>    return hbir.slice(<br/>        data, begin=new_start, end=new_end, step=new_step, output_type=y<br/>    )<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    data: mlir.Value,<br/>    starts: mlir.Value,<br/>    ends: mlir.Value,<br/>    axes: Optional[mlir.Value] = None,<br/>    steps: Optional[mlir.Value] = None,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    input_rank = len(input_shape)<br/>    starts = adaptor.operands[1].value<br/>    ends = adaptor.operands[2].value<br/>    steps = (<br/>        np.ones(input_rank, dtype=np.int64)<br/>        if steps is None<br/>        else adaptor.operands[3].value<br/>        if axes is None<br/>        else adaptor.operands[4].value<br/>    )<br/>    axes = (<br/>        [i for i in range(input_rank)]<br/>        if axes is None<br/>        else adaptor.operands[3].value<br/>    )<br/><br/>    assert len(starts) == len(<br/>        ends<br/>    ), &quot;Incompatible attributes starts and ends for Slice.&quot;<br/><br/>    new_start = np.zeros(input_rank, dtype=np.int64)  # start from zero<br/>    new_end = np.array(input_shape)  # end with original shape limit<br/>    new_step = np.ones(input_rank, dtype=np.int64)  # step default 1<br/>    if len(starts) == len(axes):<br/>        for idx, axis in enumerate(axes):<br/>            new_start[axis] = starts[idx]<br/>            new_end[axis] = ends[idx]<br/>            new_step[axis] = steps[idx]<br/>    elif len(starts) == 1:<br/>        for index in range(input_rank):<br/>            if index in axes:<br/>                new_start[index] = starts[0]<br/>                new_end[index] = ends[0]<br/>                new_step[index] = steps[0]<br/>    else:<br/>        raise ValueError(&quot;Incompatible attributes starts and axes for Slice.&quot;)<br/><br/>    # torch2onnx, axes is all -1<br/>    if (steps == -1).any():<br/>        return hbir.flip(data, axes)<br/><br/>    return hbir.slice(<br/>        data, begin=new_start, end=new_end, step=new_step, output_type=y<br/>    )</td><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td>hbir.flip</td><td><b>input:</b><br/>Type: int8<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="8">Softmax</td><td rowspan="8">The operator is splited</td><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="2">ReduceSum</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    axes=None,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = adaptor.operands[1].value.tolist() if axes is not None else None<br/><br/>    if axes is None:<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return hbir.reduce_sum(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )</td><td>hbir.reduce_sum</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="2">ReduceMax</td><td rowspan="2">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *,<br/>    axes=None,<br/>    keepdims=1,<br/>):<br/>    if axes is None:<br/>        axes = list(range(mlir.ShapedType(x.type).rank))<br/>    return hbir.reduce_max(<br/>        x,<br/>        dims=axes,<br/>        keepDim=bool(keepdims),<br/>        output_type=y,<br/>    )<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    keepdims=1,<br/>    noop_with_empty_axes=0,<br/>):<br/>    axes = None<br/>    if len(args) != 0:<br/>        axes = adaptor.operands[1].value.tolist()<br/><br/>    if axes is None:<br/>        axes = []<br/>        if noop_with_empty_axes == 0:<br/>            # Reduce all axes<br/>            axes = list(range(mlir.ShapedType(x.type).rank))<br/>        else:<br/>            # act like identity operands, here convert to reshape<br/>            return hbir.reshape(x, adaptor.operands[0].type.shape)<br/><br/>    return self.mlir_op_func(x, dims=axes, keepDim=bool(keepdims), output_type=y)</td><td>hbir.reduce_max</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/>Dim: reduce axis dim size ∈ [1, 65535]<br/>Element : reduce Elements size ∈ [1, 65535]<br/><b>output:</b><br/>Same as input, ReduceArgMax/ReduceArgMin&#x27;s output can be of type int32 or int64, as long as the size of the reduced axis can be represented using an int16 number<br/></td></tr><tr><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Reciprocal</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>):<br/>    return hbir.reciprocal(<br/>        x,<br/>        output_type=y,<br/>    )</td><td>hbir.reciprocal</td><td><b>input:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Mul</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.mul(lhs, rhs, output_type=y)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Softplus</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Softsign</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Split</td><td rowspan="1">None</td><td rowspan="1">Split</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0, split<br/>):<br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    axis = axis if axis &gt;= 0 else (dim + axis)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(shape=dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset13<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, *args, axis=0<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        num_outputs = len(adaptor.results)<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i]<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    axis=0,<br/>    num_outputs=0,<br/>):<br/>    split = None<br/>    if len(args) != 0:<br/>        split = adaptor.operands[1].value.tolist()<br/>    # format: ele = ceil(dim / num), the last one is dim - ele * (num -1)<br/>    # for example, num_outputs is 3, dim is 128, get split [43, 43, 42]<br/>    if split is None:<br/>        split = []<br/>        tmp_shape = adaptor.operands[0].type.shape<br/>        ele = int(math.ceil(tmp_shape[axis] / num_outputs))<br/>        if tmp_shape[axis] % num_outputs == 0:<br/>            split = [int(tmp_shape[axis] / num_outputs)] * num_outputs<br/>        else:<br/>            split = [ele] * (num_outputs - 1)<br/>            split.append(tmp_shape[axis] - ele * (num_outputs - 1))<br/><br/>    ret_list = []<br/>    shape = adaptor.operands[0].type.shape<br/>    dim = len(shape)<br/>    asum = 0<br/>    for i in range(len(split)):<br/>        begin = np.array([0 if i != axis else asum for i in range(dim)])<br/>        asum += split[i]<br/>        end = np.array([shape[i] if i != axis else asum for i in range(dim)])<br/>        step = np.ones(dim, dtype=np.int64)<br/>        output_type = y[i] if isinstance(y, list) else y<br/>        ret_list.append(<br/>            hbir.slice(x, begin=begin, end=end, step=step, output_type=output_type)<br/>        )<br/>    return ret_list</td><td>hbir.slice</td><td><b>input:</b><br/>Dim: all dims &lt; 2097152 <br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sqrt</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Squeeze</td><td rowspan="1">The operator is replaced</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Sub</td><td rowspan="1">None</td><td rowspan="1">Sub</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.sub(lhs, rhs, output_type=y)</td><td>hbir.sub</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Sum</td><td rowspan="1">The operator is replaced</td><td rowspan="1">Add</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, lhs: mlir.Value, rhs: mlir.Value<br/>):<br/>    return hbir.add(lhs, rhs, output_type=y)</td><td>hbir.add</td><td><b>lhs:</b><br/>Type: int8, int16, int32, if type is int32, this hbir.add op must be fusible to a Conv op<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Tan</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Tanh</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">ThresholdedRelu</td><td rowspan="1">The operator is converted into a look-up-table operator</td><td rowspan="1">HzLut</td><td rowspan="1">//opset 9<br/>func.func @HzLut(...) {<br/>  return b30.lut(...)<br/>}</td><td>b30.lut</td><td><b>inputs:</b><br/>Type: int8, int16<br/><b>outputs:</b><br/>If input is int8, output is int8<br/>If input is int16, output is int8/int16<br/></td></tr><tr><td rowspan="1">Tile</td><td rowspan="1">None</td><td rowspan="1">Tile</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    data: mlir.Value,<br/>    repeats: mlir.Value,<br/>):<br/>    repeats = adaptor.operands[1].value<br/>    return hbir.tile(data, repeats, output_type=y)</td><td>hbir.tile</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Transpose</td><td rowspan="1">None</td><td rowspan="1">Transpose</td><td rowspan="1">//opset9<br/>def emit_mlir_op(self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, perm):<br/>    return hbir.transpose(x, perm, output_type=y)</td><td>hbir.transpose</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Trilu</td><td rowspan="1">None</td><td rowspan="1">Trilu</td><td rowspan="1">//opset13<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    *args,<br/>    upper=1,<br/>):<br/>    k = 0<br/>    if len(args) != 0:<br/>        k = adaptor.operands[1].value<br/>    shape = adaptor.operands[0].type.shape<br/>    matrix = []<br/>    if upper == 1:<br/>        matrix = np.triu(np.ones(shape), k)<br/>    else:<br/>        matrix = np.tril(np.ones(shape), k)<br/>    return hbir.mul(x, matrix)</td><td>hbir.mul</td><td><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Same as lhs<br/><b>output:</b><br/>Same as lhs<br/></td></tr><tr><td rowspan="1">Unsqueeze</td><td rowspan="1">The operator is replaced</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Upsample</td><td rowspan="1">The operator is replaced</td><td rowspan="1">Reshape</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self, adaptor: NodeAdaptor, y: mlir.Type, x: mlir.Value, shape: mlir.Value<br/>):<br/>    out_shape = adaptor.results[0].type.shape<br/>    return hbir.reshape(x, out_shape)<br/>//opset18<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    x: mlir.Value,<br/>    shape: mlir.Value,<br/>    *,<br/>    allowzero=0,<br/>):<br/>    input_shape = adaptor.operands[0].type.shape<br/>    out_shape = adaptor.results[0].type.shape<br/>    new_shape = np.copy(out_shape)<br/>    if allowzero == 0:<br/>        zeros_index = np.where(out_shape == 0)<br/>        new_shape[zeros_index] = np.array(object=input_shape)[zeros_index]<br/>    return hbir.reshape(x, shape=new_shape)</td><td>hbir.reshape</td><td><b>input:</b><br/>No limits<br/><b>output:</b><br/>Same as input<br/></td></tr><tr><td rowspan="1">Where</td><td rowspan="1">None</td><td rowspan="1">Where</td><td rowspan="1">//opset9<br/>def emit_mlir_op(<br/>    self,<br/>    adaptor: NodeAdaptor,<br/>    y: mlir.Type,<br/>    condition: mlir.Value,<br/>    X: mlir.Value,<br/>    Y: mlir.Value,<br/>):<br/>    return hbir.where(condition, X, Y, output_type=y)</td><td>hbir.where</td><td><b>condition:</b><br/>Type: bool8<br/><b>lhs:</b><br/>Type: int8, int16<br/>Shape: [*]<br/><b>rhs:</b><br/>Type: int8, int16<br/><b>output:</b><br/>Same as lhs<br/></td></tr></tbody></table></div><!--/$--></div><div class="rspress-doc-footer"><footer class="mt-8"><div class="xs:flex pb-5 px-2 justify-end items-center"></div><div class="flex flex-col"></div><div class="flex flex-col sm:flex-row sm:justify-around gap-4 pt-6"><div class="prev_e7091 flex flex-col"></div><div class="next_e7091 flex flex-col"></div></div></footer></div></div></div><div class="aside-container_edeb4"><div><div class="flex flex-col"><div class="hidden"><div id="aside-container" class="relative text-sm font-medium"><div class="leading-7 block text-sm font-semibold pl-3">On This Page</div><nav class="mt-1"><ul class="relative"></ul></nav></div></div></div></div></div></div></div></section></div></div><div id="search-container"></div></body></html>