
# Autogenerated by mlir-tblgen; don't manually edit.
from .._mlir_libs import _hbdk
from ..dialects import hbir as _ods_ext
from .common import get_value_or_create_const
from .common import get_loc_or_create_from_frames
from .common import get_type_or_create_unranked_type
from .common import create_array_attr
from .common import create_attr
from .common import parse_enum
from .common import create_elements_attr


def abs(input, *, output_type=None, loc=None):
    """
HBIR tensor abs.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AbsOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def acos(input, *, output_type=None, loc=None):
    """
HBIR tensor acos.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AcosOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def acosh(input, *, output_type=None, loc=None):
    """
HBIR tensor acosh.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AcoshOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def add(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor addition.

Applies addition operator element-wise, $y_i=lhs_i+rhs_i$.

------

Note:
* Our arithmetic operator support broadcast.

------

Prototype: Pytorch add.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AddOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def asin(input, *, output_type=None, loc=None):
    """
HBIR tensor asin.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AsinOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def asinh(input, *, output_type=None, loc=None):
    """
HBIR tensor asinh.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AsinhOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def atan(input, *, output_type=None, loc=None):
    """
HBIR tensor atan.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AtanOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def atanh(input, *, output_type=None, loc=None):
    """
HBIR tensor atanh.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AtanhOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def avg_pool(input, kernel, stride=(1), pad=(0), dilation=(1), ceilMode=False, *, output_type=None, loc=None):
    """
HBIR n-D average pooling(only support 1d and 2d currently).

Applies a nD average pooling over input.

In the simplest case, the output value of the operator with input size $(N, H, W, C)$,
output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h}, ker_{w})$ can be precisely described as:

$ out(N_i, h, w, C_j) = \frac{1} { ker_h *ker_w } \sum_{m = 0} ^ { ker_h - 1 }\sum_{n = 0} ^
{ ker_w - 1 } input(N_i, stride[0]\times h + m, stride[1] \times w + n, C_j) $

where $h,w$ respectively represent the size of H and W.

------

Note:

* parameters has the same manner as the Conv2D operator, the same goes for the output size.

* ceilMode controls output's compute is mode of floor or ceil, it's default value is false.

------

Shape:

* Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in}, W_{in})$

* Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(*, H_{out}, W_{out}, C)$,
where $*$ represents any number of dimension.

$ H_{out} = \lfloor {\frac{H_{in} + padding[0] + padding[2] - kernel[0]} {stride[0]} + 1}\rfloor $

$ W_{out} = \lfloor {\frac{W_{in} + padding[1] + padding[3] - kernel[1]} {stride[1]} + 1}\rfloor $

if ceilMode = true, please use ceil replace floor in the above output formula.

------

Prototype: Pytorch avg_pool.

    """
    input = get_value_or_create_const(input)
    kernel = create_array_attr(kernel)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    ceilMode = create_attr(ceilMode)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.AvgPoolOp(output_type, input, kernel, stride=stride, pad=pad, dilation=dilation, ceilMode=ceilMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def batchnorm(input, mean, var, eps, *, weight=None, bias=None, output_type=None, loc=None):
    """
Hbir Batch Normalize

Applies Batch Normalization over each dimension of input. This compute can be precisely described as:

$ y = \frac{x-mean[x]}{\sqrt{Var[x]+\epsilon}}*weight+bias $

This mean and standard-deviation are calculated per-dimension over the batches
and weight and bias are learnable parameter vectors of the input size.

------

Note:
* eps - a value added to the denominator for numerical stability.
* $mean(x)$ and $Var[x]$'s shape are $(C)$.
* weight and bias are learnable scalar.

------

Shape:
* Input: $(N,H,W,C)$ or $(N,M,H,W,C)$ or $(H,W,C)$ or $(*,H,W,C)$, where $*$ reprensent any number of dimension.
* Output: same shape as input.

------

Prototype: Pytorch BatchNorm.

    """
    input = get_value_or_create_const(input)
    mean = get_value_or_create_const(mean)
    var = get_value_or_create_const(var)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    eps = create_attr(eps)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BatchNormOp(output_type, input, mean, var, eps, weight=weight, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev, interval_starts, interval_lengths, bev_feat_shape, *, output_type=None, loc=None):
    """
HBIR bev_pool_v2 op, cpu operator, from mmdet3d, no corresponding operator in torch/onnx

Convert several planar image inputs into bev image outputs, thus providing support for data processing under bird's eye view.

------

Parameters:
* depth: the depth tensor.
* feat: the feat tensor.
* ranks_depth: Stores the index value of depth.
* ranks_feat: Stores the index value of feat.
* ranks_bev: Stores the Voxel index value of the valid bev space.
* interval_starts: Each element marks the starting point of each "continuation segment" of the ranks_bev feat.
* interval_lengths: Each element identifies the length of each "continuous segment" of the ranks_bev feat.
* bev_feat_shape: output's shape. Aligned with the public version of cudu kernel, no permute(0, 4, 1, 2, 3) operation is performed in the kernel. And can support rank>=4.

------

Shape:
* depth: (B, N, D, fH, fW)
* feat: (B, N, fH, fW, C)
* ranks_depth: (N_points, )
* ranks_feat: (N_points, )
* ranks_bev: (N_points, )
* interval_starts: (N_pillar, )
* interval_lengths: (N_pillar, )
* output: shape same as bev_feat_shape, (B, D_Z, D_Y, D_X, C)

  
    """
    depth = get_value_or_create_const(depth)
    feat = get_value_or_create_const(feat)
    ranks_depth = get_value_or_create_const(ranks_depth)
    ranks_feat = get_value_or_create_const(ranks_feat)
    ranks_bev = get_value_or_create_const(ranks_bev)
    interval_starts = get_value_or_create_const(interval_starts)
    interval_lengths = get_value_or_create_const(interval_lengths)
    bev_feat_shape = create_array_attr(bev_feat_shape)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BevPoolV2Op(output_type, depth, feat, ranks_depth, ranks_feat, ranks_bev, interval_starts, interval_lengths, bev_feat_shape, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bitshift(input, rshift, *, output_type=None, loc=None):
    """
HBIR bitshift op
logic shift, positive value as right shift.
    """
    input = get_value_or_create_const(input)
    rshift = get_value_or_create_const(rshift)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BitShiftOp(output_type, input, rshift, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bitwise_and(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor and.
Applies 'bitwise and' operator element-wise, $y_i = lhs_i \& rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BitwiseAndOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bitwise_not(input, *, output_type=None, loc=None):
    """
HBIR tensor bitwise not.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BitwiseNotOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bitwise_or(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor or.
Applies 'bitwise or' operator element-wise, $y_i = lhs_i | rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BitwiseOrOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def bitwise_xor(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor xor.
Applies 'bitwise xor' operator element - wise, $y_i = lhs_i xor rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.BitwiseXorOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def cast_type(input, *, forceSaturate=None, output_type=None, loc=None):
    """
elemental type cast operation
Data are actually moved.
    """
    input = get_value_or_create_const(input)
    forceSaturate = create_attr(forceSaturate)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CastTypeOp(output_type, input, forceSaturate=forceSaturate, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def ceil(input, *, output_type=None, loc=None):
    """
HBIR tensor ceil.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CeilOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def clip(input, min, max, *, output_type=None, loc=None):
    """
HBIR Clip Op.

Clamps all elements in input into the range $[min, max] $.Assume min_value and max_value be min and max, respectively, this performs:

$ y_i = min(max(x_i, min\_value_i), max\_value_i) $

------

Note:
* min(min_value): lower-bound of the range to be clamped to
* max(max_value): upper-bound of the range to be clamped to

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch hardtanh.

    """
    input = get_value_or_create_const(input)
    min = create_attr(min)
    max = create_attr(max)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ClipOp(output_type, input, min, max, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def concat(inputs, dim, *, output_type=None, loc=None):
    """
Concatenates tensors along one dimension

Concatenates the given sequence of seq tensors in the given dimension. No elemental type conversion.

------

Note:
* dim - the dimension over which the tensors are concatenated.

------

Prototype: Pytorch cat.

    """
    inputs = get_value_or_create_const(inputs)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ConcatOp(output_type, inputs, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def constant(values, *, output_type=None, loc=None):
    """
HBIR constant generation op.
Generate a constant with specified type and value
    """
    values = create_elements_attr(values)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ConstantOp(output_type, values, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def conv2d(input, weight, stride=(1, 1), pad=(0, 0, 0, 0), dilation=(1, 1), groupNum=1, *, bias=None, output_type=None, loc=None):
    """
HBIR 2-D convolution.

Applies a 2D convolution over an input signal composed of several input channels.

In the simplest case,
the output value of the layer with input size $(N, H_{in}, W_{in}, C_{in})$
and output $(N, H_{out}, W_{out}, C_{out})$ can be precisely descibed as:

$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \sum_{k=0}^{C_{in} - 1}weight(C_{out_j},k) \star input(N_i,k) $

where $\star$ is the valid 2D [cross-correlation](https://www.mathworks.com/help/signal/ref/xcorr2.html) operation,
$N$ is the batch size, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.

------

Note:

* stride controls the stride for the cross-correlation, an integer array with 2 elements, default value is (1,1).

* padding controls the amount of padding applied to the input, an integer array with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end), default value is (0,0,0,0).

* dilation controls the spacing between kernel points, an integer array with 2 elements, default value is (0,0). It's harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what dilation does.

* groups controls the connections between inputs and outputs, an integer variable, default value is 1.

* Weight:  $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH represent kernel's height and width, respectively.

------

Shape:

* Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$,
where * represent any number of dimension.

* Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$

$ H_{out}=\lfloor \frac{H_{in} + padding[0] + padding[2] - dilation[0]\times(kernel[0]-1)-1}{stride[0]}+1\rfloor $
$ W_{out}=\lfloor \frac{W_{in}+padding[1]+padding[3]-dilation[1]\times(kernel[1]-1)-1}{stride[1]}+1\rfloor $

------

Prototype: Pytorch convolution.

    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    groupNum = create_attr(groupNum)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.Conv2dOp(output_type, input, weight, stride=stride, pad=pad, dilation=dilation, groupNum=groupNum, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def conv3d(input, weight, stride=(1, 1, 1), pad=(0, 0, 0, 0, 0, 0), dilation=(1, 1, 1), groupNum=1, *, bias=None, output_type=None, loc=None):
    """
HBIR 3-D convolution.

Applies a 3D convolution over an input signal composed of several input planes.

In the simplest case, the output value of the layer with input size $(N, C_{in}, D, H, W)$
and output $(N, C_{out}, D_{out}, H_{out}, W_{out})$ can be precisely described as:

$ out(N_i, C_{out_j}) = bias(C_{out_j}) +
\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k) $

where $\star$ is the valid 3D [cross-correlation] operation,
$N$ is the batch size, $C$ denotes a number of channels, $D$, $H$ and $W$ are the size of pixels.

------

Note:

* stride controls the stride for the cross-correlation, an integer array with 3 elements, default value is (1,1,1).

* padding controls the amount of padding applied to the input,  an integer array with 5 elements,
the padding sequences is (d_begin,h_begin,w_begin,d_end,h_end,w_end), default value is (0,0,0,0,0,0).

* dilation controls the spacing between kernel points, an integer array with 3 elements, default value is (0,0,0).
It's harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)
has a nice visualization of what dilation does.

* groups controls the connections between inputs and outputs, an integer variable, default value is 1.

* Weight: $(C_{out}, KD, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH represent kernel's height and width, respectively.

------

Shape:

* Input: $(N,D_{in},H_{in},W_{in},C_{in})$ or $(D_{in},H_{in},W_{in},C_{in})$ or $(*,D_{in},H_{in},W_{in},C_{in})$,
where * represent any number of dimension.

* Output: $(N,D_{out},H_{out},W_{out},C_{out})$ or $(D_{out},H_{out},W{out},C_{out})$ or $(*,D_{out},H_{out},W_{out},C_{out})$.

$ D_{out}=\lfloor \frac{D_{in} + padding[0] + padding[3] - dilation[0]\times(kernel[0]-1)-1}{stride[0]}+1\rfloor $

$ H_{out}=\lfloor \frac{H_{in} + padding[1] + padding[4] - dilation[1]\times(kernel[1]-1)-1}{stride[1]}+1\rfloor $

$ W_{out}=\lfloor \frac{W_{in} + padding[2] + padding[5] - dilation[2]\times(kernel[2]-1)-1}{stride[2]}+1\rfloor $

------

Prototype: Pytorch convolution.

    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    groupNum = create_attr(groupNum)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.Conv3dOp(output_type, input, weight, stride=stride, pad=pad, dilation=dilation, groupNum=groupNum, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def conv(input, weight, stride=(1), pad=(0), dilation=(1), groupNum=1, channelLast=True, *, bias=None, output_type=None, loc=None):
    """
HBIR convolution.

Applies a convolution over an input signal composed of several input planes.

rank in convolution is an integer greater than or equal to 1.

In the simplest case, for 2D convolution, rank=2
the output value of the layer with input size $(N, H_{in}, W_{in}, C_{in})$
and output $(N, H_{out}, W_{out}, C_{out})$ can be precisely descibed as:

$ out(N_i,C_{out_j}) = bias(C_{out_j}) + \sum_{k=0}^{C_{in} - 1}weight(C_{out_j},k) \star input(N_i,k) $

where $\star$ is the valid 2D [cross-correlation](https://www.mathworks.com/help/signal/ref/xcorr2.html) operation,
$N$ is the batch size, $C$ denotes a number of channels, $H$ and $W$ are the size of pixels.

------

Note:

* stride controls the stride for the cross-correlation, an integer array with rank elements, default value is (1,1,...).

* padding controls the amount of padding applied to the input, an integer array with 2*rank elements, when rank=2, the padding sequences is (h_begin,w_begin,h_end,w_end), default value is (0,0,0,0), when n=3, the padding sequences is (d_begin,h_begin,w_begin,d_end,h_end,w_end), default value is (0,0,0,0,0,0).

* dilation controls the spacing between kernel points, an integer array with rank elements, default value is (0,0,...). It's harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what dilation does.

* groups controls the connections between inputs and outputs, an integer variable, default value is 1.

* Weight for 2D:  $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH represent kernel's height and width, respectively.

------

Shape for 2D:

* Input: $(N,H_{in},W_{in},C_{in})$ or $(H_{in},W_{in},C_{in})$ or $(N,M,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$,
where * represent any number of dimension.

* Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or $(N,M,H_{out},W_{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$

$ H_{out}=\lfloor \frac{H_{in} + padding[0] + padding[2] - dilation[0]\times(kernel[0]-1)-1}{stride[0]}+1\rfloor $
$ W_{out}=\lfloor \frac{W_{in}+padding[1]+padding[3]-dilation[1]\times(kernel[1]-1)-1}{stride[1]}+1\rfloor $

------

Prototype: Pytorch convolution.
  
    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    groupNum = create_attr(groupNum)
    channelLast = create_attr(channelLast)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ConvOp(output_type, input, weight, stride=stride, pad=pad, dilation=dilation, groupNum=groupNum, channelLast=channelLast, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def convtranspose(input, weight, stride=(1), pad=(0), dilation=(1), groupNum=1, illegalWeight=False, *, bias=None, output_type=None, loc=None):
    """
HBIR transposed conv1d or conv2d or conv3d op.

Inverse operation of Conv op.

------

Parameters:
* input: the input tensor
* weight: the deconvolution kernel
* stride: same as conv1d's stride, [s_w];same as conv2d's stride, [s_h, s_w];same as conv3d's stride, [s_d, s_h, s_w]
* pad: 1d pad information for clipping output [w_left,w_right];2d pad information for clipping output [h_top,w_left,h_bottom,w_right];3d pad information for clipping output [d_front,h_top,w_left,d_back,h_bottom,w_right]
* dilation: same as conv1d's dilation, [d_w];same as conv2d's dilation, [d_h, d_w];same as conv3d's dilation, [d_d, d_h, d_w]
* group: same as conv1d's or conv2d's or conv3d's group

------

1d Shape:
* input: $(*, w, in\_channel)$
* weight: $(in\_channel, kw, out\_channel / group)$
* output: $(*, wo, out\_channel)$
* bias: $out\_channel$

where:

$ wo = (w - 1) * stride[0] - (pad[0] + pad[1]) + dilation[0] * (kw - 1) + 1 $

2d Shape:
* input: $(*, h, w, in\_channel)$
* weight: $(in\_channel, kh, kw, out\_channel / group)$
* output: $(*, ho, wo, out\_channel)$
* bias: $out\_channel$

where:

$ ho = (h - 1) * stride[0] - (pad[0] + pad[2]) + dilation[0] * (kh - 1) + 1 $
$ wo = (w - 1) * stride[1] - (pad[1] + pad[3]) + dilation[1] * (kw - 1) + 1 $

3d Shape:
* input: $(*, d, h, w, in\_channel)$
* weight: $(in\_channel, kd, kh, kw, out\_channel / group)$
* output: $(*, do, ho, wo, out\_channel)$
* bias: $out\_channel$

where:

$ do = (d - 1) * stride[0] - (pad[0] + pad[3]) + dilation[0] * (kd - 1) + 1 $
$ ho = (h - 1) * stride[1] - (pad[1] + pad[4]) + dilation[1] * (kh - 1) + 1 $
$ wo = (w - 1) * stride[2] - (pad[2] + pad[5]) + dilation[2] * (kw - 1) + 1 $

    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    groupNum = create_attr(groupNum)
    illegalWeight = create_attr(illegalWeight)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ConvTransposeOp(output_type, input, weight, stride=stride, pad=pad, dilation=dilation, groupNum=groupNum, illegalWeight=illegalWeight, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def correlation(lhs, rhs, kernel=1, max_d=1, stride1=1, stride2=1, pad=0, mode="multiply", *, output_type=None, loc=None):
    """
HBIR Correlation.

Applies correlation to inputs.
The correlation operator calculate the correlation of the regions of the regions of the two inputs.

------

Note:
* kernel controls correlation kernel size.
* max_d is the max_displacement of patch2 relative to patch1.
* stride1 is stride in lhs, it controls H and W in output.
* stride2 is stride in rhs, it controls channels in output.
* pad_size is the padding size.
* is_multiply controls the operation type, true means multiplication, false means subtraction.

------

Shape:
* Input: lhs's shape is $(*,H_{in},W_{in},C_{in})$, and rhs's shape should be same as lhs,
where $*$ represents any number of dimension.

* Output: $(*, H_{out}, W_{out}, C_{out})$, $*$ represents input batch size, $H_{out}$, $W_{out}$ and $C_{out}$
are height, width and channel respectively.

$ H_{out} = \frac{H_{in} + 2pad - 2(max\_d + \frac{k - 1} {2})} {stride1} $

$ W_{out} =\frac{W_{in} + 2pad - 2(max\_d +\frac{k - 1} {2})} {s1} $

$ C_{out} = (2 *\frac{max\_d} {stride2} + 1) ^ 2 $

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    kernel = create_attr(kernel)
    max_d = create_attr(max_d)
    stride1 = create_attr(stride1)
    stride2 = create_attr(stride2)
    pad = create_attr(pad)
    mode = parse_enum(mode, 'mode')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CorrelationOp(output_type, lhs, rhs, kernel=kernel, max_d=max_d, stride1=stride1, stride2=stride2, pad=pad, mode=mode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def cos(input, *, output_type=None, loc=None):
    """
HBIR tensor cos.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CosOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def cosh(input, *, output_type=None, loc=None):
    """
HBIR tensor cosh.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CoshOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def cumsum(input, axis, exclusive=0, reverse=0, *, output_type=None, loc=None):
    """
HBIR cumsum.

Performs cumulative sum of the input elements along the given axis.
By default, it will do the sum inclusively meaning the first element is copied as is.
Through an exclusive attribute, this behavior can change to exclude the first element.
It can also perform summation in the opposite direction of the axis. For that, set reverse attribute to 1.

  Args:
        input (Tensor): the input tensor.
        output (Tensor): Output tensor of the same type as input with cumulative sums of the input elements
  Attribute:
        axis (int): Must be in the range [-rank(input), rank(input)-1]. Negative value means counting dimensions from the back.
        exclusive (int): Must be 0 or 1, defaut is 0. 0 means the first element is copied to output, 1 will not.
        reverse (int): Must be 0 or 1, defaut is 0. 1 means performing summation in the opposite direction of the axis.

------

Prototype: Pytorch cumsum.
  
    """
    input = get_value_or_create_const(input)
    axis = create_attr(axis)
    exclusive = create_attr(exclusive)
    reverse = create_attr(reverse)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CumSumOp(output_type, input, axis, exclusive=exclusive, reverse=reverse, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def custom(inputs, srcPath, entryFuncName, *, includeDirs=None, libraryDirs=None, extraCompileArgs=None, extraLinkArgs=None, runtimeLibraryDirs=None, outputs_type=None, loc=None):
    """
HBIR custom op

CPU custom operator placeholder.


------

Parameters:
* inputs - tensor inputs, which type is list
* Other non-tensor arguments for CPU operators

Returns:
* outputs - list of tensor

    """
    inputs = get_value_or_create_const(inputs)
    srcPath = create_attr(srcPath)
    entryFuncName = create_attr(entryFuncName)
    includeDirs = create_attr(includeDirs)
    libraryDirs = create_attr(libraryDirs)
    extraCompileArgs = create_attr(extraCompileArgs)
    extraLinkArgs = create_attr(extraLinkArgs)
    runtimeLibraryDirs = create_attr(runtimeLibraryDirs)
    outputs_type = get_type_or_create_unranked_type(outputs_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.CustomOp(outputs_type, inputs, srcPath, entryFuncName, includeDirs=includeDirs, libraryDirs=libraryDirs, extraCompileArgs=extraCompileArgs, extraLinkArgs=extraLinkArgs, runtimeLibraryDirs=runtimeLibraryDirs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results


def deform_conv2d(input, weight, offset, mask, stride=(1, 1), pad=(0, 0, 0, 0), dilation=(1, 1), groupNum=1, offsetGroupNum=1, useMask=True, *, bias=None, output_type=None, loc=None):
    """
HBIR deformable 2-D convolution.

Applies a deformable 2D convolution over an input signal composed of several input channels.

------

Note:

* offset controls the offset for the sampling locations in the convolution kernel.

* mask controls different weights to different positions in the convolution kernel.

* stride controls the stride for the cross-correlation, an integer array with 2 elements, default value is (1,1).

* padding controls the amount of padding applied to the input, an integer array with 4 elements, the padding sequences is (h_begin,w_begin,h_end,w_end), default value is (0,0,0,0).

* dilation controls the spacing between kernel points, an integer array with 2 elements, default value is (0,0). It's harder to describe, but this [link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) has a nice visualization of what dilation does.

* groups controls the connections between inputs and outputs, an integer variable, default value is 1.

* offsetGroup controls the connections between inputs and offset, an integer variable, default value is 1.

* weight:  $(C_{out}, KH, KW, C_{in})$, bias shape = $C_{out}$ where KW and KH represent kernel's height and width, respectively.

------

Shape:

* Input: $(N,H_{in},W_{in},C_{in})$ or $(*,H_{in},W_{in},C_{in})$, where * represent any number of dimension.

* Offset: $(N,H_{out},W_{out},2\timesoffset_groups\timeskernel[0]\timeskernel[1])$ or $(*,H_{out},W_{out},2\timesoffset_groups\timeskernel[0]\timeskernel[1])$, where * represent any number of dimension.

* Mask: $(N,H_{out},W_{out},offset_groups\timeskernel[0]\timeskernel[1])$ or $(*,H_{out},W_{out},offset_groups\timeskernel[0]\timeskernel[1])$, where * represent any number of dimension.

* Output: $(N,H_{out},W_{out},C_{out})$ or $(H_{out},W{out},C_{out})$ or $(*,H_{out},W_{out},C_{out})$

$ H_{out}=\lfloor \frac{H_{in}+padding[0]+padding[2]-dilation[0]\times(kernel[0]-1)-1}{stride[0]}+1\rfloor $
$ W_{out}=\lfloor \frac{W_{in}+padding[1]+padding[3]-dilation[1]\times(kernel[1]-1)-1}{stride[1]}+1\rfloor $

------

Prototype: Pytorch convolution.

    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    offset = get_value_or_create_const(offset)
    mask = get_value_or_create_const(mask)
    bias = get_value_or_create_const(bias)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    groupNum = create_attr(groupNum)
    offsetGroupNum = create_attr(offsetGroupNum)
    useMask = create_attr(useMask)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.DeformConv2dOp(output_type, input, weight, offset, mask, stride=stride, pad=pad, dilation=dilation, groupNum=groupNum, offsetGroupNum=offsetGroupNum, useMask=useMask, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def div(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor division.

Applies division operator element-wise, $y_i=lhs_i\div rhs_i$.

------

Note:
* Our arithmetic operator support broadcast.

------

Prototype: Pytorch div.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.DivOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def dpp(inputs, anchors, anchorNum, filterThreshold, nmsThreshold, nmsMargin, seed, useClipping, stride, clsOffset, imageSize=(-1, -1), inputShift=4, legacyMode=True, useBpuFilter=True, maxBoxNum=4096, *, boxData_type=None, loc=None):
    """
HBIR detection post process.

    Legacy Operator: Detection Process.

    ------

    Note

      * The input features only supports shift quantization.
      * On J5 and later platforms, DPP will be split into Filter OP and CPU operators. Filter Op will run on BPU.

    ------

    Shape:

      * Input:
        * [1 - 5] input features. Normally, the input feature size is 1/2, 1/4, ..., or 1/32 of the original input image.
        * anchors: anchor table of dpp op.
        * anchorNum: anchor number of each input branch.
        * filterThreshold: threshold for score value.
        * nmsThreshold: threshold for nms operation.
        * nmsMargin: score margin of nms operation.
        * seed: seed for random() function.
        * useClipping: clipping flag for each input branch. if set, the box which exceeds the original image will be clipped.
        * stride: input features' stride.
        * clsOffset: class offset of each input branch.
        * imageSize: the image size, if clippling is set, all box should within this range.
        * inputShift: the input feature's quantization coefficient. for legacy mode, only shift quantization is supported.
        * legacyMode: legacy mode to be compatible with J2J3.
        * useBpuFilter: use Filter OP on BPU to accelerate dpp.
        * maxBoxNum: max box num for output
      * Output:
        * boxData: The output box data with format [coord0, coord1, coord2, coord3, max_score, class_index, pad, pad] @ int16.
  
    """
    inputs = get_value_or_create_const(inputs)
    anchors = create_array_attr(anchors)
    anchorNum = create_array_attr(anchorNum)
    filterThreshold = create_attr(filterThreshold)
    nmsThreshold = create_attr(nmsThreshold)
    nmsMargin = create_attr(nmsMargin)
    seed = create_attr(seed)
    useClipping = create_array_attr(useClipping)
    stride = create_array_attr(stride)
    clsOffset = create_array_attr(clsOffset)
    imageSize = create_array_attr(imageSize)
    inputShift = create_attr(inputShift)
    legacyMode = create_attr(legacyMode)
    useBpuFilter = create_attr(useBpuFilter)
    maxBoxNum = create_attr(maxBoxNum)
    boxData_type = get_type_or_create_unranked_type(boxData_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.DppOp(boxData_type, inputs, anchors, anchorNum, filterThreshold, nmsThreshold, nmsMargin, seed, useClipping, stride, clsOffset, imageSize=imageSize, inputShift=inputShift, legacyMode=legacyMode, useBpuFilter=useBpuFilter, maxBoxNum=maxBoxNum, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def dynamic_slice(input, starts, ends, axes, steps, *, output_type=None, loc=None):
    """
Slice a tensor out of a tensor, support dynamic input, begin, end, step, axes. Same as onnx slice.

Slice uses the starts, ends, axes and steps inputs to select a sub-tensor of its input data tensor.

------

Note:
* starts -1-D tensor of starting indices of corresponding axis in axes.
* ends -1-D tensor of ending indices (exclusive) of corresponding axis in axes.
* axes -1-D tensor of axes that starts and ends apply to.
* steps -1-D tensor of slice step of corresponding axis in axes.


------

Prototype: onnx slice.

    """
    input = get_value_or_create_const(input)
    starts = get_value_or_create_const(starts)
    ends = get_value_or_create_const(ends)
    axes = get_value_or_create_const(axes)
    steps = get_value_or_create_const(steps)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.DynamicSliceOp(output_type, input, starts, ends, axes, steps, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def elu(input, alpha, *, output_type=None, loc=None):
    """
HBIR ELU Op.

Applies Elu funciton element - wise.Elu function defined as:

$ Elu(x) = max(0, x) + alpha * (exp(min(0, x)) - 1) $

------

Note:
* alpha - Controls the value to which an ELU saturates for negative net inputs.

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch elu.

    """
    input = get_value_or_create_const(input)
    alpha = create_attr(alpha)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ELUOp(output_type, input, alpha, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def einsum(inputs, equation, *, output_type=None, loc=None):
    """
HBIR einsum.

Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.

Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation.

The general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output.

The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output.

For example, matrix multiplication can be computed using einsum as {` '"hbir.einsum"(%a, %b) {equation="ij,jk->ik"}' `}.
Here, j is the summation subscript and i and k the output subscripts.

------

* inputs : (List[Tensor]) The tensors to compute the Einstein summation of.

* equation : (str) The subscripts for the Einstein summation.

    The equation string specifies the subscripts (letters in [a-z]) for each dimension of the input operands in the
    same order as the dimensions, separating subscripts for each operand by a comma (','), e.g. 'ij,jk' specify subscripts for two 2D operands.
    The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1.
    The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order.
    The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts,
    and then summing out the dimensions whose subscripts are not part of the output.

Optionally, the output subscripts can be explicitly defined by adding an arrow ('->') at the end of the equation followed by the subscripts for the output.

For instance, the following equation computes the transpose of a matrix multiplication: 'ij,jk->ki'.

The output subscripts must appear at least once for some input operand and at most once for the output.

Ellipsis ('...') can be used in place of subscripts to broadcast the dimensions covered by the ellipsis.

Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts,
e.g. for an input operand with 5 dimensions, the ellipsis in the equation 'ab...c' cover the third and fourth dimensions.
The ellipsis does not need to cover the same number of dimensions across the operands
but the 'shape' of the ellipsis (the size of the dimensions covered by them) must broadcast together.

If the output is not explicitly defined with the arrow ('->') notation, the ellipsis will come first in the output (left-most dimensions),
before the subscript labels that appear exactly once for the input operands.
e.g. the following equation implements batch matrix multiplication '...ij,...jk'.

------

Note:

* Diagonal summation is not supported
* Sublist format is not supported
* The opt_einsum which is used to speed up computation or to consume less memory by optimizing contraction order is not supported.

------

Prototype: Pytorch einsum.

    """
    inputs = get_value_or_create_const(inputs)
    equation = create_attr(equation)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.EinsumOp(output_type, inputs, equation, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def embedding(input, weight, paddingIdx, scaleGradGyFreq, sparse, *, output_type=None, loc=None):
    """
HBIR embedding op

A simple lookup table that stores embeddings of a fixed dictionary and size.

This operation is often used to store word embeddings and retrieve them using indices.

The input to the operation is a list of indices, and the output is the corresponding word embeddings.
"
    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    paddingIdx = create_attr(paddingIdx)
    scaleGradGyFreq = create_attr(scaleGradGyFreq)
    sparse = create_attr(sparse)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.EmbeddingOp(output_type, input, weight, paddingIdx, scaleGradGyFreq, sparse, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def empty_like(input, *, dtype=None, output_type=None, loc=None):
    """
Hbir sram allocation operation, but not initial the memory

    The `hbir.empty_like` operation allocates a region of sram on the BPU. It is similar to `torch.empty_like` op.

    Example:

    ```
    %0 = hbir.empty() : tensor<64x8xf32>
    ```
  
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.EmptyLikeOp(output_type, input, dtype=dtype, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def equal(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor equal.
Determines whether two tensors are equal element by element - wise, $y_i = (lhs_i == rhs_i) $.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.EqualOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def erf(input, *, output_type=None, loc=None):
    """
HBIR tensor erf.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ErfOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def exp(input, *, output_type=None, loc=None):
    """
HBIR tensor exp.

Applies exponential operator element - wise, $y=e^{x}$.

------

Note:
* Returns a new tensor with the exponential of the elements of the input tensor input.


    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ExpOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def fake_cast(input, dtype, *, output_type=None, loc=None):
    """
fake elemental type cast operation
Cast float input to specified dtype, and then cast back to the same float type.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.FakeCastOp(output_type, input, dtype, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def filter(input, channelBegin, channelEnd, threshold, *, maxValue_type=None, maxIndex_type=None, filterCoord_type=None, filterData_type=None, loc=None):
    """
HBIR filter op

Filter data by comparing its correspond score and given threshold.
Filter constains two steps: Apply reduce max on channel c of input to get filter score and index, followed by filter of input supported by hardware.

    """
    input = get_value_or_create_const(input)
    channelBegin = create_attr(channelBegin)
    channelEnd = create_attr(channelEnd)
    threshold = create_attr(threshold)
    maxValue_type = get_type_or_create_unranked_type(maxValue_type)
    maxIndex_type = get_type_or_create_unranked_type(maxIndex_type)
    filterCoord_type = get_type_or_create_unranked_type(filterCoord_type)
    filterData_type = get_type_or_create_unranked_type(filterData_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.FilterOp(maxValue_type, maxIndex_type, filterCoord_type, filterData_type, input, channelBegin, channelEnd, threshold, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1], op.results[2], op.results[3]


def flip(input, dims, *, output_type=None, loc=None):
    """
HBIR flip

Reverse the order of the input tensor along given axis in dims.

------

Parametes:
* input: the input tensor.
* dims: axis need to reverse.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: same as the input.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.FlipOp(output_type, input, dims, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def floor(input, *, output_type=None, loc=None):
    """
HBIR tensor floor.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.FloorOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def fpp(inputs, anchors, anchorNum, filterThreshold, nmsThreshold, nmsMargin, seed, headerPadSize, dataChannel, useClipping, stride, clsOffset, imageSize=(-1, -1), inputShift=4, legacyMode=True, maxBoxNum=4096, dataRank=3, *, boxData_type=None, loc=None):
    """
HBIR Filter Post Process.

    Operator: Filter Detection Process.

    ------

    Note

      * The input features only supports shift quantization.

    ------

    Shape:

      * Input:
        * [1 - 5] input features from bpu filter raw data.
        * anchors: anchor table to calculate boxes.
        * anchorNum: anchor number of each input branch.
        * filterThreshold: threshold for score value.
        * nmsThreshold: threshold for nms operation.
        * nmsMargin: score margin of nms operation.
        * seed: seed for random() function.
        * headerPadSize: padding size of filtered box header, bayes is 0, nash is 8.
        * dataChannel: the origin filter input data channel
        * useClipping: clipping flag for each input branch. if set, the box which exceeds the original image will be clipped.
        * stride: input features' stride.
        * clsOffset: class offset of each input branch.
        * imageSize: the image size, if clippling is set, all box should within this range.
        * inputShift: the input feature's quantization coefficient. for legacy mode, only shift quantization is supported.
        * legacyMode: legacy mode to be compatible with J2J3.
        * maxBoxNum: config max box num for output, if not 4096, output is static shape.
        * dataRank: the origin filter input data rank, since bpu filter is rank1 output.
      * Output:
        * boxData: The output box data with format [coord0, coord1, coord2, coord3, max_score, class_index, pad, pad] @ int16.
  
    """
    inputs = get_value_or_create_const(inputs)
    anchors = create_array_attr(anchors)
    anchorNum = create_array_attr(anchorNum)
    filterThreshold = create_attr(filterThreshold)
    nmsThreshold = create_attr(nmsThreshold)
    nmsMargin = create_attr(nmsMargin)
    seed = create_attr(seed)
    headerPadSize = create_attr(headerPadSize)
    dataChannel = create_array_attr(dataChannel)
    useClipping = create_array_attr(useClipping)
    stride = create_array_attr(stride)
    clsOffset = create_array_attr(clsOffset)
    imageSize = create_array_attr(imageSize)
    inputShift = create_attr(inputShift)
    legacyMode = create_attr(legacyMode)
    maxBoxNum = create_attr(maxBoxNum)
    dataRank = create_attr(dataRank)
    boxData_type = get_type_or_create_unranked_type(boxData_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.FppOp(boxData_type, inputs, anchors, anchorNum, filterThreshold, nmsThreshold, nmsMargin, seed, headerPadSize, dataChannel, useClipping, stride, clsOffset, imageSize=imageSize, inputShift=inputShift, legacyMode=legacyMode, maxBoxNum=maxBoxNum, dataRank=dataRank, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def gelu(input, approximate="none", *, output_type=None, loc=None):
    """
HBIR GELU activation.

Applies gelu funciton element-wise. Gelu functon defined as:

$ Gelu(x) = xP(X\leq x) = x*\phi(x) $

------

Note:
* Gelu is shorthand for Gaussian Error Linear Unit.
* approximate - support 'none' or 'tanh' mode

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch gelu.

    """
    input = get_value_or_create_const(input)
    approximate = parse_enum(approximate, 'approximate')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GELUOp(output_type, input, approximate=approximate, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def gather_elements(input, indices, dim, *, output_type=None, loc=None):
    """
HBIR gather op for onnx GatherElements.
 HBIR gather op for onnx GatherElements. 
    """
    input = get_value_or_create_const(input)
    indices = get_value_or_create_const(indices)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GatherElementsOp(output_type, input, indices, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def gather_nd(input, indices, batchDim, *, output_type=None, loc=None):
    """
HBIR gather_nd op for onnx gather_nd.
 HBIR gather_nd op for onnx gather_nd. 
    """
    input = get_value_or_create_const(input)
    indices = get_value_or_create_const(indices)
    batchDim = create_attr(batchDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GatherNdOp(output_type, input, indices, batchDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def greater_equal(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor greater_equal.
Applies greater_equal operator element - wise, $y_i = lhs_i > = rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GreaterEqualOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def greater(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor greater.
Applies greater operator element - wise, $y_i = lhs_i > rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GreaterOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def grid_sample(input, grid, mode="bilinear", expansionMode="constant", alignCorner=False, *, padValue=None, output_type=None, loc=None):
    """
HBIR grid_sample.

From the input and a flow-field grid, computes the output using input values and pixel locations from the grid.

------

Shape:
* input: input of shape $(*, H_{in}, W_{in}, C_{in}) $, where * represent any number of dimension.
* grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$
* output: $(*, H_{out}, W_{out}, C_{in})$

    """
    input = get_value_or_create_const(input)
    grid = get_value_or_create_const(grid)
    mode = parse_enum(mode, 'mode')
    expansionMode = parse_enum(expansionMode, 'expansionMode')
    alignCorner = create_attr(alignCorner)
    padValue = create_attr(padValue)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.GridSampleOp(output_type, input, grid, mode=mode, expansionMode=expansionMode, alignCorner=alignCorner, padValue=padValue, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def identity(input, *, output_type=None, loc=None):
    """
Identity operator.
Return tensor copy from input tensor.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.IdentityOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def image_convert(inputs, mode="NV12", *, output_type=None, loc=None):
    """
Image format conversion, such as NV12 to YUV444.

Convert images from various forms into tensor and record image information.

------

Parameters:
* inputs - input image: 8-bit unsigned
* mode - the source image format. Currently, only support "NV12"
* output_type - output image data type: 8-bit signed, 8-bit unsigned

    """
    inputs = get_value_or_create_const(inputs)
    mode = parse_enum(mode, 'mode')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ImageConvertOp(output_type, inputs, mode=mode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def image_preprocess(input, csc="YUVBT601FULL2RGB", divisor=255, mean=(0.485, 0.456, 0.406), sd=(0.229, 0.224, 0.225), *, output_type=None, loc=None):
    """
Image preprocess, convert to bgr/rgb then normalize.

Convert input to rgb/bgr then normalize with given mean and standard deviation

------

Parameters:
* input - input image(yuv or rgb/bgr), 8-bit singed/unsigned
* csc - the color scale conversion mode. The modes are "SKIP", "YUVBT601FULL2RGB"(default), "YUVBT601FULL2BGR", "YUVBT601VIDEO2RGB", "YUVBT601VIDEO2BGR", "RGB2BGR", "BGR2RGB"
* divisor - divisor when convert from int to float
* mean - mean per channel in normalization
* sd - standard deviation per channel in normalization
* output_type - float32 type

    """
    input = get_value_or_create_const(input)
    csc = parse_enum(csc, 'csc')
    divisor = create_attr(divisor)
    mean = create_array_attr(mean)
    sd = create_array_attr(sd)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ImagePreProcessOp(output_type, input, csc=csc, divisor=divisor, mean=mean, sd=sd, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def index(input, index, dim, *, output_type=None, loc=None):
    """
HBIR index op for aten::index_select

  Returns a new tensor which indexes the :attr:`input` tensor along dimension
  :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`.

  The returned tensor has the same number of dimensions as the original tensor
  (:attr:`input`).  The :attr:`dim`\ th dimension has the same size as the length
  of :attr:`index`; other dimensions have the same size as in the original tensor.

  Args:
        input (Tensor): the input tensor.
        dim (int): the dimension in which we index
        index (IntTensor or LongTensor): the tensor containing the indices to index
  
    """
    input = get_value_or_create_const(input)
    index = get_value_or_create_const(index)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.IndexOp(output_type, input, index, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def kv_cache_update(cachedInput, input, dim, repeatNum, *, output_type=None, cachedOut_type=None, loc=None):
    """
KV cache update op that concat cached KV and current token.
Concat cached KV and new KV of current token, output updated input.
    """
    cachedInput = get_value_or_create_const(cachedInput)
    input = get_value_or_create_const(input)
    dim = create_attr(dim)
    repeatNum = create_attr(repeatNum)
    output_type = get_type_or_create_unranked_type(output_type)
    cachedOut_type = get_type_or_create_unranked_type(cachedOut_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.KvCacheUpdateOp(output_type, cachedOut_type, cachedInput, input, dim, repeatNum, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def layernorm(input, dims, eps, *, weight=None, bias=None, output_type=None, loc=None):
    """
Hbir Layer Normalize

Applies Layer Normalization over a mini - batch of inputs. This compute can be precisely described as:

$ y = \frac{x - mean[x]} {\sqrt{Var[x] +\epsilon}} * weight + bias $

The Mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of normalized_shape(dims).

------

Note:
* Unlike Batch Normalization, which compute mean and standard - deviation, LayerNormalization compute these in single sample's different dimension.
* dims controls normalized_shape.

------

Shape:
* Input: $(N, *)$
* Output: $(N, *)$ (same shape as input)

------

Prototype: Pytorch layerNorm.


    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    dims = create_array_attr(dims)
    eps = create_attr(eps)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LayerNormOp(output_type, input, dims, eps, weight=weight, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def leaky_relu(input, slop, *, output_type=None, loc=None):
    """
HBIR Leaky ReLU Op.

Applies LeakyRelu funciton element - wise.LeakyRelu function defined as:

$ LeakyRelu(x) = max(0, x) + slope * min(0, x) $

------

Note:
* slope - Controls the angle of the negative slope.

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch leaky_relu.

    """
    input = get_value_or_create_const(input)
    slop = create_attr(slop)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LeakyReLUOp(output_type, input, slop, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def less_equal(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor less_equal.
Applies less_equal operator element - wise, $y_i = lhs_i < = rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LessEqualOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def less(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor less.
Applies less operator element - wise, $y_i = lhs_i < rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LessOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def linear(input, weight, *, bias=None, output_type=None, loc=None):
    """
HBIR Linear.

Applies a linear transformation to the incoming data: $y = xW ^ T + b$

------

Note:
* Weight's shape is $(C_{in},C_{out})$.

------

Shape:
* Input: $(*, C_{in})$ where $*$ represents any number of dimensions and $C_{in}$ = in_features.
* Output: $(*, C_{out})$ where all but the last dimension are the same shape as the input and $C_{out}$ = out_features.

------

Prototype: pytorch linear.

    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    bias = get_value_or_create_const(bias)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LinearOp(output_type, input, weight, bias=bias, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def log(input, *, output_type=None, loc=None):
    """
HBIR tensor log.

Applies natural logarithm operator element - wise, $y = \log_e{(x)}$.

------

Note:
* Returns a new tensor with the natural logarithm of the elements of input.


    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def log_softmax(input, dim, *, output_type=None, loc=None):
    """
HBIR LogSoftmax Op.

Applies the LogSoftmax function to an n - dimensional input Tensor rescaling them so that the elements of the n-dimensional output.
The output tensor has the same dimension and shape as the input with values in the range [-inf, 0)

LogSoftmax function is defined as:

$ LogSoftmax(x_i) = log(\frac{exp(x_i)} {\sum_jexp(x_j)}) $

------

Shape:
* Input(*), where * means, any number of additional dimensions.
* Output(*), same shape as the input.

------

Prototype: Pytorch LogSoftmax.

    """
    input = get_value_or_create_const(input)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogSoftmaxOp(output_type, input, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def logical_and(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor and.
Applies 'logical and' operator element - wise, $y_i = lhs_i \&\& rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogicalAndOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def logical_not(input, *, output_type=None, loc=None):
    """
HBIR tensor logical not, output bool.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogicalNotOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def logical_or(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor or.
Applies 'logical or' operator element - wise, $y_i = lhs_i || rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogicalOrOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def logical_xor(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor xor.
Applies 'logical xor' operator element - wise, $y_i = lhs_i xor rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LogicalXorOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def lp_normalize(input, p=2.0, dim=1, eps=1e-12, *, output_type=None, loc=None):
    """
HBIR LpNormalization.

Performs $L_p$ normalization of inputs over specified dimension.

------

For a tensor $input$ of sizes $(n_0, ..., n_{dim}, ..., n_k)$, each $n_{dim}$ -element vector $v$ along dimension $dim$ is transformed as
$ v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)} $.

With the default arguments it uses the Euclidean norm over vectors along dimension $1$ for normalization.

------

Prototype: Pytorch normalize.

    """
    input = get_value_or_create_const(input)
    p = create_attr(p)
    dim = create_attr(dim)
    eps = create_attr(eps)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LpNormalizeOp(output_type, input, p=p, dim=dim, eps=eps, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def lp_pool(input, kernel, stride, pad, dilation, p, ceilMode=False, *, output_type=None, loc=None):
    """
Hbir LpPool

Applies a n-D power-average pooling over an input signal composed of several input planes.

On each window, the function computed is:

$ f(X) = \sqrt[p]{\sum_{x \in X} x^{p}} $

- At p = $\infty$, one gets Max Pooling
- At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)

------

Note:

* If the sum to the power of $p$ is zero, the gradient of this function is not defined.
This implementation will set the gradient to zero in this case.

------

Prototype: Pytorch LPPool1d & LPPool2d.

    """
    input = get_value_or_create_const(input)
    kernel = create_array_attr(kernel)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    p = create_attr(p)
    ceilMode = create_attr(ceilMode)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LpPoolOp(output_type, input, kernel, stride, pad, dilation, p, ceilMode=ceilMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def lut(input, config, *, output_type=None, loc=None):
    """
HBIR lut op
lut on int8
    """
    input = get_value_or_create_const(input)
    config = get_value_or_create_const(config)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.LutOp(output_type, input, config, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def make_tuple(inputs, *, output_type=None, loc=None):
    """
make tuple from anything

    """
    inputs = get_value_or_create_const(inputs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MakeTupleOp(output_type, inputs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def masked_select(input, mask, *, output_type=None, loc=None):
    """
HBIR masked_select op

Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.

The shapes of the mask tensor and the input tensor don't need to match, but they must be broadcastable.

    """
    input = get_value_or_create_const(input)
    mask = get_value_or_create_const(mask)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MaskedSelectOp(output_type, input, mask, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def matmul(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR Matrix Multiplication.

Applies matrix multiplication between two inputs: $C = A \times B$,
where $\times$ means matrix multiplication.

------

Note:
* If both tensors are 1-dimensional, the dot product (scalar) is returned
* If both arguments are 2-dimensional, the matrix-matrix product is returned
* If the first argument is 1-dimensional and the second argument is 2-dimensional, the matrix-vector product is returned
* If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned
* If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned

------

Shape:
* lhs: $(B_{l}, M, C)$, where $B_{l}$ represent any number of dimension.
* rhs: $(B_{r}, C, N)$, where $B_{r}$ represent any number of dimension.
* output: $(B_{o}, M, N)$, where $B_{o}$ represent represent the result of broadcast between $B_{l}$ and $V_{r}$.

------

Prototype: Pytorch Matmul.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MatMulOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def max(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor max.

Applies maximum operator element-wise, $y_i=max(lhs_i,rhs_i)$.

------

Note:
* Our arithmetic operator support broadcast.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MaxOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def max_pool(input, kernel, stride=(1), pad=(0), dilation=(1), ceilMode=False, *, output_type=None, loc=None):
    """
HBIR n-D max pooling(supports 1d, 2d and 3d).

Applies a n-D max Pooling over an input.

In the 2d case, for example, the output value of the operator with input size $(N, H, W, C)$,
output $(N, H_{out}, W_{out}, C)$ and kernel size $(ker_{h}, ker_{w})$ can be precisely described as:

$ out(N_i, h, w, C_j) = \frac{1} { ker_h *ker_w }\max_{m = 0} ^ { ker_h - 1 }\max_{n = 0} ^
{ ker_w - 1 } input(N_i, stride[0] \times h + m, stride[1] \times w + n, C_j) $

where $h,w$ respectively represent the size of H and W.

------

Note:

* parameters has the same manner as the Conv2D operator, the same goes for the output size.
* ceilMode controls output 's compute is mode of floor or ceil, it' s default value is false.

------

Shape:

* Input: $(N, H_{in}, W_{in}, C)$ or $(H_{in}, W_{in}, C)$ or $(*, H_{in}, W_{in})$

* Output: $(N, H_{out}, W_{out}, C)$ or $(H_{out}, W_{out}, C)$ or $(*, H_{out}, W_{out}, C)$,
where $*$ represents any number of dimension.

$ H_{out} = \lfloor {\frac{H_{in} + padding[0] + padding[2] - kernel[0]} {stride[0]} + 1}\rfloor $

$ W_{out} = \lfloor {\frac{W_{in} + padding[1] + padding[3] - kernel[1]} {stride[1]} + 1}\rfloor $

if ceilMode = true, please use ceil replace floor in the above ouput formula.

------

Prototype: Pytorch max_pool.

    """
    input = get_value_or_create_const(input)
    kernel = create_array_attr(kernel)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    ceilMode = create_attr(ceilMode)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MaxPoolOp(output_type, input, kernel, stride=stride, pad=pad, dilation=dilation, ceilMode=ceilMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def max_pool_with_indices(input, kernel, stride=(1), pad=(0), dilation=(1), ceilMode=False, *, output_type=None, indices_type=None, loc=None):
    """
HBIR max_pool_with_indices2d op

Use Hbir_MaxPoolWithIndicesOp instead of Hbir_MaxPoolOp when MaxUnPool op is followed.
When converting from torch op or onnx op, converting method should set legal attributes.

    """
    input = get_value_or_create_const(input)
    kernel = create_array_attr(kernel)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    ceilMode = create_attr(ceilMode)
    output_type = get_type_or_create_unranked_type(output_type)
    indices_type = get_type_or_create_unranked_type(indices_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MaxPoolWithIndicesOp(output_type, indices_type, input, kernel, stride=stride, pad=pad, dilation=dilation, ceilMode=ceilMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def max_unpool(input, indices, kernel, outputShape=(0), stride=(1), pad=(0), dilation=(1), *, output_type=None, loc=None):
    """
HBIR max_unpool op

  MaxUnpool2d takes in as input the output of MaxPool2d including the indices of
  the maximal values and computes a partial inverse in which all non-maximal values are set to zero.
  When converting from torch op or onnx op, converting method should set legal attributes.
  
    """
    input = get_value_or_create_const(input)
    indices = get_value_or_create_const(indices)
    kernel = create_array_attr(kernel)
    outputShape = create_array_attr(outputShape)
    stride = create_array_attr(stride)
    pad = create_array_attr(pad)
    dilation = create_array_attr(dilation)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MaxUnpoolOp(output_type, input, indices, kernel, outputShape=outputShape, stride=stride, pad=pad, dilation=dilation, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def min(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor min.

Applies minimum operator element-wise, $y_i=min(lhs_i,rhs_i)$.

------

Note:
* Our arithmetic operator support broadcast.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MinOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def mish(input, *, output_type=None, loc=None):
    """
HBIR Mish activation.

Applies the mish function element-wise. Mish fucntion defined as:

$ Mish(x) = x * Tanh(\zeta(x)), \zeta(x) = ln(1+e^x) $

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MishOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def mod(lhs, rhs, sameSignAsDividend=True, *, output_type=None, loc=None):
    """
Hbir get modulo of two dividing tensors

Computes the modulo function.
It is equivalent to the operator x1 % x2

------

Parameters:
* sameSignAsDividend: result has the same sign as dividend or divisor. Default true means same sign as dividend.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    sameSignAsDividend = create_attr(sameSignAsDividend)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ModOp(output_type, lhs, rhs, sameSignAsDividend=sameSignAsDividend, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def mul(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor multiplication.

Applies multiplication operator element-wise, $y_i=lhs_i\times rhs_i$.

------

Note:
* Our arithmetic operator support broadcast.

------

Prototype: Pytorch mul.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.MulOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def nan_to_num(input, nan=0.0, *, posinf=None, neginf=None, output_type=None, loc=None):
    """
HBIR tensor nan_to_num.

Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf.

------

Note:
* Returns a new tensor with the replace value of the elements of input.


    """
    input = get_value_or_create_const(input)
    nan = create_attr(nan)
    posinf = create_attr(posinf)
    neginf = create_attr(neginf)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NanToNumOp(output_type, input, nan=nan, posinf=posinf, neginf=neginf, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def neg(input, *, output_type=None, loc=None):
    """
HBIR tensor neg.

Applies negation operator element - wise, $y = x\times - 1$.

------

Note:
* Returns a new tensor with the negative of the elements of input.


    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NegOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def nms(boxes, scores, mode="xyxy", iouThreshold=0.0, scoreThreshold=0.0, maxOutputBoxesPerClass=1, *, indices_type=None, loc=None):
    """
HBIR NonMaxSuppression

Filter out boxes that have high intersection-over-union (IOU) overlap with previously selected boxes.

------

Parameters:
* boxes: The input boxes.
* scores: The dimension to sort along.
* boxType: The format of the box data. Support mode are "xyxy": [x1,y1,x2,y2],  "yxyx": [y1,x1,y2,x2], "xywh": [x_center, y_center, width, height]
* iouThreshold: Representing the threshold for deciding whether boxes overlap too much with respect to IOU. It is scalar. Value range [0, 1]. Default to 0.
* scoreThreshold: Representing the threshold for deciding when to remove boxes based on score. It is a scalar.
* maxOutputBoxesPerClass: Representing the maximum number of boxes to be selected per batch per class. It is a scalar. Default to 1.

------

Shape:
* boxes: $(batch, boxesNum, 4)$
* scores: $(batch, classNum, boxesNum)$
* Indices: $(N, 3)$. N is euqal batch*classNum*min(boxesNum,maxOutputBoxesPerClass). The last dim means selected index, format is [batch_index, class_index, box_index].


------

Prototype: ONNX NonMaxSuppression

    """
    boxes = get_value_or_create_const(boxes)
    scores = get_value_or_create_const(scores)
    mode = parse_enum(mode, 'mode')
    iouThreshold = create_attr(iouThreshold)
    scoreThreshold = create_attr(scoreThreshold)
    maxOutputBoxesPerClass = create_attr(maxOutputBoxesPerClass)
    indices_type = get_type_or_create_unranked_type(indices_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NonMaxSuppressionOp(indices_type, boxes, scores, mode=mode, iouThreshold=iouThreshold, scoreThreshold=scoreThreshold, maxOutputBoxesPerClass=maxOutputBoxesPerClass, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def nonzero(input, *, output_type=None, loc=None):
    """
HBIR nonzero op for torch.nonzero

Find all indices in tensor that are not 0

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NonZeroOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def not_equal(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor not_equal.
Applies not_equal operator element - wise, $y_i = lhs_i != rhs_i$.
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NotEqualOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def numba(inputs, path, irFileName, funcName, libraryMode="NONE", *, libraryName=None, outputs_type=None, loc=None):
    """
HBIR numba op
CPU code segment generated from numba jit compiler
    """
    inputs = get_value_or_create_const(inputs)
    path = create_attr(path)
    irFileName = create_attr(irFileName)
    funcName = create_attr(funcName)
    libraryMode = parse_enum(libraryMode, 'libraryMode')
    libraryName = create_attr(libraryName)
    outputs_type = get_type_or_create_unranked_type(outputs_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.NumbaOp(outputs_type, inputs, path, irFileName, funcName, libraryMode=libraryMode, libraryName=libraryName, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results


def pad(input, begin, end, expansionMode="constant", foldable=True, *, padValue=None, output_type=None, loc=None):
    """
Pad at both edges of Tensor.
Padding at the begin and end position with constant / border value.
    """
    input = get_value_or_create_const(input)
    begin = create_array_attr(begin)
    end = create_array_attr(end)
    expansionMode = parse_enum(expansionMode, 'expansionMode')
    padValue = create_attr(padValue)
    foldable = create_attr(foldable)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.PadOp(output_type, input, begin, end, expansionMode=expansionMode, foldable=foldable, padValue=padValue, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def point_pillar_preprocess(points, pcRanges, voxelSizes, maxVoxelNum, maxPointsPerVoxel, normDims=(0, 1, 2), *, normRanges=None, voxels_type=None, coords_type=None, loc=None):
    """
HBIR point pillar preprocess op.
HBIR point pillar preprocess.Voxelization and Normalization
    """
    points = get_value_or_create_const(points)
    pcRanges = create_array_attr(pcRanges)
    normRanges = create_array_attr(normRanges)
    voxelSizes = create_array_attr(voxelSizes)
    maxVoxelNum = create_attr(maxVoxelNum)
    maxPointsPerVoxel = create_attr(maxPointsPerVoxel)
    normDims = create_array_attr(normDims)
    voxels_type = get_type_or_create_unranked_type(voxels_type)
    coords_type = get_type_or_create_unranked_type(coords_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.PointPillarPreProcessOp(voxels_type, coords_type, points, pcRanges, voxelSizes, maxVoxelNum, maxPointsPerVoxel, normDims=normDims, normRanges=normRanges, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def point_pillar_scatter(voxels, coords, outShape, *, output_type=None, loc=None):
    """
HBIR point pillar scatter op.
HBIR point pillar scatter.
    """
    voxels = get_value_or_create_const(voxels)
    coords = get_value_or_create_const(coords)
    outShape = create_array_attr(outShape)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.PointPillarScatterOp(output_type, voxels, coords, outShape, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def pow(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor pow.
Pow takes input data(lhs) and exponent Tensor(rhs),
                        and produces one output data where the function $f(x) = x ^ {exponent}$,
                        is applied to the data tensor element - wise
    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.PowOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def prelu(input, slope, *, output_type=None, loc=None):
    """
HBIR tensor prelu
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    slope = get_value_or_create_const(slope)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.PreluOp(output_type, input, slope, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def relu(input, *, output_type=None, loc=None):
    """
HBIR ReLU activation.

Applies the rectified linear unit function element - wise.Relu function is defined as:

$ ReLU(x) = (x) ^ + = max(0, x) $

------

Shape:
* Input(*), where * means any number of dimensions.
* Output(*), same shapes as the input.

------

Prototype: Pytorch Relu.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReLUOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reciprocal(input, *, output_type=None, loc=None):
    """
HBIR tensor reciprocal.

Applies reciprocal operator element - wise, $y = \frac{1}{x}$.

------

Note:
* Returns a new tensor with the reciprocal of the elements of input.


    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReciprocalOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_all(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Tests if all elements in input evaluate to True.

Return True if all elements in the row evaluate to True and False otherwise.

------

Parameters:
* input: the input tensor.
* dims: dimensions to perform reduce all on. If it's a list, reduce over all of them. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output shape will be 1x4).


    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceAllOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_argmax(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate max on multiple axes and return its index.

Return the indices of the max elements of the input tensor's element along the provided axis.

------

Parameters:
* input: the input tensor.
* dims: dimension to perform reduce argmax on. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output shape will be 1x3x4).

------

Prototype: ONNX ReduceArgMax.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceArgmaxOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_argmin(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate min on multiple axes and return its index.

Return the indices of the min elements of the input tensor's element along the provided axis.

------

Parameters:
* input: the input tensor.
* dims: dimension to perform reduce argmin on. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1], the output shape will be 1x3x4).

------

Prototype: ONNX ReduceArgMin.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceArgminOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_logsum(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate the logsum (i.e. reduce sum -> log) on multiple axes.

Return the logsum of all elements in the input tensor.

------

Note:
* Returns the logsum of each row of the input tensor in the given dimension dims.If dims is a list of dimensions, reduce over all of them.
* keepdim is True, the output tensor is of the same size as input.

------

Prototype: ONNX ReduceLogSum

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceLogSumOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_max(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate max on multiple axes.

Return the max value of all elements in the provided axes of the input tensor.

------

Parameters:
* input: the input tensor.
* dims: dimensions to perform reduce max on. If it's a list, reduce over all of them. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output shape will be 1x4).

------

Prototype: ONNX ReduceMax.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceMaxOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_mean(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate mean on multiple axes.

Return the mean of all elements in the provided axes of the input tensor.

------

Parameters:
* input: the input tensor.
* dims: dimensions to perform reduce mean on. If it's a list, reduce over all of them. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output shape will be 1x4).

------

Prototype: ONNX ReduceMean.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceMeanOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_min(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate min on multiple axes.

Return the min value of all elements in the provided axes of the input tensor.

------

Parameters:
* input: the input tensor.
* dims: dimensions to perform reduce min on. If it's a list, reduce over all of them. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output shape will be 1x4).

------

Prototype: ONNX ReduceMin

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceMinOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_prod(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate product (i.e multiplication) on multiple axes.

Return the product of all elements in the input tensor.

------

Note:
* Returns the product of each row of the input tensor in the given dimension dims.If dims is a list of dimensions, reduce over all of them.
* keepdim is True, the output tensor is of the same size as input.

------

Prototype: TensorFlow reduce_prod

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceProdOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_sum(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate sum on multiple axes.

Return the sum of all elements in the provided axes of the input tensor.

------

Parameters:
* input: the input tensor.
* dims: dimensions to perform reduce sum on. If it's a list, reduce over all of them. Accepted range is [-r, r - 1] where r = rank(input).
* keepDim: keep the reduced dimensions or not. Default true means keep reduced dimensions.

------

Shape:
* input: $(*)$, where * represents any dimension.
* output: if keepDim is True, same as input. Otherwise, all reduced dims will be discarded (e.g. if input is of shape 1x2x3x4 and dims=[1, 2], the output shape will be 1x4).

------

Prototype: ONNX ReduceSum.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceSumOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reduce_sumsquare(input, dims, keepDim=True, *, output_type=None, loc=None):
    """
Calculate the sum square on multiple axes.

Return the sum square of all elements in the input tensor.

------

Note:
* Returns the sum square of each row of the input tensor in the given dimension dims.If dims is a list of dimensions, reduce over all of them.
* keepdim is True, the output tensor is of the same size as input.

------

Prototype: ONNX ReduceSumSquare

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    keepDim = create_attr(keepDim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReduceSumSquareOp(output_type, input, dims, keepDim=keepDim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def reshape(input, shape, foldable=True, *, output_type=None, loc=None):
    """
View a tensor as another shape

Returns a tensor with the same data and number of elements as input, but with the specified shape.When possible, the returned tensor will be a view of input.

------

Note:
* shape - the new shape.

------

Prototype: Pytorch reshape.

    """
    input = get_value_or_create_const(input)
    shape = create_array_attr(shape)
    foldable = create_attr(foldable)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ReshapeOp(output_type, input, shape, foldable=foldable, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def resize2d(input, step, initialOffset=(0.0, 0.0), mode="bilinear", expansionMode="constant", *, ratio=None, size=None, padValue=None, output_type=None, loc=None):
    """
HBIR 2-D resizing.

Scale the input proportionally.

------

Note:
* ratio controls zoom size.*mode controls interpolation type, it's default value is nearest.

------

Shape:
* Input: $(*, H_{in}, W_{in}, C)$
* Output: $(*, H_{out}, W_{out}, C)$, where

$ H_{out} = \lfloor{H_{in} * ratio}\rfloor $

$ W_{out} = \lfloor{W_{in} * ratio}\rfloor $

------

Prototype: Pytorch upsample_nearest2d.

    """
    input = get_value_or_create_const(input)
    ratio = create_array_attr(ratio)
    size = create_array_attr(size)
    step = create_array_attr(step)
    initialOffset = create_array_attr(initialOffset)
    mode = parse_enum(mode, 'mode')
    expansionMode = parse_enum(expansionMode, 'expansionMode')
    padValue = create_attr(padValue)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.Resize2dOp(output_type, input, step, initialOffset=initialOffset, mode=mode, expansionMode=expansionMode, ratio=ratio, size=size, padValue=padValue, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def rle(input, *, output_type=None, loc=None):
    """
HBIR rle op
Run length encode along with writing data from L1M to Ddr.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RleOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def rms_norm(input, dims, eps, *, weight=None, output_type=None, loc=None):
    """
Hbir Root Mean Square Normalization

Applies Root Mean Square Normalization over a mini - batch of inputs. This compute can be precisely described as:

$ y = \frac{x}{\sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2}} $

The Mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of normalized_shape(dims).

------

Note:
* Unlike Batch Normalization, which compute mean and standard - deviation, RMS Normalization compute these in single sample's different dimension.
* dims controls normalized_shape.

------

Shape:
* Input: $(N, *)$
* Output: $(N, *)$ (same shape as input)


    """
    input = get_value_or_create_const(input)
    weight = get_value_or_create_const(weight)
    dims = create_array_attr(dims)
    eps = create_attr(eps)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RmsNormOp(output_type, input, dims, eps, weight=weight, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def roi_align(inputs, shape, featureStrides, samplingRatio, interpolateMode, canonicalBoxSize, canonicalLevel, *, boxClipRatio=None, output_type=None, loc=None):
    """
HBIR roi align.

Crop the ROIs from a few feature maps, and resize them to the specified shape.The first input represents the ROI,
and the rest of the inputs serve as features.

------

Shape:
* Inputs: this first input is Roi: $(N, 4)$ or $(N, 6)$ represent the N ROIs, the rest of the inputs is input features: $(H_{in}, W_{in}, C_{in})$
* Output: $(N, H_{out}, W_{out}, C_{in})$,
where $H_{out}$ and $W_{out}$ are the specified shape controled by the parameter.

    """
    inputs = get_value_or_create_const(inputs)
    shape = create_array_attr(shape)
    featureStrides = create_array_attr(featureStrides)
    samplingRatio = create_attr(samplingRatio)
    interpolateMode = create_attr(interpolateMode)
    canonicalBoxSize = create_attr(canonicalBoxSize)
    canonicalLevel = create_attr(canonicalLevel)
    boxClipRatio = create_array_attr(boxClipRatio)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RoiAlignOp(output_type, inputs, shape, featureStrides, samplingRatio, interpolateMode, canonicalBoxSize, canonicalLevel, boxClipRatio=boxClipRatio, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def roi_resize(y, roi, size, padValue=(0, 0), mode="NV12", interpolateMode="bilinear", expansionMode="constant", *, uv=None, output_type=None, loc=None):
    """
HBIR roi resize.
Crop the image based on the given ROI and SIZE, then resize them to the specified shape.

  ------

  Parameters:
  * y: 8-bit signed/unsigned image of y plane or yuv444
  * uv: 8-bit signed/unsigned iamge containing interleaved u/v plane(nv12), the input is optional
  * size: the destination size after resizing, target shape of H&W
  * padValue: fill value for 'constant' padding, which is a array of size 2, the first value is for y, and the second value is for uv
  * roi: regions of interest to resize, is a 2-D input of shape (num_rois, 4) given as [[left, top, right, bottom], ...]. The roi should be a tensor and the element type can be f32 or int32
  * mode: the destimation image mode, three image modes: "NV12"(default), "YUV444", "GRAY"
  * interpolateMode: two interpolation modes: "bilinear"(default), "nearest"
  * expansionMode: two expansionMode modes: "constant"(default), "border"
  
    """
    y = get_value_or_create_const(y)
    uv = get_value_or_create_const(uv)
    roi = get_value_or_create_const(roi)
    size = create_array_attr(size)
    padValue = create_array_attr(padValue)
    mode = parse_enum(mode, 'mode')
    interpolateMode = parse_enum(interpolateMode, 'interpolateMode')
    expansionMode = parse_enum(expansionMode, 'expansionMode')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RoiResizeOp(output_type, y, roi, size, padValue=padValue, mode=mode, interpolateMode=interpolateMode, expansionMode=expansionMode, uv=uv, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def roll(input, shifts, dims, *, output_type=None, loc=None):
    """
Roll the tensor along the given dimensions

Roll the tensor along the given dimension.Elements that are shifted beyond the last position are re-introduced at the first position.

------

Note:
* shifts - The number of places by which the elements of the tensor are shifted.
* dims -Axis along which to roll.

------
Prototype: Pytorch roll.

    """
    input = get_value_or_create_const(input)
    shifts = create_array_attr(shifts)
    dims = create_array_attr(dims)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RollOp(output_type, input, shifts, dims, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def round(input, decimals=0, *, output_type=None, loc=None):
    """
HBIR tensor round.

Rounds elements of input to the nearest integer.

------

Note:
* This function implements the round half to even.
* Returns a new tensor with the round of the elements of input.


    """
    input = get_value_or_create_const(input)
    decimals = create_attr(decimals)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RoundOp(output_type, input, decimals=decimals, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def rpp(roi, inputs, boxnum, *, output_type=None, loc=None):
    """
HBIR RCNN post process.
Process a few feature maps, generating a few boxes(ROIs).
    """
    roi = get_value_or_create_const(roi)
    inputs = get_value_or_create_const(inputs)
    boxnum = create_attr(boxnum)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RppOp(output_type, roi, inputs, boxnum, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def rpp_v2(bbox_data, bbox_score, bbox_deltas, original_img_h=1024, original_img_w=1024, nms_threshold=0.3, score_threshold=0.1, class_number=1, nms_top_n=-1, bbox_delta_mean=(0.0, 0.0, 0.0, 0.0), bbox_delta_std=(1.0, 1.0, 1.0, 1.0), image_size_fixed=True, *, int_output_type=None, float_output_type=None, loc=None):
    """
HBIR RCNN post process.Real useful version
Process a few feature maps, generating a few boxes(ROIs).
    """
    bbox_data = get_value_or_create_const(bbox_data)
    bbox_score = get_value_or_create_const(bbox_score)
    bbox_deltas = get_value_or_create_const(bbox_deltas)
    original_img_h = create_attr(original_img_h)
    original_img_w = create_attr(original_img_w)
    nms_threshold = create_attr(nms_threshold)
    score_threshold = create_attr(score_threshold)
    class_number = create_attr(class_number)
    nms_top_n = create_attr(nms_top_n)
    bbox_delta_mean = create_array_attr(bbox_delta_mean)
    bbox_delta_std = create_array_attr(bbox_delta_std)
    image_size_fixed = create_attr(image_size_fixed)
    int_output_type = get_type_or_create_unranked_type(int_output_type)
    float_output_type = get_type_or_create_unranked_type(float_output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RppV2Op(int_output_type, float_output_type, bbox_data, bbox_score, bbox_deltas, original_img_h=original_img_h, original_img_w=original_img_w, nms_threshold=nms_threshold, score_threshold=score_threshold, class_number=class_number, nms_top_n=nms_top_n, bbox_delta_mean=bbox_delta_mean, bbox_delta_std=bbox_delta_std, image_size_fixed=image_size_fixed, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def rsqrt(input, *, output_type=None, loc=None):
    """
HBIR tensor rsqrt.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.RsqrtOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def scatter_elements(data, indices, updates, axis=0, scatterReduceMode="none", *, output_type=None, loc=None):
    """
HBIR scatter elements op. Same semantics as scatter elements in onnx. In addition, it supports the mean mode of torch scatter_reduce

HBIR scatter elements scatter.The ONNX op link:
https: // github.com/onnx/onnx/blob/main/docs/Operators.md#ScatterElements

Copy the data to output, the specify a direction axis, use the values in updates to update the values in output at specific location according to indices.

In addition, it supports the mean mode of torch scatter_reduce.

    """
    data = get_value_or_create_const(data)
    indices = get_value_or_create_const(indices)
    updates = get_value_or_create_const(updates)
    axis = create_attr(axis)
    scatterReduceMode = parse_enum(scatterReduceMode, 'scatterReduceMode')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ScatterElementsOp(output_type, data, indices, updates, axis=axis, scatterReduceMode=scatterReduceMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def scatter_nd(data, indices, updates, scatterReduceMode="none", *, output_type=None, loc=None):
    """
HBIR scatterND op. Same semantics as scatterND in onnx.

HBIR scatterDN.The ONNX op link:
https: // github.com/onnx/onnx/blob/main/docs/Operators.md#ScatterND

Copy the data to output, then use the values in updates to update the values in the output at some directions given by the indices.

    """
    data = get_value_or_create_const(data)
    indices = get_value_or_create_const(indices)
    updates = get_value_or_create_const(updates)
    scatterReduceMode = parse_enum(scatterReduceMode, 'scatterReduceMode')
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ScatterNDOp(output_type, data, indices, updates, scatterReduceMode=scatterReduceMode, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def select(input, dim, index, *, output_type=None, loc=None):
    """
select a tensor from a bigger tensor on a specific dim and index
Slices the input tensor along the selected dimension at the given index.

------

Note:
* dim - the dimension to slice.
* index - the index to select with.
* Select operator is equivalent to slicing.

------

Prototype:Pytorch select.

    """
    input = get_value_or_create_const(input)
    dim = create_attr(dim)
    index = create_attr(index)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SelectOp(output_type, input, dim, index, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def shufflefactor(input, factors, dims, *, output_type=None, loc=None):
    """
View a tensor as another shape
Returns a tensor with the same number of elements as input,
                    but may shuffle the data after expanding the data into small factors.
    """
    input = get_value_or_create_const(input)
    factors = create_array_attr(factors)
    dims = create_array_attr(dims)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.ShuffleFactorOp(output_type, input, factors, dims, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sigmoid(input, *, output_type=None, loc=None):
    """
HBIR Sigmoid activation.

Applies the element - wise function.Sigmoid function is defined as:

$ Sigmoid(x) = \frac{1} {1 + exp(-x)} $

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch sigmoid.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SigmoidOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sign(input, *, output_type=None, loc=None):
    """
HBIR tensor sign.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SignOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sin(input, *, output_type=None, loc=None):
    """
HBIR tensor sin.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SinOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sinh(input, *, output_type=None, loc=None):
    """
HBIR tensor sinh.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SinhOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def slice(input, begin, end, step, foldable=True, *, output_type=None, loc=None):
    """
Slice a tensor out of a tensor

Slicing like python's style means taking elements from one given index to another given index.

------

Note:
* begin -the index start to pick(inclusive).
* end -the index end to pick(exclusive).
* step -the step interval of picking.

------

Prototype: Pytorch slice.

    """
    input = get_value_or_create_const(input)
    begin = create_array_attr(begin)
    end = create_array_attr(end)
    step = create_array_attr(step)
    foldable = create_attr(foldable)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SliceOp(output_type, input, begin, end, step, foldable=foldable, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def slice_scatter(input, src, dim, start, end, step=1, *, output_type=None, loc=None):
    """
Embeds the values of the src tensor into input at the given dimension

Embeds the values of the src tensor into input at the given dimension. This function returns a tensor with fresh storage; it does not create a view.

------

Note:
* src (Tensor) -the tensor to embed into input.
* dim (int) -the dimension to insert the slice into.
* start (int) -the start index of where to insert the slice.
* end (int) -the end index of where to insert the slice.
* step (int) -the how many elements to skip in.

------

Prototype: Pytorch slice_scatter.

    """
    input = get_value_or_create_const(input)
    src = get_value_or_create_const(src)
    dim = create_attr(dim)
    start = create_attr(start)
    end = create_attr(end)
    step = create_attr(step)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SliceScatterOp(output_type, input, src, dim, start, end, step=step, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def softmax(input, dim, *, output_type=None, loc=None):
    """
HBIR Softmax Op.

Applies the Softmax function to an n - dimensional input Tensor rescaling them so that the elements of the n-dimensional output.
Tensor lie in the range $[0, 1] $ and sum to 1.

Softmax function is defined as:

$ Softmax(x_i) = \frac{exp(x_i)} {\sum_jexp(x_j)} $

------

Shape:
* Input(*), where $*$ means, any number of additional dimensions.*Output(*), same shape as the input.

------

Prototype: Pytorch softmax.

    """
    input = get_value_or_create_const(input)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SoftmaxOp(output_type, input, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def softplus(input, beta, threshold, *, output_type=None, loc=None):
    """
HBIR Softplus Op.

Applies the SoftPlus function element-wise. SoftPlus function defined as:

$ SoftPlus(x) = \frac{1}{\beta}*log(1+exp(\beta * x)) $

SoftPlus is  a smooth approximation to the ReLU function.

------

Note:
* beta - the $\beta$ value for the Softplus formulation.
* max - values is for numerical stability, when $\beta *x > max, SoftPlus(x)=x$.

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch softplus.

    """
    input = get_value_or_create_const(input)
    beta = create_attr(beta)
    threshold = create_attr(threshold)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SoftplusOp(output_type, input, beta, threshold, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sort(input, dim=-1, descending=False, stable=False, *, values_type=None, indices_type=None, loc=None):
    """
HBIR tensor sort

Sorts the elements of the input tensor along a given dimension in ascending order by value.

------

Parameters:
* input: the input tensor.
* dim: the dimension to sort along.
* descending: controls the sorting order (ascending or descending).
* stable: makes the sorting routine stable, which guarantees that the order of equivalent elements is preserved.

------

Shape:
* Input: $(N, *)$
* Values: $(N, *)$ (same shape as input)
* Indices: $(N, *)$ (same shape as input)

------

Prototype: Pytorch sort.

    """
    input = get_value_or_create_const(input)
    dim = create_attr(dim)
    descending = create_attr(descending)
    stable = create_attr(stable)
    values_type = get_type_or_create_unranked_type(values_type)
    indices_type = get_type_or_create_unranked_type(indices_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SortOp(values_type, indices_type, input, dim=dim, descending=descending, stable=stable, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def sqrt(input, *, output_type=None, loc=None):
    """
HBIR tensor sqrt.

Applies square root operator element - wise, $y = \sqrt{x}$.

------

Note:
* Returns a new tensor with the square root of the elements of input. If input is negative, then it will return NaN.


    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SqrtOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def stack(inputs, dim, *, output_type=None, loc=None):
    """
Stack multiple tensors along one extra dimension

Concatenates a sequence of tensors along a new dimension.No elemental type conversion.

------

Note:
* All tensors need to be of the same size.

------

Prototype: Pytorch stack.

    """
    inputs = get_value_or_create_const(inputs)
    dim = create_attr(dim)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.StackOp(output_type, inputs, dim, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def sub(lhs, rhs, *, output_type=None, loc=None):
    """
HBIR tensor substraction.

Applies substraction operator element-wise, $y_i=lhs_i-rhs_i$.

------

Note:
* Our arithmetic operator support broadcast.

    """
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SubOp(output_type, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def swish(input, *, output_type=None, loc=None):
    """
HBIR Swish activation.

Applies the swish function element-wise.Swish function defined as:

$ Swish(x) = x * Sigmoid(x) $

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch silu.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.SwishOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def tan(input, *, output_type=None, loc=None):
    """
HBIR tensor tan.
Return tensor after the operation, which has the same shape as the input.
    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TanOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def tanh(input, *, output_type=None, loc=None):
    """
HBIR Tanh activation.

Applies the tanh function element - wise.Tanh function is defined as:

$ Tanh(x) =\frac{exp(x) - exp(-x)} {exp(x) + exp(-x)} $

------

Shape:
* Input: (*), where * means any number of dimensions.
* Output: (*), same shape as the input.

------

Prototype: Pytorch tanh.

    """
    input = get_value_or_create_const(input)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TanhOp(output_type, input, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def tile(input, multiplies, *, output_type=None, loc=None):
    """
Constructs a tensor by tiling a given tensor.



    """
    input = get_value_or_create_const(input)
    multiplies = create_array_attr(multiplies)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TileOp(output_type, input, multiplies, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def topk(input, k, dim=-1, largest=True, sorted=True, *, values_type=None, indices_type=None, loc=None):
    """
HBIR tensor topk

Returns the k largest elements of the given input tensor along a given dimension.

If dim is not given, the last dimension of the input is chosen.

If largest is False then the k smallest elements are returned.

values, indices are returned in separate tensors, where the indices are the indices of the elements in the original input tensor.

The boolean option sorted if True, will make sure that the returned k elements are themselves sorted.

    """
    input = get_value_or_create_const(input)
    k = create_attr(k)
    dim = create_attr(dim)
    largest = create_attr(largest)
    sorted = create_attr(sorted)
    values_type = get_type_or_create_unranked_type(values_type)
    indices_type = get_type_or_create_unranked_type(indices_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TopkOp(values_type, indices_type, input, k, dim=dim, largest=largest, sorted=sorted, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0], op.results[1]


def transpose(input, dims, *, output_type=None, loc=None):
    """
Reverse or permute the dims of an array; returns the modified array.

Returns a tensor that is a view of the original tensor input with its dimesions permuted.

------

Note:
* input: the input tensor.
* dims: the desired ordering of dimensions.

------

Prototype: Pytorch permute.

    """
    input = get_value_or_create_const(input)
    dims = create_array_attr(dims)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TransposeOp(output_type, input, dims, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def triton_call(inputs, signature, inputIndices, outputIndices, ttir, *, inputOutputIndices=None, parameters=None, library=None, loc=None):
    """
Triton kernel call for custom operator.

    The `hbir.triton_call` operation is similar to `hbir.call` op, but it is a zero-output operator and has side effect.
  
    """
    inputs = get_value_or_create_const(inputs)
    signature = create_attr(signature)
    inputIndices = create_array_attr(inputIndices)
    outputIndices = create_array_attr(outputIndices)
    ttir = create_attr(ttir)
    inputOutputIndices = create_array_attr(inputOutputIndices)
    parameters = create_array_attr(parameters)
    library = create_attr(library)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TritonCallOp(inputs, signature, inputIndices, outputIndices, ttir, inputOutputIndices=inputOutputIndices, parameters=parameters, library=library, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return 

def tuple_get(input, index, *, name=None, output_type=None, loc=None):
    """
Get one element in tuple

    """
    input = get_value_or_create_const(input)
    index = create_attr(index)
    name = create_attr(name)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.TupleGetOp(output_type, input, index, name=name, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def warp(input, move, mode="bilinear", expansionMode="constant", *, padValue=None, output_type=None, loc=None):
    """
HBIR warp.

From the input, sample(bi - linear interpolation) pixels specified by grid.

------

Shape:
* input: input of shape $(*, H_{in}, W_{in}, C_{in})$, where * represent any number of dimension.
* grid: flow - field of shape $(*, H_{out}, W_{out}, 2)$
* output: $(*, H_{out}, W_{out}, C_{in})$

    """
    input = get_value_or_create_const(input)
    move = get_value_or_create_const(move)
    mode = parse_enum(mode, 'mode')
    expansionMode = parse_enum(expansionMode, 'expansionMode')
    padValue = create_attr(padValue)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.WarpOp(output_type, input, move, mode=mode, expansionMode=expansionMode, padValue=padValue, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


def where(condition, lhs, rhs, *, output_type=None, loc=None):
    """
HBIR where op

Return a tensor of elements selected from either lhs or rhs, depending on condition.

    """
    condition = get_value_or_create_const(condition)
    lhs = get_value_or_create_const(lhs)
    rhs = get_value_or_create_const(rhs)
    output_type = get_type_or_create_unranked_type(output_type)
    loc = get_loc_or_create_from_frames(loc)
    op = _ods_ext.WhereOp(output_type, condition, lhs, rhs, loc=loc)
    _hbdk.infer_type_if_failed_infer_shape(op)
    return op.results[0]


