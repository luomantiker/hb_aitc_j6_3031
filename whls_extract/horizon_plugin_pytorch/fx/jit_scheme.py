# Use jit trace to get model graph. Graph format is same as torch fx, but
# some model attribute will recorded as constant value (for non Tensor) or
# TensorInfo (for Tensor) in the graph, so it cannot be recompiled.
# Because we do not regenerate forward code, following quantization procedure
# is different with symbolic trace:
# 1. Function Replacement: This functionallty is achieved by
#    Dispatcher, we register a impl that jump to generated
#    Module for each torch func call.
# 2. OP fusion: Must replace module which fused into other module with
#    Identity.

import copy
import inspect
from contextlib import contextmanager
from typing import Any, Dict, Tuple, Type

import torch
from torch import Tensor
from torch.fx.graph import Graph
from torch.fx.node import Node
from torch.nn import Module
from torch.utils._pytree import tree_flatten, tree_unflatten

from horizon_plugin_pytorch.fx.fx_helper import match_node_operation
from horizon_plugin_pytorch.nn.qat import FloatFunctional as QATFunctional
from horizon_plugin_pytorch.nn.quantized import FloatFunctional, QFunctional
from horizon_plugin_pytorch.tensor_dispatch_wrapper import Dispatcher
from horizon_plugin_pytorch.torch_patch import _is_namedtuple_instance
from horizon_plugin_pytorch.utils._swap_horizon_float_nn import (
    swap_nn_with_horizonnn,
)
from horizon_plugin_pytorch.utils.location_info import (
    LocationManager,
    TorchLocationInfo,
)
from horizon_plugin_pytorch.utils.misc import is_called_by_plugin
from horizon_plugin_pytorch.utils.model_helper import (
    HookAndTorchFunctionHelper,
    ModelState,
)
from horizon_plugin_pytorch.utils.typeguard import typechecked
from .fx_helper import (
    FxWrapManager,
    _torch_horizon_nn_op_mapping,
    _torch_horizon_op_mapping,
    get_supported_method,
)

__all__ = [
    "Tracer",
    "FunctionReplacer",
]


class GraphModule(Module):
    """GraphModule for jit trace graph.

    Must be generated by class method `pack`.
    """

    graph: Graph = None
    node_name_to_scope: Dict[str, str] = None
    node_name_to_dtype: Dict[str, torch.dtype] = None
    node_name_to_location: Dict[str, TorchLocationInfo] = None
    mod_called_times: Dict[str, int] = None

    def __init__(self, *args, **kwargs) -> None:
        raise NotImplementedError(
            "{} is not supposed to be inited directly".format(
                self.__class__.__name__
            )
        )

    @classmethod
    def pack(
        cls,
        model: Module,
        graph: Graph,
        node_name_to_scope: Dict[str, str],
        node_name_to_dtype: Dict[str, torch.dtype],
        node_name_to_location: Dict[str, TorchLocationInfo],
        mod_called_times: Dict[str, int],
    ) -> "Type[GraphModule]":
        """Inplaced modify a nn.Module to GraphModule.

        Args:
            model (Module): Input nn.Module.
            graph (Graph): Compute graph of input model.
            node_name_to_scope (Dict[str, str]): A mapping from node name to
                the module name that generate this node.
            node_name_to_dtype (Dict[str, torch.dtype]): A mapping from node
                name to the output dtype of this node.
            node_name_to_location (Dict[str, TorchLocationInfo]): A mapping
                from node name to the python code location that generate
                this node.
            mod_called_times (Dict[str, int]): A mapping from mod name to its
                called time.

        Returns:
            Type[GraphModule]: Output GraphModule.
        """
        # If origin has an attr called 'graph'
        has_graph_attr = False
        # If the origin 'graph' attr is a property
        has_graph_property = False
        if hasattr(model, "graph"):
            has_graph_attr = True
            if not hasattr(model.__class__, "graph") or not isinstance(
                model.__class__.graph, property
            ):
                # if graph is not property, rename it to _ori_graph
                assert not hasattr(model, "_ori_graph")
                model._ori_graph = model.graph
                del model.graph
            else:
                has_graph_property = True

        class GraphModuleImpl(  # type: ignore[valid-type]
            model.__class__, cls
        ):
            @property
            def graph(self):
                # 0: is_called_by_plugin
                # 1: here
                # 2: caller
                if is_called_by_plugin(2) or not has_graph_attr:
                    return self._jit_fx_graph
                else:
                    if has_graph_property:
                        return super().graph
                    else:
                        return self._ori_graph

            @graph.setter
            def graph(self, v):
                # 0: is_called_by_plugin
                # 1: here
                # 2: torch.nn.Module.__setattr__
                # 3: caller
                if is_called_by_plugin(3) or not has_graph_attr:
                    self._jit_fx_graph = v
                else:
                    if has_graph_property:
                        super().graph = v
                    else:
                        self._ori_graph = v

        model.__class__ = GraphModuleImpl
        model.graph = graph
        model.node_name_to_scope = node_name_to_scope
        model.node_name_to_dtype = node_name_to_dtype
        model.node_name_to_location = node_name_to_location
        model.mod_called_times = mod_called_times

        return model

    def strip(self):
        """Remove the graph node outside QuantStub -> Dequantstub."""
        stripped_graph, node_mapping = strip_no_quant_nodes(self.graph, self)

        node_name_to_scope = {}
        node_name_to_dtype = {}
        node_name_to_location = {}
        for node in self.graph.nodes:
            node: Node
            if node in node_mapping:
                node_name_to_scope[
                    node_mapping[node].name
                ] = self.node_name_to_scope[node.name]
                node_name_to_location[
                    node_mapping[node].name
                ] = self.node_name_to_location[node.name]
                if node.name in self.node_name_to_dtype:
                    node_name_to_dtype[
                        node_mapping[node].name
                    ] = self.node_name_to_dtype[node.name]

        self.graph = stripped_graph
        self.node_name_to_scope = node_name_to_scope
        self.node_name_to_dtype = node_name_to_dtype
        self.node_name_to_location = node_name_to_location

        return self

    def replace_node(self, ori_node: Node, new_node: Node):
        """Replace node in graph with a new node.

        Update the mappings aboud node in GraphModule.
        """
        self.node_name_to_scope[new_node.name] = self.node_name_to_scope.pop(
            ori_node.name
        )
        self.node_name_to_location[
            new_node.name
        ] = self.node_name_to_location.pop(ori_node.name)
        if ori_node.name in self.node_name_to_dtype:
            self.node_name_to_dtype[
                new_node.name
            ] = self.node_name_to_dtype.pop(ori_node.name)
        ori_node.replace_all_uses_with(new_node)
        self.graph.erase_node(ori_node)


class TensorInfo:
    """Store the tensor attribute of op input."""

    def __init__(self, tensor: Tensor) -> None:
        self.dtype = tensor.dtype
        self.shape = tensor.shape


class Tracer(HookAndTorchFunctionHelper):
    """Getting graph with module hook and __torch_function__ of tensor."""

    # Only for accessing Tracer obj in TracerTensor
    _current = None
    _dynamic_block_prefix = "_dynamic_block_"

    def __init__(self) -> None:
        self._model = None
        self._graph = None
        self._node_name_to_scope = {}
        self._node_name_to_dtype = {}
        self._node_name_to_location = {}

    @staticmethod
    def scope_end(scope_name):
        """Define a helper func marking the end of a scope."""
        pass

    def mark_scope_end(self, loc: TorchLocationInfo):
        """Add a call_func(scope_end) node to graph at current position."""
        node = self._graph.call_function(self.scope_end, (loc.mod_name,))
        self._node_name_to_scope[node.name] = loc.mod_name
        self._node_name_to_location[node.name] = loc

    @classmethod
    def is_tracing(cls):
        return cls._current is not None

    @classmethod
    @contextmanager
    def dynamic_block(cls, mod: torch.nn.Module, name: str):
        """Use this user interface to mark dynamic python code block.

        Used to mark the python code that called times will vary in different
        forward. So that each func call within a dynamic_block will be
        replaced by one module (other than n modules that n is the func
        called times).
        Dynamic block will generate a new scope named _dynamic_block_<name>

        Args:
            mod (torch.nn.Module): The module that hold this dynamic_block.
                A Dispatcher about this dynamic_block will
                be added to the module.
            name (str): An identity of this dynamic_block, must be unique in
                one module!


        """
        try:
            if cls.is_tracing():
                current_mod_name = LocationManager.get("").mod_name
                if current_mod_name == "":
                    LocationManager.push(
                        "{}{}".format(cls._dynamic_block_prefix, name)
                    )
                else:
                    LocationManager.push(
                        "{}.{}{}".format(
                            current_mod_name, cls._dynamic_block_prefix, name
                        )
                    )

            dispatcher_name = (
                FunctionReplacer.dispatcher_attr_name
                + cls._dynamic_block_prefix
                + name
            )
            if hasattr(mod, dispatcher_name):
                dispatcher: Dispatcher = getattr(mod, dispatcher_name)
                dispatcher.enter(mod)

            yield
        finally:
            if cls.is_tracing():
                loc = LocationManager.get("dynamic_block", True)
                cls._current.mark_scope_end(loc)
                LocationManager.pop()

            dispatcher_name = (
                FunctionReplacer.dispatcher_attr_name
                + cls._dynamic_block_prefix
                + name
            )
            if hasattr(mod, dispatcher_name):
                dispatcher: Dispatcher = getattr(mod, dispatcher_name)
                dispatcher.exit()

    class TracerTensor(HookAndTorchFunctionHelper.TracerTensor):
        """Patch Tensor for tracing."""

        node: Node
        _enabled = True

        def __new__(cls, data: Tensor, node: Node):
            instance = super().__new__(cls, data)
            return instance

        def __init__(self, data: Tensor, node: Node):
            super().__init__(data)
            self.node = node

        @classmethod
        def enabled(cls):
            return cls._enabled

        @classmethod
        def enable(cls):
            cls._enabled = True

        @classmethod
        def disable(cls):
            cls._enabled = False

        @classmethod
        @contextmanager
        def disabled(cls):
            old_state = cls._enabled
            try:
                cls._enabled = False
                yield
            finally:
                cls._enabled = old_state

        @classmethod
        def _get_node_args(cls, args):
            """Convert function or module args to fx node args."""
            node_args = []
            flatten_args, spec = tree_flatten(args)
            for flatten_arg in flatten_args:
                if isinstance(flatten_arg, cls):
                    node_args.append(flatten_arg.node)
                elif isinstance(flatten_arg, Tensor):
                    node_args.append(TensorInfo(flatten_arg))
                else:
                    node_args.append(flatten_arg)

            return tree_unflatten(node_args, spec)

        @classmethod
        def _attach_node(cls, node: Node, tensors, loc: TorchLocationInfo):
            Tracer._current._node_name_to_scope[node.name] = loc.mod_name
            Tracer._current._node_name_to_location[node.name] = loc

            if isinstance(tensors, Tensor):
                Tracer._current._node_name_to_dtype[node.name] = tensors.dtype
                if isinstance(tensors, cls):
                    # For inplace operation, we should replace its node
                    # Do not gen new tensor!
                    tensors.node = node
                    return tensors
                else:
                    return cls(tensors, node)
            # logic for namedtuple should be different from normal tuple
            elif _is_namedtuple_instance(tensors):
                rets = {}
                for i, (k, v) in enumerate(tensors._asdict().items()):
                    rets[k] = cls._attach_node(
                        Tracer._current._graph.call_function(
                            type(tensors).__getitem__,
                            (node, i),
                            {},
                        ),
                        v,
                        loc,
                    )
                return type(tensors)(**rets)
            elif isinstance(tensors, (list, tuple)):
                rets = []
                for i, v in enumerate(tensors):
                    rets.append(
                        cls._attach_node(
                            Tracer._current._graph.call_function(
                                type(tensors).__getitem__,
                                (node, i),
                                {},
                            ),
                            v,
                            loc,
                        )
                    )
                return type(tensors)(rets)
            elif isinstance(tensors, dict):
                rets = {}
                for k, v in tensors.items():
                    rets[k] = cls._attach_node(
                        Tracer._current._graph.call_function(
                            dict.get,
                            (node, k),
                            {},
                        ),
                        v,
                        loc,
                    )
                return rets
            else:
                return tensors

        @classmethod
        def _torch_function_postprocess(
            cls, func, types, args, kwargs, func_ret
        ):
            """Postprocess of __torch_function__."""
            # If TracerTensor is saved during forward,
            # it should behave as its base after tracing.
            if Tracer.is_tracing() and cls.enabled():
                with cls.disabled():
                    node = Tracer._current._graph.call_function(
                        func,
                        cls._get_node_args(args),
                        cls._get_node_args(kwargs),
                    )
                    loc = LocationManager.get(func, True)

                    # if func_ret is None, assume this is a inplaced func
                    # and attach node to args[0]
                    if func_ret is None:
                        cls._attach_node(node, args[0], loc)
                    else:
                        func_ret = cls._attach_node(node, func_ret, loc)
                        return func_ret

            return func_ret

        def __deepcopy__(self, memo):
            return self.__class__(copy.deepcopy(self._base), self.node)

    def _forward_pre_hook(self, mod, args, kwargs):
        """Implement module forward pre hook."""
        # Do not unwrap TracerTensor in this hook,
        # or we cannot get node in _forward_hook.
        self.TracerTensor.disable()

    def _forward_hook(self, mod, args, kwargs, output):
        """Implement module forward hook."""

        loc = LocationManager.get(mod)
        assert (
            loc.mod_name == mod._qualified_name
        ), f"{loc.mod_name} != {mod._qualified_name}"

        if isinstance(mod, (FloatFunctional, QATFunctional, QFunctional)):
            assert hasattr(mod, "_last_called_method_name")
            getattr_node = self._graph.get_attr(loc.mod_name)
            self._current._node_name_to_scope[getattr_node.name] = loc.mod_name
            self._current._node_name_to_location[getattr_node.name] = loc
            node = self._graph.call_method(
                mod._last_called_method_name,
                (getattr_node,) + self.TracerTensor._get_node_args(args),
                self.TracerTensor._get_node_args(kwargs),
            )
        else:
            node = self._graph.call_module(
                mod._qualified_name,
                self.TracerTensor._get_node_args(args),
                self.TracerTensor._get_node_args(kwargs),
            )

        output = self.TracerTensor._attach_node(
            node, self.TracerTensor.unwrap(output), loc
        )

        self.TracerTensor.enable()

        return output

    def _example_inputs_preprocess(self, example_inputs, example_kw_inputs):
        """Preprocess example inputs before running forward."""
        fn_for_analysis = inspect.unwrap(self._model.forward)
        co = fn_for_analysis.__code__
        total_args = co.co_argcount + co.co_kwonlyargcount
        arg_names = list(co.co_varnames)[1:total_args]

        input_with_node = []
        kw_input_with_node = {}

        loc = LocationManager.get(self._model)

        for i, input in enumerate(example_inputs):
            if i < len(arg_names):
                name = arg_names[i]
            else:
                name = "input_{}".format(i)
            node = self._graph.placeholder(name)
            input_with_node.append(
                self.TracerTensor._attach_node(node, input, loc)
            )

        for k, v in example_kw_inputs.items():
            node = self._graph.placeholder(k)
            kw_input_with_node[k] = self.TracerTensor._attach_node(
                node, v, loc
            )

        return tuple(input_with_node), kw_input_with_node

    def _record_graph_output(self, output):
        """Implement hook to record output and add output node in graph."""
        output_args = self.TracerTensor._get_node_args(output)
        output_node = self._graph.output(
            output_args if isinstance(output_args, tuple) else (output_args,),
        )
        self._node_name_to_scope[output_node.name] = ""
        self._node_name_to_location[output_node.name] = TorchLocationInfo(
            "output", "", 0
        )

    @typechecked
    def trace(
        self,
        model: Module,
        example_inputs: Any = None,
        example_kw_inputs: Any = None,
    ) -> GraphModule:
        """
        Trace model to get graph.

        Args:
            model (Module): The model being traced.
            example_inputs (Any): Inputs used to run model forward.

        Returns:
            Graph: FX graph.
        """
        # Freeze bn when trace.
        def _freeze_bn(m):
            classname = m.__class__.__name__
            if classname.find("BatchNorm") != -1:
                m.eval()

        model.apply(_freeze_bn)

        old_obj = Tracer._current
        Tracer._current = self

        self._model = model
        self._graph = Graph(owning_module=model)
        self._node_name_to_scope = {}
        self._node_name_to_dtype = {}
        self._node_name_to_location = {}
        mod_called_times = {}

        handles = []
        for _, mod in model.named_modules():

            def mod_count_hook(mod, args, output):
                # count module called times
                mod_called_times[mod._qualified_name] = (
                    mod_called_times.get(mod._qualified_name, 0) + 1
                )

            handles.append(mod.register_forward_hook(mod_count_hook))

        for _, mod in model.named_modules():
            if not self._is_leaf_module(mod):

                def mark_scope_end_hook(mod, args, output):
                    loc = LocationManager.get(mod, True)
                    self.mark_scope_end(loc)

                handles.append(mod.register_forward_hook(mark_scope_end_hook))

        swap_nn_with_horizonnn(model)

        model_rets = self._register_hook_and_forward(
            self._model, example_inputs, example_kw_inputs
        )

        for h in handles:
            h.remove()

        self._record_graph_output(model_rets)

        # Do not remove dead node, because we avoid recompile, dead node is
        # always be called! (so need to do `replace_function_with_module`)
        # self._graph.eliminate_dead_code()

        self._graph.lint()

        Tracer._current = old_obj

        return GraphModule.pack(
            model,
            self._graph,
            self._node_name_to_scope,
            self._node_name_to_dtype,
            self._node_name_to_location,
            mod_called_times,
        )


class FunctionReplacer:
    dispatcher_attr_name = "_qtensor_dispatcher"

    @classmethod
    def get_dispatcher(cls, mod: Module, name_prefix=None) -> Dispatcher:
        """Get or generate needed Dispatcher in a module.

        For a dynamic_block within the module, the `name_prefix` shound be
        _dynamic_block_<name>.
        For ordinary situation, the `name_prefix` should be None.
        """
        dispatcher_name = cls.dispatcher_attr_name
        if name_prefix is not None:
            dispatcher_name = dispatcher_name + name_prefix

        if not hasattr(mod, dispatcher_name):
            setattr(mod, dispatcher_name, Dispatcher())

            if name_prefix is None:

                def forward_pre_hook(mod, *args, **kwargs):
                    dispatcher: Dispatcher = getattr(mod, dispatcher_name)
                    dispatcher.enter(mod)

                def forward_hook(mod, *args, **kwargs):
                    dispatcher: Dispatcher = getattr(mod, dispatcher_name)
                    dispatcher.exit()

                mod.register_forward_pre_hook(forward_pre_hook)
                mod.register_forward_hook(forward_hook)

        return getattr(mod, dispatcher_name)

    @classmethod
    def add_qtensor_dispatch_for_func(
        cls,
        mod: Module,
        ori_func,
        func_info: str,
        called_mod_name: str,
        method_name: str = None,
        name_prefix: str = None,
    ):
        """Register impl to dispatcher for a func call.

        All func args and kwargs will be passed to the generated module.

        Args:
            mod (Module): The module that func call happened in, used to hold
                the dispatcher.
            ori_func (callable): A torch function.
            func_info (str): The python code that generate this func call.
            called_mod_name (str): The name of generated mod for this func
                call, should be a submod of `mod`.
            method_name (str, optional): If the generated mod is a
                Floatfunctional, give the method name. Defaults to None.
            name_prefix (str, optional): See the docs of `get_dispatcher`.
                Defaults to None.
        """
        dispatcher = cls.get_dispatcher(mod, name_prefix)

        def called_func(current_mod, *args, **kwargs):
            called_mod = current_mod.get_submodule(called_mod_name)
            if method_name is None:
                return called_mod(*args, **kwargs)
            else:
                return getattr(called_mod, method_name)(*args, **kwargs)

        dispatcher.add_call(ori_func, called_func, func_info)

    @classmethod
    def add_qtensor_dispatch_for_functional(
        cls,
        mod: Module,
        ori_func,
        func_info: str,
        called_mod_name: str,
        name_prefix: str = None,
    ):
        """Register impl to dispatcher for a func call.

        Only Tensors in func args and kwargs will be passed to the
        generated module.

        Args:
            mod (Module): The module that func call happened in, used to hold
                the dispatcher.
            ori_func (callable): A torch function.
            func_info (str): The python code that generate this func call.
            called_mod_name (str): The name of generated mod for this func
                call, should be a submod of `mod`.
            name_prefix (str, optional): See the docs of `get_dispatcher`.
                Defaults to None.
        """
        dispatcher = cls.get_dispatcher(mod, name_prefix)

        def called_func(current_mod, *args, **kwargs):
            tensor_args = []
            tensor_kwargs = {}

            for arg in args:
                if isinstance(arg, Tensor):
                    tensor_args.append(arg)
            for k, v in kwargs.items():
                if isinstance(v, Tensor):
                    tensor_kwargs[k] = v

            called_mod = current_mod.get_submodule(called_mod_name)
            return called_mod(*tensor_args, **tensor_kwargs)

        dispatcher.add_call(ori_func, called_func, func_info)

    @classmethod
    def replace_function_with_module(cls, model: GraphModule):
        """Replace torch func calls in a GraphModule with module.

        The graph is modified correspondingly to agree with the computation
        logic after replacement.
        """
        model_state = ModelState.record(model)

        FxWrapManager.apply_wrap()

        node_name_to_scope = model.node_name_to_scope
        node_name_to_dtype = model.node_name_to_dtype
        node_name_to_location = model.node_name_to_location

        supported_functional = get_supported_method()[FloatFunctional]

        # A mapping to count the generated mod for particular func in a module
        generated_ops_count: Dict[str, Dict[Any, int]] = {}

        def _clear_generated_ops_count(
            mod_name: str, dynamic_block_name: str = None
        ):
            if dynamic_block_name is not None:
                mod_name += dynamic_block_name
            if mod_name in generated_ops_count:
                mod_generated_ops_count = generated_ops_count[mod_name]
                for k in mod_generated_ops_count:
                    mod_generated_ops_count[k] = 0

        def _get_generated_mod_name(
            mod_name: str, op_name: str, dynamic_block_name: str = None
        ):
            if dynamic_block_name is not None:
                mod_name += dynamic_block_name
            mod_generated_ops_count = generated_ops_count.get(mod_name, {})
            op_count = mod_generated_ops_count.get(op_name, 0)
            mod_generated_ops_count[op_name] = op_count + 1
            generated_ops_count[mod_name] = mod_generated_ops_count

            return "_generated_{}_{}".format(op_name, op_count)

        def _replace(
            op_name, module_class, move_nontensor_args, is_call_method
        ):
            # add Module to model
            non_tensor_args = []
            non_tensor_kwargs = {}
            if move_nontensor_args:
                for arg in node.args:
                    if not isinstance(arg, (Node, TensorInfo)):
                        non_tensor_args.append(arg)

                for k, v in node.kwargs.items():
                    if not isinstance(v, (Node, TensorInfo)):
                        non_tensor_kwargs[k] = v
            module_name = _get_generated_mod_name(
                current_scope_name, op_name, dynamic_block_name
            )
            if not hasattr(submod, module_name):
                generated_mod = module_class(
                    *non_tensor_args, **non_tensor_kwargs
                )
                if is_call_method:
                    generated_mod._last_called_method_name = op_name
                submod.add_module(module_name, generated_mod)
                if move_nontensor_args:
                    cls.add_qtensor_dispatch_for_functional(
                        submod,
                        node.target,
                        current_location.user_stack,
                        module_name,
                        dynamic_block_name,
                    )
                else:
                    cls.add_qtensor_dispatch_for_func(
                        submod,
                        node.target,
                        current_location.user_stack,
                        module_name,
                        op_name if is_call_method else None,
                        dynamic_block_name,
                    )

            # modify graph
            if current_scope_name == "":
                qualified_name = module_name
            else:
                qualified_name = "{}.{}".format(
                    current_scope_name, module_name
                )

            model.graph.inserting_before(node)
            if is_call_method:
                get_attr_node = model.graph.get_attr(qualified_name)
                call_method_node = model.graph.call_method(
                    op_name,
                    args=(get_attr_node,) + node.args,
                    kwargs=node.kwargs,
                )
                node_name_to_scope[get_attr_node.name] = current_scope_name
                node_name_to_location[get_attr_node.name] = current_location
                model.replace_node(node, call_method_node)
            else:
                call_module_node = model.graph.call_module(
                    qualified_name,
                    args=node.args,
                    kwargs=node.kwargs,
                )
                model.replace_node(node, call_module_node)

        for node in list(model.graph.nodes):
            node: Node

            current_scope_name = node_name_to_scope[node.name]
            current_location = node_name_to_location[node.name]

            # If current scope is a dynamic block
            if Tracer._dynamic_block_prefix in current_scope_name:
                words = current_scope_name.split(".")
                dynamic_block_name = words[-1]
                # Deal with nested dynamic block
                words = tuple(
                    filter(
                        lambda word: Tracer._dynamic_block_prefix not in word,
                        words,
                    )
                )
                if len(words) == 0:
                    current_scope_name = ""
                else:
                    current_scope_name = ".".join(words)
                # remove dynamic_block in node_name_to_scope
                node_name_to_scope[node.name] = current_scope_name
            else:
                dynamic_block_name = None

            if node.op != "call_function":
                continue

            # if encounter with scope_end, clear generated ops count
            if node.target is Tracer.scope_end:
                _clear_generated_ops_count(
                    current_scope_name, dynamic_block_name
                )
                continue

            # Skip if op output is not float
            # TODO: Shoule we skip non-float output ops?
            if node_name_to_dtype.get(node.name, "unknown") not in (
                torch.float,
                torch.float16,
            ):
                continue

            submod = model.get_submodule(current_scope_name)

            # replace torch func with horizon.nn module
            op_name = match_node_operation(node, _torch_horizon_op_mapping)
            if op_name is not None:
                _replace(
                    op_name, _torch_horizon_op_mapping[op_name], False, False
                )
                continue

            # replace torch.nn.functional func with torch.nn module
            op_name = match_node_operation(node, _torch_horizon_nn_op_mapping)
            if op_name is not None:
                _replace(
                    op_name, _torch_horizon_nn_op_mapping[op_name], True, False
                )
                continue

            # replace torch func with FloatFunctional
            op_name = match_node_operation(node, supported_functional)
            if op_name is not None:
                _replace(op_name, FloatFunctional, False, True)
                continue

        # model.graph.eliminate_dead_code()
        model_state.apply(model)


def strip_no_quant_nodes(
    graph: Graph, model: GraphModule
) -> Tuple[Graph, Dict[Node, Node]]:
    stripped_graph = Graph(graph.owning_module)

    # mapping from node in ori graph to node in striped graph
    node_mapping = {}
    quant_nodes = set()
    input_num = 0

    def record_node_mapping(
        node_in_ori_graph, node_in_stripprd_graph, is_quant=True
    ):
        node_mapping[node_in_ori_graph] = node_in_stripprd_graph
        if is_quant:
            quant_nodes.add(node_in_ori_graph)

    def load_args(ori_arg: Node):
        nonlocal input_num

        if ori_arg in node_mapping:
            return node_mapping[ori_arg]
        else:
            input_node = stripped_graph.placeholder(
                "input_{}".format(input_num)
            )
            record_node_mapping(ori_arg, input_node, False)
            input_num += 1
            return input_node

    from horizon_plugin_pytorch.nn import qat, quantized
    from horizon_plugin_pytorch.quantization import QuantStub

    entry_modules = (
        torch.quantization.QuantStub,
        QuantStub,
        qat.QuantStub,
        quantized.Quantize,
    )
    exit_modules = (
        torch.quantization.DeQuantStub,
        qat.DeQuantStub,
        quantized.DeQuantize,
    )

    output_nodes = []

    has_entry_module = False
    for node in graph.nodes:
        if node.op == "call_module" and isinstance(
            model.get_submodule(node.target), entry_modules
        ):
            has_entry_module = True
            break

    for node in graph.nodes:
        node: Node

        if node.op == "output":
            if len(output_nodes) == 0:
                # If there is no exit_modules in the model, use origin output
                record_node_mapping(
                    node, stripped_graph.node_copy(node, load_args)
                )
            else:
                output_node = stripped_graph.output(tuple(output_nodes))
                record_node_mapping(node, output_node, False)
            # output node must be the last node
            break
        if node.op == "call_module" and isinstance(
            model.get_submodule(node.target), entry_modules
        ):
            # Always copy QuantStub node
            # placeholder is generated automatically in load_args
            record_node_mapping(
                node, stripped_graph.node_copy(node, load_args)
            )
        elif node.op == "call_module" and isinstance(
            model.get_submodule(node.target), exit_modules
        ):
            # For DequantStub, copy node but do not put it in quant_nodes
            # because as graph output, it is not allowed to be used
            # by other nodes
            output_node = stripped_graph.node_copy(node, load_args)
            record_node_mapping(node, output_node, False)
            output_nodes.append(output_node)
        elif node.op == "call_function" and node.target is Tracer.scope_end:
            # Always copy scope_end
            record_node_mapping(
                node, stripped_graph.node_copy(node, load_args), False
            )
        elif node.op == "get_attr":
            # Always copy get_attr node
            record_node_mapping(
                node, stripped_graph.node_copy(node, load_args), False
            )
        elif node.op == "placeholder":
            # If no entry module, copy placeholder as entry node
            if not has_entry_module:
                record_node_mapping(
                    node, stripped_graph.node_copy(node, load_args)
                )
        else:
            # Only copy entry module's downstream nodes
            flat_args = tree_flatten((node.args, node.kwargs))[0]
            if any(
                isinstance(arg, Node)
                and arg.op != "get_attr"
                and arg not in output_nodes
                and arg in quant_nodes
                for arg in flat_args
            ):
                record_node_mapping(
                    node, stripped_graph.node_copy(node, load_args)
                )

    # Delete unused get_attr nodes
    for node in stripped_graph.nodes:
        if node.op == "get_attr" and len(node.users) == 0:
            stripped_graph.erase_node(node)

    # stripped_graph.eliminate_dead_code()

    return stripped_graph, node_mapping
